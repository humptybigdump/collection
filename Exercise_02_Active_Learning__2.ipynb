{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TQCBh950qOLe"
   },
   "source": [
    "# Active Learning:\n",
    "\n",
    "## Motivation\n",
    "A key problem in deep learning is data efficiency. While excellent performance can be obtained with modern tools, these are often data-hungry, rendering the deployment of deep learning in the real-world challenging for many tasks.  In Active Learning we use a “human in the loop” approach to data labelling, reducing the amount of data that needs to be labelled drastically, and making machine learning applicable when labelling costs would be too high [1].\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tCc9Ds2Lzqms"
   },
   "source": [
    "## Passive Learning\n",
    "Tasks which involve gathering a large amount of data randomly sampled from the underlying distribution and using this large dataset to train a model that can perform some sort of prediction. This method has many disadvantages:\n",
    "* Too many wasted samples. \n",
    "* Learning is limited by sampling resolution \n",
    "\n",
    "<img src=https://imgur.com/M7Afc39.png width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ewo3mFbzjID"
   },
   "source": [
    "## Active Learning: Definition and Concepts\n",
    "Active Learning is a semi-supervised technique whose main hypothesis is that if a learning algorithm can select the data it wants to learn from, it can perform better than traditional methods with significantly less data for training. In other words, **Active Learning (AL) is an interactive approach to simultaneously build a labelled data set and train a machine learning model.**\n",
    "\n",
    "**How to make machines curious to Learn?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qQrT2uXR63ub"
   },
   "source": [
    "### AL algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gPDfcYBHMnLm"
   },
   "source": [
    "1. A relatively large unlabeled dataset is gathered.\n",
    "2. A domain expert labels a few positive examples in the dataset.\n",
    "\n",
    "<img src=https://imgur.com/BHi6GRx.png width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nx8gol6OMnLp"
   },
   "source": [
    "3. A classifier is trained on labeled samples.\n",
    "4. The classifier is applied to the rest of the corpus.\n",
    "\n",
    "<img src=https://imgur.com/ZPya6mX.png width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sec3f0qEMnLs"
   },
   "source": [
    "5. Few most “useful” examples are selected (e.g., that increase classification performance).\n",
    "6. The examples labeled by the expert are added to the training set.\n",
    "\n",
    "<img src=https://imgur.com/yCSp1kU.png width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "00RU5phDMnLv"
   },
   "source": [
    "7. Goto 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPAzx2hB7qX5"
   },
   "source": [
    "<img src=https://imgur.com/smisThj.png width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dSaZwMqk7qdI"
   },
   "source": [
    "<img src=https://imgur.com/NuD954f.png width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1N5cvDyZyjkc"
   },
   "source": [
    "## Classification Uncertainty \n",
    "The simplest measure is the uncertainty of classification defined by ([3] slide 22) $$U(x)=1-P(\\hat{x}|x)$$\n",
    "\n",
    "Where x is the instance to be predicted and $\\hat{x}$ is the most likely prediction. \n",
    "\n",
    "For example, if you have three classes [0, 1, 2] and your network's classification probabilities are [0.1, 0.2, 0.7], the most likely class according to your classifier is class. Uncertainty in its prediction is equal to 0.3. \n",
    "\n",
    "If you have three instances, and the output probabilities of your model are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0mVDJtTKyjuU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "proba = np.array([[0.1 , 0.85, 0.05],  # instance 1 \n",
    "                  [0.6 , 0.3 , 0.1 ],  # instance 2\n",
    "                  [0.39, 0.61, 0.0 ]]) # instance 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y8b67cz5zh3Q"
   },
   "source": [
    "the corresponding uncertainties are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0obFeUhzic1"
   },
   "outputs": [],
   "source": [
    "1 - proba.max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "reV0FrNRztZF"
   },
   "source": [
    "In the example above, the most uncertain sample is the second. When querying labels based on this measure, the strategy selects the sample with the highest uncertainty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WEADWT0DMnL1"
   },
   "source": [
    "## Active Learning on MNIST\n",
    "First we will introduce `modAL`, a modular active learning framework for Python. modAL is built on top of scikit-learn, but you can also use TensorFlow Keras or PyTorch models, if those are your frameworks of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JgLZlcB2FzwM"
   },
   "outputs": [],
   "source": [
    "!pip install modAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mnL08guWvbq-"
   },
   "source": [
    "To start, you will import the packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0BUbIMUtSEu"
   },
   "outputs": [],
   "source": [
    "## Packages for Dirctories\n",
    "import pathlib\n",
    "import shutil\n",
    "#visualization\n",
    "from matplotlib import pyplot as plt\n",
    "##\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model \n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g63Z28Pz5320"
   },
   "source": [
    "## Dataset preparation\n",
    "**MNIST** is a dataset which is 60K pictures of handwritten digits with labels and 10K test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gTBQ3sXe4tAZ"
   },
   "outputs": [],
   "source": [
    "# Load and prepare the MNIST dataset.\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6_uvFu1A1IUU"
   },
   "source": [
    "### Verify the data\n",
    "To verify that the dataset looks correct, let's plot the first 25 images from the training set and display the class name below each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqWnwgns1KCZ"
   },
   "outputs": [],
   "source": [
    "class_names = ['0', '1', '2', '3', '4',\n",
    "               '5', '6', '7', '8', '9']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([]) # Disable xticks (text labels)\n",
    "    plt.yticks([]) # Disable yticks (text labels)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    # The Minist labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[y_train[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the network with a small part of dataset to make the training for the exercise faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a small dataset to make the training faster\n",
    "x_train = x_train[:5000]\n",
    "y_train = y_train[:5000]\n",
    "\n",
    "# Take 400 samples for validation  neural network will never see it during the training\n",
    "x_test = x_test[:400]\n",
    "y_test = y_test[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVZfwUEPISUJ"
   },
   "outputs": [],
   "source": [
    "print (\"The shape of training examples (Features):  \" , x_train.shape)\n",
    "print (\"The shape of training examples (Labels):  \" , y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This form of input is not compatible with the Conv2D layer. This layer expects a dimension of 4. Therefore, we need to extend the dimension of the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0bN_4YKA7-d"
   },
   "outputs": [],
   "source": [
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The shape of training examples:  \" , x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (5000, 28, 28, 1):\n",
    " * 5000 samples\n",
    " * 28*28 pixels\n",
    " * 1 gray image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOsze2-e58oy"
   },
   "source": [
    "For normalization, we can divide the grayscale image points by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V3yGmKjFBWDk"
   },
   "outputs": [],
   "source": [
    "# Convert the samples from integers to floating-point numbers:\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TWQiGu9rfEbG"
   },
   "source": [
    "### Pool-Based Sampling\n",
    "In pool-based sampling the machine has access to a large number of examples  (in our example 200 samples) and samples from a pool (4000 samples) based on “informativeness.” Informativeness is quantified based on a user-selected metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bhe7fKw5AGjj"
   },
   "outputs": [],
   "source": [
    "# assemble initial data\n",
    "n_initial = 200\n",
    "initial_idx = np.random.choice(range(len(x_train)), size=n_initial, replace=False)\n",
    "x_initial = x_train[initial_idx]\n",
    "y_initial = y_train[initial_idx]\n",
    "\n",
    "# generate the pool\n",
    "# remove the initial data from the training dataset\n",
    "x_pool = np.delete(x_train, initial_idx, axis=0)\n",
    "y_pool = np.delete(y_train, initial_idx, axis=0)\n",
    "\n",
    "print (\"The shape of training examples:  \" , x_initial[0].shape)\n",
    "print (\"The number of all training examples: \", x_train.shape[0])\n",
    "print (\"The number of training examples with labels: \", x_initial.shape[0])\n",
    "print (\"The number of training examples without labels: \", x_pool.shape[0])\n",
    "print (\"The number of testing examples:  \", x_test.shape[0])\n",
    "print (\"The number of testing examples:  \", x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Gbwt4FbOAA5"
   },
   "source": [
    "### Create the Model\n",
    "Build the `tf.keras.Sequential` model by stacking layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jmipeJAF6NiV"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3),activation='relu',name='convolution_1', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2),name='pooling_1'),\n",
    "    Conv2D(64, kernel_size=(3, 3),activation='relu',name='convolution_2'),\n",
    "    Dense(64, activation='relu',  name='dense_1'),\n",
    "    Flatten(name='flatten'),\n",
    "    Dropout(0.5,name='dropout_2'),\n",
    "    Dense(10, activation='softmax',name='dense_2')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UGOsMk72XC7"
   },
   "source": [
    "Let's display the architecture of our model so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KA-6q1Mp2XLf"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HZyNBFEY2ozX"
   },
   "source": [
    "Above, you can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 32 or 64). Typically, as the width and height shrink, you can afford (computationally) to add more output channels in each Conv2D layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, \n",
    "           show_shapes=True, \n",
    "           show_layer_names=True, \n",
    "           to_file='my_model.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TesnorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `callbacks.TensorBoard` to generate TensorBoard logs for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(name):\n",
    "    return [\n",
    "        TensorBoard(logdir/name),\n",
    "        EarlyStopping(monitor='val_binary_crossentropy', patience=10),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = pathlib.Path.home() / '.keras' /\"tensorboard_logs\"\n",
    "# Delete an entire directory tree\n",
    "shutil.rmtree(logdir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model\n",
    "Before the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n",
    "* Loss function —This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
    "* Optimizer —This is how the model is updated based on the data it sees and its loss function.\n",
    "* Metrics —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNHrppAo3v-Z"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XjdTUFKiBd3w"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "          x_initial, \n",
    "          y_initial,\n",
    "          epochs=30,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks= get_callbacks(\"passive_learning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wzmfcOnjgcRQ"
   },
   "outputs": [],
   "source": [
    "# evaluate has two output (test_loss, test_acc) only the accuracy output will be used \n",
    "_, test_acc = model.evaluate(x_test, y_test)\n",
    "print(\"the accuracy of model after 3 epochs of training: \", test_acc*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zr94KP0ACw8S"
   },
   "source": [
    "### Active Learning\n",
    "Suppose that you can query the label of an unlabelled instance, but it costs you a lot. Which one would you choose? By querying an instance in the uncertain region, surely you obtain more information than querying by random. \n",
    "\n",
    "The key components of any workflow are the **model** you choose, the **informativeness** measure you use and the **query** strategy you apply to request labels. modAL was designed with modularity, flexibility and extensibility in mind. With using the scikit-learn API, it allows you to rapidly create active learning workflows with nearly complete freedom. What is more, you can easily replace parts with your custom built solutions, allowing you to design novel algorithms with ease. With it, instead of choosing from a small set of built-in components, you have the freedom to seamlessly integrate scikit-learn, TensorFlow/Keras or PyTorch models into your algorithm and easily tailor your custom query strategies and uncertainty measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SbkH-At6OMMj"
   },
   "outputs": [],
   "source": [
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_KorN0vYWg5o"
   },
   "source": [
    "Use the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0aqk0owIWjhA"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3),activation='relu',name='convolution_1', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2),name='pooling_1'),\n",
    "    Conv2D(64, kernel_size=(3, 3),activation='relu',name='convolution_2'),\n",
    "    Dense(64, activation='relu',  name='dense_1'),\n",
    "    Flatten(name='flatten'),\n",
    "    Dropout(0.5,name='dropout_2'),\n",
    "    Dense(10, activation='softmax',name='dense_2')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1N1P6bCp_X7X"
   },
   "outputs": [],
   "source": [
    "# initialize ActiveLearner\n",
    "learner = ActiveLearner(\n",
    "    estimator=model,\n",
    "    query_strategy=uncertainty_sampling,\n",
    "    # the following argumets will be used to train the model (model.fit)\n",
    "    X_training=x_initial, \n",
    "    y_training=y_initial,\n",
    "    epochs=2,\n",
    "    validation_data=(x_test, y_test),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AKRJ7QDGjCt0"
   },
   "source": [
    "Evaluate the model on the test data using `evaluate`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVVfQdZhrzxf"
   },
   "outputs": [],
   "source": [
    "# We'll plot the accuracy scores after we have actively queried the human user.\n",
    "accuracy_scores = []\n",
    "# evaluate has two output (test_loss, test_acc) only the accuracy output will be used \n",
    "_, test_acc = model.evaluate(x_test, y_test)\n",
    "print(\"the accuracy of model after 2 epochs of training: \", test_acc*100, \"%\")\n",
    "# add test_acc to the list\n",
    "accuracy_scores.append(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZvDvd6Tr70i"
   },
   "source": [
    "Actively query the human user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNLm-uq9MnMi"
   },
   "outputs": [],
   "source": [
    "# query for labels\n",
    "query_idx, query_inst = learner.query(x_pool)\n",
    "# training=False is needed only if there are layers with different\n",
    "# behavior during training versus inference (e.g. Dropout).\n",
    "predictions = model(query_inst, training=False)[0]\n",
    "# the predicted class:\n",
    "class_idx = tf.argmax(predictions).numpy()\n",
    "\n",
    "# print the results\n",
    "print(\"the index of query: \", query_idx)\n",
    "print(\"the network prediction: \", class_idx)\n",
    "print(\"the network output: \", predictions.numpy())\n",
    "plt.imshow(query_inst[0].reshape([28, 28]), cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n4lH-5EPDaA2"
   },
   "outputs": [],
   "source": [
    "# supply label for queried instance\n",
    "user_answer = [3]\n",
    "learner.teach(x_pool[query_idx], user_answer)\n",
    "print(\"The right answer was: \", y_pool[query_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qDIDfGl0rMdA"
   },
   "source": [
    "Remove this query from the pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxGKp710rLRc"
   },
   "outputs": [],
   "source": [
    "x_pool, y_pool = np.delete(x_pool, query_idx, axis=0), np.delete(y_pool, query_idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzB2KWc7sUry"
   },
   "source": [
    "Evaluate the model using the test data after the first query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7a9MiT08UKev"
   },
   "outputs": [],
   "source": [
    "# evaluate has two output (test_loss, test_acc) only the accuracy output will be used \n",
    "_, test_acc = model.evaluate(x_test, y_test)\n",
    "# add test_acc to the list\n",
    "accuracy_scores.append(test_acc)\n",
    "print(\"the accuracy of model after the first query \", test_acc*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gkKqRayVsgwB"
   },
   "source": [
    "### Active Learning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uss4K_pxMnMk",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_queries = 6\n",
    "for i in range(n_queries):\n",
    "    query_idx, query_inst = learner.query(x_pool)\n",
    "    plt.title('Digit to label')\n",
    "    plt.imshow(query_inst.reshape(28, 28), cmap=plt.cm.binary)\n",
    "    plt.show()\n",
    "    print(\"Which digit is this?\")\n",
    "    y_new = np.array([int(input())], dtype=int)\n",
    "    print(\"The right answer was: \",y_pool[query_idx])\n",
    "    learner.teach(query_inst, y_new)\n",
    "    x_pool, y_pool = np.delete(x_pool, query_idx, axis=0), np.delete(y_pool, query_idx, axis=0)\n",
    "    _,test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    accuracy_scores.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5sXKLsCAiRk4"
   },
   "outputs": [],
   "source": [
    "print(\"the accuracy of model after the five queries \", test_acc*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X46cofxVjkqk"
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-white'):\n",
    "      plt.figure(figsize=(10, 5))\n",
    "      plt.title('Accuracy of your model')\n",
    "      plt.plot(range(n_queries+2), accuracy_scores)\n",
    "      plt.scatter(range(n_queries+2), accuracy_scores)\n",
    "      plt.xlabel('number of queries')\n",
    "      plt.ylabel('accuracy')\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9EkSxsJrCdC4"
   },
   "source": [
    "## Active Learning for Robotics:\n",
    "In Reinforcement Learning the reward function are the feedback signal (labels) that the agent uses to learn a new skill or achieve a new task. Real world tasks often involve high dimensional observations, like images. Unfortunately, in practice, the design of reward functions for robotic skills is very challenging, especially when learning skills from raw observations such as images. \n",
    "\n",
    "<img src=https://bair.berkeley.edu/static/blog/end_to_end/drape.gif width=\"200\">\n",
    "<img src=https://bair.berkeley.edu/static/blog/end_to_end/push.gif width=\"200\">\n",
    "\n",
    "One solution to learn the **rewards** for such tasks from a small number of user-provided goal examples and samples collected by the policy. The RL algorithm then utilizes this updated classifier as reward for learning a policy to achieve the desired goal, and this alternating process continues until the samples collected by the policy are indistinguishable from the user-proved goal examples [2].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yujukzgHZMAl"
   },
   "source": [
    "### Method: \n",
    "1. User provides examples of successful outcomes (80 user-provided examples are used in [2]).\n",
    "2. Learn a reward function on images using a success classifier, in [2]\n",
    "they used a convolutional neural network for learning a success classifier on image data.\n",
    "3. Run RL with this reward\n",
    "4. Actively query the human user: These active queries are selected based\n",
    "on uncertainty estimates from the classifier that is being used as a reward function, and allow us to learn effective rewards from a small number of initial examples.\n",
    "\n",
    "\n",
    "<img src=https://imgur.com/mGli9UF.png width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m3SlInM_a1a8"
   },
   "source": [
    "Some example queries made by the algorithm, and the corresponding labels provided by a human user. This data is fed back into the classifier.\n",
    "\n",
    "<img src=https://imgur.com/v41mHCE.png width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UeXUWjFqUckt"
   },
   "source": [
    "## References:\n",
    "[1] A. Kirsch, \"BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning\"  \n",
    "[2] A. Singh, \"End-to-End Robotic Reinforcement Learning without Reward Engineering\"  \n",
    "[3] J.M. Zöllner, \"Machine Learning 2, Lecture 4\"  \n",
    "[4] https://www.tensorflow.org/tutorials/  \n",
    "[5] https://modal-python.readthedocs.io/   \n",
    "[6] https://medium.com/@ODSC/crash-course-pool-based-sampling-in-active-learning-cb40e30d49df   \n",
    "[7] https://sites.google.com/view/reward-learning-rl/home\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise_02_Active_Learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv_tf2",
   "language": "python",
   "name": "venv_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
