{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from scipy.stats import norm\n",
    "from tqdm import trange, tqdm_notebook\n",
    "import os.path as osp\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: KL Divergence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.normal import Normal\n",
    "\n",
    "class MOG(nn.Module):\n",
    "    def __init__(self, weights, locs, scales, torch_device): \n",
    "        super().__init__()\n",
    "        self.weights = torch.tensor(weights, device=torch_device)\n",
    "        self.locs = torch.tensor(locs, device=torch_device)\n",
    "        self.scales = torch.tensor(scales, device=torch_device)\n",
    "        self.n_components = len(self.weights)\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        weights = self.weights.unsqueeze(0).repeat(x.shape[0], 1)\n",
    "        return (Normal(self.locs, self.scales).log_prob(x.unsqueeze(1).repeat(1, self.n_components)).exp() * weights).sum(dim=1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-3, 3, num=1000)\n",
    "data_distribution = MOG(np.array([0.7, 0.3]),    # mixture weights\n",
    "                        np.array([-1, 1]),       # means\n",
    "                        np.array([0.25, 0.25]),# scales\n",
    "                        torch_device = 'cpu')  \n",
    "ys = data_distribution.log_prob(torch.tensor(xs)).exp().numpy()\n",
    "plt.plot(xs, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).\n",
    "        self.loc = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "        self.log_scale = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        return Normal(self.loc, self.log_scale.exp()).log_prob(x)\n",
    " \n",
    "    # Compute loss as negative log-likelihood\n",
    "    def nll(self, x):\n",
    "        return - self.log_prob(x).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_distribution = Gaussian()  # scales\n",
    "y_gauss = model_distribution.log_prob(torch.tensor(xs)).exp().detach().numpy()\n",
    "plt.plot(xs, y_gauss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Kullback-Leibler Divergence\n",
    "\n",
    "Kullback-Leibler Divergence is a measure of how one probability distribution is different from a second, reference probability distribution. It is defined as the expectation of the logarithmic difference between the probabilities P and  Q, where the expectation is taken using the probabilities P:\n",
    "$$D_{KL}[P||Q]  = \\mathbb{E}_{x \\sim P}\\left[\\log P(x) - \\log Q(x)\\right] $$\n",
    "For discrete probability distributions P and Q defined on the same probability space, KL-divergence is defined as a sum:\n",
    "$$D_{KL}[P||Q]  = \\sum_{i} P(i) log\\frac{P(i)}{Q(i)}$$\n",
    "For distributions P and Q of a continuous random variable, the KL-divergence as an integral:\n",
    "$$D_{KL}[P||Q]  =  \\int_{} P(x) log\\frac{P(x)}{Q(x)}dx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q, interval_size=1):\n",
    "    return (p * (p.log() - q.log())).sum() * interval_size\n",
    "\n",
    "def kl_divergence_with_logs(p, q, interval_size=1):\n",
    "    return (p.exp() * (p - q)).sum() * interval_size\n",
    "\n",
    "def forward_kl(model, data_dist):\n",
    "    num_steps = 100000\n",
    "    interval_size = 10.0 / num_steps\n",
    "\n",
    "    xs = torch.linspace(-5, 5, steps=num_steps).to(ptu.device)\n",
    "    return kl_divergence_with_logs(data_dist.log_prob(xs), model.log_prob(xs), interval_size=interval_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_model = Gaussian().to(ptu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Train the model using forward KL or maximum log-likelihood estimate (MLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(model, data_distribution, train_args, loss_fn):\n",
    "    epochs, lr = train_args['epochs'], train_args['lr']\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    ## Evaluation\n",
    "   # model.eval()\n",
    "    #test_loss = loss_fn(model, data_distribution).item()\n",
    "    #test_losses.append(test_loss)  # loss at init\n",
    "\n",
    "    ## Training\n",
    "    for epoch in tqdm_notebook(range(epochs), desc='Epoch', leave=False):\n",
    "        # start the training\n",
    "        mle_model.train()\n",
    "        loss = forward_kl(mle_model, data_distribution)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=3E-3\n",
    "epochs=1000\n",
    "\n",
    "optimizer = optim.Adam(mle_model.parameters(), lr=lr)\n",
    "mle_model.train()\n",
    "\n",
    "for epoch in tqdm_notebook(range(epochs), desc='Epoch', leave=False):\n",
    "\n",
    "    loss = forward_kl(mle_model, data_distribution)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    print(loss)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs, ys, label='data')\n",
    "plt.plot(xs, ptu.get_numpy(mle_model.log_prob(ptu.tensor(xs)).exp()), c='g', linestyle='dashed', label='learned model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2:  A Simple Latent Variable Model (LVM)\n",
    "In this part, we train a simple LVM modeled as $z \\sim \\text{Multinomial}(3), x \\sim N(\\mu_\\theta(z), 1)$, where $\\mu_\\theta(z)$ is a small neural network outputting the mean of a guassian. We fit this LVM using maximum likelhood by marginalizing out $z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_blobs(n):\n",
    "    centers = np.array([[5, 5], [-5, 5], [0, -5]])\n",
    "    st_devs = np.array([[1.0, 1.0], [0.2, 0.2], [3.0, 0.5]])\n",
    "    labels = np.random.randint(0, 3, size=(n,), dtype='int32')\n",
    "    x = np.random.randn(n, 2) * st_devs[labels] + centers[labels]\n",
    "    return x.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_2d(points, title='', labels=None):\n",
    "    plt.figure()\n",
    "    if labels is not None:\n",
    "        plt.scatter(points[:, 0], points[:, 1], c=labels,\n",
    "                    cmap=matplotlib.colors.ListedColormap(['red', 'blue', 'green', 'purple']))\n",
    "    else:\n",
    "        plt.scatter(points[:, 0], points[:, 1])\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sample_blobs(10000)\n",
    "test_data = sample_blobs(2500)\n",
    "plot_scatter_2d(train_data, title='Train Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the numpy dataset to pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=128)\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLVM(nn.Module):\n",
    "    def __init__(self, n_mix):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_mix = n_mix\n",
    "        self.pi_logits = nn.Parameter(torch.zeros(n_mix, dtype=torch.float32), requires_grad=True)\n",
    "        self.mus = nn.Parameter(torch.randn(n_mix, 2, dtype=torch.float32), requires_grad=True)\n",
    "        self.log_stds = nn.Parameter(-torch.ones(n_mix, 2, dtype=torch.float32), requires_grad=True)\n",
    "\n",
    "    def loss(self, x):\n",
    "        log_probs = []\n",
    "        for i in range(self.n_mix):\n",
    "            mu_i, log_std_i = self.mus[i].unsqueeze(0), self.log_stds[i].unsqueeze(0)\n",
    "            log_prob = -0.5 * (x - mu_i) ** 2 * torch.exp(-2 * log_std_i)\n",
    "            log_prob = log_prob - 0.5 * np.log(2 * np.pi) - log_std_i\n",
    "            log_probs.append(log_prob.sum(1))\n",
    "        log_probs = torch.stack(log_probs, dim=1)\n",
    "\n",
    "        log_pi = F.log_softmax(self.pi_logits, dim=0)\n",
    "        log_probs = log_probs + log_pi.unsqueeze(0)\n",
    "        loss = -torch.logsumexp(log_probs, dim=1).mean()\n",
    "        return OrderedDict(loss=loss)\n",
    "\n",
    "    def sample(self, n):\n",
    "        with torch.no_grad():\n",
    "            probs = F.softmax(self.pi_logits, dim=0)\n",
    "            labels = torch.multinomial(probs, n, replacement=True)\n",
    "            mus, log_stds = self.mus[labels], self.log_stds[labels]\n",
    "            x = torch.randn(n, 2) * log_stds.exp() + mus\n",
    "        return x.numpy(), labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train_epochs\n",
    "\n",
    "n_mix = 3\n",
    "model = SimpleLVM(n_mix)\n",
    "\n",
    "def fn(epoch):\n",
    "    x, labels = model.sample(10000)\n",
    "    plot_scatter_2d(x, title=f'Epoch {epoch} Samples', labels=labels)\n",
    "\n",
    "train_epochs(model, train_loader, test_loader, device, dict(epochs=10, lr=7e-2),\n",
    "             fn=fn, fn_every=2, quiet=True)\n",
    "\n",
    "x, labels = model.sample(10000)\n",
    "plot_scatter_2d(x, title='Final Samples', labels=labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
