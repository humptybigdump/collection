{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed6669-5a7c-4fb3-a111-986033c4224c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow gymnasium pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d79d6f3-1466-4f1e-b1ee-93eb7169d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ad28f-50d6-4d39-95a3-53eee142cd5e",
   "metadata": {},
   "source": [
    "# Tabular Q-learning limitations\n",
    "\n",
    "- relatively small state-action space (discrete and finite)\n",
    "\n",
    "- learns solution for given scenario, each time the environment changed, we had to retrain the model\n",
    "\n",
    "- to update an action-state value, the state has to be visited and the action has to be executed, not feasible in large state-action space\n",
    "\n",
    "## Idea: replace the table with a function approximator, a neural network\n",
    "We approximate the q-value function $Q$ by a $\\theta$ parametrized neural network $q_\\theta$.\n",
    "\n",
    "Generally, the state $s_t$ and the action $a_t$ are in different domains. For this reason, we will not use both as an input for the neural network, instead we design our network to take the state $s_t$ as an input, and to have an output vector with each element corresponding to a possible action.\n",
    "$$\n",
    "Q(s_t, a_t) = q_\\theta(s_t)[a_t]\n",
    "$$\n",
    "Similarly to tabular Q-learining, the policy derived from the approximated q-value function is then\n",
    "$$\n",
    "\\pi(s_t) = argmax_a(Q(s_t, a)) = argmax_a(q_\\theta(s_t)[a])\n",
    "$$\n",
    "and the value for the given state\n",
    "$$\n",
    "v(s_t) = max_a(Q(s_t, a)) = max_a(q_\\theta(s_t)[a])\n",
    "$$\n",
    "\n",
    "We used TD-learning to update our approximation of the q-value function. In case of neural networks, we use Bellman's equation directly as a target and minimize $MSE$ (mean squared error) loss with stochastic gradient descent.\n",
    "$$\n",
    "output = q_\\theta(s_t)[a_t]\n",
    "$$\n",
    "$$\n",
    "target = R(s_t, a_t) + \\gamma v(s_{t+1})\n",
    "$$\n",
    "$$\n",
    "loss = MSE(output, target)\n",
    "$$\n",
    "To keep the implementation of the weight-update simple, we use the whole output vector of the neural network to calculate the loss. For that, we need to update the value of the target output vector at $a_t$ position.\n",
    "$$\n",
    "output = q_\\theta(s_t)\n",
    "$$\n",
    "$$\n",
    "target = q_\\theta(s_{t + 1})\n",
    "$$\n",
    "$$\n",
    "target[a_t] \\leftarrow R(s_t, a_t) + \\gamma v(s_{t+1})\n",
    "$$\n",
    "$$\n",
    "loss = MSE(output, target)\n",
    "$$\n",
    "## Problems\n",
    "### Unstable training\n",
    "Unfortunately simply replacing the table with a neural network won't work. One main problem is, that calculating the $target$ also executes the same neural network, that we would want to train. Thus the $output$ and $target$ are not independent and this makes the training unstable.\n",
    "\n",
    "**Solution**: target network $q_{target}$\n",
    "- a \"frozen\" copy of $q_\\theta$\n",
    "- not trained\n",
    "- network parameters (weights and biases) periodically synchronized with $q_\\theta$\n",
    "\n",
    "This changes the loss calculation to\n",
    "$$\n",
    "output = q_\\theta(s_t)\n",
    "$$\n",
    "$$\n",
    "target = q_{target}(s_{t + 1})\n",
    "$$\n",
    "$$\n",
    "target[a_t] \\leftarrow R(s_t, a_t) + \\gamma max_a(q_{target}(s_t)[a])\n",
    "$$\n",
    "$$\n",
    "loss = MSE(output, target)\n",
    "$$\n",
    "### Overfitting and catastrophic forgetting\n",
    "Neural netoworks are trained with stochastic gradient descent. It works only properly if the training data in the batch is independent and identically distributed. This is not the case if we sequentially interact with the environment.\n",
    "\n",
    "**Solution**: replay buffer\n",
    "- does not update q-value approximator after each step\n",
    "- stores gathered experience in a buffer\n",
    "- samples from buffer from training\n",
    "- helps to avoid overfitting and catastrophic forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a884f16d",
   "metadata": {},
   "source": [
    "## Step 1 (TODO): Implement the network\n",
    "First we will implement the network that learns to approximate the q-value function.\n",
    "\n",
    "We define a class `DQN` that inherits from tf.keras.layers.Layer. This class represents a deep neural network with a certain number of layers, each of which is a dense (fully connected) layer with a specified number of units and a ReLU activation function. The last layer is also a dense layer with a number of units equal to the number of possible actions (`n_actions`). The call method defines the forward pass of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6b1c88-419c-4f9f-9fc5-9485388ea95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Deep Q-Network (DQN)\n",
    "class DQN(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=(100, 50), n_actions=2, **kwargs):\n",
    "        # Call the initializer of the parent tf.keras.layers.Layer class.\n",
    "        super(DQN, self).__init__(**kwargs)\n",
    "        # Initialize an empty list for layers of the neural network.\n",
    "        self.layers = []\n",
    "        # TODO Loop over the given number of units in each layer and \n",
    "        # create dense layers with Rectified Linear Unit (ReLU) activation function.\n",
    "        # hint: use tf.keras.layers.Dense(n_units, activation='relu')\n",
    "        ...\n",
    "        # TODO Append the output layer to the list of layers.\n",
    "        # The output layer has n_actions neurons which is the number of possible actions.\n",
    "        ...\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        # Define the forward pass of the network.\n",
    "        # Initialize outputs with inputs\n",
    "        outputs = inputs\n",
    "        # TODO Loop over the layers and pass the inputs through each layer\n",
    "        # hint: the output of a layer is the input for the next layer\n",
    "        # hint: you can call a layer l as a function with \"(<input>)\"\n",
    "        ...\n",
    "        # Return the final output\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e83da2",
   "metadata": {},
   "source": [
    "## Step 2 (TODO): Implement the agent\n",
    "The `DQNAgent` class represents an agent that uses a deep Q-network to learn to take actions in a given environment. The agent has an exploration rate (`epsilon`), which determines how often the agent selects a random action, and a discount factor (`gamma`), which determines how much future rewards are valued compared to immediate rewards. The agent is trained to minimize the difference between the estimated q-values and the target q-values, which are calculated based on the estimated q-values of the next state and the immediate reward. The agent periodically updates the target network with the weights of the online network to stabilize learning.\n",
    "\n",
    "In our implementation, we make use of OpenAI's `gym` implementation for descrete action spaces: https://gymnasium.farama.org/api/spaces/fundamental/#discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67001e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, action_space, observation_shape, epsilon=0.9, gamma=0.95):\n",
    "        # TODO set class variables\n",
    "        # The action space of the environment\n",
    "        ...\n",
    "        # The exploration rate\n",
    "        ...\n",
    "        # The discount factor\n",
    "        ...\n",
    "        # TODO Initialize the model and the target model.\n",
    "        # hint: you can access the number of possible actions of a discrete action space by action_space.n\n",
    "        ...\n",
    "        # The optimizer for training the network\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        # Initialize the network weights. This creates the tensorflow graph and intializes the weights of the model.\n",
    "        # This is required for copying the model's weights to the target model's weights\n",
    "        self._init_networks(observation_shape)\n",
    "\n",
    "    def target_update(self):\n",
    "        # Get the weights of the online model\n",
    "        weights = self.model.get_weights()\n",
    "        # Set the weights of the target model to the weights of the model\n",
    "        self.target_model.set_weights(weights)\n",
    "        \n",
    "    def _init_networks(self, observation_shape):\n",
    "        # Define the initializer. This is just a tensor in the shape of the input to the model. The values do not matter.\n",
    "        initializer = np.zeros([1, observation_shape])\n",
    "        # Initialize the model.\n",
    "        self.model(initializer)\n",
    "        # Initialize the target model.\n",
    "        self.target_model(initializer)\n",
    "        # Copy the model's weights to the target model.\n",
    "        self.target_update()\n",
    "        \n",
    "    def act(self, observation, explore=True):\n",
    "        if explore:\n",
    "            # TODO With probability epsilon, select a random action and return it\n",
    "            # hint: action space documentation\n",
    "            ...\n",
    "        # TODO Otherwise, select the action with the highest estimated q-value\n",
    "        # hint: execute model and use argmax\n",
    "        ...\n",
    "        return action\n",
    "        \n",
    "    def learn(self, states, actions, rewards, next_states, dones):\n",
    "        # TODO Get the current target q-values for the current state. Cast the results to numpy with .numpy()\n",
    "        ...\n",
    "        # TODO compute the value of the next state next_states\n",
    "        ...\n",
    "        # Calculate the new target q-values for the actions that were taken. \n",
    "        # If the episode ended, there are no future rewards, so the target q-value is just the \n",
    "        # immediate reward.\n",
    "        targets[range(actions.shape[0]), actions] = rewards + (1-dones) * next_value * self.gamma\n",
    "        with tf.GradientTape() as tape:\n",
    "            # TODO Get the current q-values\n",
    "            ...\n",
    "            # TODO: Calculate the MSE loss, the mean squared error between the target and \n",
    "            # current q-values.\n",
    "            ...\n",
    "        # Get the gradients for the weights\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        # We also clip the gradients, this leads to a more stable training usually\n",
    "        gradients = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gradients]\n",
    "        # Update the model's weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        return loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099fd0f8",
   "metadata": {},
   "source": [
    "## Step 3: Replay buffer\n",
    "The ReplayBuffer is an important component of DQN. It stores past experiences that the agent can then sample from when learning. This helps to break correlations in the observation sequence, which can significantly improve learning stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69529796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        # Initialize the ReplayBuffer with a maximum capacity. \n",
    "        # The buffer is implemented as a deque (double-ended queue) which automatically\n",
    "        # removes the oldest elements when appending new elements beyond its maximum length.\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def put(self, state, action, reward, next_state, done):\n",
    "        # This function is used to add a new experience to the buffer. \n",
    "        # An experience consists of the current state, the action taken in that state, \n",
    "        # the reward received, the next state that resulted from the action, \n",
    "        # and a flag 'done' indicating whether this next state is terminal (i.e., the episode ended).\n",
    "        self.buffer.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def sample(self, batch_size=1):\n",
    "        # This function is used to retrieve a batch of experiences from the buffer. \n",
    "        # It randomly selects 'batch_size' experiences.\n",
    "        # It separates the list of experiences into separate lists for states, actions, rewards,\n",
    "        # next_states, and 'done' flags, and returns these lists.\n",
    "        sample = random.sample(self.buffer, batch_size)\n",
    "        # We cast the sampled elements to numpy arrays\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*sample))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def size(self):\n",
    "        # This function returns the current size of the buffer.\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b4f61-10a5-42df-8aee-2003043036ca",
   "metadata": {},
   "source": [
    "## Step 4 (TODO): Implement the Training\n",
    "\n",
    "The training process involves a few different steps, so we will break it down into sections. It includes gathering experiences from the environment, learning from the experiences by updating the agent's Q-network, and periodically evaluating the agent's performance and updating the target network.\n",
    "\n",
    "In the following code snippets, we will outline the process of training your DQN agent.\n",
    "\n",
    "Firstly, let's start by implementing a method to calculate the average return of our agent over some episodes, for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef5fbe7-8e6a-4254-866a-d50da41a6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(env, agent, num_episodes=1, max_steps=200, render=False):\n",
    "    # Initialize the total_return to 0. This variable will accumulate the returns of each episode.\n",
    "    total_return = 0.0\n",
    "\n",
    "    # Iterate over the number of episodes\n",
    "    for _ in range(num_episodes):\n",
    "        # Reset the environment at the beginning of each episode and ignore the initial reward and extra info\n",
    "        obs, _ = env.reset()\n",
    "        \n",
    "        # Initialize the return of the current episode\n",
    "        episode_return = 0.0\n",
    "\n",
    "        # Initialize done to False. Done is a flag that indicates whether the episode has ended.\n",
    "        done = False\n",
    "\n",
    "        # Initialize the number of steps in the current episode\n",
    "        steps = 0\n",
    "\n",
    "        # Continue the loop until the episode ends or the number of steps exceeds max_steps\n",
    "        while not (done or steps > max_steps):\n",
    "            # If render is True, clear the previous output, render the environment, and display it\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "\n",
    "            # Choose the action using the agent's policy without exploration\n",
    "            action = agent.act(np.array([obs]), explore=False)\n",
    "\n",
    "            # Execute the action in the environment and get the new observation, reward, and done flag\n",
    "            obs, r, done, _, _ = env.step(action)\n",
    "            \n",
    "            # Add the reward to the return of the current episode\n",
    "            episode_return += r\n",
    "\n",
    "            # Increment the number of steps\n",
    "            steps += 1\n",
    "\n",
    "        # After the episode ends, add the return of the episode to the total return\n",
    "        total_return += episode_return\n",
    "\n",
    "    # Return the average return over the episodes\n",
    "    return total_return / num_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c5ce6f",
   "metadata": {},
   "source": [
    "Initially, when implementing a model, it is advised to test it on simple problems. Since the CartPole environment is the \"Hello World!\" of reinforcement learning, we will first test our agent on this environment.\n",
    "\n",
    "OpenAI Gym is a Python library for developing and comparing reinforcement learning algorithms. It provides a wide variety of environments for your algorithms to interact with. It's goal is to provide a standardized interface for comparing and reproducing the results of RL algorithms.\n",
    "\n",
    "The CartPole environment, which we are using here, is one of the simplest environments provided by gym. The goal is to balance a pole, connected with one joint on top of a cart. The cart moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. A reward of +1 is provided for every timestep the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "The CartPole environment's observation is a 4D vector, containing the cart's position, velocity and the pole's angle and velocity. It has two possible actions: moving left and moving right.\n",
    "\n",
    "Now we can initialize our environment and DQN agent, and test them without training to see the performance of an untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190003ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment using gym.make(), you can choose the 'CartPole-v1' for example.\n",
    "# We use render_mode='rgb_array' to enable visualization compute_avg_return\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "obs, _ = env.reset()\n",
    "\n",
    "print(env.observation_space.shape)\n",
    "# TODO Initialize the DQN agent using the DQNAgent class.\n",
    "...\n",
    "\n",
    "# TODO Compute the average return over 5 episodes using the untrained agent, also, enable rendering.\n",
    "...\n",
    "\n",
    "print(\"Average return of an untrained agent:\", avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27bc125-dbcf-4459-9f79-c6ec30c5e695",
   "metadata": {},
   "source": [
    "Next, we define the main training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9720b9cb-8078-4acb-9666-f1088c9bee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize the replay buffer with a capacity of 10000.\n",
    "replay_buffer = ReplayBuffer(10000)\n",
    "\n",
    "# Define the number of epochs and the number of steps per epoch.\n",
    "num_epochs = 200\n",
    "num_steps = 400\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Reset the environment at the beginning of each epoch.\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # TODO Gather experience for num_steps steps.\n",
    "    ...\n",
    "\n",
    "    # Learn from the experiences in the replay buffer.\n",
    "    for _ in range(128):\n",
    "        # TODO Sample 64 samples from the replay buffer\n",
    "        ...\n",
    "        # TODO Learn from the sampled data\n",
    "        ...\n",
    "\n",
    "    # TODO Periodically evaluate the agent's performance and print the average return.\n",
    "    ...\n",
    "\n",
    "    # TODO Periodically update the target network with the online network weights.\n",
    "    ...\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ddbc7",
   "metadata": {},
   "source": [
    "## Step 5: Test the trained model\n",
    "\n",
    "After training the agent, you want to test it to see how well it performs. In the code below, we will run the agent for a number of episodes and average the returns.\n",
    "\n",
    "We will also set `render=True` in `compute_avg_return()` function, so that we can see the agent interacting with the environment in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce822a5-92b6-41a8-8b89-94f3c9c7b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# Run the trained agent on the environment for a number of episodes and print the average return\n",
    "avg_return = compute_avg_return(env, agent, num_episodes=3, render=True)\n",
    "\n",
    "print(\"Average return of the trained agent:\", avg_return)\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe0bb4-e492-477f-8572-38b1edb4ca77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
