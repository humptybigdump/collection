{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Heuristic algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will implement 4 heuristic algorithms for link prediction of nodes in graph, i.e.\n",
    "\n",
    "* Common neighbours (CN)\n",
    "* Adamic Adar (AA)\n",
    "* Resource Allocation (RA)\n",
    "* Personalized PageRank (PPR)\n",
    "\n",
    "We will look into details of these algorithms about how they process link prediction and how well they can achieve it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Import library\n",
    "\n",
    "Firstly, import the necessary libraries supporting all the exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "import scipy.sparse as ssp\n",
    "\n",
    "\n",
    "from typing import Tuple, List, Dict, Any, Optional\n",
    "from torch import FloatTensor\n",
    "from scipy.sparse import csr_matrix\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Load data and split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Cora'  \n",
    "path = '.'\n",
    "undirected = True\n",
    "val_pct = 0.2\n",
    "test_pct = 0.1 \n",
    "include_negatives = True\n",
    "dataset = Planetoid(path, dataset_name)\n",
    "transform = RandomLinkSplit(is_undirected=undirected,\n",
    "                            num_val=val_pct,\n",
    "                            num_test=test_pct,\n",
    "                            add_negative_train_samples=include_negatives)\n",
    "train_data, val_data, test_data = transform(dataset._data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Get positive and negative edges\n",
    "\n",
    "In the input data, there are positive and negative edges (as opposed to message passing edges). The following functions are for extracting them by RandomLinkSplit, and load the samples which will be actually input to algorithms by percentage or sample number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_neg_edges(data, sample_frac=1):\n",
    "    \"\"\"\n",
    "    :param data: A train, val or test split returned by RandomLinkSplit\n",
    "    :return: positive edge_index, negative edge_index.\n",
    "    \"\"\"\n",
    "    device = data.edge_index.device\n",
    "    edge_index = data['edge_label_index'].to(device)\n",
    "    labels = data['edge_label'].to(device)\n",
    "    pos_edges = edge_index[:, labels == 1].t()\n",
    "    neg_edges = edge_index[:, labels == 0].t()\n",
    "    if sample_frac != 1:\n",
    "        n_pos = pos_edges.shape[0]\n",
    "        np.random.seed(123)\n",
    "        perm = np.random.permutation(n_pos)\n",
    "        perm = perm[:int(sample_frac * n_pos)]\n",
    "        pos_edges = pos_edges[perm, :]\n",
    "        neg_edges = neg_edges[perm, :]\n",
    "    return pos_edges.to(device), neg_edges.to(device)\n",
    "\n",
    "pos_train_edge, neg_train_edge = get_pos_neg_edges(train_data)\n",
    "pos_val_edge, neg_val_edge = get_pos_neg_edges(val_data)\n",
    "pos_test_edge, neg_test_edge = get_pos_neg_edges(test_data)\n",
    "\n",
    "train_data.pos_edges, train_data.neg_edges = pos_train_edge, neg_train_edge\n",
    "val_data.pos_edges, val_data.neg_edges = pos_val_edge, neg_val_edge\n",
    "test_data.pos_edges, test_data.neg_edges = pos_test_edge, neg_test_edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Prepare adjacency matrix A\n",
    "\n",
    "The adjacency matrix, denoted as A, represents a graph's connections. In a binary form, $A[i][j]$ is 1 if nodes i and j are connected and 0 otherwise. It succinctly captures the graph's structure.\n",
    "\n",
    "Create the adjacency matrix A for further testing of heuristic algorithms with loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = dataset._data.num_nodes\n",
    "labels = torch.tensor([1] * pos_train_edge.size(0) + [0] * neg_train_edge.size(0))\n",
    "edge_weight = torch.ones(dataset._data.edge_index.shape[1])\n",
    "A = ssp.csr_matrix((edge_weight, (dataset._data.edge_index[0,:], dataset._data.edge_index[1,:])),shape=(num_nodes, num_nodes), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Complete the algorithms\n",
    "\n",
    "There comes to heuristic algorithms themselves. Please based on your knowledge and previous assignments, fill the TODO parts in functions to complete the algorithms.\n",
    "\n",
    "**Hint: refer to formulas of corresponding algorithms.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CN(A: csr_matrix, edge_index: torch.Tensor, batch_size: int = 100000) -> Tuple[FloatTensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Common neighbours\n",
    "    :param A: scipy sparse adjacency matrix\n",
    "    :param edge_index: pyg edge_index (torch.Tensor of shape [2, num_edges])\n",
    "    :param batch_size: int\n",
    "    :return: Tuple containing a FloatTensor of scores and the pyg edge_index\n",
    "    \"\"\"\n",
    "    edge_index = edge_index.t()\n",
    "    link_loader = DataLoader(range(edge_index.size(0)), batch_size)\n",
    "    scores = []\n",
    "    for ind in tqdm(link_loader):\n",
    "        src, dst = edge_index[ind, 0], edge_index[ind, 1]\n",
    "        cur_scores = np.array(np.sum(A[src].multiply(A[dst]), 1)).flatten()\n",
    "        scores.append(cur_scores)\n",
    "    scores = np.concatenate(scores, 0)\n",
    "    print(f'evaluated Common Neighbours for {len(scores)} edges')\n",
    "    return torch.FloatTensor(scores), edge_index\n",
    "\n",
    "\n",
    "def AA(A: csr_matrix, edge_index: torch.Tensor, batch_size: int = 100000) -> Tuple[FloatTensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Adamic Adar\n",
    "    :param A: scipy sparse adjacency matrix\n",
    "    :param edge_index: pyg edge_index (torch.Tensor of shape [2, num_edges])\n",
    "    :param batch_size: int\n",
    "    :return: Tuple containing a FloatTensor of scores and the pyg edge_index\n",
    "    \"\"\"\n",
    "    edge_index = edge_index.t()\n",
    "    multiplier = 1 / np.log(A.sum(axis=0))\n",
    "    multiplier[np.isinf(multiplier)] = 0\n",
    "    A_ = A.multiply(multiplier).tocsr()\n",
    "    link_loader = DataLoader(range(edge_index.size(0)), batch_size)\n",
    "    scores = []\n",
    "    for ind in tqdm(link_loader):\n",
    "        src, dst = edge_index[ind, 0], edge_index[ind, 1]\n",
    "        cur_scores = np.array(np.sum(A[src].multiply(A_[dst]), 1)).flatten()\n",
    "        scores.append(cur_scores)\n",
    "    scores = np.concatenate(scores, 0)\n",
    "    print(f'evaluated Adamic Adar for {len(scores)} edges')\n",
    "    return torch.FloatTensor(scores), edge_index\n",
    "\n",
    "\n",
    "def RA(A: csr_matrix, edge_index: torch.Tensor, batch_size: int = 100000) -> Tuple[FloatTensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Resource Allocation https://arxiv.org/pdf/0901.0553.pdf\n",
    "    :param A: scipy sparse adjacency matrix\n",
    "    :param edge_index: pyg edge_index (torch.Tensor of shape [2, num_edges])\n",
    "    :param batch_size: int\n",
    "    :return: Tuple containing a FloatTensor of scores and the pyg edge_index\n",
    "    \"\"\"\n",
    "    edge_index = edge_index.t()\n",
    "    multiplier = 1 / A.sum(axis=0)\n",
    "    multiplier[np.isinf(multiplier)] = 0\n",
    "    A_ = A.multiply(multiplier).tocsr()\n",
    "    link_loader = DataLoader(range(edge_index.size(0)), batch_size)\n",
    "    scores = []\n",
    "    for ind in tqdm(link_loader):\n",
    "        src, dst = edge_index[ind, 0], edge_index[ind, 1]\n",
    "        cur_scores = np.array(np.sum(A[src].multiply(A_[dst]), 1)).flatten()\n",
    "        scores.append(cur_scores)\n",
    "    scores = np.concatenate(scores, 0)\n",
    "    print(f'evaluated Resource Allocation for {len(scores)} edges')\n",
    "    return torch.FloatTensor(scores), edge_index\n",
    "\n",
    "\n",
    "def PPR(A, edge_index):\n",
    "    \"\"\"\n",
    "    The Personalized PageRank heuristic score.\n",
    "    Need to install fast_pagerank by \"pip install fast-pagerank\"\n",
    "    Too slow for large datasets now.\n",
    "    :param A: A CSR matrix using the 'message passing' edges\n",
    "    :param edge_index: The supervision edges to be scored\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    edge_index = edge_index.T\n",
    "    from fast_pagerank import pagerank_power\n",
    "    num_nodes = A.shape[0]\n",
    "    src_index, sort_indices = torch.sort(edge_index[:, 0])\n",
    "    dst_index = edge_index[sort_indices, 1]\n",
    "    edge_reindex = torch.stack([src_index, dst_index])\n",
    "    scores = []\n",
    "    visited = set([])\n",
    "    j = 0\n",
    "    for i in tqdm(range(edge_reindex.shape[1])):\n",
    "        if i < j:\n",
    "            continue\n",
    "        src = edge_reindex[0, i]\n",
    "        personalize = np.zeros(num_nodes)\n",
    "        personalize[src] = 1\n",
    "        # get the ppr for the current source node\n",
    "        ppr = pagerank_power(A, p=0.85, personalize=personalize, tol=1e-7)\n",
    "        j = i\n",
    "        # get ppr for all links that start at this source to save recalculating the ppr score\n",
    "        while edge_reindex[0, j] == src:\n",
    "            j += 1\n",
    "            if j == edge_reindex.shape[1]:\n",
    "                break\n",
    "        all_dst = edge_reindex[1, i:j]\n",
    "        cur_scores = ppr[all_dst]\n",
    "        if cur_scores.ndim == 0:\n",
    "            cur_scores = np.expand_dims(cur_scores, 0)\n",
    "        scores.append(np.array(cur_scores))\n",
    "\n",
    "    scores = np.concatenate(scores, 0)\n",
    "    print(f'evaluated PPR for {len(scores)} edges')\n",
    "    return torch.FloatTensor(scores), edge_reindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Test heuristic algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 69.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluated Common Neighbours for 10556 edges\n",
      "tensor([0., 1., 1.,  ..., 2., 3., 2.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 70.28it/s]\n",
      "/scratch/ipykernel_3437702/2261758863.py:30: RuntimeWarning: divide by zero encountered in divide\n",
      "  multiplier = 1 / np.log(A.sum(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluated Resource Allocation for 10556 edges\n",
      "tensor([0.0000, 0.3333, 0.2500,  ..., 0.5000, 0.5303, 0.5000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 78.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluated Adamic Adar for 10556 edges\n",
      "tensor([0.0000, 0.9102, 0.7213,  ..., 1.4427, 1.7287, 1.4427])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10556/10556 [00:12<00:00, 824.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluated PPR for 10556 edges\n",
      "tensor([0.1125, 0.0734, 0.0991,  ..., 0.1089, 0.0896, 0.0837])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('starting training')\n",
    "CN_scores, edge_index = CN(A, dataset._data.edge_index)\n",
    "print(CN_scores)\n",
    "PA_scores, edge_index = RA(A, dataset._data.edge_index)\n",
    "print(PA_scores)\n",
    "AA_scores, edge_index = AA(A, dataset._data.edge_index)\n",
    "print(AA_scores)\n",
    "PPR_scores, edge_index = PPR(A, dataset._data.edge_index)\n",
    "print(PPR_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network\n",
    "\n",
    "Heuristic algorithms works in a intuitive way to capture the underneath patterns among nodes and links in graph. Now we try another thought with neural networks, which implement the same purpose as heuristic algorithm but through **learning** capacities of models. In a nutshell, we let the models themselves to find the pattern, instead of we giving primitive guess and designing. We will try two kinds of neural networks:\n",
    "\n",
    "* MLP: Multi-layer Perceptron\n",
    "* GCN: Graph convolutional network\n",
    "\n",
    "Note that in this project, both models only possess basic structures, and training methods are also primitive without fine-tunning. Hence, this project might not be able to give outcomes as good as heuristic algorithms, but aims to help you get familiar with the idea of how to construct a basic neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Import torch\n",
    "\n",
    "torch provides with abundant modularized functions and classes especially for neural networks. Import corresponding ones with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Load data\n",
    "\n",
    "Load data and split it into training set, validation set and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of dataset:\n",
      "Classes of labels: {0, 1, 2, 3, 4, 5, 6}\n",
      "row of adjacency matrix A: [ 633 1862 2582 ...  598 1473 2706]\n",
      "col of adjacency matrix A: [   0    0    0 ... 2707 2707 2707]\n",
      "data of adjacency matrix A: [1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='.', name='Cora') \n",
    "data = dataset[0]\n",
    "adj = to_scipy_sparse_matrix(data.edge_index)\n",
    "print(f'Overview of dataset:')\n",
    "print(f'Classes of labels: {set(list(i.item() for i in data.y))}')\n",
    "print(f'row of adjacency matrix A: {adj.row}')\n",
    "print(f'col of adjacency matrix A: {adj.col}')\n",
    "print(f'data of adjacency matrix A: {adj.data}')\n",
    "\n",
    "# split_data\n",
    "transform = RandomLinkSplit(is_undirected=True,\n",
    "                            num_val=0.2,\n",
    "                            num_test=0.1,\n",
    "                            add_negative_train_samples=True\n",
    "                            )\n",
    "train_data, val_data, test_data = transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) define model\n",
    "\n",
    "#### 1. MLP: Multi-layer perceptron\n",
    "\n",
    "MLP is universal approximator consisting of fully connected layers, which is capable of finding patterns of almost all datasets, but of course, in theory, and only to some degrees. Before a threshold, inceasing number of layers can indeed be more powerful to learn from data, but afterwards, it will cause plenty of problems. If it interests you, feel free to search some classic problems when training neural networks:\n",
    "\n",
    "* training not converges\n",
    "* overfitting vs underfitting\n",
    "* gradient explosion / vanishing\n",
    "* bias-variance tradeoff\n",
    "* computational complexity of MLP\n",
    "\n",
    "Here we only set up 3 layers for MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    \"\"\" Multiple Layers' Perceptron\n",
    "    \n",
    "    Params\n",
    "    ----------\n",
    "    nfeat : dimension of input \n",
    "    nhid : dim of hidden neurons\n",
    "    nclass : number of classes ground truth\n",
    "    dropout : dropout probability\n",
    "    with_bias: None\n",
    "    \"\"\"\n",
    "    def __init__(self, num_feat, nclass, n_hidden, dropout=0.5, with_bias=True):\n",
    "        super(MLP, self).__init__()\n",
    "        self.num_feat = num_feat\n",
    "        self.class_label = nclass\n",
    "        self.n_hidden = n_hidden\n",
    "        self.linear1 = nn.Linear(self.num_feat, self.n_hidden)\n",
    "        self.linear2 = nn.Linear(self.n_hidden, self.n_hidden)\n",
    "        self.linear3 = nn.Linear(self.n_hidden, self.class_label)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize\"\"\"\n",
    "        self.linear1.reset_parameters()\n",
    "        self.linear2.reset_parameters()\n",
    "        self.linear3.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"Forward Your code\"\"\"\n",
    "        x = F.dropout(self.linear1(data.x), p=self.dropout, training=self.training)\n",
    "        x = F.dropout(self.linear2(x), p=self.dropout, training=self.training)\n",
    "        x = F.dropout(self.linear3(x), p=self.dropout, training=self.training)        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Proposed SOTA: GCN: Graph convolutional network\n",
    "\n",
    "Convolution neural network transfers the convolutional operation on singal processing to deep learning realm, which captures local features with convolutional kernels (i.e. filters) on different form of data (e.g. images, signals). It requires extremely less parameters than MLP but usually gives better performance, if the model fits the situation well.\n",
    "\n",
    "Graph convolutional network (GCN) is especially designed for graph-structured data, which combine the feature of graph and convolutional neural network. Here we test a GCN model with 2 GCN layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    \"\"\" Two Layers' GCN\n",
    "    \n",
    "    Params\n",
    "    ----------\n",
    "    nfeat : dimension of input \n",
    "    nhid : dim of hidden neurons\n",
    "    nclass : number of classes ground truth\n",
    "    dropout : dropout probability\n",
    "    with_bias: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nclass, nhid, dropout=0.5, with_bias=True):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(nfeat, nhid, bias=with_bias, activation=F.relu)\n",
    "        self.conv2 = GCNConv(nhid, nclass, bias=with_bias)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize\"\"\"\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, data, features=None):\n",
    "        \"\"\"Forward Your code\"\"\"\n",
    "        x, edge_index = data.x, data.edge_index \n",
    "        x = F.relu(self.conv1(x, edge_index)) \n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Train & Test\n",
    "\n",
    "Then we define the process of training and testing.\n",
    "\n",
    "Usually, training of a neural network includes:\n",
    "\n",
    "* send train data into model and get output\n",
    "* calculate loss between output and ground truth (i.e. target)\n",
    "* conduct back-propagation to get gradients\n",
    "* update parameters of model with gradients and optimizer\n",
    "* repeat the whole process for multiple epochs\n",
    "\n",
    "Since testing is to evaluate how well the trained model can already learn , we send test data into trained model on a new data (split from original dataset but not invovlved in training phase) and only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, lr=0.001, weight_decay=5e-4, epochs=200):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    labels = data.y\n",
    "    train_mask = data.train_mask\n",
    "    best_loss_val = 100\n",
    "\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # only training nodes will be used to calculate loss \n",
    "        loss = F.nll_loss(output[train_mask], labels[train_mask]) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch {}, training loss: {}'.format(i, loss.item()))\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    \"\"\"Evaluate GAT performance on test set.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_mask = data.test_mask\n",
    "    labels = data.y \n",
    "    output = model(data) \n",
    "    loss_test = F.nll_loss(output[test_mask], labels[test_mask])\n",
    "    preds = output[test_mask].argmax(1) \n",
    "    acc_test = preds.eq(labels[test_mask]).cpu().numpy().mean() \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test))\n",
    "    return preds, output, acc_test.item()\n",
    "\n",
    "num_feat = data.num_features\n",
    "n_hidden = 16\n",
    "labels = data.y  \n",
    "nclass = labels.max().item()+1\n",
    "device = 'cpu' # device ='cuda'\n",
    "train_data = train_data.to(device)\n",
    "test_data = test_data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Proposed Baseline MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 1.9597594738006592\n",
      "Epoch 10, training loss: 1.9177745580673218\n",
      "Epoch 20, training loss: 1.853309154510498\n",
      "Epoch 30, training loss: 1.7658307552337646\n",
      "Epoch 40, training loss: 1.6853845119476318\n",
      "Epoch 50, training loss: 1.6468523740768433\n",
      "Epoch 60, training loss: 1.5682454109191895\n",
      "Epoch 70, training loss: 1.5421398878097534\n",
      "Epoch 80, training loss: 1.545398473739624\n",
      "Epoch 90, training loss: 1.4439668655395508\n",
      "Test set results: loss= 1.6905 accuracy= 0.5430\n"
     ]
    }
   ],
   "source": [
    "model = MLP(num_feat, nclass, n_hidden, dropout=0.5, with_bias=True)\n",
    "model = model.to(device)\n",
    "train(model, train_data, epochs=100)\n",
    "preds, output, acc_test = test(model, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
