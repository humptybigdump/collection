{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cdb363a-9ea1-4b52-99c6-1537f22908cc",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "## Environment\n",
    "Almost the same as before.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/grid_example.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "We do not inteact with human agent anymore.\n",
    "\n",
    "Changes to the GridWorld environment:\n",
    "- The possible actions \"up\", \"down\", \"right\", \"left\" were changed to integers, to make handling simpler.\n",
    "- Observations changed from visualizing the whole environment to just the updated position of the agent.\n",
    "- <code>render()</code> function for visualization only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "116270ad-7949-4f54-a460-f27ecd83db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "legend = {\n",
    "    'empty': 0,\n",
    "    'agent': 4,\n",
    "    'blocking': 8\n",
    "}\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, world_shape, agent_init_pos, blocking_states, terminal_states, reward_states):\n",
    "        self.world_shape = world_shape\n",
    "        self.agent_init_pos = agent_init_pos\n",
    "        self.blocking_states = blocking_states\n",
    "        self.terminal_states = terminal_states\n",
    "        self.reward_states = reward_states\n",
    "        \n",
    "        # the action representations are now integers, to make indexing and sampling for TD learning simpler\n",
    "        self.possible_actions = {\n",
    "            0: np.array([-1, 0]), # up\n",
    "            1: np.array([1, 0]),  # down\n",
    "            2: np.array([0, 1]),  # right\n",
    "            3: np.array([0, -1])  # left\n",
    "        }\n",
    "        \n",
    "        # set initial agent position\n",
    "        self.agent_current_pos = self.agent_init_pos\n",
    "        # list of collected rewards, to not collect rewards twice\n",
    "        self.collected_rewards = []\n",
    "        \n",
    "    def reset(self):\n",
    "        # reset agent position\n",
    "        self.agent_current_pos = self.agent_init_pos\n",
    "        # reset list of collected rewards\n",
    "        self.collected_rewards = []\n",
    "            \n",
    "        # render initial observation\n",
    "        observation = np.copy(self.agent_current_pos)\n",
    "        return observation\n",
    "    \n",
    "    def move_agent(self, action):\n",
    "        # move agent\n",
    "        new_agent_pos = np.array(self.agent_current_pos) + self.possible_actions[action]\n",
    "\n",
    "        # check if new position is blocked\n",
    "        if tuple(new_agent_pos) in self.blocking_states:\n",
    "            return self.agent_current_pos\n",
    "\n",
    "        # check if new position is out of bounds\n",
    "        if (new_agent_pos < 0).any() or (new_agent_pos >= self.world_shape).any():\n",
    "            return self.agent_current_pos\n",
    "\n",
    "        return tuple(new_agent_pos)\n",
    "        \n",
    "    def step(self, action):\n",
    "        # execute action\n",
    "        self.agent_current_pos = self.move_agent(action)\n",
    "        \n",
    "        reward = 0.0\n",
    "        done = False\n",
    "        \n",
    "        # check if there is any reward\n",
    "        if tuple(self.agent_current_pos) in self.reward_states.keys() and tuple(self.agent_current_pos) not in self.collected_rewards:\n",
    "            reward += self.reward_states[tuple(self.agent_current_pos)]\n",
    "            self.collected_rewards.append(tuple(self.agent_current_pos))\n",
    "        \n",
    "        # check if there is any reward and whether the game ended\n",
    "        if tuple(self.agent_current_pos) in self.terminal_states:\n",
    "            done = True\n",
    "            \n",
    "        # render observation\n",
    "        observation = np.copy(self.agent_current_pos)\n",
    "        return observation, reward, done\n",
    "    \n",
    "    def render(self, show_render=True):\n",
    "        # initialize empty states\n",
    "        states = np.ones(self.world_shape) * legend['empty']\n",
    "\n",
    "        # add agent\n",
    "        states[tuple(self.agent_current_pos)] = legend['agent']\n",
    "\n",
    "        # add blocking states\n",
    "        for blocking_state in self.blocking_states:\n",
    "            states[blocking_state] = legend['blocking']\n",
    "\n",
    "        # add rewards\n",
    "        for state, reward in self.reward_states.items():\n",
    "            if state not in self.collected_rewards:\n",
    "                states[state] = reward\n",
    "        if show_render:\n",
    "            print(states)\n",
    "        return states\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5978c1-8710-4905-8b26-c8a8ec70948a",
   "metadata": {},
   "source": [
    "## Agent\n",
    "### Step 1 (TODO): Tabular Q-learning\n",
    "We have a discrete state space (bounded integer coordinates) and a discrete action space (up, down, right, left). In such cases, we can represent the approximated **q-value function** $Q$ by a **table** $q$ with dimensions $[\\text{number of actions}]\\times[\\text{number of states}]$. Since we have a 2D state space in this environment, we will represent the q-value function with a 3D **array**. In our case, the dimensions are $[\\text{number of actions}]\\times[\\text{number of rows}]\\times[\\text{number of columns}] = 4\\times3\\times4$.\n",
    "\n",
    "To retrieve an action value given a state $s_t =(r_t, c_t)$ – with $r_t$ and $c_t$ the corresponding row and column of the state – and action $a_t$, we just retrieve it from the array at the indices $[a_t, r_t, c_t]$\n",
    "$$\n",
    "Q(s_t, a_t) = q[a_t, r_t, c_t]\n",
    "$$\n",
    "The policy derived from the approximated q-value function is then\n",
    "$$\n",
    "\\pi(s_t) = argmax_a(Q(s_t, a)) = argmax_a(q[a, r_t, c_t])\n",
    "$$\n",
    "and the value for the given state\n",
    "$$\n",
    "v(s_t) = max_a(Q(s_t, a)) = max_a(q[a, r_t, c_t])\n",
    "$$\n",
    "During TD-learning, we update the array with the new values based on the received reward $R$.\n",
    "$$\n",
    "Q'(s_t, a_t) = R(s_t, a_t) + \\gamma v(s_{t+1})\n",
    "$$\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow (1-\\alpha) Q(s_t, a_t)+ \\alpha Q'(s_t, a_t) \n",
    "$$\n",
    "As we can see, we need the current state $s_t$ (at timestep $t$), the action taken $a_t$, the reward received $R(s_t, a_t)$ and the next state $s_{t+1}$ (at timestep $t+1$) resulting from $a_t$ to update the q-value approximation. $\\alpha$ is the agent's learining rate and $\\gamma$ is the discount factor from Bellman's equation.\n",
    "\n",
    "While learning we enable exploration, that yields a random action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b04922f-8d51-4e93-9142-2d99c4f248b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    def __init__(self, n_actions, world_shape, learning_rate=0.2, discount_factor=0.9):\n",
    "        # TODO set class variables: n_actions, learning_rate and discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # TODO initialize q-table with zeros of shape [n_actions, n_rows, n_columns]\n",
    "        self.q_table = np.zeros([n_actions, *world_shape])\n",
    "\n",
    "    def act(self, observation, explore=True):\n",
    "        if explore:\n",
    "            # TODO sample random action during training e.g. a random integer in [0, n_actions - 1]\n",
    "            action = np.random.randint(self.n_actions - 1)\n",
    "        else:\n",
    "            # TODO get the q-values from the q-table at the given position (observation is the same as the agent's position)\n",
    "            # hint: use numpy array slicing, with the \":\" operator\n",
    "            q_values = self.q_table[:, observation[0], observation[1]]\n",
    "            # TODO find the action with the maximal q-value\n",
    "            action = np.argmax(q_values)\n",
    "        return action\n",
    "\n",
    "    def learn(self, obs_0, action_0, reward_0, obs_1):\n",
    "        # TODO retrieve the current q-value approximation for obs_0 and action_0 from the q-table\n",
    "        q_value = self.q_table[action_0, obs_0[0], obs_0[1]]\n",
    "        \n",
    "        # TODO compute the value of the next state obs_1\n",
    "        next_value = np.max(self.q_table[:, obs_1[0], obs_1[1]])\n",
    "        \n",
    "        # TODO compute new estimation of q-value for obs_0 and action_0\n",
    "        new_q_value = reward_0 + self.discount_factor * next_value\n",
    "        \n",
    "        # TODO adjust old estimation using the learning rate\n",
    "        adapted_q_value = (1 - self.learning_rate) * q_value + self.learning_rate * new_q_value \n",
    "        # TODO set the new q-value approximation in the q-table\n",
    "        self.q_table[action_0, obs_0[0], obs_0[1]] = adapted_q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be2e40-c9e6-41bf-b928-3b12a5d9a3cd",
   "metadata": {},
   "source": [
    "### Step 2 (TODO): Training\n",
    "The training method is similar to the interaction between the human player and the environment from the previous excersize. We specify a number of training steps, for wich the agent colletcts experience and updates its q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f604465e-6a3a-4716-aa7c-22fc8897ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, steps):\n",
    "    # TODO reset environment and get initial observation\n",
    "    obs = env.reset()\n",
    "    for i in range(steps):\n",
    "        # TODO get action from agent, given the observation\n",
    "        action = agent.act(obs)\n",
    "        # TODO execute action, get reward, new observation and termination flag\n",
    "        new_obs, reward, done = env.step(action)\n",
    "        # TODO learn from the gathered experience\n",
    "        agent.learn(obs, action, reward, new_obs)\n",
    "        # TODO set obs to new observation obs_1\n",
    "        obs = new_obs\n",
    "        # TODO reset environment if game terminated\n",
    "        if done:\n",
    "            obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98405c02",
   "metadata": {},
   "source": [
    "To run the training, we first need an environment. Here we define the environment's properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1180df46-ede7-42b6-a854-c0aef60cc0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of the GridWorld\n",
    "world_shape = (3, 4)\n",
    "\n",
    "# initial position of the agent\n",
    "agent_init_pos = (2, 0)\n",
    "\n",
    "# list of blocking state positions\n",
    "blocking_states = [(1, 1)]\n",
    "\n",
    "# list of terminal state positions\n",
    "terminal_states = [(0,3), (1,3)]\n",
    "\n",
    "# dictionary of rewards with key: position and value: reward\n",
    "reward_states = {\n",
    "    (0,3): 1,\n",
    "    (1,3): -1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478e5875",
   "metadata": {},
   "source": [
    "Now, we initialze the environment and the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e5ead09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  8.  0. -1.]\n",
      " [ 4.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# TODO initialize environment\n",
    "env = GridWorld(world_shape, agent_init_pos, blocking_states, terminal_states, reward_states)\n",
    "# TODO render initial setup\n",
    "env.render()\n",
    "\n",
    "# TODO initialize agent; use len(env.possible_actions) to get the number of actions from the environment\n",
    "agent = QAgent(len(env.possible_actions), world_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a64b20d",
   "metadata": {},
   "source": [
    "We train the agent and print the learned values ans the policy (argmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd9ddea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "values\n",
      "[[0.81   0.9    1.     0.    ]\n",
      " [0.729  0.     0.9    0.    ]\n",
      " [0.6561 0.729  0.81   0.    ]]\n",
      "\n",
      "argmax q\n",
      "[[2 2 2 0]\n",
      " [0 0 0 0]\n",
      " [0 2 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# TODO train agent for 10000 steps\n",
    "train(agent, env, 20000)\n",
    "\n",
    "# show values\n",
    "print()\n",
    "print('values')\n",
    "print(agent.q_table.max(axis=0))\n",
    "\n",
    "# show actions with highest q-value\n",
    "print()\n",
    "print('argmax q')\n",
    "print(agent.q_table.argmax(axis=0))\n",
    "\n",
    "# Reminder: integer-action\n",
    "# 0: up\n",
    "# 1: down\n",
    "# 2: right\n",
    "# 3: left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bde15d-535d-4611-a89f-d5598a08c167",
   "metadata": {},
   "source": [
    "### Step 3 (TODO): Test\n",
    "We implement a method to test the agent. This is similar to the training loop, except the agent does not collect experience and does not learn from it. While testing, we only want to play a single game, and compute the cumulative reward for that game collected by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d013e63-27c5-41ab-8767-4d420668191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent, env, max_steps=30):\n",
    "    # TODO reset environment and get initial observation\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # TODO record path of agent in a list of positions; the fist position is the initial observation\n",
    "    path = [obs]\n",
    "    \n",
    "    # initialize the cumulated reward as 0\n",
    "    cumulated_reward = 0.0\n",
    "    \n",
    "    # we want to execute only one episode --> until the game is done or until a maximum nuber of steps is reached\n",
    "    done = False\n",
    "    n_steps = 0\n",
    "    while not done and n_steps < max_steps:\n",
    "        # TODO get action from agent, don't forget to set the explore flag to False\n",
    "        action = agent.act(obs, explore=False)\n",
    "        # TODO execute action, get reward and new observation\n",
    "        new_obs, reward, done = env.step(action)\n",
    "        # TODO record path of agent\n",
    "        path.append(new_obs)\n",
    "        # TODO increment cumulated reward by received reward\n",
    "        cumulated_reward += reward\n",
    "        # TODO set obs to new observation obs_1\n",
    "        obs = new_obs\n",
    "        n_steps += 1\n",
    "    return cumulated_reward, path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbfd7ab",
   "metadata": {},
   "source": [
    "Test the agent and print the cumlative reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d49655ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulated reward 0.0\n"
     ]
    }
   ],
   "source": [
    "cumulated_reward, path = test(agent, env)\n",
    "print('cumulated reward', cumulated_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b08f0ec",
   "metadata": {},
   "source": [
    "We also want to show the path, the agent has taken. For this we implement a render method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1099f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_path(env, path):\n",
    "    states = env.render(show_render=False)\n",
    "    for position in path:\n",
    "        states[tuple(position)] = 3\n",
    "    print(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d34ce",
   "metadata": {},
   "source": [
    "The rendered path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e9ece54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  0.  0.  8.  0.  0.  0.  1.]\n",
      " [ 3.  0.  0.  8.  0.  0.  0. -1.]\n",
      " [ 3.  0.  0.  8.  0.  0.  0.  0.]\n",
      " [ 3.  0.  0.  8.  0.  0.  0.  0.]\n",
      " [ 3.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 3.  0.  0.  8.  0.  0.  0.  0.]\n",
      " [ 3.  0.  0.  8.  0.  0.  0.  0.]\n",
      " [ 3.  0.  0.  8.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "render_path(env, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3083111-500c-43f4-8212-cce61584d24a",
   "metadata": {},
   "source": [
    "### Step 4: Let's scale it up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bed690",
   "metadata": {},
   "source": [
    "Now we want to test whether the agent performs in a larger GridWorld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2277b27f-2832-469f-b26b-8e860426bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of the GridWorld\n",
    "world_shape = (8, 8)\n",
    "\n",
    "# initial position of the agent\n",
    "agent_init_pos = (7, 0)\n",
    "\n",
    "# list of blocking state positions\n",
    "blocking_states = [(0, 3),\n",
    "                   (1, 3),\n",
    "                   (2, 3),\n",
    "                   (3, 3),\n",
    "                   (5, 3),\n",
    "                   (6, 3),\n",
    "                   (7, 3)]\n",
    "\n",
    "# list of terminal state positions\n",
    "terminal_states = [(0,7), (1,7)]\n",
    "\n",
    "# dictionary of rewards with key: position and value: reward\n",
    "reward_states = {\n",
    "    (0,7): 1,\n",
    "    (1,7): -1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f84d151",
   "metadata": {},
   "source": [
    "Like before, we initialze the environment and the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aebbc752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  8.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  8.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  8.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  8.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  8.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  8.  0.  0.  0.  0.]\n",
      " [ 4.  0.  0.  8.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(world_shape, agent_init_pos, blocking_states, terminal_states, reward_states)\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "agent = QAgent(len(env.possible_actions), world_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e428cf",
   "metadata": {},
   "source": [
    "We train the agent and print the learned policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cac8f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argmax q\n",
      "[[0 2 1 0 2 2 2 0]\n",
      " [0 0 1 0 1 0 0 0]\n",
      " [2 2 1 0 2 2 0 1]\n",
      " [2 2 1 0 2 2 0 0]\n",
      " [2 2 2 2 2 0 0 0]\n",
      " [2 2 0 0 2 0 0 0]\n",
      " [2 2 0 0 2 0 0 0]\n",
      " [2 2 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "train(agent, env, 15000)\n",
    "\n",
    "print('argmax q')\n",
    "print(agent.q_table.argmax(axis=0))\n",
    "\n",
    "# Reminder: integer-action\n",
    "# 0: up\n",
    "# 1: down\n",
    "# 2: right\n",
    "# 3: left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8653066",
   "metadata": {},
   "source": [
    "We test the agent and show the path it has taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9d2f781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulated reward 1.0\n",
      "[[ 0.  0.  0.  8.  0.  0.  3.  3.]\n",
      " [ 0.  0.  0.  8.  0.  0.  3. -1.]\n",
      " [ 0.  0.  0.  8.  0.  0.  3.  0.]\n",
      " [ 0.  0.  0.  8.  0.  3.  3.  0.]\n",
      " [ 0.  0.  3.  3.  3.  3.  0.  0.]\n",
      " [ 0.  0.  3.  8.  0.  0.  0.  0.]\n",
      " [ 0.  0.  3.  8.  0.  0.  0.  0.]\n",
      " [ 3.  3.  3.  8.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "cumulated_reward, path = test(agent, env)\n",
    "print('cumulated reward', cumulated_reward)\n",
    "render_path(env, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a51446-c3b0-4747-801c-334b2083dedb",
   "metadata": {},
   "source": [
    "### Step 5 (TODO): Epsilon-greedy policy\n",
    "In our Agent implementation, we sample random actions during training to ensure explocation. An alternative would be the **epsilon-greedy** policy, that samples random actions only with epsilon probability, and acts according to the already partially learned policy otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88a9bf26-c2fb-42c5-9a42-ad5ae6aeed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We inherit from the QAgent class\n",
    "class EGreedyQAgent(QAgent):\n",
    "    # We override the constructor of QAgent. *args is just an unpacked list of arguments \n",
    "    def __init__(self, epsilon, n_actions, world_shape, learning_rate=0.2, discount_factor=0.9):\n",
    "        # We call the constructor of the superclass QAgent, with the unpacked list of arguments\n",
    "        super(EGreedyQAgent, self).__init__(n_actions, world_shape, learning_rate=0.2, discount_factor=0.9)\n",
    "        # TODO set new class variable epsilon\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # We only override the act method of the superclass to implement the epsilon-greedy policy\n",
    "    def act(self, observation, explore=True):\n",
    "        # TODO get the q-values from the q-table at the given position (observation is the same as the agent's position)\n",
    "        # hint: you have access to the class variables of the superclass, e.g. self.q_table\n",
    "        q_values = self.q_table[:, observation[0], observation[1]]\n",
    "        if explore:\n",
    "            # We sample a random action with epsilon probability and act according to the learned policy otherwise\n",
    "            if np.random.uniform(0, 1) < self.epsilon:\n",
    "                # TODO explore the action space sample random action during training e.g. a random integer in [0, n_actions - 1]\n",
    "                # hint: you have access to the class variables of the superclass, e.g. self.n_actions\n",
    "                action = np.random.randint(self.n_actions - 1)\n",
    "            else:\n",
    "                # TODO get the q-values from the q-table at the given position (observation is the same as the agent's position)\n",
    "                # hint: use numpy array slicing, with the \":\" operator\n",
    "                q_values = self.q_table[:, observation[0], observation[1]]\n",
    "                # TODO find the action with the maximal q-value\n",
    "                action = np.argmax(q_values)\n",
    "        else:\n",
    "            # TODO find the action with the maximal q-value\n",
    "            q_values = self.q_table[:, observation[0], observation[1]]\n",
    "            action = np.argmax(q_values)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a406eb",
   "metadata": {},
   "source": [
    "We now want to test the new agent. We reset the environment and initialiye the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a268f0f5-c230-485e-8c2e-350aeae072fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "agent = EGreedyQAgent(0.9, len(env.possible_actions), world_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93c6f7",
   "metadata": {},
   "source": [
    "Like before, we train the agent and print the learned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "869c7186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argmax q\n",
      "[[0 0 1 0 2 2 2 0]\n",
      " [0 0 1 0 2 2 0 0]\n",
      " [0 0 1 0 0 0 0 1]\n",
      " [0 1 1 0 2 2 0 0]\n",
      " [0 1 2 2 2 0 0 0]\n",
      " [2 2 0 0 0 0 0 0]\n",
      " [2 2 0 0 0 0 0 0]\n",
      " [2 2 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "train(agent, env, 5000)\n",
    "        \n",
    "print('argmax q')\n",
    "print(agent.q_table.argmax(axis=0))\n",
    "\n",
    "# Reminder: integer-action\n",
    "# 0: up\n",
    "# 1: down\n",
    "# 2: right\n",
    "# 3: left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f68eb",
   "metadata": {},
   "source": [
    "After training, the agent is tested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5509a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulated reward 1.0\n",
      "[[ 0.  0.  0.  8.  0.  0.  3.  3.]\n",
      " [ 0.  0.  0.  8.  0.  0.  3. -1.]\n",
      " [ 0.  0.  0.  8.  0.  0.  3.  0.]\n",
      " [ 0.  0.  0.  8.  0.  3.  3.  0.]\n",
      " [ 0.  0.  3.  3.  3.  3.  0.  0.]\n",
      " [ 0.  0.  3.  8.  0.  0.  0.  0.]\n",
      " [ 0.  0.  3.  8.  0.  0.  0.  0.]\n",
      " [ 3.  3.  3.  8.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "cumulated_reward, path = test(agent, env)\n",
    "print('cumulated reward', cumulated_reward)\n",
    "render_path(env, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87831cf",
   "metadata": {},
   "source": [
    "### Step 6: Comparison: random vs. epsilon-greedy\n",
    "Now we want tu compare the random agend and the epsilon-greedy agent. For this, we train multiple agents for the same amount of steps, and test them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "233a97ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "n_runs = 50\n",
    "n_steps = 5000\n",
    "\n",
    "# Initialize environment\n",
    "env = GridWorld(world_shape, agent_init_pos, blocking_states, terminal_states, reward_states)\n",
    "\n",
    "cumulated_rewards_random = []\n",
    "cumulated_rewards_epsilon_greedy = []\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    # Initialize, train and test QAgent\n",
    "    random_agent = QAgent(len(env.possible_actions), world_shape)\n",
    "    train(random_agent, env, n_steps)\n",
    "    cumulated_reward, path = test(random_agent, env)\n",
    "    # Store the cumulative reward\n",
    "    cumulated_rewards_random.append(cumulated_reward)\n",
    "    \n",
    "    # Initialize, train and test EGreedyQAgent\n",
    "    epsilon_greedy_agent = EGreedyQAgent(0.99, len(env.possible_actions), world_shape)\n",
    "    train(epsilon_greedy_agent, env, n_steps)\n",
    "    cumulated_reward, path = test(epsilon_greedy_agent, env)\n",
    "    # Store the cumulative reward\n",
    "    cumulated_rewards_epsilon_greedy.append(cumulated_reward)\n",
    "\n",
    "# Print results\n",
    "print(sum(cumulated_rewards_random) / n_runs)\n",
    "print(sum(cumulated_rewards_epsilon_greedy) / n_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302af3c7",
   "metadata": {},
   "source": [
    "In this case, the epsilon-greedy policy does not necessarily outperform the random policy. Why? (Hint: initial q-values are zeros)\n",
    "\n",
    "What happens if we change the initial q-values to random values instead of zeros?\n",
    "\n",
    "To implement this, we simply inherit from our agents and overwrite the initial q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43d0e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgentRandomInit(QAgent):\n",
    "    def __init__(self, n_actions, world_shape, learning_rate=0.2, discount_factor=0.9):\n",
    "        # We call the constructor of the superclass QAgent, with the unpacked list of arguments\n",
    "        super(QAgentRandomInit, self).__init__(n_actions, world_shape, learning_rate=0.2, discount_factor=0.9)\n",
    "        # We overwrite the q_table with random values\n",
    "        self.q_table = np.random.rand(n_actions, *world_shape)\n",
    "        \n",
    "class EGreedyQAgentRandomInit(EGreedyQAgent):\n",
    "    def __init__(self, epsilon, n_actions, world_shape, learning_rate=0.2, discount_factor=0.9):\n",
    "        # We call the constructor of the superclass EGreedyQAgent, with the unpacked list of arguments\n",
    "        super(EGreedyQAgentRandomInit, self).__init__(epsilon, n_actions, world_shape, learning_rate=0.2, discount_factor=0.9)\n",
    "        # We overwrite the q_table with random values\n",
    "        self.q_table = np.random.rand(n_actions, *world_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e7e309",
   "metadata": {},
   "source": [
    "Now we compare both models the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f271e56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "n_runs = 50\n",
    "n_steps = 25000\n",
    "\n",
    "# Initialize environment\n",
    "env = GridWorld(world_shape, agent_init_pos, blocking_states, terminal_states, reward_states)\n",
    "\n",
    "cumulated_rewards_random = []\n",
    "cumulated_rewards_epsilon_greedy = []\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    # Initialize, train and test QAgentRandomInit\n",
    "    random_agent = QAgentRandomInit(len(env.possible_actions), world_shape)\n",
    "    train(random_agent, env, n_steps)\n",
    "    cumulated_reward, path = test(random_agent, env)\n",
    "    # Store the cumulative reward\n",
    "    cumulated_rewards_random.append(cumulated_reward)\n",
    "    \n",
    "    # Initialize, train and test EGreedyQAgentRandomInit\n",
    "    epsilon_greedy_agent = EGreedyQAgentRandomInit(0.99, len(env.possible_actions), world_shape)\n",
    "    train(epsilon_greedy_agent, env, n_steps)\n",
    "    cumulated_reward, path = test(epsilon_greedy_agent, env)\n",
    "    # Store the cumulative reward\n",
    "    cumulated_rewards_epsilon_greedy.append(cumulated_reward)\n",
    "    \n",
    "print(sum(cumulated_rewards_random) / n_runs)\n",
    "print(sum(cumulated_rewards_epsilon_greedy) / n_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d873507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
