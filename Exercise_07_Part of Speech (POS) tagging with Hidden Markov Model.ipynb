{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78cf33d7",
   "metadata": {},
   "source": [
    "## What is Part of Speech (POS) tagging?\n",
    "\n",
    "Back in elementary school, we have learned the differences between the various parts of speech tags such as nouns, verbs, adjectives, and adverbs. Associating each word in a sentence with a proper POS (part of speech) is known as POS tagging or POS annotation. POS tags are also known as word classes, morphological classes, or lexical tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc37b141",
   "metadata": {},
   "source": [
    "### Stochastic (Probabilistic) tagging: \n",
    "A stochastic approach includes frequency, probability or statistics. The simplest stochastic approach finds out the most frequently used tag for a specific word in the annotated training data and uses this information to tag that word in the unannotated text. But sometimes this approach comes up with sequences of tags for sentences that are not acceptable according to the grammar rules of a language. One such approach is to calculate the probabilities of various tag sequences that are possible for a sentence and assign the POS tags from the sequence with the highest probability. Hidden Markov Models (HMMs) are probabilistic approaches to assign a POS Tag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f8f7f0",
   "metadata": {},
   "source": [
    "### POS tagging with Hidden Markov Model\n",
    "HMM (Hidden Markov Model) is a Stochastic technique for POS tagging. Hidden Markov models are known for their applications to reinforcement learning and temporal pattern recognition such as speech, handwriting, gesture recognition, musical score following, partial discharges, and bioinformatics.\n",
    "\n",
    "Let us consider an example proposed by Dr.Luis Serrano and find out how HMM selects an appropriate tag sequence for a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38afeb00",
   "metadata": {},
   "source": [
    "<img src=\"https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/04/16134154/pos2.png\" style=\"width:600px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa7967b",
   "metadata": {},
   "source": [
    "In this example, we consider only 3 POS tags that are **noun, model and verb**. Let the sentence “ Ted will spot Will ” be tagged as noun, model, verb and a noun and to **calculate the probability associated with this particular sequence of tags** we require their **Transition probability** and **Emission probability**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a8f537",
   "metadata": {},
   "source": [
    "### Hidden Markov Model (HMM):\n",
    "HMM has no input, and the probability distribution for the output should be given. An HMM is a five-tuple $\\lambda = \\{S,V,A,B,\\Pi\\} $\n",
    "\n",
    "with:\n",
    "\n",
    "* **State Vector:** $S= \\{0,\\ldots,N\\}$\n",
    "* **Output Vector:** $O= \\{0,\\ldots,M\\}$\n",
    "* **Matrix of transition probabilities:** $ A = (a_{ij}) $,  where  $ a_{ij} $ is the probability $s_j $ comes after $s_i $\n",
    "* **Matrix of emission probabilities:** $ B $, where $b_i(k)$ is the probability to observe $v_k$ in the state $ s_i $\n",
    "* **Initial state distribution:** $ \\Pi $. where $ \\pi_i $ is the probability that $ s_i $  is the intial  state\n",
    "\n",
    "\n",
    "<img src=\"https://imgur.com/MiIxToo.png\" width=\"520\" height=\"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f362c",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Let us calculate the above two probabilities for the set of sentences below\n",
    "\n",
    "* Mary Jane can see Will\n",
    "* Spot will see Mary\n",
    "* Will Jane spot Mary?\n",
    "* Mary will pat Spot\n",
    "\n",
    "<img src=\"https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/04/17112900/pos3-1.png\" style=\"width:600px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755cb322",
   "metadata": {},
   "source": [
    "### Emission probabilities\n",
    "\n",
    "Now, what is the probability that the word Ted is a noun, will is a model, spot is a verb and Will is a noun. These sets of probabilities are **Emission probabilities** and should be high for our tagging to be likely.\n",
    "\n",
    "\n",
    "In the above sentences, the word Mary appears four times as a noun. To calculate the emission probabilities, let us create a counting table in a similar manner.\n",
    "\n",
    "| Words | Noun | Model  | Verb |\n",
    "|-------|------|--------|------|\n",
    "| Mary  | 4    | 0      | 0    |\n",
    "| Jane  | 2    | 0      | 0    |\n",
    "| Will  | 1    | 3      | 0    |\n",
    "| Spot  | 2    | 0      | 1    |\n",
    "| Can   | 0    | 1      | 0    |\n",
    "| See   | 0    | 0      | 2    |\n",
    "| pat   | 0    | 0      | 1    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6831bf32",
   "metadata": {},
   "source": [
    "Now let us divide each column by the total number of their appearances for example, ‘noun’ appears nine times in the above sentences so divide each term by 9 in the noun column. We get the following table after this operation. \n",
    "\n",
    "| Words | Noun | Model  | Verb |\n",
    "|-------|------|--------|------|\n",
    "| Mary  | 4/9  | 0      | 0    |\n",
    "| Jane  | 2/9  | 0      | 0    |\n",
    "| Will  | 1/9  | 3/4    | 0    |\n",
    "| Spot  | 2/9  | 0      | 1/4  |\n",
    "| Can   | 0    | 1/4    | 0    |\n",
    "| See   | 0    | 0      | 2/4  |\n",
    "| pat   | 0    | 0      | 1/4  |\n",
    "\n",
    "\n",
    "From the above table, we infer that\n",
    "* The probability that Will  is Noun = 1/9\n",
    "* The probability that Will is Model = 3/4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc14243b",
   "metadata": {},
   "source": [
    "### Transition Probability\n",
    "\n",
    "The **transition probability** is the likelihood of a particular sequence for example, how likely is that a noun is followed by a model and a model by a verb and a verb by a noun. This probability is known as Transition probability. It should be high for a particular sequence to be correct.\n",
    "\n",
    "Next, we have to calculate the transition probabilities, so define two more tags $<S>$ and $<E>$. \n",
    "* $<S>$ is placed at the beginning of each sentence and \n",
    "* $<E>$ at the end as shown in the figure below.\n",
    "\n",
    "\n",
    "<img src=\"https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/04/16134911/pos4.png\" style=\"width:600px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c2bf30",
   "metadata": {},
   "source": [
    "Next, we divide each term in a row of the table by the total number of co-occurrences of the tag in consideration, for example, The Model tag is followed by any other tag four times as shown below, thus we divide each element in the third row by four.\n",
    "\n",
    "|     | N   | M   | V   | \\<E> |\n",
    "|-----|-----|-----|-----|-----|\n",
    "|  \\<S> | 3/4 | 1/4 | 0   | 0   |\n",
    "| N   | 1/9 | 3/9 | 1/9 | 4/9 |\n",
    "| M   | 1/4 | 0   | 3/4 | 0   |\n",
    "| V   | 4/4 | 0   | 0   | 0   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e5cb71",
   "metadata": {},
   "source": [
    "Three fundamental problems should characterize the hidden Markov models:\n",
    "\n",
    "* **Problem 1 (Likelihood):** Given an HMM λ = (A, B) and an observation sequence O, determine the likelihood $P(O|λ )$.\n",
    "* **Problem 2 (Decoding):** Given an observation sequence O and an HMM λ = (A, B), discover the best-hidden state sequence X.\n",
    "* **Problem 3 (Learning):** Given an observation sequence O and the set of states in the HMM, learn the HMM parameters A and B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc83fc",
   "metadata": {},
   "source": [
    "Now how does the HMM determine the appropriate sequence of tags for a particular sentence from the above tables?\n",
    "\n",
    "Take a new sentence and tag them with wrong tags. Let the sentence, ‘ Will can spot Mary’.One possilbe tagging would                     mbe\n",
    "\n",
    "\n",
    "<img src=\"https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/04/16135107/pos6-1.png\" style=\"width:600px;height:200px;\">\n",
    "\n",
    "To go from $\\pi$ to $O$ you need to multiply the corresponding transition probabilities $(1/4)$ and the corresponding emission probability $(3/4)$. You keep doing that for all the words, until you get the probability of an entire sequence. \n",
    "\n",
    "Now calculate the probability of the sequence of hidden state in the following manner:\n",
    "$$P(S|O,λ )= 1/4*3/4*3/4*0*1*2/9*1/9*4/9*4/9=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4141e624",
   "metadata": {},
   "source": [
    "Other possible words tagging as shown below\n",
    "\n",
    "<img src=\"https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/04/16135135/pos7.png\" style=\"width:600px;height:200px;\">\n",
    "\n",
    "\n",
    "Calculating  the product of these terms we get,\n",
    "$$P(S|O,λ )= 3/4*1/9*3/9*1/4*3/4*1/4*1*4/9*4/9=0.00025720164$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f95fb",
   "metadata": {},
   "source": [
    "keeping into consideration just three POS tags we have mentioned, but 81 different combinations of tags can be formed. Now let us visualize these 81 combinations as paths and using the transition and emission probability mark each vertex and edge as shown below.\n",
    "\n",
    "<img src=\"https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/04/16135201/pos8.png\" style=\"width:600px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123e74e",
   "metadata": {},
   "source": [
    "The next step is to delete all the vertices and edges with probability zero, also the vertices which do not lead to the endpoint are removed. Also, we will mention-\n",
    "\n",
    "<img src=\"https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/04/16135225/pos9.png\" style=\"width:600px;height:200px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c97be92",
   "metadata": {},
   "source": [
    "$$<S>→N→M→N→N→<E> =3/4*1/9*3/9*1/4*1/4*2/9*1/9*4/9*4/9=0.00000846754$$\n",
    "\n",
    "$$<S>→N→M→N→V→<E>=3/4*1/9*3/9*1/4*3/4*1/4*1*4/9*4/9=0.00025720164$$\n",
    "\n",
    "Clearly, the probability of the second sequence is much higher and hence the HMM is going to tag each word in the sentence according to this sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7f0068",
   "metadata": {},
   "source": [
    "## Optimizing HMM with Viterbi Algorithm \n",
    "\n",
    "\"The Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states—called the Viterbi path—that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad48c47a",
   "metadata": {},
   "source": [
    "Let us use the same example we used before and apply the Viterbi algorithm to it\n",
    "\n",
    "<img src=\"https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/04/16135201/pos8.png\" style=\"width:600px;height:200px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a88505",
   "metadata": {},
   "source": [
    "## Viterbi Initialization\n",
    "\n",
    "You will now populate a matrix C of dimension (num_tags, num_words). This matrix will have the probabilities that will tell you what part of speech each word belongs to. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c50a5d",
   "metadata": {},
   "source": [
    "Now to populate the first column, you just multiply the initial $\\pi$ distribution, for each tag, times $b_{i, \\operatorname{cindex}\\left(w_{1}\\right)}$. Where the $i$, corresponds to the tag of the initial distribution and the $cindex(w_1)$ is the index of word 1 in the emission matrix.\n",
    "\n",
    "|   | w_1     | w_2 | ... | w_k |\n",
    "|---|---------|-----|-----|-----|\n",
    "| N | c_(N,1) |     |     |     |\n",
    "| M | c_(M,1) |     |     |     |\n",
    "| V | c_(V,1) |     |     |     |\n",
    "\n",
    "\n",
    "<img src=\"https://imgur.com/i05hwKj.png\" width=\"220\" height=\"300\" />\n",
    "<img src=\"https://imgur.com/qrEJYxO.png\" width=\"720\" height=\"400\" />\n",
    "\n",
    "$$c_{(N,1)} = 1/4 * 3/4 = 0.1875$$\n",
    "$$c_{(M,1)} = 3/4 * 1/9 = 0.08333$$\n",
    "$$c_{(V,1)} = 0  *  0   = 0 $$\n",
    "\n",
    "And that's it, you are done with populating the first column of your new $C$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaa233c",
   "metadata": {},
   "source": [
    "You will now need to keep track what part of speech you are coming from. Hence we introduce a matrix $D$, which allows you to store the labels that represent the different states you are going through when finding the most likely sequence of POS tags for the given sequence of words $ w_1,... ,w_{K_w} $ \n",
    "\n",
    "At first you set the first column to $0$, because you are not coming from any POS tag. \n",
    "\n",
    "|   | w_1     | w_2 | ... | w_k |\n",
    "|---|---------|-----|-----|-----|\n",
    "| N | d_(N,1) = 0 |     |     |     |\n",
    "| M | d_(M,1) = 0 |     |     |     |\n",
    "| V | d_(V,1) = 0 |     |     |     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf558e7c",
   "metadata": {},
   "source": [
    "## Viterbi: Forward Pass\n",
    "\n",
    "\n",
    "So to populate a cell (i.e. 1,2) in the image above, you have to take the max of (kth cells in the previous column, times the corresponding transition probability of the kth POS to the first POS times the emission probability of the first POS and the current word you are looking at). You do that for all the cells.\n",
    "\n",
    "<img src=\"https://imgur.com/MYqpamU.png\" width=\"280\" height=\"300\" />\n",
    "<img src=\"https://imgur.com/WllgNdh.png\" width=\"720\" height=\"400\" />\n",
    "\n",
    "$$c_{(N,2)} = \\max [ c_{(N,1)} *  a_{(N,N)} * b_{N,\\operatorname{cindex}\\left(w_{2}\\right)}\\\\,c_{(M,1)} *  a_{(N,M)} * b_{N,\\operatorname{cindex}\\left(w_{2}\\right)}\\\\,c_{(V,1)} *  a_{(N,V)} * b_{N,\\operatorname{cindex}\\left(w_{2}\\right)}] \\\\= \\max [0.1875*1/9*0, 0.08333* 1/4* 0 , 0] = 0 $$\n",
    "\n",
    "\n",
    "|   | w_1     | w_2 | ... | w_k |\n",
    "|---|---------|-----|-----|-----|\n",
    "| N | c_(N,1) | c_(N,2)   |     |     |\n",
    "| M | c_(M,1) | c_(M,2)   |     |     |\n",
    "| V | c_(V,1) | c_(V,2)   |     |     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60155d01",
   "metadata": {},
   "source": [
    "Now to populate the D matrix, you will keep track of the argmax of where you came from as follows: \n",
    "<img src=\"https://imgur.com/EGzZS3K.png\" width=\"340\" height=\"300\" />\n",
    "\n",
    "|   | w_1     | w_2 | ... | w_k |\n",
    "|---|---------|-----|-----|-----|\n",
    "| N | d_(N,1) = 0 | d_(N,2) = {} |     |     |\n",
    "| M | d_(M,1) = 0 | d_(M,2) = N  |     |     |\n",
    "| V | d_(V,1) = 0 | d_(V,2) = {}   |     |     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c6e3d",
   "metadata": {},
   "source": [
    "There are two paths leading to this vertex as shown below along with the probabilities of the two mini-paths\n",
    "<img src=\"https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/04/16135641/pos11-3.png\" style=\"width:600px;height:200px;\">\n",
    "\n",
    "<img src=\"https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/04/16135742/pos1-5.png\" style=\"width:600px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cea3589",
   "metadata": {},
   "source": [
    "## Viterbi: Backward Pass\n",
    "\n",
    "Great, now that you know how to compute A, B, C, and D, we will put it all together and show you how to construct the path that will give you the part of speech tags for your sentence. \n",
    "\n",
    "Let us first consider a general example:\n",
    "\n",
    "<img src=\"https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/GbakvlBQRk-2pL5QUNZPQg_dfd81697a97845e6809fb91cb80e2038_Screen-Shot-2021-03-10-at-3.34.53-PM.png?expiry=1643760000000&hmac=aulIWYQkywdtz0Tf-Wh3X7IPaN8gvsNHcsRvPfT_uSc\" style=\"width:600px;height:200px;\">\n",
    "\n",
    "The equation above just gives you the index of the highest row in the last column of C. Once you have that, you can go ahead and start using your D matrix as follows: \n",
    "\n",
    "<img src=\"https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/eKIMiBJVQZ6iDIgSVVGeRQ_eca8f4f04a464168ace61d69f63bc29c_Screen-Shot-2021-03-10-at-3.36.07-PM.png?expiry=1643760000000&hmac=RQ6R2O677zV2NFAt5aqFPQmon7vQ_nCjQ8rjM1OBt4s\" style=\"width:600px;height:200px;\">\n",
    "\n",
    "\n",
    "Note that since we started at index one, hence the last word $(w_5)$. Then we go to the first row of $D$ and what ever that number is, it indicated the row of the next part of speech tag. Then next part of speech tag indicates the row of the next and so forth. This allows you to reconstruct the POS tags for your sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0352a6a7",
   "metadata": {},
   "source": [
    "### Back to our example\n",
    "The Forward Pass:\n",
    "<img src=\"https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/04/16135838/pos13.png\" style=\"width:600px;height:200px;\">\n",
    "\n",
    "The Backward Pass:\n",
    "<img src=\"https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/04/16135907/pos14.png\" style=\"width:600px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462bec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install the library\n",
    "!pip install hmmlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f786065b",
   "metadata": {},
   "source": [
    "### Import Libraries:\n",
    "first we will import all the packages that are required for this exercise. \n",
    "- [numpy](www.numpy.org) is the main package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs in Python.\n",
    "- np.random.seed(1) is used to keep all the random function calls consistent\n",
    "- `hmmlearn` implements the Hidden Markov Models (HMMs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1171a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hmm.MultinomialHMM Hidden Markov Model with multinomial (discrete) emissions.\n",
    "class HMM(hmm.MultinomialHMM):\n",
    "    def __init__(self,A,B,pi,**kwargs): #  keyword argument \n",
    "        n_components        = A.shape[0]\n",
    "        super().__init__(n_components,**kwargs)\n",
    "        self.transmat_     = A\n",
    "        self.emissionprob_ = B\n",
    "        self.startprob_    = pi\n",
    "        \n",
    "    def likelihood(self,obs_seq):\n",
    "        if len(obs_seq.shape)==1:\n",
    "            obs_seq = obs_seq.reshape(-1, 1)\n",
    "        # logprob -> probability\n",
    "        return np.exp(self.score(obs_seq))\n",
    "         \n",
    "    def decoding(self,obs_seq):\n",
    "        if len(obs_seq.shape)==1:\n",
    "            obs_seq = obs_seq.reshape(-1, 1)\n",
    "        # logprob -> probability\n",
    "        logprob, seq = self.decode(obs_seq)\n",
    "        return np.exp(logprob), seq\n",
    "    \n",
    "    def learning(self,obs_seq):\n",
    "        if len(obs_seq.shape)==1:\n",
    "            obs_seq = obs_seq.reshape(-1, 1)\n",
    "            \n",
    "        self.fit(obs_seq)\n",
    "    \n",
    "    def show_model(self):\n",
    "        np.set_printoptions(precision=4, suppress=True)\n",
    "        print('A: Transition probability matrix')\n",
    "        print(self.transmat_)\n",
    "        print('------------------------------')\n",
    "        print('B: Emission probability matrix')\n",
    "        print(self.emissionprob_)\n",
    "        print('-------------------------------')\n",
    "        print('pi: Initital state distribution')\n",
    "        print(self.startprob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f49ede",
   "metadata": {},
   "source": [
    "| Words | Noun | Model  | Verb |\n",
    "|-------|------|--------|------|\n",
    "| Mary  | 4/9  | 0      | 0    |\n",
    "| Jane  | 2/9  | 0      | 0    |\n",
    "| Will  | 1/9  | 3/4    | 0    |\n",
    "| Spot  | 2/9  | 0      | 1/4  |\n",
    "| Can   | 0    | 1/4    | 0    |\n",
    "| See   | 0    | 0      | 2/4  |\n",
    "| pat   | 0    | 0      | 1/4  |\n",
    "\n",
    "|     | N   | M   | V   | \\<E> |\n",
    "|-----|-----|-----|-----|-----|\n",
    "|  \\<S> | 3/4 | 1/4 | 0   | 0   |\n",
    "| N   | 1/9 | 3/9 | 1/9 | 4/9 |\n",
    "| M   | 1/4 | 0   | 3/4 | 0   |\n",
    "| V   | 4/4 | 0   | 0   | 0   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07067d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['Noun', 'Modal', 'Verb','End']\n",
    " \n",
    "observations = ['Mary','Jane','Will','Spot','Can','See','pat','.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfc48d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Multinomial HMM \n",
    "pi= np.array([3/4, 1/4 , 0  , 0])  # initial probability  \n",
    "A = np.array([[1/9 ,3/9,1/9 , 4/9],\n",
    "              [1/4 , 0 ,3/4,  0],\n",
    "              [4/4 , 0 , 0 ,  0],\n",
    "              [ 0 , 0 , 0 ,  4/4]]) # transmition probability\n",
    "\n",
    "B = np.array([[4/9, 2/9, 1/9 , 2/9 , 0, 0, 0, 0],\n",
    "              [0, 0, 3/4 ,0 , 1/4 ,0 , 0, 0 ],\n",
    "              [0, 0, 0 ,1/4,  0,  2/4 , 1/4, 0],\n",
    "              [0, 0, 0 ,0 , 0 ,0 , 0 ,1 ]\n",
    "             ]) # Emission probability\n",
    "\n",
    "model = HMM(A,B,pi)   # n_components: number of state\n",
    "model.show_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5acf3bc",
   "metadata": {},
   "source": [
    "**Problem 1 (Likelihood):** Given an HMM $λ = (A, B)$ and an observation sequence $O$, determine the likelihood $P(O|λ )$\n",
    "\n",
    "**Note:** The log likelihood is provided from calling `.likelihood.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd6cd28",
   "metadata": {},
   "source": [
    "How likely is a given sequence?\n",
    "* $ O= \\{\\text{\"Mary\"}\\}$\n",
    "* $ O= \\{\\text{\"Mary Jane\"}\\}$\n",
    "* $ O= \\{\\text{\"Mary can\"}\\}$\n",
    "* $ O= ...$ Test your own example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdd29a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_seq = np.array([0])\n",
    "print(\"Prob(Mary | pi, A, B) = {:0.4f}\".format(model.likelihood(obs_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146f5610",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.75 * 0.4444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691bc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_seq = np.array([0,1])\n",
    "print(\"Prob(Mary Jane | pi, A, B) = {:0.4f}\".format(model.likelihood(obs_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2183221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_seq = np.array([0,4])\n",
    "print(\"Prob(Mary can | pi, A, B) = {:0.4f}\".format(model.likelihood(obs_seq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a9b29",
   "metadata": {},
   "source": [
    "**Problem 2 (Decoding):** Given an observation sequence {O} and an HMM {λ = (A, B)}, discover the best-hidden state sequence {X}.\n",
    "\n",
    "The **Viterbi algorithm** is one of most common decoding algorithms for HMM. Its goal is to find the most likely hidden state sequence corresponding to a series of observations. \n",
    "\n",
    "**Note:** The decoding is provided from calling `.decoding.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e60ea88",
   "metadata": {},
   "source": [
    "What is the most probable “path” for generating a given sequence?\n",
    "* $ O= \\{\\text{\"Mary\"}\\}$\n",
    "* $ O= \\{\\text{\"Mary Jane\"}\\}$\n",
    "* $ O= \\{\\text{\"Mary can\"}\\}$\n",
    "* $ O= \\{\\text{\"Will can sport Mary\"}\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_seq = np.array([0])\n",
    "prob,state_seq = model.decoding(obs_seq)\n",
    "print (\"Most likely state sequence for observation (Mary): \", state_seq)\n",
    "print(\"Probability: {:0.6f}\".format(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a758b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_seq = np.array([0,1])\n",
    "prob,state_seq = model.decoding(obs_seq)\n",
    "print (\"Most likely state sequence for observation (Mary Jane): \", state_seq)\n",
    "print(\"Probability: {:0.6f}\".format(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2542af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_seq = np.array([0,4])\n",
    "prob,state_seq = model.decoding(obs_seq)\n",
    "print (\"Most likely state sequence for observation (Mary can): \", state_seq)\n",
    "print(\"Probability: {:0.6f}\".format(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e7a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_seq = np.array([2,4,3,0])\n",
    "prob,state_seq = model.decoding(obs_seq)\n",
    "print (\"Most likely state sequence for observation (Will can sport Mary): \", state_seq)\n",
    "print(\"Probability: {:0.6f}\".format(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759875e3",
   "metadata": {},
   "source": [
    "## Generate Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e4baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset a sequence of 100 measurements\n",
    "O, _ = model.sample(5)\n",
    "\n",
    "words = []\n",
    "for o in O:\n",
    "    words.append(observations[o[0]])\n",
    "    \n",
    "print(\"The generated sentence: \", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98bff4",
   "metadata": {},
   "source": [
    "## References:\n",
    "* https://www.mygreatlearning.com/blog/pos-tagging/\n",
    "* https://www.coursera.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wmec",
   "language": "python",
   "name": "venv_wmec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
