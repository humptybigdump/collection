{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q1MRJCXvoVjZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Cross-Validation with Ridge Regression"
      ],
      "metadata": {
        "id": "1Fl97pG6t34f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are given data from the following function:\n",
        "$f(x) = - 0.1x^3 + x^2 - 0.1x + 1 $.\n",
        "\n",
        "However, your data set is highly noisy and unfortunately also seems to be quite unbalanced and small.\n",
        "\n",
        "In this exercise your task is to apply ridge regression to approximate the data distribution. In ridge regression the regularization term $\\alpha \\sum_{j=1}^p \\beta_j^2$ is added to the error function.\n",
        "The main goal of this task is to optimize the hyperparameter $\\alpha$ using k-fold cross validation."
      ],
      "metadata": {
        "id": "QeZ3ZQOct2KG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we load the data. You do not have to change anything here."
      ],
      "metadata": {
        "id": "n8Q9khZNXvIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(X, noisy=False):\n",
        "  y = np.zeros(X.shape)\n",
        "  param = [1, -.1, 1, -.1]\n",
        "  for i, weight in enumerate(param):\n",
        "    y += weight * X**(i)\n",
        "\n",
        "  # Adding noise\n",
        "  if noisy:\n",
        "    y += np.random.normal(0, 1, X.shape)\n",
        "  return y"
      ],
      "metadata": {
        "id": "fYuRPKSKIqAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of data points\n",
        "np.random.seed(2)\n",
        "data_points = 150\n",
        "n_outliers = 50\n",
        "\n",
        "# Generating synthetic data\n",
        "X = np.concatenate((np.random.uniform(-6, -5, int(data_points/2)),\n",
        "                    np.random.uniform(-5, -4, 2),\n",
        "                    np.random.uniform(-4, -1, int(data_points/4)),\n",
        "                    np.random.uniform(0, 1.5, int(data_points/4)),\n",
        "                    np.random.uniform(1.5, 3.9, 3),\n",
        "                    np.random.uniform(4, 6, int(data_points/2))))\n",
        "\n",
        "np.random.shuffle(X)\n",
        "y = f(X, noisy=True)\n",
        "X = X[:, np.newaxis]\n",
        "\n",
        "# Introduce outliers\n",
        "outlier_indices = np.random.choice(data_points, n_outliers, replace=False)\n",
        "outlier_indices = np.concatenate((outlier_indices,\n",
        "                                  np.where(np.logical_and(X < 4, X > 1))[0],\n",
        "                                  np.where(np.logical_and(X > -5, X < -4))[0]))\n",
        "y[outlier_indices] += np.random.normal(0, 10, outlier_indices.shape[0])\n",
        "\n",
        "# generate test/holdout data set to compare cross validation\n",
        "# and train-test-split validation at the end\n",
        "X_holdout = np.random.uniform(-6, 6, 100)[:, np.newaxis]\n",
        "y_holdout = f(X_holdout, noisy=True)\n",
        "\n",
        "# number of weights to fit in regression\n",
        "degree = 10\n",
        "poly_features = PolynomialFeatures(degree=degree, include_bias=True)\n",
        "\n",
        "# number of folds\n",
        "num_folds = 5\n",
        "\n",
        "# plot data\n",
        "# Generate a dense set of X values for smooth plotting\n",
        "X_dense = np.linspace(X.min(), X.max(), 500)[:, np.newaxis]\n",
        "y_dense = f(X_dense)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X, y, color='black', label='Training data')\n",
        "plt.scatter(X_holdout, y_holdout, color='blue', alpha=0.4, label='Holdout data')\n",
        "plt.plot(X_dense, y_dense, color='red', label='True function')\n",
        "plt.legend()\n",
        "plt.title('Training Data and True Function')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qrNb9307tajl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function will be used later to use your implementation of the k-fold cross validation with different values of alphas. You do not have to change anything in this function."
      ],
      "metadata": {
        "id": "SrP9LWTxTVV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_alphas(cross_validation=True, verbose=False):\n",
        "  \"\"\"\n",
        "  Tests out a range of values for alphas for ridge regression\n",
        "  @param cross_validation: Whether to use cross validation or one test and one train split\n",
        "  @return: alphas, losses, best_alpha, best_loss\n",
        "  \"\"\"\n",
        "  np.random.seed(1)\n",
        "  num_alphas = 200\n",
        "  alphas = np.logspace(-5, 4, num_alphas)\n",
        "\n",
        "  min_loss = np.inf\n",
        "  best_alpha = None\n",
        "\n",
        "  losses = []\n",
        "\n",
        "  for i in range(num_alphas):\n",
        "    alpha = alphas[i]\n",
        "\n",
        "    # initialize model with current alpha\n",
        "    model = make_pipeline(poly_features, Ridge(alpha, solver='svd'))\n",
        "\n",
        "    if cross_validation:\n",
        "      # do k-fold cross validation of model\n",
        "      score = k_fold_cross_validation(X, y, num_folds, model)\n",
        "    else:\n",
        "      # do validation with train-test-split\n",
        "      score = train_test_split_validation(X, y, model)\n",
        "\n",
        "    if verbose:\n",
        "      print(f\"alpha: {alpha}, mse loss: {score}\")\n",
        "    losses.append(score)\n",
        "\n",
        "    # store best alpha\n",
        "    if score < min_loss:\n",
        "      min_loss = score\n",
        "      best_alpha = alpha\n",
        "\n",
        "  print(\"-------------------------------------------------------------------------------------\")\n",
        "  print(f\"Best alpha = {best_alpha} (log(alpha) = {np.log10(best_alpha)}), mse loss {min_loss}\")\n",
        "\n",
        "  return alphas, losses, best_alpha, min_loss\n"
      ],
      "metadata": {
        "id": "GDz5OtfrW07h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Complete the function, implementing the k-fold cross validation on the data set (X, y), given the model we want to train. You are free to implement the ridge regression yourself. However, as our main goal is to implement cross-validation and not ridge regression, you can also use the class `sklearn.linear_model.Ridge`. For score we will use the mean squared error. You can implement this by yourself of use the function `mean_squared_error` from scikit learn."
      ],
      "metadata": {
        "id": "uGR_UsfWNIBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import error\n",
        "def k_fold_cross_validation(X, y, k, model):\n",
        "  \"\"\"\n",
        "  Performs k-fold cross validation on the model\n",
        "  @param X: Data x-values\n",
        "  @param y: Data true Labels\n",
        "  @param k: Number of folds\n",
        "  @param model: Model to train\n",
        "  @return: Mean  mse scores over the k validations\n",
        "  \"\"\"\n",
        "  # TODO: implement k-fold cross validation\n",
        "  raise NotImplementedError\n"
      ],
      "metadata": {
        "id": "_XMgGuWDUfv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- use this cell to test your implementation"
      ],
      "metadata": {
        "id": "1rX2Y4Smneae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphas_cross, losses_cross, best_alpha_cross, min_loss_cross = test_alphas(verbose=True)\n",
        "\n",
        "# initialize model with the alpha that returned the lowest average loss\n",
        "model_cross = make_pipeline(poly_features, Ridge(best_alpha_cross, solver='svd'))\n",
        "model_cross.fit(X, y)\n",
        "\n",
        "# generate a dense set of X values for smooth plotting\n",
        "X_dense = np.linspace(X.min(), X.max(), 500)[:, np.newaxis]\n",
        "y_pred_cross = model_cross.predict(X_dense)\n",
        "\n",
        "# plot\n",
        "plt.figure()\n",
        "plt.scatter(X, y, color='black', label='Test data')\n",
        "plt.plot(X_dense, y_pred_cross, color='blue', label=f'model approximation using alpha {best_alpha_cross:.5f}')\n",
        "plt.plot(X_dense, f(X_dense), color='black', linestyle='--', label='true function')\n",
        "plt.title('Fit with Best Alpha')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Op1DI37mmXQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot all the alphas and their average losses\n",
        "plt.figure()\n",
        "plt.plot(np.log10(alphas_cross), losses_cross, color='blue')\n",
        "plt.xlabel('log(Alpha)')\n",
        "plt.ylabel('Average MSE Loss')\n",
        "plt.title('Average Loss vs. Alpha')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HnAXq-YIYO82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) We want to compare k-fold cross validation to a validation, where the data is only split once into one train and one test set and trained once on the train set, before it is evaluated once on the test set. Complete the function implementing the training of the model, where the data is only split into one train and one test set"
      ],
      "metadata": {
        "id": "2xLV2z49UpNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split_validation(X, y, model):\n",
        "  \"\"\"\n",
        "  Performs validation on the model using one test and one train set\n",
        "  @param X: Data x-values\n",
        "  @param y: Data true Labels\n",
        "  @param model: Model to train\n",
        "  @return: mse score from testing\n",
        "  \"\"\"\n",
        "\n",
        "  # TODO: split the data into test and training data\n",
        "\n",
        "  # TODO: train the model on the training data\n",
        "\n",
        "  # TODO: use the trained model to make predictions on the test data\n",
        "\n",
        "  # TODO: calculate the mse loss of the predictions on the test data\n",
        "\n",
        "  # TODO: return the loss\n",
        "  raise NotImplementedError"
      ],
      "metadata": {
        "id": "aXpQUPumgGZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alphas_split, losses_split, best_alpha_split, min_loss_split = test_alphas(cross_validation=False, verbose=True)\n",
        "\n",
        "# initialize model with the alpha that returned the lowest average loss\n",
        "model_split = make_pipeline(poly_features, Ridge(best_alpha_split, solver='svd'))\n",
        "model_split.fit(X, y)\n",
        "\n",
        "#print(\"------------------------------------------------------------\")\n",
        "#print(f\"Train-test-split weights: \\n{model_split['ridge'].coef_}\")\n",
        "#print(f\"Cross validation weights: \\n{model_cross['ridge'].coef_}\")\n",
        "\n",
        "# Generate a dense set of X values for smooth plotting\n",
        "X_dense = np.linspace(X.min(), X.max(), 500)[:, np.newaxis]\n",
        "y_pred_split = model_split.predict(X_dense)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X, y, color='black', label='Test data')\n",
        "plt.plot(X_dense, y_pred_split, color='red', label=f'train-test-split alpha {best_alpha_split:.5f}')\n",
        "plt.plot(X_dense, y_pred_cross, color='blue', label=f'cross validation alpha {best_alpha_cross:.5f}')\n",
        "plt.plot(X_dense, f(X_dense), color='black', linestyle='--', label='true function')\n",
        "plt.title('Fit with Best Alpha')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D1XHeKa-hQmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Compare the performance of the model using the alpha optimized with cross validation and with the model using the alpha optimized with one train-test-split. For this use the Test data (holdout data) stored in the variables `X_holdout`, `y_holdout`. This data was never used before for testing in this task."
      ],
      "metadata": {
        "id": "owrjUyobtaxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: calculate mse error on holdout data for both models\n",
        "loss_cross = None\n",
        "loss_split = None\n",
        "\n",
        "print(f\"loss on holdout data from cross validation = {loss_cross}, with alpha = {best_alpha_cross}\")\n",
        "print(f\"loss on holdout data from train-test-split validation = {loss_split}, with alpha = {best_alpha_split}\")"
      ],
      "metadata": {
        "id": "A076ipfyRrc9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}