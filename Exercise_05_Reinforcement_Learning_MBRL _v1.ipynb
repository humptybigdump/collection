{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "The goal of this exercise is to implement a simple model-based reinforcement learning algorithm.  First, we will learn a dynamics function to model observed state transitions, and then we will use model decision timing planning to maximize predicted rewards [paper](https://arxiv.org/pdf/1708.02596.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miro Board Link: https://miro.com/app/board/uXjVOtP-obk=/?share_link_id=685135424478"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we install some necessary packages to visualise the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing an RL cycle using OpenAI GYM\n",
    "`Gym` is a toolkit for developing and comparing reinforcement learning algorithms. `Gym` has a lot of built-in environments like the cartpole, pendulum,... In this [link](https://gym.openai.com/envs/), you can find a list of all defined environments.\n",
    "\n",
    "<img src=img/rl.png width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np \n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ml2_utils import Dataset, Logger\n",
    "from ml2_utils import normalize, unnormalize, weight_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the global seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seeds(i):\n",
    "    torch.manual_seed(i)\n",
    "    np.random.seed(i)\n",
    "    random.seed(i)\n",
    "\n",
    "\n",
    "# Set the random seed:\n",
    "seed = 999\n",
    "set_global_seeds(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "print(\"the shape of the observation space: \", obs_dim)\n",
    "print(\"the shape of the action space: \", act_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space of our system contains 3 Measurements $ [\\cos(\\phi), \\sin(\\phi), \\dot{\\phi}] $. The instance attributes  `low` and `high` return the minimum and maximum values of the observation space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The minimum value of the observation space :\", env.observation_space.low)\n",
    "print(\"The maximum value of the observation space :\", env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task aims to control the pendulum to its rest position using motor torque $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The minimum value of the observation space :\", env.action_space.low)\n",
    "print(\"The maximum value of the observation space :\", env.action_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the lecture, if the system is not complex, like the pendulum, the model can be represented mathematically. Here you can find all the mathematical equations describing the motion of the pendulum [link](https://en.wikipedia.org/wiki/Pendulum_(mechanics)). However, today we will assume that the dynamic for this system is unknown and will try to learn it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Based RL with Model Predictive Control (MPC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Run some policy (e.g. random policy) to collect data $D_{env} = \\{s_t, a_t, r_t, s_{t+1}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code lets the RL agent plays for **4 episodes** in which Agent makes **100 moves**. At the same time, the game is rendered at each step and prints the accumulated reward for each game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play 4 games\n",
    "number_episodes = 20\n",
    "max_rollout_length = 300\n",
    "dataset = Dataset()\n",
    "\n",
    "for i in range(number_episodes):\n",
    "    # initialize the environment\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    t = 0        \n",
    "    episode_rew = 0  # accumulated reward\n",
    "    while not done:\n",
    "        # update the counter\n",
    "        t += 1\n",
    "        # choose a random action\n",
    "        action = env.action_space.sample()\n",
    "        # take a step in the environment\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # is the episode done? \n",
    "        done = done or (t >= max_rollout_length)\n",
    "        episode_rew += reward\n",
    "        #env.render()\n",
    "        # add the transition to the dataset\n",
    "        dataset.add(state, action, next_state, reward, done)\n",
    "        # update the state\n",
    "        state = next_state\n",
    "\n",
    "    # when is done, print the cumulative reward of the game and reset the environment\n",
    "    print('Episode %d finished, reward:%d, the lenght of the episode:%d'% (i, episode_rew,t))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is initialized by calling `reset()`. After doing so, the cycle loops 10 times. In each iteration, `env.action_space.sample()` samples a random action, executes it in the environment with `env.step()`, and displays the result with the `render()` method; that is, the current state of the game, as in the preceding screenshot. In the end, the environment is closed by calling `env.close()`.  Indeed, the `step()` method returns four variables that provide information about the interaction with the environment; namely, Observation, Reward, Done, and Info.\n",
    "\n",
    "Whenever `done` is True, this means that the episode has terminated and that the environment should be reset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `Dataset` provides some functions for obtaining summary statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The state mean: \", dataset.state_mean)\n",
    "print(\"The state std: \",  dataset.state_std)\n",
    "print(\"The action mean: \",dataset.action_mean)\n",
    "print(\"The action std: \", dataset.action_std)\n",
    "print(\"shape of the random dataset: \", dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.random_iterator(batch_size=3)\n",
    "next(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** the batch is not torch tensores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Learn model $P_\\phi(s_t,a_t)$ \n",
    "We parameterize our learned dynamics function $P_\\phi (s_t, a_t)$ as a deep neural network, where the parameter vector $\\phi$ represents the network's weights. \n",
    "\n",
    "\n",
    "We don't want to learn a network to predict the next state $s_{t+1}$, given the current state and the current action $s_t, a_t$.  This function can be challenging to learn when the states $s_t$  and $s_{t+1}$ are too similar, and the action has seemingly little effect on the output. This difficulty becomes more evident as the time between states $∆t$ becomes small.\n",
    "\n",
    "Note that increasing this $∆t$ increases the information available from each data point and can help with dynamics learning and planning using the learned dynamics model. However, increasing $∆t$ also increases the discretization and complexity of the underlying continuous-time dynamics, making the learning process more difficult.\n",
    "\n",
    "We will learn a neural network dynamics model encodes the change in state that occurs as a result of executing the action $a_t$from state $s_t$ of the form:\n",
    "$$\\hat{\\Delta}_{t+1} = P_\\phi (s_t, a_t)$$\n",
    "such that\n",
    "$$ s_{t+1} =  s_t + \\hat{\\Delta}_{t+1} $$\n",
    "\n",
    "We will train $P_\\phi$ in a standard supervised learning setup, by performing gradient descent on the following objective:\n",
    "$$L(\\phi) =   \\sum_{(s_t, a_t,s_{t+1} ) \\in D}  \\lVert (s_t + \\hat{\\Delta}_{t+1}) − s_{t+1}\\rVert_2^2$$\n",
    "\n",
    "\n",
    "<img src=img/img1.png width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a neural network dynamics model and train it using a fixed dataset consisting of rollouts collected by a random policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firs part: one Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_bn_relu_block(in_feat, out_feat, normalize=True):\n",
    "    \"\"\" linear + batchnorm + leaky relu \"\"\"\n",
    "    layers = [nn.Linear(in_feat, out_feat)]\n",
    "    if normalize:\n",
    "        layers.append(nn.BatchNorm1d(out_feat))\n",
    "    layers.append(nn.ReLU(inplace=True))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second part: Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim,  nn_size=64):\n",
    "        super(MLP, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            *linear_bn_relu_block(state_dim + action_dim, nn_size, normalize=True),\n",
    "            *linear_bn_relu_block(nn_size, nn_size, normalize=True),\n",
    "            nn.Linear(nn_size, state_dim)\n",
    "        )\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    # actions and the states should be normalizd tensores\n",
    "    def forward(self, states, actions):\n",
    "        state_action_input = torch.cat((states, actions), dim=-1)\n",
    "        return self.network(state_action_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MLP(state_dim=3,action_dim=1)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third part: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, state_shape, action_shape, nn_size):\n",
    "        super().__init__()\n",
    "        self.model = MLP(state_dim = state_shape[0],\n",
    "                        action_dim = action_shape[0],\n",
    "                        nn_size = nn_size)\n",
    "\n",
    "\n",
    "        self.state_mean = None\n",
    "        self.state_std = None\n",
    "        self.action_mean = None\n",
    "        self.action_std = None\n",
    "        self.delta_state_mean = None\n",
    "        self.delta_state_std = None\n",
    "\n",
    "    def set_statistics(self, dataset):\n",
    "        # dataset is on cpu and numpy\n",
    "        self.state_mean  = torch.from_numpy(dataset.state_mean).cuda().unsqueeze(dim=0)\n",
    "        self.state_std   = torch.from_numpy(dataset.state_std).cuda().unsqueeze(dim=0)\n",
    "        self.action_mean = torch.from_numpy(dataset.action_mean).cuda().unsqueeze(dim=0)\n",
    "        self.action_std  = torch.from_numpy(dataset.action_std).cuda().unsqueeze(dim=0)\n",
    "        self.delta_state_mean = torch.from_numpy(dataset.delta_state_mean).cuda().unsqueeze(dim=0)\n",
    "        self.delta_state_std  = torch.from_numpy(dataset.delta_state_std).cuda().unsqueeze(dim=0)\n",
    "    \n",
    "    # Funtion required to train the model\n",
    "    def predicted_delta_state_normalized(self, states, actions):\n",
    "        # normalize the state and the action\n",
    "        states_normalized = normalize(states, self.state_mean, self.state_std)\n",
    "        actions_normalized = normalize(actions, self.action_mean, self.action_std)\n",
    "        # predict the normalized delta\n",
    "        predicted_delta_state_normalized = self.model(states_normalized, actions_normalized)\n",
    "        \n",
    "        return predicted_delta_state_normalized\n",
    "\n",
    "    # Funtion to evaluate the model\n",
    "    def predict_next_states(self, states, actions):\n",
    "        # predict the normalized delta\n",
    "        predicted_delta_state_normalized = self.predicted_delta_state_normalized(states, actions)\n",
    "        # unnormalize the normalized delta\n",
    "        predicted_delta_state = unnormalize(predicted_delta_state_normalized, self.delta_state_mean,\n",
    "                                            self.delta_state_std)\n",
    "        # next state: state + delta\n",
    "        return states + predicted_delta_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_tensor(self, actions, states):\n",
    "    # convert to tensores \n",
    "    cos_th = states[:,0].cuda()\n",
    "    sin_th = states[:,1].cuda()\n",
    "    th_dot = states[:,2].cuda()\n",
    "    th = np.arctan2(sin_th,cos_th)\n",
    "    th_normalize = (((th+np.pi) % (2*np.pi)) - np.pi)\n",
    "    action = np.clip(actions,-2.0, 2.0)[0]\n",
    "    reward = - (th_normalize ** 2 + .1 * th_dot ** 2 + .001 * (action ** 2))\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last part: Our Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBRL(object):\n",
    "    def __init__(self, model, planner, device, action_shape, args):\n",
    "        self.model                     = model\n",
    "        self.planner                   = planner\n",
    "        self.device                    = device\n",
    "        self.update_freq               = args.update_freq\n",
    "        self.log_interval              = args.log_interval\n",
    "        \n",
    "        # optimizers\n",
    "        self.model_optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=args.model_lr, betas=(args.model_beta, 0.999))\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def train(self, training=True):\n",
    "        self.training = training\n",
    "        self.model.train(training)\n",
    "\n",
    "    def get_action(self,env, model,state, step):\n",
    "        state = np.squeeze(state.numpy(), axis=0)\n",
    "        actions_argmin = self.planner.obtain_solution(state)\n",
    "        actions = actions_argmin.cpu().detach().numpy()\n",
    "        action = actions[0][0:self.action_dim]\n",
    "        return action\n",
    "\n",
    "    def update_model(self,state, action, next_state, L,step):\n",
    "        delta_state_normalized  = self.model.predicted_delta_state_normalized(state, action)\n",
    "        target_delta             = next_state - state\n",
    "        target_delta_normalized  = normalize(target_delta, self.model.delta_state_mean, self.model.delta_state_std)\n",
    "        model_loss               = F.mse_loss(delta_state_normalized,target_delta_normalized)\n",
    "\n",
    "        if step % self.log_interval == 0:\n",
    "            L.log('train_model/loss', model_loss, step)\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.model_optimizer.zero_grad()\n",
    "        model_loss.backward()\n",
    "        self.model_optimizer.step()\n",
    "\n",
    "    def update(self, iterator,L, step):\n",
    "        #iterator = dataset.random_iterator(args.batch_size)\n",
    "        state, action, next_state, reward, done = next(iterator)\n",
    "        # Convert numpy to tensores\n",
    "        state      =  torch.from_numpy(state).cuda().float()\n",
    "        action     =  torch.from_numpy(action).cuda().float()\n",
    "        next_state =  torch.from_numpy(next_state).cuda().float() \n",
    "        #if step % self.log_interval == 0:\n",
    "        #    L.log('train', step)\n",
    "\n",
    "        self.update_model(state, action, next_state,L, step)\n",
    "\n",
    "    def save_model(self, dir, step):\n",
    "        torch.save(self.model.state_dict(), os.path.join(dir, f'{step}.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = lambda:x\n",
    "# Training and optimization arguments\n",
    "args.update_freq = 1\n",
    "args.model_lr    = 0.01\n",
    "args.model_beta  = 0.99\n",
    "args.batch_size  = 128\n",
    "args.num_train_steps = 10\n",
    "# model argument\n",
    "args.nn_size     = 64\n",
    "\n",
    "# prints logs\n",
    "args.log_interval = 1\n",
    "args.work_dir     = '.'\n",
    "args.save_tb      = './tb'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = Logger(args.work_dir, use_tb=args.save_tb, config='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "state_shape    =  env.observation_space.shape\n",
    "action_shape  =  env.action_space.shape\n",
    "model         =  Model(state_shape, action_shape, nn_size = args.nn_size)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Planner\n",
    "planner = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model based RL + MPC\n",
    "mbrl = MBRL(model, planner, device, action_shape, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update the statustics of the model (required for normalization)\n",
    "mbrl.model.set_statistics(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.random_iterator(args.batch_size)\n",
    "#mbrl.update(iterator, step = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(args.num_train_steps+1):\n",
    "    iterator = dataset.random_iterator(args.batch_size)\n",
    "    # evaluate agent periodically\n",
    "\n",
    "    #if step > 0 and step % args.eval_freq == 0:\n",
    "    #    print(\"evaluation\")\n",
    "    #    print('eval/episode', episode, step)\n",
    "    #    with torch.no_grad():\n",
    "    #        #evaluate(eval_env, agent, video, args.num_eval_episodes, L, step)\n",
    "    #        evaluate(eval_env, agent, video, 3, L, step)\n",
    "    #    if args.save_model:\n",
    "    #        agent.save_model(model_dir, step)\n",
    "    for _ in range(3):\n",
    "        mbrl.update(iterator, L,step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPC():\n",
    "    def __init__(self,state_dim, action_dim):\n",
    "        #update regularly\n",
    "        self.model = None\n",
    "        self.env = None\n",
    "        self.init_state = None\n",
    "\n",
    "        #only init once\n",
    "        self.args     = args\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim  = state_dim\n",
    "        self.horizon    = args.horizon\n",
    "        self.discount   = args.discount\n",
    "        self.lb         = args.lb\n",
    "        self.ub         = args.ub\n",
    "        #self.popsize = self.config[\"popsize\"]\n",
    "        #self.elites = int(round(self.config[\"elites\"]*self.popsize))\n",
    "        self.epsilon = 1e-8\n",
    "        self.init_mean = torch.zeros(self.action_dim).cuda()\n",
    "        self.init_std = torch.ones(self.action_dim).cuda()\n",
    "\n",
    "    def get_actions_uniform(self):\n",
    "        m = torch.distributions.uniform.Uniform(\n",
    "            torch.squeeze(torch.full((1,self.action_dim),self.lb,dtype=torch.float32)),\n",
    "            torch.squeeze(torch.full((1,self.action_dim),self.ub,dtype=torch.float32))\n",
    "            )\n",
    "        actions = m.sample_n(1024).cuda()\n",
    "        actions[:1,:self.action_dim] = 0\n",
    "        return actions\n",
    "        \n",
    "    def cost_fn(self):\n",
    "        #inititialze Tensors\n",
    "        self.action_samples = torch.FloatTensor(torch.zeros((self.popsize,self.horizon*self.action_dim))).cuda() \n",
    "        self.next_states_0 = torch.FloatTensor(torch.zeros((self.popsize,self.state_dim))).cuda() \n",
    "        init_states = torch.FloatTensor(np.repeat([self.init_state], len(self.action_samples), axis=0)).cuda() if self.model.cuda_enabled else torch.FloatTensor(np.repeat([self.init_state], len(action_samples), axis=0)) #[popsize[state_dim]]\n",
    "        self.reward_trusted_out = torch.FloatTensor(np.zeros((self.horizon,len(self.action_samples)))).cuda()\n",
    "        all_costs = torch.FloatTensor(np.zeros((self.horizon,len(self.action_samples)))).cuda() if self.model.cuda_enabled else torch.FloatTensor(np.zeros(len(self.action_samples)))\n",
    "\n",
    "        #define number of batches and action_samples per batch\n",
    "        n_batch = max(1, int(len(self.action_samples)/1024))\n",
    "        per_batch = len(self.action_samples)/n_batch\n",
    "        \n",
    "        #predict rewards for batch of action with horizon h\n",
    "        for i in range(n_batch):\n",
    "            start_index = int(i*per_batch)\n",
    "            end_index = len(self.action_samples) if i == n_batch - \\\n",
    "                1 else int(i*per_batch + per_batch)\n",
    "\n",
    "            start_states = init_states[start_index:end_index]\n",
    "\n",
    "            #call model\n",
    "            dyn_model = self.model\n",
    "\n",
    "            h=0\n",
    "            for h in range(self.horizon):\n",
    "                # sample an action batch, concatenate model input and predict next states\n",
    "                actions = self.action_sampler() \n",
    "                model_input = torch.cat((start_states, actions), dim=1)\n",
    "                next_states = dyn_model.predict_tensor(model_input)+start_states #de-normalized model output\n",
    "\n",
    "                # predict reward based on cost function from environment\n",
    "                predicted_reward = self.env.get_reward_tensor(actions, start_states,next_states)\n",
    "\n",
    "                # add negative rewards (costs) to the cost function for evaluation of the best action sequence\n",
    "                all_costs[h,start_index: end_index] += torch.neg(predicted_reward*self.discount**h)\n",
    "\n",
    "                self.action_samples[start_index:end_index][:, h*self.action_dim: h * self.action_dim + self.action_dim] = 0\n",
    "                self.action_samples[start_index:end_index][:, h*self.action_dim: h * self.action_dim + self.action_dim] += actions\n",
    "\n",
    "                if h == 0:\n",
    "                    self.next_states_0[start_index:end_index] += next_states\n",
    "\n",
    "                start_states = next_states\n",
    "\n",
    "                h=h+1\n",
    "        \n",
    "        return all_costs\n",
    "\n",
    "    def obtain_solution(self):\n",
    "        all_costs = self.cost_fn()\n",
    "        costs = torch.sum(all_costs,axis=0)\n",
    "        indices = torch.argsort(costs)\n",
    "        return self.action_samples[indices], -1*self.all_costs[0,indices[0]], self.next_states_0[indices[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View in TensorBoard\n",
    "Open an embedded  TensorBoard viewer inside a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir {'./tb'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wmec",
   "language": "python",
   "name": "venv_wmec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
