{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b358710c-7fbd-482a-84a8-d220cda6de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041b7af-9a74-45db-b58b-d4433987d60f",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization\n",
    "\n",
    "PPO is a policy gradient Actor-Critic algorithm. The policy model, the **actor** network  produces a stochastic policy. It maps the state to a probability distribution over the set of possible actions. The **critic** network is used to approximate the value function and then, the advantage is calculated:\n",
    "\n",
    "$$\n",
    "A_\\Phi (s_t, a_t) = q_\\Phi (s_t,a_t) - v_\\Phi (s_t) = R_t + \\gamma v_{\\Phi'} (s_{t+1}) - v_\\Phi (s_t)\n",
    "$$\n",
    "\n",
    "The critic, $v_\\Phi$ is trained in the same manner, as the DQN model and the critic of DDPG, with TD-learning and a \"frozen\" and periodically updated target critic network, $v_{\\Phi'}$. Instead of approximating a q-value, it approximates the value.\n",
    "\n",
    "To train the actor, PPO uses the ratio of two policies:\n",
    "- a current policy $\\pi_\\Theta$, that is learned currently\n",
    "- a baseline policy $\\pi_{\\Theta´}$, an earlier version of the policy\n",
    "\n",
    "$$\n",
    "r^t (\\Theta)=r_\\Theta (s_t,a_t) = \\frac{\\pi_\\Theta (a_t | s_t)}{\\pi_{\\Theta'} (a_t | s_t)}\n",
    "$$\n",
    "\n",
    "It is the ratio of the probabilities of selecting $a_t$ given $\\pi_\\Theta$ and the probability of selecting the same action with $\\pi_{\\Theta´}$.\n",
    "\n",
    "When multiplied with the the approximated advantage, calculated using the critic network, it can be used as the objective function (maximize with SGA)\n",
    "\n",
    "$$\n",
    "loss_{actor} = - r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t)\n",
    "$$\n",
    "\n",
    "as when\n",
    "- the advantage is positive, meaning, that selecting the action would increase the value, the probability of selecting this action would increase\n",
    "- the advantage is negative, meaning, that selecting the action would decrease the value, the probability of selecting this action would decrease\n",
    "\n",
    "Instead of using this directly as loss function, to stabilize the implementation by adjusting the policy optimization step size, the loss is extended in a pessimistic way:\n",
    "\n",
    "$$\n",
    "loss_{actor} = \\min [r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t), clip(r_\\Theta (s_t, a_t), 1-\\epsilon, 1+\\epsilon) A_\\Phi (s_t, a_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b5d89-2006-432e-a7fd-9c6eb930c5b0",
   "metadata": {},
   "source": [
    "PPO uses 2 main models. The actor network learns the stochastic policy. It maps the state to a probability distribution over the set of possible actions. The critic network learns the value function. It maps the state to a scalar.\n",
    "\n",
    "The critic, $v_\\Phi$ is trained in the same manner, as the DQN model and the critic of DDPG, with TD-learning and a \"frozen\" and periodically updated target critic network, $v_{\\Phi'}$. Instead of approximating a q-value, it approximates the value.\n",
    "\n",
    "To train the actor, PPO uses the ratio of two policies:\n",
    "- a current policy $\\pi_\\Theta$, that is learned currently\n",
    "- a baseline policy $\\pi_{\\Theta´}$, an earlier version of the policy\n",
    "\n",
    "$$\n",
    "r^t (\\Theta)=r_\\Theta (s_t,a_t) = \\frac{\\pi_\\Theta (a_t | s_t)}{\\pi_{\\Theta'} (a_t | s_t)}\n",
    "$$\n",
    "\n",
    "It is the ratio of the probabilities of selecting $a_t$ given $\\pi_\\Theta$ and the probability of selecting the same action with $\\pi_{\\Theta´}$.\n",
    "\n",
    "When multiplied with the the approximated advantage, calculated using the critic network, it can be used as the objective function (maximize with SGA)\n",
    "\n",
    "$$\n",
    "loss_{actor} = - r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t)\n",
    "$$\n",
    "\n",
    "as when\n",
    "- the advantage is positive, meaning, that selecting the action would increase the value, the probability of selecting this action would increase\n",
    "- the advantage is negative, meaning, that selecting the action would decrease the value, the probability of selecting this action would decrease\n",
    "\n",
    "Instead of using this directly as loss function, to stabilize the implementation by adjusting the policy optimization step size, the loss is extended in a pessimistic way:\n",
    "\n",
    "$$\n",
    "loss_{actor} = \\min [r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t), clip(r_\\Theta (s_t, a_t), 1-\\epsilon, 1+\\epsilon) A_\\Phi (s_t, a_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9179d247-1c6d-4307-bea3-def396311e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, units=(400, 300), n_actions=2, **kwargs):\n",
    "        super(Actor, self).__init__(**kwargs)\n",
    "        self._layers = []\n",
    "        for i, u in enumerate(units):\n",
    "            self._layers.append(tf.keras.layers.Dense(u, activation='relu'))\n",
    "        self._layers.append(tf.keras.layers.Dense(n_actions, activation='softmax'))\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = inputs\n",
    "        for l in self._layers:\n",
    "            outputs = l(outputs)\n",
    "        return outputs\n",
    "    \n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, units=(400, 300), **kwargs):\n",
    "        super(Critic, self).__init__(**kwargs)\n",
    "        self._layers = []\n",
    "        for i, u in enumerate(units):\n",
    "            self._layers.append(tf.keras.layers.Dense(u, activation='relu'))\n",
    "        self._layers.append(tf.keras.layers.Dense(1))\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = inputs\n",
    "        for l in self._layers:\n",
    "            outputs = l(outputs)\n",
    "        return outputs\n",
    "    \n",
    "class PPOAgent:\n",
    "    def __init__(self, action_space, observation_shape, gamma=0.99, epsilon = 0.1):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.actor = Actor(n_actions=action_space.n)\n",
    "        self.actor_old = Actor(n_actions=action_space.n)\n",
    "        self.critic = Critic()\n",
    "        self.target_critic = Critic()\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.00025)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self._init_networks(observation_shape)\n",
    "        \n",
    "    def _init_networks(self, observation_shape):\n",
    "        initializer = np.zeros([1, *observation_shape.shape])\n",
    "        self.actor(initializer)\n",
    "        self.actor_old(initializer)\n",
    "        \n",
    "        self.critic(initializer)\n",
    "        self.target_critic(initializer)\n",
    "        \n",
    "        self.update_frozen_nets()\n",
    "        \n",
    "    def act(self, observation):\n",
    "        probs = self.actor(observation).numpy()\n",
    "        probs = np.squeeze(probs)\n",
    "        action = np.random.choice(env.action_space.n, p=probs)\n",
    "        return action\n",
    "    \n",
    "    def get_critic_grads(self, states, rewards, next_states, dones):\n",
    "        with tf.GradientTape() as tape:\n",
    "            next_value = self.target_critic(next_states)\n",
    "            q_value = rewards + (1-dones) * self.gamma * next_value\n",
    "            value = self.critic(states)\n",
    "            \n",
    "            advantage = q_value - value\n",
    "            loss = tf.reduce_mean(tf.square(advantage))\n",
    "        gradients = tape.gradient(loss, self.critic.trainable_variables)\n",
    "        return gradients, loss, advantage\n",
    "    \n",
    "    def get_actor_grads(self, states, actions, advantage):\n",
    "        with tf.GradientTape() as tape:\n",
    "            p_current = tf.gather(self.actor(states), actions, axis=1)\n",
    "            p_old = tf.gather(self.actor_old(states), actions, axis=1)\n",
    "            ratio = p_current / p_old\n",
    "            clip_ratio = tf.clip_by_value(ratio, 1-self.epsilon, 1+self.epsilon)\n",
    "            # standardize advantage\n",
    "            advantage = (advantage - tf.reduce_mean(advantage)) / (tf.keras.backend.std(advantage) + 1e-8)\n",
    "            objective = ratio * advantage\n",
    "            clip_objective = clip_ratio * advantage\n",
    "            loss = -tf.reduce_mean(tf.where(objective < clip_objective, objective, clip_objective))\n",
    "        gradients = tape.gradient(loss, self.actor.trainable_variables)\n",
    "        return gradients, loss\n",
    "        \n",
    "\n",
    "    def learn(self, states, actions, rewards, next_states, dones):\n",
    "        critic_grads, critic_loss, advantage = self.get_critic_grads(states, rewards, next_states, dones)\n",
    "        actor_grads, actor_loss = self.get_actor_grads(states, actions, advantage)\n",
    "        \n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "        return actor_loss, critic_loss\n",
    "    \n",
    "    def update_frozen_nets(self):\n",
    "        # TODO: set discount factor\n",
    "        weights = self.actor.get_weights()\n",
    "        self.actor_old.set_weights(weights)\n",
    "        \n",
    "        weights = self.critic.get_weights()\n",
    "        self.target_critic.set_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f274993-caca-4c6d-b92a-ba5589d87831",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "PPO is an on-policy method. We allways complete a full episode, record the trajectory and the rewards. We then use these to update our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5c53a-7ef2-4876-bf80-1bfc696e0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(env, agent, num_episodes=1, max_steps=200, render=False):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_return = 0.0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not (done or steps > max_steps):\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            action = agent.act(np.array([obs]))\n",
    "            obs, r, done, _, _ = env.step(action)\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "        total_return += episode_return\n",
    "    return total_return / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "\n",
    "agent = PPOAgent(env.action_space, env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5119ca2a-ef65-4676-b51b-41743eb2a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rollouts = 5\n",
    "batch_size = 8\n",
    "learn_steps = 16\n",
    "\n",
    "for i in range(150):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    next_states = []\n",
    "    dones = []\n",
    "    \n",
    "    for _ in range(n_rollouts):\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = agent.act(np.array([obs]))\n",
    "            new_obs, r, done, _, _ = env.step(action)\n",
    "\n",
    "            states.append(obs)\n",
    "            rewards.append([r])\n",
    "            actions.append([action])\n",
    "            obs = new_obs\n",
    "            next_states.append(obs)\n",
    "            dones.append([done])\n",
    "    states, actions, rewards, next_states, dones = map(np.array, [states, actions, rewards, next_states, dones])\n",
    "    \n",
    "    for _ in range(learn_steps):\n",
    "        indices = np.arange(states.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        shuffled_states = states[indices]\n",
    "        shuffled_actions = actions[indices]\n",
    "        shuffled_rewards = rewards[indices]\n",
    "        shuffled_next_states = next_states[indices]\n",
    "        shuffled_dones = dones[indices]\n",
    "        for j in range(0, states.shape[0], batch_size):\n",
    "            states_batch = shuffled_states[j:j + batch_size]\n",
    "            actions_batch = shuffled_actions[j:j + batch_size]\n",
    "            rewards_batch = shuffled_rewards[j:j + batch_size]\n",
    "            next_states_batch = shuffled_next_states[j:j + batch_size]\n",
    "            dones_batch = shuffled_dones[j:j + batch_size]\n",
    "            actor_loss, critic_loss = agent.learn(states_batch,\n",
    "                                                  actions_batch,\n",
    "                                                  rewards_batch,\n",
    "                                                  next_states_batch,\n",
    "                                                  dones_batch)\n",
    "    agent.update_frozen_nets()\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        avg_return = compute_avg_return(env, agent, num_episodes=2)\n",
    "        print(f'epoch {i + 1}, actor loss {actor_loss}, critic loss {critic_loss}, avg_return {avg_return}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db997f26",
   "metadata": {},
   "source": [
    "# Storing and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6304c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def store_models(agent, path='./model'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    actor_path = f'{path}/actor'\n",
    "    agent.actor.save_weights(actor_path)\n",
    "    \n",
    "    critic_path = f'{path}/critic'\n",
    "    agent.critic.save_weights(critic_path)\n",
    "\n",
    "\n",
    "def load_models(agent, path='./model'):\n",
    "    actor_path = f'{path}/actor'\n",
    "    if not os.path.exists(actor_path + '.index'):\n",
    "        raise FileNotFoundError(f\"Actor model not found at {actor_path}.\")\n",
    "    \n",
    "    critic_path = f'{path}/critic'\n",
    "    if not os.path.exists(critic_path + '.index'):\n",
    "        raise FileNotFoundError(f\"Critic model not found at {critic_path}.\")\n",
    "    \n",
    "    agent.actor.load_weights(actor_path)\n",
    "    agent.critic.load_weights(critic_path)\n",
    "    agent.update_frozen_nets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2821f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_models(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579e1d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the trained agent, initialize an agent\n",
    "agent = PPOAgent(env.action_space, env.observation_space)\n",
    "# and load the stored weights\n",
    "load_models(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9fff65-511c-46e2-b01c-0290e7f0548e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compute_avg_return(env, agent, num_episodes=2, render=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
