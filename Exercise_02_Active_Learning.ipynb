{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8NSBab6fyLN"
   },
   "outputs": [],
   "source": [
    "## unimportant, makes the notebook deterministic\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "import numpy\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQCBh950qOLe"
   },
   "source": [
    "# Active Learning:\n",
    "\n",
    "## Motivation\n",
    "A key problem in deep learning is data efficiency. While excellent performance can be obtained with modern tools, these are often data-hungry, rendering the deployment of deep learning in the real-world challenging for many tasks.  In Active Learning we use a “human in the loop” approach to data labelling, reducing the amount of data that needs to be labelled drastically, and making machine learning applicable when labelling costs would be too high.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCc9Ds2Lzqms"
   },
   "source": [
    "## Passive Learning\n",
    "Tasks which involve gathering a large amount of data randomly sampled from the underlying distribution and using this large dataset to train a model that can perform some sort of prediction. This method has many disadvantages:\n",
    "* Too many wasted samples. \n",
    "* Learning is limited by sampling resolution \n",
    "\n",
    "<img src=https://imgur.com/M7Afc39.png width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ewo3mFbzjID"
   },
   "source": [
    "## Active Learning: Definition and Concepts\n",
    "Active Learning is a semi-supervised technique whose main hypothesis is that if a learning algorithm can select the data it wants to learn from, it can perform better than traditional methods with significantly less data for training. In other words, **Active Learning (AL) is an interactive approach to simultaneously build a labelled data set and train a machine learning model.**\n",
    "\n",
    "**How to make machines curious to Learn?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQrT2uXR63ub"
   },
   "source": [
    "### AL algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPDfcYBHMnLm"
   },
   "source": [
    "1. A relatively large unlabeled dataset is gathered.\n",
    "2. A domain expert labels a few positive examples in the dataset.\n",
    "\n",
    "<img src=https://imgur.com/BHi6GRx.png width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nx8gol6OMnLp"
   },
   "source": [
    "3. A classifier is trained on labeled samples.\n",
    "4. The classifier is applied to the rest of the corpus.\n",
    "\n",
    "#<img src=https://imgur.com/ZPya6mX.png width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sec3f0qEMnLs"
   },
   "source": [
    "5. Few most “useful” examples are selected (e.g., that increase classification performance).\n",
    "6. The examples labeled by the expert are added to the training set.\n",
    "\n",
    "<img src=https://imgur.com/yCSp1kU.png width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00RU5phDMnLv"
   },
   "source": [
    "7. Goto 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPAzx2hB7qX5"
   },
   "source": [
    "<img src=https://imgur.com/smisThj.png width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSaZwMqk7qdI"
   },
   "source": [
    "<img src=https://imgur.com/NuD954f.png width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8shfd8_3J_2R"
   },
   "source": [
    "Full Loop\n",
    "\n",
    "<img src=https://modal-python.readthedocs.io/en/latest/_images/active-learning.png width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N5cvDyZyjkc"
   },
   "source": [
    "## Measures of uncertainty\n",
    "\n",
    "#### Low Confidence\n",
    "The simplest measure of uncertainty is ***low confidence*** defined by slide 22. $$x_{LC}=1-P(\\hat{y}|x)$$\n",
    "\n",
    "Where x is the instance to be predicted and $\\hat{y}$ is the certainty of the most likely prediction. \n",
    "\n",
    "For example, if you have three classes [car, tree, plane] and your network's classification probabilities are [0.1 | 0.2 | 0.7] the most likely class according to your classifier is plane with 70%. We take 100% and subtract the highest probability to get a 0.3 uncertainty score. The samples with the lowest uncertainty score are then queried.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oxm4RFIcMbLg"
   },
   "source": [
    "Exercise (from exam):\n",
    "\n",
    "*   Which of these samples should be labeld by an expert who uses the ***low confidence*** metric?\n",
    "    *   a) [0.5 | 0.3 | 0.2]\n",
    "    *   b) [1.0 | 0.0 | 0.0]\n",
    "    *   c) [0.0 | 1.0 | 0.0]\n",
    "    *   d) [0.4 | 0.3 | 0.3 ]\n",
    "    *   e) [0.5 | 0.0 | 0.5 ]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mVDJtTKyjuU"
   },
   "outputs": [],
   "source": [
    "proba = torch.tensor([\n",
    "                    [0.5, 0.3, 0.2],  # a) Position 0\n",
    "                    [1.0, 0.0, 0.0],  # b) Position 1\n",
    "                    [0.0, 1.0, 0.0],  # c) Position 2\n",
    "                    [0.4, 0.3, 0.3],  # d) Position 3\n",
    "                    [0.5, 0.0, 0.5]]) # e) position 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8b67cz5zh3Q"
   },
   "source": [
    "the corresponding uncertainties are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0obFeUhzic1"
   },
   "outputs": [],
   "source": [
    "highest_confidence = proba.max(axis=1)[0]\n",
    "print(f\"the highest confidences for a class are : {highest_confidence.data} \\n\")\n",
    "subtract = 1-highest_confidence\n",
    "print(f\"Subtract from one {subtract} \\n\")\n",
    "print(f\"the lowest confidence is on position {subtract.argmax()} with value {subtract.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reV0FrNRztZF"
   },
   "source": [
    "In the example above, the most uncertain sample is d) because 0.4 was the highest certainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5E0XXoGJd_S_"
   },
   "outputs": [],
   "source": [
    "## Code for plotting no understanding necessary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from itertools import product\n",
    "\n",
    "n_res = 100\n",
    "p1, p2 = np.meshgrid(np.linspace(0, 1, n_res), np.linspace(0, 1, n_res))\n",
    "p3 = np.maximum(1 - p1 - p2, 0)\n",
    "\n",
    "uncertainty = 1 - np.maximum.reduce([p1, p2, p3])\n",
    "\n",
    "with plt.style.context('seaborn-white'):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.contourf(p1, p2, uncertainty*((p1+p2) < 1), 100)\n",
    "    plt.title('Lowest Confidence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x9018GZQrnq"
   },
   "source": [
    "#### Smallest Margin Confidence\n",
    "The measure of uncertainty ***smallest margin*** is defined by slide 24. \n",
    "\n",
    "$$ x_{M}=\\underset{x}{\\operatorname{arg min}} P(\\hat{y_1}|x) - P(\\hat{y_2}|x)$$\n",
    "\n",
    "We are searching for the instances where the probability of the two most certain classes is very similar to each other. This means that the neural net can't decide between two classes.\n",
    "\n",
    "For example, if you have three classes [car, tree, plane] and your network's classification probabilities are [0.1 | 0.2 | 0.7] the most likely class according to your classifier is plane with 70% and the second highest is tree with 20%. We subtract 0.2 from 0.7 and get a 0.5 uncertainty score. The samples with the lowest confidence scores are then queried.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w75478LCU7Ct"
   },
   "source": [
    "Exercise (from exam):\n",
    "\n",
    "*   Which of these samples should be labeld by an expert, who is following the ***smallest margin*** metric?\n",
    "    *   a) [0.5 | 0.3 | 0.2]\n",
    "    *   b) [1.0 | 0.0 | 0.0]\n",
    "    *   c) [0.0 | 1.0 | 0.0]\n",
    "    *   d) [0.4 | 0.3 | 0.3 ]\n",
    "    *   e) [0.5 | 0.0 | 0.5 ]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBxSpdA5VE_H"
   },
   "outputs": [],
   "source": [
    "proba = torch.tensor([\n",
    "                    [0.5, 0.3, 0.2],  # a) Position 0\n",
    "                    [1.0, 0.0, 0.0],  # b) Position 1\n",
    "                    [0.0, 1.0, 0.0],  # c) Position 2\n",
    "                    [0.4, 0.3, 0.3],  # d) Position 3\n",
    "                    [0.5, 0.0, 0.5]]) # e) position 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acw6_DY7V4Ft"
   },
   "outputs": [],
   "source": [
    "two_highest_probabilities = torch.topk(proba, k=2, axis=1)[0]\n",
    "print(f\"top two classes \\n{two_highest_probabilities}\")\n",
    "\n",
    "subtraction = torch.abs(two_highest_probabilities[:,0] - two_highest_probabilities[:,1])\n",
    "print(f\"\\nsubtract the highest class with the second highest class \\n{subtraction}\" )\n",
    "print(f\"\\nThe lowest margin is on Position {torch.argmin(subtraction)} with value {torch.min(subtraction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qj3RurvTfdlB"
   },
   "outputs": [],
   "source": [
    "# just for visualization\n",
    "proba = np.vstack((p1.ravel(), p2.ravel(), p3.ravel())).T\n",
    "\n",
    "part = np.partition(-proba, 1, axis=1)\n",
    "margin = - part[:, 0] + part[:, 1]\n",
    "\n",
    "margin = margin.reshape(p1.shape)\n",
    "\n",
    "with plt.style.context('seaborn-white'):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.contourf(p1, p2, (1-margin)*((p1+p2) < 1), 100)\n",
    "    plt.title('Classification margin for ternary classification')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IPmtARuVB_L"
   },
   "source": [
    "Entropy\n",
    "\n",
    "The measure of uncertainty ***highest entropy*** is defined by slide 24. \n",
    "\n",
    "$$ x_{E}=\\underset{x}{\\operatorname{arg max}} \\sum_y -P(y|x) log P(y|x)$$\n",
    "\n",
    "For example, if you have three classes [car, tree, plane] and your network's classification probabilities are [0.1 | 0.2 | 0.7].\n",
    "Then we calculate \n",
    "\n",
    "$$ -0.1 \\log(0.1) - 0.2 \\log(0.2) - 0.7 \\log(0.7) = 0.8$$ \n",
    "\n",
    "which is the uncertainty score. We are searching for the instances with the highest entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-uY1Ju-Dpwm"
   },
   "source": [
    "Exercise (not in exam but it could be):\n",
    "\n",
    "*   Which of these samples should be labeld by an expert, who is following the ***highest entropy*** metric?\n",
    "    *   a) [0.5 | 0.3 | 0.2]\n",
    "    *   b) [1.0 | 0.0 | 0.0]\n",
    "    *   c) [0.0 | 1.0 | 0.0]\n",
    "    *   d) [0.4 | 0.3 | 0.3 ]\n",
    "    *   e) [0.5 | 0.0 | 0.5 ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFP8IOplZjqG"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy # let scipy calculate our entropy\n",
    "proba = torch.tensor([\n",
    "                    [0.5, 0.3, 0.2],  # a) Position 0\n",
    "                    [1.0, 0.0, 0.0],  # b) Position 1\n",
    "                    [0.0, 1.0, 0.0],  # c) Position 2\n",
    "                    [0.4, 0.3, 0.3],  # d) Position 3\n",
    "                    [0.5, 0.0, 0.5]]) # e) position 4\n",
    "\n",
    "ent = entropy(proba.T)\n",
    "print(f\"Entropy of all instances is {ent} \\nmaximum Entropy is {ent.max()} \\nat Position {ent.argmax()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsxiPflv6sHl"
   },
   "outputs": [],
   "source": [
    "# just for visualization\n",
    "proba = np.vstack((p1.ravel(), p2.ravel(), p3.ravel())).T\n",
    "entr = entropy(proba.T).reshape(p1.shape)\n",
    "\n",
    "with plt.style.context('seaborn-white'):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.contourf(p1, p2, entr*((p1+p2) < 1), 100)\n",
    "    plt.title('Entropy for ternary classification')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEADWT0DMnL1"
   },
   "source": [
    "## Active Learning on CIFAR10\n",
    "We will be using `modAL`, a modular active learning framework for Python. modAL is built on top of scikit-learn, but you can also use TensorFlow, Keras or PyTorch models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnL08guWvbq-"
   },
   "source": [
    "To start, you will import some pytorch packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0BUbIMUtSEu"
   },
   "outputs": [],
   "source": [
    "## Packages for Dirctories\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g63Z28Pz5320"
   },
   "source": [
    "## Dataset preparation\n",
    "**CIFAR10** is a dataset which contains 60K low resolution RGB pictures. 50k images are normally used for training and 10k images are used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTBQ3sXe4tAZ"
   },
   "outputs": [],
   "source": [
    "# Load and prepare the CIFAR dataset.\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=50000,\n",
    "                                            shuffle=False, num_workers=0,\n",
    "                                            worker_init_fn=seed_worker,\n",
    "                                            generator=g)\n",
    "\n",
    "validset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=10000,\n",
    "                                            shuffle=False, num_workers=0,\n",
    "                                            worker_init_fn=seed_worker,\n",
    "                                            generator=g)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_uvFu1A1IUU"
   },
   "source": [
    "### Verify the data\n",
    "To verify that the dataset looks correct, let's plot the first 25 images from the training set and display the class name below each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqWnwgns1KCZ"
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "# we are loading the full dataset into RAM. This might cause OOM Errors here.\n",
    "images, labels = dataiter.next()\n",
    "vis_imgs = images[:6]\n",
    "vis_labels = labels[:6]\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(vis_imgs))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[vis_labels[j]]:5s}' for j in range(6)))\n",
    "\n",
    "# convert training images in numpy\n",
    "images = images.detach().numpy()\n",
    "labels = labels.detach().numpy()\n",
    "\n",
    "# also get the validation data\n",
    "valid_images, valid_labels = iter(validloader).next()\n",
    "valid_images = valid_images.detach().numpy()\n",
    "valid_labels = valid_labels.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcrsQJZNIjTS"
   },
   "source": [
    "We will train the network with a small part of dataset to make the training for the exercise faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVZfwUEPISUJ"
   },
   "outputs": [],
   "source": [
    "print (f\"The full training dataset contains {len(trainset)} elements\")\n",
    "print (f\"The full validation dataset contains {len(validset)} elements\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWQiGu9rfEbG"
   },
   "source": [
    "### Pool-Based Sampling\n",
    "In pool-based sampling the machine has access to a large number of examples  (in our example 1000 samples) and samples from a pool (50000 samples) based on “informativeness.” Informativeness is quantified based on a user-selected metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bhe7fKw5AGjj"
   },
   "outputs": [],
   "source": [
    "# assemble initial data\n",
    "n_initial = 1000\n",
    "\n",
    "# take 1000 random images and labels from our training dataset\n",
    "initial_idx = np.random.choice(range(len(images)), size=n_initial, replace=False)\n",
    "x_initial = images[initial_idx]\n",
    "y_initial = labels[initial_idx]\n",
    "\n",
    "# delete the initial images from our pool\n",
    "# we are using our pool to find samples, that have a high uncertainty score and \"label\" them\n",
    "x_pool = np.delete(images, initial_idx, axis=0)\n",
    "y_pool = np.delete(labels, initial_idx, axis=0)\n",
    "\n",
    "print(f\"our initial dataset containes {len(x_initial)} images \")\n",
    "print(f\"our remaining 'unlabeled' dataset contains {len(x_pool)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Gbwt4FbOAA5"
   },
   "source": [
    "### Create the Model\n",
    "\n",
    "We are wrapping our PyTorch model with Skorch to make it compatible with our Active Learning Library  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m509SSEi3njs"
   },
   "outputs": [],
   "source": [
    "%pip install skorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7JF4BVo3IaN"
   },
   "source": [
    "Build CNN, define lossfunction and set optimizer. \n",
    "Wrap model with skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmipeJAF6NiV"
   },
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "# create very small neural net\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"we are working on\", device)\n",
    "\n",
    "model = NeuralNetClassifier(net,\n",
    "                            criterion=nn.CrossEntropyLoss(),\n",
    "                            optimizer=optim.Adam,\n",
    "                            optimizer__weight_decay=0.001,\n",
    "                            warm_start=True,\n",
    "                            lr = 0.001,\n",
    "                            train_split=None,\n",
    "                            verbose=1,\n",
    "                            device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zr94KP0ACw8S"
   },
   "source": [
    "### Active Learning\n",
    "Suppose that you can query the label of an unlabelled instance, but it costs you a lot. Which one would you choose? By querying an instance in the uncertain region, surely you obtain more information than querying by random. \n",
    "\n",
    "The key components of any workflow are the **model** you choose, the **informativeness** measure you use and the **query** strategy you apply to request labels. modAL was designed with modularity, flexibility and extensibility in mind. With using the scikit-learn API, it allows you to rapidly create active learning workflows. You have the freedom to seamlessly integrate scikit-learn, TensorFlow/Keras or PyTorch models into your algorithm and easily tailor your custom query strategies and uncertainty measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUfhcDpY4yoQ"
   },
   "outputs": [],
   "source": [
    "%pip install modAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbkH-At6OMMj"
   },
   "outputs": [],
   "source": [
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling, margin_sampling, entropy_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TTSMnLd5y2J"
   },
   "source": [
    "Here we initialize our Active Learner. It takes as input:\n",
    "\n",
    "*   Our skorch neural net\n",
    "*   A query strategy:\n",
    "    * lowest confidence strategy is called classification uncertainty\n",
    "    * smallest margin strategy is called classification margin\n",
    "    * highest entropy is called classification entropy\n",
    "* the initial training data\n",
    "* how many epochs we want to train on this original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1N1P6bCp_X7X"
   },
   "outputs": [],
   "source": [
    "# initialize ActiveLearner\n",
    "learner = ActiveLearner(\n",
    "    estimator=model,\n",
    "    query_strategy=uncertainty_sampling,\n",
    "    # the following argumets will be used to train the model\n",
    "    X_training=x_initial, \n",
    "    y_training=y_initial,\n",
    "    epochs=10,\n",
    ")\n",
    "\n",
    "print(\"accuracy on validation\",learner.score(valid_images, valid_labels)*100,\"%\")\n",
    "print(\"acc score on training\", learner.score(x_initial, y_initial)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZvDvd6Tr70i"
   },
   "source": [
    "Actively query the human user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mNLm-uq9MnMi",
    "outputId": "6b2638a6-3ae5-4354-f02d-72e4795d1f76"
   },
   "outputs": [],
   "source": [
    "n_queries = 5\n",
    "\n",
    "uncertain_images = []\n",
    "uncertain_labels = []\n",
    "for i in range(n_queries):\n",
    "\n",
    "    # learner.query(x_pool) calculates the uncertainty for each element in our\n",
    "    # pool and returns the element with the highest uncertainty\n",
    "    query_idx, query_inst = learner.query(x_pool)\n",
    "    # visualize image with highest uncertainty\n",
    "    imshow(torchvision.utils.make_grid(torch.tensor(query_inst)))\n",
    "    # what is our neural net currently predicting?\n",
    "    predictions = net(torch.tensor(query_inst).to(device))\n",
    "    # the predicted class:\n",
    "    class_idx = torch.argmax(predictions).cpu().numpy()\n",
    "\n",
    "    # print the results\n",
    "    print(\"the network prediction: \", classes[class_idx])\n",
    "    print(\"the class in our dataset\", classes[y_pool[query_idx][0]])\n",
    "    print(\"class percentages: \", F.softmax(predictions, dim=1).detach().cpu().numpy() * 100, \"%\")\n",
    "\n",
    "    print(\"Which class is this?\")\n",
    "    for j in range(len(classes)):\n",
    "        print(f\"{j}: {classes[j]}\")\n",
    "    \n",
    "    y_new = np.array([int(input())], dtype=int)\n",
    "    uncertain_images.append(query_inst[0])\n",
    "    uncertain_labels.append(y_new[0])\n",
    "    # delete the sample with the high uncertainty from the pool\n",
    "    x_pool, y_pool = np.delete(x_pool, query_idx, axis=0), np.delete(y_pool, query_idx, axis=0)\n",
    "\n",
    "# this adds the high uncertainty images with labels to our initial dataset and retrains\n",
    "learner.teach(np.array(uncertain_images), np.array(uncertain_labels), epochs=1)\n",
    "print(\"accuracy on validation\",learner.score(valid_images, valid_labels)*100,\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiM3wMzmiaQy"
   },
   "source": [
    "#Let's cheat: \n",
    "Our dataset already contains the correct labels.\n",
    "Let's take them instead of asking a human to label them.\n",
    "\n",
    "(It makes no sense to use active learning on an already labeled dataset. But it's easier to show the advantages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4lH-5EPDaA2"
   },
   "outputs": [],
   "source": [
    "n_queries = 10\n",
    "\n",
    "for i in range(n_queries):\n",
    "    print('Query no. %d' % (i + 1))\n",
    "    \n",
    "    # take the 500 most uncertain images \n",
    "    query_idx, query_inst = learner.query(x_pool,n_instances=1000)\n",
    "    learner.teach(X=x_pool[query_idx], y=y_pool[query_idx])\n",
    "    x_pool = np.delete(x_pool, query_idx, axis=0)\n",
    "    y_pool = np.delete(y_pool, query_idx, axis=0)\n",
    "    print(\"accuracy on validation\",learner.score(valid_images, valid_labels)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3eV1cmh0zmk"
   },
   "source": [
    "We had 1,000 initial images and added 24,000 images with active learning. With these 25000 labes we got a validation accuracy of ~47.5%\n",
    "\n",
    "How high is our validation accuracy on 25,000 random images?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYn2D4LSyN6z"
   },
   "outputs": [],
   "source": [
    "random_model = NeuralNetClassifier(Net(),\n",
    "                            criterion=nn.CrossEntropyLoss(),\n",
    "                            optimizer=optim.Adam,\n",
    "                            optimizer__weight_decay=0.001,\n",
    "                            lr = 0.001,\n",
    "                            train_split=None,\n",
    "                            verbose=1,\n",
    "                            device=device)\n",
    "random_idx = np.random.choice(range(len(images)), size=25000, replace=False)\n",
    "x_random = images[initial_idx]\n",
    "y_random = labels[initial_idx]\n",
    "\n",
    "random_model.fit(x_random, y_random, epochs=100)\n",
    "print(\"accuracy on training\", random_model.score(x_random, y_random))\n",
    "print(\"accuracy on validation\",random_model.score(valid_images, valid_labels)*100,\"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is less than 47.5%. So if labeling is expensive, it's better to use active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise_02_Active_Learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
