{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0977dad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:11:02.906372651Z",
     "start_time": "2023-06-14T11:11:00.925104518Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from lib.grid_world import GridWorld\n",
    "from lib.dqn import DQNAgent, ReplayBuffer, DQN\n",
    "from lib.util import train_dqn, test_agent, render_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029cff92",
   "metadata": {},
   "source": [
    "# Back to the GridWorld\n",
    "We explored DQNs and applied them to the CartPole environment which has a continuous observation space and a discrete action space. Today, we're going to revisit our first environment: GridWorld.\n",
    "\n",
    "Unlike the CartPole, the state in GridWorld is represented by the agent's position on the grid, which is a discrete value. This difference in state representation brings us to an important point: the use of one-hot encoding for discrete states.\n",
    "\n",
    "One-hot encoding is a process of converting discrete variables into a form that could be provided to machine learning algorithms to improve their performance. In one-hot, we map each category to a vector that contains 1 and 0, denoting the presence of the feature or not. This method is important because it allows us to use categorical data in algorithms that require numerical input. It also helps to prevent any potential misinterpretations by the algorithm due to arbitrary number assignments for categories.\n",
    "\n",
    "In our previous session, we've implicitly used a form of one-hot encoding for actions. Our Q network took only the state as input and output a vector, with each element representing a possible action. We then selected the action corresponding to the maximum value in the vector. This is similar to one-hot encoding, where each action corresponds to a unique element in the output vector. The use of the argmax function to select the action is analogous to selecting the '1' in a one-hot encoded vector.\n",
    "\n",
    "By using one-hot encoding for both the states and actions in GridWorld, we can ensure that our DQN accurately interprets the discrete nature of the environment and makes optimal decisions.\n",
    "\n",
    "DQNGridWorld extends the base `GridWorld` class to create a new environment suitable for a DQN agent. The state in this environment is represented by the agent's one-hot encoded position. This is achieved by adding the `get_observation` method. The `reset` and `step` methods are overridden to add use `get_observation` and `step` also adds a small negative reward for each step, encouraging the agent to find faster solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c76c059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:11:02.912715803Z",
     "start_time": "2023-06-14T11:11:02.910875952Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a new GridWorld environment for DQN where the state is represented by the agent's position\n",
    "class DQNGridWorld(GridWorld):\n",
    "    def get_observation(self):\n",
    "        # Create a grid with the agent's position marked as 1 and the rest as 0\n",
    "        agent = np.zeros(self.world_shape)\n",
    "        agent[tuple(self.agent_current_pos)] = 1\n",
    "        # Flatten the grid to create the observation\n",
    "        observation = agent.flatten()\n",
    "        return observation\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment and return the initial observation\n",
    "        super().reset()\n",
    "        return self.get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Perform the action and get the new state, reward and done flag\n",
    "        _, reward, done = super().step(action)\n",
    "        # Add a small negative reward for each step to encourage faster solutions\n",
    "        reward += -0.01\n",
    "        return self.get_observation(), reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def38be",
   "metadata": {},
   "source": [
    "Now we test our environment and model. First we initialize the environment, the agent and the replay buffer. Then we train the agent while periodically validating its performance. Finally, we test the trained agent and render its path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebd60154",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:11:02.937104328Z",
     "start_time": "2023-06-14T11:11:02.913620169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  8.  0. -1.]\n",
      " [ 4.  0.  0.  0.]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "(12,)\n"
     ]
    }
   ],
   "source": [
    "# dimensions of the GridWorld\n",
    "world_shape = (3, 4)\n",
    "# initial position of the agent\n",
    "agent_init_pos = (2, 0)\n",
    "# list of blocking state positions\n",
    "blocking_states = [(1, 1)]\n",
    "# list of terminal state positions\n",
    "terminal_states = [(0, 3), (1, 3)]\n",
    "# dictionary of rewards with key: position and value: reward\n",
    "reward_states = {\n",
    "    (0, 3): 1,\n",
    "    (1, 3): -1\n",
    "}\n",
    "# Create the environment and check the observation space\n",
    "env = DQNGridWorld(world_shape, agent_init_pos, blocking_states, terminal_states, reward_states)\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "print(obs)\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e067c159",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:11:04.496502158Z",
     "start_time": "2023-06-14T11:11:02.930930805Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the DQN agent\n",
    "agent = DQNAgent(env.action_space, obs.shape)\n",
    "# Create the replay buffer\n",
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2df02c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:12:22.930403923Z",
     "start_time": "2023-06-14T11:11:04.499583687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.007327901264943648, avg_return -0.2\n",
      "epoch 2, loss 0.0050391774657327915, avg_return -0.2\n",
      "epoch 3, loss 0.0045160068566474365, avg_return -0.2\n",
      "epoch 4, loss 0.0036988739029766293, avg_return -0.2\n",
      "epoch 5, loss 0.0034128879033232806, avg_return -0.2\n",
      "epoch 6, loss 0.003345584276758018, avg_return -0.2\n",
      "epoch 7, loss 0.0035492531078489264, avg_return -0.2\n",
      "epoch 8, loss 0.0037231964161037467, avg_return -0.2\n",
      "epoch 9, loss 0.004095938687896705, avg_return -0.2\n",
      "epoch 10, loss 0.004443292569703772, avg_return 0.95\n",
      "epoch 11, loss 0.005256044471025234, avg_return 0.95\n",
      "epoch 12, loss 0.005523191048268927, avg_return 0.95\n",
      "epoch 13, loss 0.0053218105604173616, avg_return 0.95\n",
      "epoch 14, loss 0.005325290167093044, avg_return 0.95\n",
      "epoch 15, loss 0.005272434573271312, avg_return 0.95\n",
      "epoch 16, loss 0.004802814710274106, avg_return 0.95\n",
      "epoch 17, loss 0.004224048729156493, avg_return 0.95\n",
      "epoch 18, loss 0.003963031776947901, avg_return 0.95\n",
      "epoch 19, loss 0.00358435119051137, avg_return 0.95\n",
      "epoch 20, loss 0.003261269986978732, avg_return 0.95\n",
      "epoch 21, loss 0.002584951967946836, avg_return 0.95\n",
      "epoch 22, loss 0.0023481457410525763, avg_return 0.95\n",
      "epoch 23, loss 0.0017268454630539054, avg_return 0.95\n",
      "epoch 24, loss 0.0012807185512428987, avg_return 0.95\n",
      "epoch 25, loss 0.0009393304749210074, avg_return 0.95\n",
      "epoch 26, loss 0.0007884466754148889, avg_return 0.95\n",
      "epoch 27, loss 0.0006120183566054038, avg_return 0.95\n",
      "epoch 28, loss 0.00046855186883476563, avg_return 0.95\n",
      "epoch 29, loss 0.00037523269065786735, avg_return 0.95\n",
      "epoch 30, loss 0.00030813717194178025, avg_return 0.95\n",
      "epoch 31, loss 0.0002041761350710658, avg_return 0.95\n",
      "epoch 32, loss 0.00013351399570638023, avg_return 0.95\n",
      "epoch 33, loss 9.240075286243155e-05, avg_return 0.95\n",
      "epoch 34, loss 6.834446446646325e-05, avg_return 0.95\n",
      "epoch 35, loss 4.371651917267627e-05, avg_return 0.95\n",
      "epoch 36, loss 2.847393825788913e-05, avg_return 0.95\n",
      "epoch 37, loss 2.0686496192467985e-05, avg_return 0.95\n",
      "epoch 38, loss 1.3979377285977534e-05, avg_return 0.95\n",
      "epoch 39, loss 9.886038959905363e-06, avg_return 0.95\n",
      "epoch 40, loss 7.950038821036287e-06, avg_return 0.95\n"
     ]
    }
   ],
   "source": [
    "# Train the DQN agent\n",
    "train_dqn(env, agent, replay_buffer, n_epochs=40, n_steps=400, eval_after_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1433aff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:12:22.945062764Z",
     "start_time": "2023-06-14T11:12:22.932246049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulated reward 0.95\n",
      "[[ 0.  0.  3.  3.]\n",
      " [ 0.  8.  3. -1.]\n",
      " [ 3.  3.  3.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent and render its path\n",
    "cumulated_reward, path = test_agent(agent, env)\n",
    "print('cumulated reward', cumulated_reward)\n",
    "render_path(env, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bac066",
   "metadata": {},
   "source": [
    "The model should also perform on larger and complexer environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b28d6d32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:12:22.981865765Z",
     "start_time": "2023-06-14T11:12:22.948576007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  8.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  8.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  8.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  8.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  8.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  8.  0.  0.  0.  0.]\n",
      " [ 4.  0.  0.  8.  0.  0.  0.  0.]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "# dimensions of the GridWorld\n",
    "world_shape = (8, 8)\n",
    "# initial position of the agent\n",
    "agent_init_pos = (7, 0)\n",
    "# list of blocking state positions\n",
    "blocking_states = [(0, 3),\n",
    "                   (1, 3),\n",
    "                   (2, 3),\n",
    "                   (3, 3),\n",
    "                   (5, 3),\n",
    "                   (6, 3),\n",
    "                   (7, 3)]\n",
    "# list of terminal state positions\n",
    "terminal_states = [(0, 7), (1, 7)]\n",
    "# dictionary of rewards with key: position and value: reward\n",
    "reward_states = {\n",
    "    (0, 7): 1,\n",
    "    (1, 7): -1\n",
    "}\n",
    "# Create the environment and check the observation space\n",
    "env_large = DQNGridWorld(world_shape, agent_init_pos, blocking_states, terminal_states, reward_states)\n",
    "obs = env_large.reset()\n",
    "env_large.render()\n",
    "print(obs)\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afcfa830",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:12:23.027453909Z",
     "start_time": "2023-06-14T11:12:22.951908981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the DQN agent\n",
    "agent_large = DQNAgent(env_large.action_space, obs.shape)\n",
    "# Create the replay buffer\n",
    "replay_buffer_large = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e7db232",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:13:50.176510088Z",
     "start_time": "2023-06-14T11:12:23.005977491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.002964810000776197, avg_return -0.2\n",
      "epoch 2, loss 0.0022253918050409993, avg_return -0.2\n",
      "epoch 3, loss 0.0013253529696157784, avg_return -0.2\n",
      "epoch 4, loss 0.0008619869963695237, avg_return -0.2\n",
      "epoch 5, loss 0.0005547847863454081, avg_return -0.2\n",
      "epoch 6, loss 0.0004024379597922234, avg_return -0.2\n",
      "epoch 7, loss 0.00028751534841831017, avg_return -0.2\n",
      "epoch 8, loss 0.0002197890057686891, avg_return -0.2\n",
      "epoch 9, loss 0.0001840024727925993, avg_return -0.2\n",
      "epoch 10, loss 0.0001392982356946959, avg_return -0.2\n",
      "epoch 11, loss 0.00011619759885661551, avg_return -0.2\n",
      "epoch 12, loss 9.345242403924203e-05, avg_return -0.2\n",
      "epoch 13, loss 8.035337353362593e-05, avg_return -0.2\n",
      "epoch 14, loss 7.650910904999364e-05, avg_return -0.2\n",
      "epoch 15, loss 7.700516340491959e-05, avg_return -0.2\n",
      "epoch 16, loss 5.311181911338281e-05, avg_return -0.2\n",
      "epoch 17, loss 5.654819543110534e-05, avg_return -0.2\n",
      "epoch 18, loss 6.076910958086046e-05, avg_return -0.2\n",
      "epoch 19, loss 7.441735465363308e-05, avg_return -0.2\n",
      "epoch 20, loss 7.88546472847429e-05, avg_return -0.2\n",
      "epoch 21, loss 8.158736835639502e-05, avg_return -0.2\n",
      "epoch 22, loss 8.822375630757051e-05, avg_return -0.2\n",
      "epoch 23, loss 0.00010688829058835836, avg_return -0.2\n",
      "epoch 24, loss 0.00011111995132750963, avg_return -0.2\n",
      "epoch 25, loss 0.00013608121517449945, avg_return -0.2\n",
      "epoch 26, loss 0.00016445503740669665, avg_return -0.2\n",
      "epoch 27, loss 0.00022827563486771396, avg_return -0.2\n",
      "epoch 28, loss 0.00022201491233886372, avg_return -0.2\n",
      "epoch 29, loss 0.00023104123613393313, avg_return -0.2\n",
      "epoch 30, loss 0.00032324142318884697, avg_return -0.2\n",
      "epoch 31, loss 0.00030717299699745126, avg_return -0.2\n",
      "epoch 32, loss 0.00031208866579390815, avg_return -0.2\n",
      "epoch 33, loss 0.0004486387790052504, avg_return -0.2\n",
      "epoch 34, loss 0.0004864852596710989, avg_return -0.2\n",
      "epoch 35, loss 0.0005128542211423337, avg_return -0.2\n",
      "epoch 36, loss 0.0005141132354538058, avg_return -0.2\n",
      "epoch 37, loss 0.00048653563703737746, avg_return -0.2\n",
      "epoch 38, loss 0.0004744296968510753, avg_return -0.2\n",
      "epoch 39, loss 0.00045385313796941773, avg_return 0.8600000000000001\n",
      "epoch 40, loss 0.00045725458539891406, avg_return 0.8600000000000001\n"
     ]
    }
   ],
   "source": [
    "# Train the DQN agent\n",
    "train_dqn(env_large, agent_large, replay_buffer_large, n_epochs=40, n_steps=400, eval_after_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a06dd21d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:13:50.244042288Z",
     "start_time": "2023-06-14T11:13:50.180187274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulated reward 0.86\n",
      "[[ 0.  0.  0.  8.  0.  0.  3.  3.]\n",
      " [ 0.  0.  0.  8.  0.  3.  3. -1.]\n",
      " [ 0.  0.  0.  8.  0.  3.  0.  0.]\n",
      " [ 0.  0.  0.  8.  3.  3.  0.  0.]\n",
      " [ 0.  0.  3.  3.  3.  0.  0.  0.]\n",
      " [ 0.  0.  3.  8.  0.  0.  0.  0.]\n",
      " [ 0.  0.  3.  8.  0.  0.  0.  0.]\n",
      " [ 3.  3.  3.  8.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent and render its path\n",
    "cumulated_reward, path = test_agent(agent_large, env_large)\n",
    "print('cumulated reward', cumulated_reward)\n",
    "render_path(env_large, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c75c72",
   "metadata": {},
   "source": [
    "## Convolutional DQN\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are particularly well-suited for processing grid-like data, such as images. In the case of our GridWorld environment, the state can be represented as a grid where each cell corresponds to a particular state of the environment. This grid-like structure makes it a good fit for a CNN.\n",
    "\n",
    "CNNs have the ability to automatically and adaptively learn spatial hierarchies of features. This is particularly useful in our GridWorld scenario where the agent's decision at a particular location may depend on the surrounding cells. For example, the agent may need to avoid a blocking state that is nearby or move towards a reward state. The convolutional layers can learn to recognize these spatial patterns and make decisions accordingly.\n",
    "\n",
    "Convolutional DQNs extend the traditional DQN by replacing the fully connected layers with convolutional layers. This allows the ConvDQN to take advantage of the spatial structure in the input data, which can lead to more efficient learning.\n",
    "\n",
    "In contrast, fully connected DQNs, while powerful, do not take into account the spatial structure of the input data. The network does not have a built-in mechanism to learn spatial hierarchies of features. This can make fully connected DQNs less efficient at learning in environments where spatial relationships are important.\n",
    "\n",
    "In general, ConvDQNs can be useful in any scenario where the input data has a grid-like structure. This includes not only game environments like GridWorld, but also real-world applications such as image processing, robotics, and any other task where spatial relationships in the input data are important.\n",
    "\n",
    "\n",
    "The `ConvDQN` class extends the base `DQN` class to create a new DQN model with convolutional layers. The `__init__` method is overridden to initialize the layers of the model with convolutional layers, a flatten layer, and a dense layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48c8bc54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:13:50.249495824Z",
     "start_time": "2023-06-14T11:13:50.247013473Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a new DQN model with convolutional layers\n",
    "class ConvDQN(DQN):\n",
    "    def __init__(self, filters=(32, 16, 8, 8), kernel_size=(3, 3), n_actions=4, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Initialize the layers of the model\n",
    "        self.layers = []\n",
    "        # Add convolutional layers with the specified filters and kernel size\n",
    "        for f in filters:\n",
    "            self.layers.append(tf.keras.layers.Conv2D(f, kernel_size, activation='relu', padding='same'))\n",
    "        # Add a flatten layer to convert the 2D output of the convolutional layers to 1D\n",
    "        self.layers.append(tf.keras.layers.Flatten())\n",
    "        # Add a dense layer with the number of units equal to the number of actions\n",
    "        self.layers.append(tf.keras.layers.Dense(n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4059ca",
   "metadata": {},
   "source": [
    "The `ConvDQNAgent` class extends the base `DQNAgent` class to create a new DQN agent that uses a convolutional DQN model.  Only the `__init__` method is overridden to initialize the model and the target model with a convolutional DQN model instead of a dense DQN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "018329e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:13:50.262214786Z",
     "start_time": "2023-06-14T11:13:50.249798228Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a new DQN agent with the convolutional DQN model\n",
    "class ConvDQNAgent(DQNAgent):\n",
    "    def __init__(self, action_space, observation_shape, epsilon=0.9, gamma=0.95):\n",
    "        self.action_space = action_space\n",
    "        # Initialize the model and the target model with the convolutional DQN model\n",
    "        self.model = ConvDQN(n_actions=self.action_space.n)\n",
    "        self.target_model = ConvDQN(n_actions=self.action_space.n)\n",
    "        self.epsilon = epsilon  # exploration vs exploitation\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        # Initialize the networks of the model and the target model\n",
    "        self._init_networks(observation_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3b6a2",
   "metadata": {},
   "source": [
    "The `ConvDQNGridWorld` class extends the `DQNGridWorld` class to create a new environment where the state is represented by the entire grid. We only override the `get_observation` method to enable convolutional DQN agents to capture spatial dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8853cb51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:13:50.313713131Z",
     "start_time": "2023-06-14T11:13:50.264784507Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a new GridWorld environment for Convolutional DQN where the state is represented by the entire grid\n",
    "class ConvDQNGridWorld(DQNGridWorld):\n",
    "    def get_observation(self):\n",
    "        # Create separate grids for the agent, rewards, negative rewards and blocking states\n",
    "        agent = np.zeros(self.world_shape)\n",
    "        agent[tuple(self.agent_current_pos)] = 1\n",
    "        rewards = np.zeros(self.world_shape)\n",
    "        negative_rewards = np.zeros(self.world_shape)\n",
    "        for state, reward in self.reward_states.items():\n",
    "            if state not in self.collected_rewards:\n",
    "                if reward > 0:\n",
    "                    rewards[state] = reward\n",
    "                else:\n",
    "                    negative_rewards[state] = reward\n",
    "        blocking_states = np.zeros(self.world_shape)\n",
    "        for blocking_state in self.blocking_states:\n",
    "            blocking_states[blocking_state] = 1\n",
    "        # Stack the grids to create the observation\n",
    "        observation = np.stack([agent, rewards, negative_rewards, blocking_states], axis=-1)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee9e533",
   "metadata": {},
   "source": [
    "Now we test our environment and model. First we initialize the environment, the agent and the replay buffer. Then we train the agent while periodically validating its performance. Finally, we test the trained agent and render its path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c93d485",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:13:50.314452425Z",
     "start_time": "2023-06-14T11:13:50.313458591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  8.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  8.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  8.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  8.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  8.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  8.  0.  0.  0.  0.]\n",
      " [ 4.  0.  0.  8.  0.  0.  0.  0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "(8, 8, 4)\n"
     ]
    }
   ],
   "source": [
    "# dimensions of the GridWorld\n",
    "world_shape = (8, 8)\n",
    "# initial position of the agent\n",
    "agent_init_pos = (7, 0)\n",
    "# list of blocking state positions\n",
    "blocking_states = [(0, 3),\n",
    "                   (1, 3),\n",
    "                   (2, 3),\n",
    "                   (3, 3),\n",
    "                   (5, 3),\n",
    "                   (6, 3),\n",
    "                   (7, 3)]\n",
    "\n",
    "# list of terminal state positions\n",
    "terminal_states = [(0, 7), (1, 7)]\n",
    "# dictionary of rewards with key: position and value: reward\n",
    "reward_states = {\n",
    "    (0, 7): 1,\n",
    "    (1, 7): -1\n",
    "}\n",
    "# Create the environment and check the observation space\n",
    "env_conv = ConvDQNGridWorld(world_shape, agent_init_pos, blocking_states, terminal_states, reward_states)\n",
    "obs = env_conv.reset()\n",
    "env_conv.render()\n",
    "for i in range(obs.shape[-1]):\n",
    "    print(obs[..., i])\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b184b48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:13:51.184266552Z",
     "start_time": "2023-06-14T11:13:50.313994462Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the Convolutional DQN agent\n",
    "agent_conv = ConvDQNAgent(env_conv.action_space, obs.shape)\n",
    "# Create the replay buffer\n",
    "replay_buffer_conv = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d5bd09d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:16:17.754021381Z",
     "start_time": "2023-06-14T11:13:51.184470612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0011825622469814334, avg_return -0.2\n",
      "epoch 2, loss 0.00048678940189006425, avg_return -0.2\n",
      "epoch 3, loss 0.00031929314144463206, avg_return -0.2\n",
      "epoch 4, loss 0.0001877865820461011, avg_return -0.2\n",
      "epoch 5, loss 0.0005851912990948449, avg_return -0.2\n",
      "epoch 6, loss 0.0003869798361932908, avg_return -0.2\n",
      "epoch 7, loss 0.00023286131830957402, avg_return -0.2\n",
      "epoch 8, loss 0.00016616802852809087, avg_return -0.2\n",
      "epoch 9, loss 0.0001624102603159372, avg_return -0.2\n",
      "epoch 10, loss 0.0001388551022358797, avg_return -0.2\n",
      "epoch 11, loss 0.00013964975022418002, avg_return -0.2\n",
      "epoch 12, loss 0.00012604870517662903, avg_return -0.2\n",
      "epoch 13, loss 0.00011814487517369798, avg_return -0.2\n",
      "epoch 14, loss 0.00010638092263093313, avg_return -0.2\n",
      "epoch 15, loss 0.00010834162805650749, avg_return -0.2\n",
      "epoch 16, loss 0.00013261970926237154, avg_return -0.2\n",
      "epoch 17, loss 0.00015229240335656868, avg_return -0.2\n",
      "epoch 18, loss 0.00017328833251895048, avg_return -0.2\n",
      "epoch 19, loss 0.0001789305428303578, avg_return -0.2\n",
      "epoch 20, loss 0.00018680800670267672, avg_return -0.2\n",
      "epoch 21, loss 0.00022360541720445326, avg_return -0.2\n",
      "epoch 22, loss 0.00025024756433822404, avg_return -0.2\n",
      "epoch 23, loss 0.00022957040829396647, avg_return -0.2\n",
      "epoch 24, loss 0.0002816031944803399, avg_return -0.2\n",
      "epoch 25, loss 0.0003050563751685331, avg_return -0.2\n",
      "epoch 26, loss 0.0003300789544482541, avg_return -0.2\n",
      "epoch 27, loss 0.00034781833244323934, avg_return -0.2\n",
      "epoch 28, loss 0.000387044876561049, avg_return 0.8600000000000001\n",
      "epoch 29, loss 0.00043592581494067417, avg_return 0.8600000000000001\n",
      "epoch 30, loss 0.00046388874852709705, avg_return 0.8600000000000001\n",
      "epoch 31, loss 0.0004691202659614646, avg_return 0.8600000000000001\n",
      "epoch 32, loss 0.00043555226056923857, avg_return 0.8600000000000001\n",
      "epoch 33, loss 0.00044707900019602675, avg_return 0.8600000000000001\n",
      "epoch 34, loss 0.0005050870936429419, avg_return 0.8600000000000001\n",
      "epoch 35, loss 0.00060249025932535, avg_return 0.8600000000000001\n",
      "epoch 36, loss 0.000624545862137893, avg_return 0.8600000000000001\n",
      "epoch 37, loss 0.0006205875765772362, avg_return 0.8600000000000001\n",
      "epoch 38, loss 0.0006308580527729646, avg_return 0.8600000000000001\n",
      "epoch 39, loss 0.0007007845820226066, avg_return 0.8600000000000001\n",
      "epoch 40, loss 0.000707245577359572, avg_return 0.8600000000000001\n"
     ]
    }
   ],
   "source": [
    "# Train the Convolutional DQN agent\n",
    "train_dqn(env_conv, agent_conv, replay_buffer_conv, n_epochs=40, n_steps=400, eval_after_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "041f290d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:16:17.808129602Z",
     "start_time": "2023-06-14T11:16:17.756639982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulated reward 0.86\n",
      "[[ 0.  0.  0.  8.  0.  0.  3.  3.]\n",
      " [ 0.  0.  0.  8.  0.  0.  3. -1.]\n",
      " [ 0.  0.  0.  8.  3.  3.  3.  0.]\n",
      " [ 0.  0.  0.  8.  3.  0.  0.  0.]\n",
      " [ 0.  0.  3.  3.  3.  0.  0.  0.]\n",
      " [ 0.  3.  3.  8.  0.  0.  0.  0.]\n",
      " [ 3.  3.  0.  8.  0.  0.  0.  0.]\n",
      " [ 3.  0.  0.  8.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent and render its path\n",
    "cumulated_reward, path = test_agent(agent_conv, env_conv)\n",
    "print('cumulated reward', cumulated_reward)\n",
    "render_path(env_conv, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b74296",
   "metadata": {},
   "source": [
    "## Generalization\n",
    "\n",
    "The `RandomConvDQNGridWorld` class extends the `ConvDQNGridWorld` class to create a new environment where the reward (and terminal) states are randomly generated. This is achieved by overriding the `reset` method and adding a new `generate_config` method. In the constructor we have `reward_dict` instead of `terminal_states` and  `reward_states`, mapping reward values to number of states with that reward. This class is useful for testing the generalization ability of a convolutional DQN agent, as the agent needs to adapt to different reward and terminal states in each episode. We don't need to change the implementation of the Convolutional DQN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3913a65a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:16:17.816621424Z",
     "start_time": "2023-06-14T11:16:17.814414589Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a new GridWorld environment for Convolutional DQN where the state is represented by the entire grid\n",
    "# and the reward (and terminal) states are randomly generated\n",
    "class RandomConvDQNGridWorld(ConvDQNGridWorld):\n",
    "    def __init__(self, world_shape, agent_init_pos, blocking_states, reward_dict):\n",
    "        terminal_states, reward_states = self.generate_config(world_shape, agent_init_pos, blocking_states, reward_dict)\n",
    "        super().__init__(world_shape, agent_init_pos, blocking_states, terminal_states, reward_states)\n",
    "        self.reward_dict = reward_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_config(world_shape, agent_init_pos, blocking_states, reward_dict):\n",
    "        # Generate random reward and terminal states\n",
    "        n_positions = sum([v for v in reward_dict.values()])\n",
    "        possible_positions = np.meshgrid(np.arange(world_shape[0]), np.arange(world_shape[1]))\n",
    "        possible_positions = np.array(possible_positions).T.reshape(-1, 2)\n",
    "        # Remove agent initial position and blocking states from possible positions\n",
    "        possible_positions = possible_positions[np.logical_not(np.all(possible_positions == agent_init_pos, axis=1))]\n",
    "        for blocking_state in blocking_states:\n",
    "            possible_positions = possible_positions[\n",
    "                np.logical_not(np.all(possible_positions == blocking_state, axis=1))]\n",
    "        positions = possible_positions[np.random.choice(possible_positions.shape[0], n_positions, replace=False), :]\n",
    "        current_index = 0\n",
    "        terminal_states = []\n",
    "        reward_states = {}\n",
    "        for k, v in reward_dict.items():\n",
    "            terminal_states.append(tuple(positions[current_index]))\n",
    "            for i in range(v):\n",
    "                reward_states[tuple(positions[current_index])] = k\n",
    "            current_index += 1\n",
    "        return terminal_states, reward_states\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment and return the initial observation\n",
    "        self.terminal_states, self.reward_states = self.generate_config(self.world_shape, self.agent_init_pos,\n",
    "                                                                        self.blocking_states, self.reward_dict)\n",
    "        return super().reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330a461",
   "metadata": {},
   "source": [
    "Now we test our environment and model. First we initialize the environment, the agent and the replay buffer. Then we train the agent while periodically validating its performance. Finally, we test the trained agent and render its path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2208acb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:16:17.842288826Z",
     "start_time": "2023-06-14T11:16:17.818342375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 8. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 8. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 8. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 8. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 8. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 8. 0. 0. 0. 0.]\n",
      " [4. 0. 0. 8. 0. 0. 0. 0.]]\n",
      "(8, 8, 4)\n"
     ]
    }
   ],
   "source": [
    "# initial position of the agent\n",
    "agent_init_pos = (7, 0)\n",
    "\n",
    "# list of blocking state positions\n",
    "blocking_states = [(0, 3),\n",
    "                   (1, 3),\n",
    "                   (2, 3),\n",
    "                   (3, 3),\n",
    "                   (5, 3),\n",
    "                   (6, 3),\n",
    "                   (7, 3)]\n",
    "# dictionary mapping reward to number of states with that reward\n",
    "reward_dict = {\n",
    "    1: 1\n",
    "}\n",
    "# Create the environment and check the observation space\n",
    "env_conv_random = RandomConvDQNGridWorld(world_shape, agent_init_pos, blocking_states, reward_dict)\n",
    "obs = env_conv_random.reset()\n",
    "env_conv_random.render()\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "588abacf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:16:17.925242372Z",
     "start_time": "2023-06-14T11:16:17.834238870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the Convolutional DQN agent for the random environment\n",
    "agent_conv_random = ConvDQNAgent(env_conv_random.action_space, obs.shape)\n",
    "# Create the replay buffer\n",
    "replay_buffer_conv_random = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba19c55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:25:30.934110965Z",
     "start_time": "2023-06-14T11:16:17.925125438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 4.869260916962048e-05, avg_return -0.2\n",
      "epoch 2, loss 0.0005204806265197703, avg_return -0.2\n",
      "epoch 3, loss 0.00042627325994715193, avg_return -0.2\n",
      "epoch 4, loss 0.0009700416095625997, avg_return -0.08100000000000004\n",
      "epoch 5, loss 0.0005691390172160027, avg_return -0.2\n",
      "epoch 6, loss 0.00047958690086602473, avg_return -0.08200000000000005\n",
      "epoch 7, loss 0.0006423462054669926, avg_return -0.2\n",
      "epoch 8, loss 0.0007211429442577355, avg_return -0.2\n",
      "epoch 9, loss 0.0008188191991393978, avg_return -0.08100000000000002\n",
      "epoch 10, loss 0.000786642007710725, avg_return 0.26999999999999996\n",
      "epoch 11, loss 0.000723100112963948, avg_return 0.03399999999999996\n",
      "epoch 12, loss 0.0006775240714773645, avg_return -0.2\n",
      "epoch 13, loss 0.0007386754757590097, avg_return 0.036999999999999963\n",
      "epoch 14, loss 0.0006334689053346665, avg_return -0.2\n",
      "epoch 15, loss 0.0006690150692065799, avg_return -0.08500000000000005\n",
      "epoch 16, loss 0.0007070919906482231, avg_return 0.146\n",
      "epoch 17, loss 0.0008106426591893978, avg_return 0.254\n",
      "epoch 18, loss 0.0009767392434696376, avg_return -0.08600000000000005\n",
      "epoch 19, loss 0.0009865290844572883, avg_return -0.2\n",
      "epoch 20, loss 0.0008641501563033671, avg_return -0.08200000000000002\n",
      "epoch 21, loss 0.0009551298367114214, avg_return 0.15099999999999997\n",
      "epoch 22, loss 0.0010644040933129872, avg_return -0.2\n",
      "epoch 23, loss 0.0009817777875014144, avg_return 0.2679999999999999\n",
      "epoch 24, loss 0.0011587280398543953, avg_return 0.378\n",
      "epoch 25, loss 0.0009364879667828063, avg_return 0.14699999999999996\n",
      "epoch 26, loss 0.0009025742294852535, avg_return -0.08300000000000005\n",
      "epoch 27, loss 0.0012354711973330268, avg_return 0.03199999999999996\n",
      "epoch 28, loss 0.0009782343781807867, avg_return 0.034999999999999955\n",
      "epoch 29, loss 0.0010332078049941629, avg_return 0.38799999999999985\n",
      "epoch 30, loss 0.001170677487152716, avg_return 0.026999999999999948\n",
      "epoch 31, loss 0.0010667656410987547, avg_return 0.02899999999999996\n",
      "epoch 32, loss 0.0010140071226487635, avg_return 0.13199999999999995\n",
      "epoch 33, loss 0.0009495073024936573, avg_return 0.018999999999999954\n",
      "epoch 34, loss 0.0008781368342170026, avg_return 0.2579999999999999\n",
      "epoch 35, loss 0.0008154226832175482, avg_return 0.14299999999999996\n",
      "epoch 36, loss 0.0008772296503138932, avg_return 0.35699999999999993\n",
      "epoch 37, loss 0.0007455360137100797, avg_return 0.02899999999999996\n",
      "epoch 38, loss 0.0007527085949732282, avg_return 0.03399999999999996\n",
      "epoch 39, loss 0.0007359966782587435, avg_return 0.7020000000000001\n",
      "epoch 40, loss 0.0009928189349466265, avg_return 0.3609999999999999\n",
      "epoch 41, loss 0.0008670125866956369, avg_return 0.13999999999999996\n",
      "epoch 42, loss 0.0007761446934182459, avg_return 0.022999999999999958\n",
      "epoch 43, loss 0.0009014211320845789, avg_return 0.5850000000000001\n",
      "epoch 44, loss 0.0008796617635198345, avg_return 0.491\n",
      "epoch 45, loss 0.0008619579994046944, avg_return 0.24499999999999997\n",
      "epoch 46, loss 0.000854038946727087, avg_return 0.37799999999999995\n",
      "epoch 47, loss 0.000902476938563268, avg_return 0.2639999999999999\n",
      "epoch 48, loss 0.0008831831237330334, avg_return 0.4919999999999999\n",
      "epoch 49, loss 0.0008986274083326862, avg_return 0.592\n",
      "epoch 50, loss 0.0008832551939121913, avg_return 0.48199999999999993\n",
      "epoch 51, loss 0.0009225165317729989, avg_return 0.24299999999999997\n",
      "epoch 52, loss 0.0008984542196230905, avg_return 0.24599999999999994\n",
      "epoch 53, loss 0.0008333801251865225, avg_return 0.712\n",
      "epoch 54, loss 0.0008094926656667667, avg_return 0.37599999999999995\n",
      "epoch 55, loss 0.0008197776542147039, avg_return 0.48699999999999993\n",
      "epoch 56, loss 0.0007945638749333739, avg_return 0.48200000000000004\n",
      "epoch 57, loss 0.000766363801631087, avg_return 0.258\n",
      "epoch 58, loss 0.0007431911571984529, avg_return 0.472\n",
      "epoch 59, loss 0.0006776669199552998, avg_return 0.585\n",
      "epoch 60, loss 0.000656577777590428, avg_return 0.7070000000000001\n",
      "epoch 61, loss 0.0007056519636989833, avg_return 0.13699999999999998\n",
      "epoch 62, loss 0.0008391114301957714, avg_return 0.601\n",
      "epoch 63, loss 0.0008522530138179718, avg_return 0.596\n",
      "epoch 64, loss 0.0008304658053930325, avg_return 0.485\n",
      "epoch 65, loss 0.0006727680083713494, avg_return 0.14499999999999996\n",
      "epoch 66, loss 0.0006876110571738536, avg_return 0.3539999999999999\n",
      "epoch 67, loss 0.0006400449160537391, avg_return 0.24799999999999991\n",
      "epoch 68, loss 0.0006720420983583608, avg_return 0.581\n",
      "epoch 69, loss 0.000566316277399892, avg_return 0.374\n",
      "epoch 70, loss 0.0005607008788501844, avg_return 0.472\n",
      "epoch 71, loss 0.0005537489705602638, avg_return 0.5820000000000001\n",
      "epoch 72, loss 0.000523742791301629, avg_return 0.3649999999999999\n",
      "epoch 73, loss 0.0004939774935337482, avg_return 0.24499999999999988\n",
      "epoch 74, loss 0.0004919858081393613, avg_return 0.25599999999999995\n",
      "epoch 75, loss 0.0004906833157747315, avg_return 0.46499999999999997\n",
      "epoch 76, loss 0.0004978533447683731, avg_return 0.13899999999999998\n",
      "epoch 77, loss 0.00044686033152174787, avg_return 0.13699999999999996\n",
      "epoch 78, loss 0.000436815276998459, avg_return 0.47599999999999987\n",
      "epoch 79, loss 0.0004004107438504434, avg_return 0.3639999999999999\n",
      "epoch 80, loss 0.0004493654514590162, avg_return 0.699\n",
      "epoch 81, loss 0.0004357568338946294, avg_return 0.36499999999999994\n",
      "epoch 82, loss 0.0003808549747645884, avg_return 0.48199999999999993\n",
      "epoch 83, loss 0.0004370724896034517, avg_return 0.585\n",
      "epoch 84, loss 0.00042708505043265177, avg_return 0.372\n",
      "epoch 85, loss 0.0004114375990411645, avg_return 0.588\n",
      "epoch 86, loss 0.0003925528671970824, avg_return 0.13699999999999998\n",
      "epoch 87, loss 0.0004235501944549469, avg_return 0.5889999999999999\n",
      "epoch 88, loss 0.0004302293937143986, avg_return 0.583\n",
      "epoch 89, loss 0.0004187178346910514, avg_return 0.358\n",
      "epoch 90, loss 0.00044751028917744406, avg_return 0.581\n",
      "epoch 91, loss 0.0003989390434071538, avg_return 0.469\n",
      "epoch 92, loss 0.00041091548041549686, avg_return 0.594\n",
      "epoch 93, loss 0.0004086983512934239, avg_return 0.6\n",
      "epoch 94, loss 0.00039504431265413587, avg_return 0.591\n",
      "epoch 95, loss 0.00038902944947949436, avg_return 0.47800000000000004\n",
      "epoch 96, loss 0.00036081839164125995, avg_return 0.7129999999999999\n",
      "epoch 97, loss 0.00035000261209461314, avg_return 0.582\n",
      "epoch 98, loss 0.0003200950330892738, avg_return 0.363\n",
      "epoch 99, loss 0.00031619500100532605, avg_return 0.4959999999999999\n",
      "epoch 100, loss 0.00029499269567168085, avg_return 0.588\n",
      "epoch 101, loss 0.00027403026740557834, avg_return 0.255\n",
      "epoch 102, loss 0.00026022249733159697, avg_return 0.46499999999999997\n",
      "epoch 103, loss 0.0002890009180873676, avg_return 0.489\n",
      "epoch 104, loss 0.0002566459213539929, avg_return 0.5939999999999999\n",
      "epoch 105, loss 0.00022686957254336448, avg_return 0.572\n",
      "epoch 106, loss 0.0002146975690493491, avg_return 0.38399999999999995\n",
      "epoch 107, loss 0.00021138966985745355, avg_return 0.13599999999999998\n"
     ]
    }
   ],
   "source": [
    "# Train the Convolutional DQN agent\n",
    "train_dqn(env_conv_random, agent_conv_random, replay_buffer_conv_random, n_epochs=160, n_steps=400,\n",
    "          eval_after_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca234747",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:25:30.988664302Z",
     "start_time": "2023-06-14T11:25:30.937534419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulated reward 0.9\n",
      "[[0. 0. 0. 8. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 8. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 8. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 8. 0. 0. 3. 0.]\n",
      " [0. 3. 3. 3. 3. 3. 3. 0.]\n",
      " [3. 3. 0. 8. 0. 0. 0. 0.]\n",
      " [3. 0. 0. 8. 0. 0. 0. 0.]\n",
      " [3. 0. 0. 8. 0. 0. 0. 0.]]\n",
      "[(7, 0), (6, 0), (5, 0), (5, 1), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (3, 6)]\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent and render its path\n",
    "cumulated_reward, path = test_agent(agent_conv_random, env_conv_random)\n",
    "print('cumulated reward', cumulated_reward)\n",
    "render_path(env_conv_random, path)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d6b9203",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:26:06.997859338Z",
     "start_time": "2023-06-14T11:26:06.953163415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulated reward -0.3000000000000001\n",
      "[[0. 0. 0. 8. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 8. 3. 3. 3. 0.]\n",
      " [0. 0. 0. 8. 0. 3. 3. 0.]\n",
      " [0. 0. 0. 8. 0. 3. 0. 0.]\n",
      " [0. 0. 3. 3. 3. 3. 0. 0.]\n",
      " [0. 3. 3. 8. 0. 0. 0. 0.]\n",
      " [0. 3. 0. 8. 0. 0. 0. 0.]\n",
      " [3. 3. 0. 8. 0. 0. 0. 0.]]\n",
      "[(7, 0), (7, 1), (6, 1), (5, 1), (5, 2), (4, 2), (4, 3), (4, 4), (4, 5), (3, 5), (2, 5), (2, 6), (1, 6), (1, 5), (1, 4), (1, 5), (1, 4), (1, 5), (1, 4), (1, 5), (1, 4), (1, 5), (1, 4), (1, 5), (1, 4), (1, 5), (1, 4), (1, 5), (1, 4), (1, 5), (1, 4)]\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent and render its path\n",
    "cumulated_reward, path = test_agent(agent_conv_random, env_conv_random)\n",
    "print('cumulated reward', cumulated_reward)\n",
    "render_path(env_conv_random, path)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9846d237",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T11:26:13.371962377Z",
     "start_time": "2023-06-14T11:26:13.343103903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulated reward 0.89\n",
      "[[0. 0. 0. 8. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 8. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 8. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 8. 0. 0. 0. 0.]\n",
      " [0. 3. 3. 3. 3. 3. 0. 0.]\n",
      " [3. 3. 0. 8. 0. 3. 3. 0.]\n",
      " [3. 0. 0. 8. 0. 0. 3. 0.]\n",
      " [3. 0. 0. 8. 0. 0. 0. 0.]]\n",
      "[(7, 0), (6, 0), (5, 0), (5, 1), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6)]\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent and render its path\n",
    "cumulated_reward, path = test_agent(agent_conv_random, env_conv_random)\n",
    "print('cumulated reward', cumulated_reward)\n",
    "render_path(env_conv_random, path)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4364e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
