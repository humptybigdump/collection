{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "The goal of this Notebook is to implement a simple model-based reinforcement learning algorithm.  First we will learn a dynamics function to model observed state transitions, and then we will use model predictions directly in an optimization setup to maximize predicted rewards, we will call this step the process of creating a controller. If you want to read more about this topic then you can read this [paper](https://arxiv.org/pdf/1708.02596.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we install some necessary packages to visualise the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install graphviz\n",
    "!pip install pydot\n",
    "!pip install gym\n",
    "!python -m pip install git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing an RL cycle using OpenAI GYM\n",
    "`Gym` is a toolkit for developing and comparing reinforcement learning algorithms. `Gym` has a lot of built-in environments like the cartpole, pendulum,... In this [link](https://gym.openai.com/envs/), you can find a list of all defined environments.\n",
    "\n",
    "<img src=img/rl.png width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code lets the RL Agent plays for four episodes in which Agent makes 100 moves while the game is rendered at each step and prints the accumulated reward for each game using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import gym\n",
    "\n",
    "# create and initialize the environment\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "print(\"the shape of the observation space: \", obs_dim)\n",
    "print(\"the shape of the action space: \", act_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play 4 games\n",
    "number_episodes = 4\n",
    "number_moves    = 100\n",
    "for i in range(number_episodes):\n",
    "    # initialize the environment\n",
    "    env.reset()\n",
    "    done = False\n",
    "    game_rew = 0  # accumulated reward\n",
    "    for j in range(number_moves):\n",
    "        # choose a random action\n",
    "        action = env.action_space.sample()\n",
    "        # take a step in the environment\n",
    "        new_obs, rew, done, info = env.step(action)\n",
    "        game_rew += rew\n",
    "        env.render()\n",
    "        # when is done, print the cumulative reward of the game and reset the environment\n",
    "        if done:\n",
    "            print(\"Done\")\n",
    "            break\n",
    "    print('Episode %d finished, reward:%d, the lenght of the episode:%d'% (i, game_rew,j))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is initialized by calling `reset()`. After doing so, the cycle loops 10 times. In each iteration, `env.action_space.sample()` samples a random action, executes it in the environment with `env.step()`, and displays the result with the `render()` method; that is, the current state of the game, as in the preceding screenshot. In the end, the environment is closed by calling `env.close()`.  Indeed, the `step()` method returns four variables that provide information about the interaction with the environment; namely, Observation, Reward, Done, and Info.\n",
    "\n",
    "Whenever `done` is True, this means that the episode has terminated and that the environment should be reset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Gym Spaces\n",
    "The actions and observations are mostly instances of the `Discrete` or `Box` class. These two classes represent different spaces. `Box` represents an n-dimensional array, while `Discrete`, on the other hand, is a space that allows a fixed range of non-negative numbers. The observation of `Pendulum` is encoded by three floats, meaning that it's an instance of the `Box` class and the action space is an instance of the `Box` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The observation space of Pendulum is \", env.observation_space)\n",
    "print(\"The action space of Pendulum is \", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `low` and `high` instance attributes return the minimum and maximum values allowed by a `Box` space: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The minimum value of the observation space :\", env.observation_space.low)\n",
    "print(\"The maximum value of the observation space :\", env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Gym Wrappers\n",
    "\n",
    "you will want to extend the environment’s functionality in some generic way. For example, an environment gives you some observations, but you want to accumulate them in some buffer and provide to the agent the N last observations.\n",
    "\n",
    "Another example is when you want to be able to crop or preprocess an image’s pixels to make it more convenient for the agent to digest, or if you want to normalize reward scores somehow. There are many such situations which have the same structure: you’d like to “wrap” the existing environment and add some extra logic doing something. Gym provides you with a convenient framework for these situations, called a Wrapper class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructEnv(gym.Wrapper):\n",
    "    '''\n",
    "    Gym Wrapper to store information like number of steps and total reward of the last espisode.\n",
    "    '''\n",
    "    def __init__(self, env,max_rollout_length = 1000):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.n_obs = self.env.reset()\n",
    "        self._max_rollout_length = max_rollout_length\n",
    "        self.total_rew = 0\n",
    "        self.len_episode = 0\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        self.n_obs = self.env.reset(**kwargs)\n",
    "        self.total_rew = 0\n",
    "        self.len_episode = 0\n",
    "        return self.n_obs.copy()\n",
    "        \n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.total_rew += reward\n",
    "        self.len_episode += 1\n",
    "        return ob, reward, done, info\n",
    "        \n",
    "    @property\n",
    "    def get_episode_reward(self):\n",
    "        return self.total_rew\n",
    "    @property\n",
    "    def get_episode_length(self):\n",
    "        return self.len_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and initialize the environment\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "env = StructEnv(env)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play 4 games\n",
    "number_episodes = 4\n",
    "number_moves    = 100\n",
    "game_rew        = 0\n",
    "for i in range(number_episodes):\n",
    "    # initialize the environment\n",
    "    env.reset()\n",
    "    done = False\n",
    "    for j in range(number_moves):\n",
    "        # choose a random action\n",
    "        action = env.action_space.sample()\n",
    "        # take a step in the environment\n",
    "        new_obs, rew, done, _ = env.step(action)\n",
    "        game_rew += rew\n",
    "        #env.render()\n",
    "        # when is done, print the cumulative reward of the game and reset the environment\n",
    "        if done:\n",
    "            print(\"Done\")\n",
    "            break\n",
    "    print('Episode %d finished, reward:%d, the lenght of the episode:%d' % (i, env.get_episode_reward, env.get_episode_length))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with TF 2.0 (Recap)\n",
    "\n",
    "As a recap of what we  used in the last exercises \n",
    "\n",
    "```python\n",
    "## Load Dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "## Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "## Define the loss function\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "## Create the optimizer by minimizing the loss using the Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "## Compile the model\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_object,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "## Train the model\n",
    "model.fit(x_train, \n",
    "          y_train,\n",
    "          epochs=3,\n",
    "          validation_data=(x_test, y_test),\n",
    "          verbose=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 2.x comes with some great benefits, one of them is **Eager execution**. TensorFlow 1.x defines static computational graphs. However, Python is typically more dynamic. TensorFlow 2.x  **still defines a graph**, but we can define, change, and execute nodes on-the-fly, with no special session interfaces or placeholders. The **eager execution**  means that the model definitions are dynamic, and the execution is immediate. Another benefit of using TensorFlow 2.x is **Training from `tf.data.datasets`**. The `tf.data` API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Based Reinforcement Learning with TF 2.0\n",
    "Model-Based Reinforcement Learning consists primarily of two steps:\n",
    "1. learning a dynamics model and\n",
    "2. using the learned dynamics models to plan and execute actions that maximize a reward function or to train a policy (Dyna Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to import all required packages for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import gym\n",
    "import ml2_utils\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "if(int(tf.__version__[0]) <= 1):\n",
    "    print('tensorflow {} detected; Please install tensorflow >= 2.0.0'.format(tf.__version__))\n",
    "else:\n",
    "    print('tensorflow {} detected'.format(tf.__version__))\n",
    "    \n",
    "from tensorflow.keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics Model\n",
    "We parameterize our learned dynamics function $f_\\theta (s_t, a_t)$ as a deep neural network, where the parameter vector $\\theta$ represents the weights of the network. \n",
    "We don't want to learn a network to predict the next state $s_{t+1}$, given the current state and the current  action $s_t, a_t$  this function can be difficult to learn when the states $s_t$  and $s_{t+1}$ are too similar and the action has seemingly little effect on the output; this difficulty becomes more pronounced as the time between states $∆t$ becomes smaller and the state differences do not indicate the underlying dynamics well.\n",
    "\n",
    "Note that increasing this $∆t$ increases the information available from each data point, and can help\n",
    "with not only dynamics learning but also with planning using the learned dynamics model. However, increasing\n",
    "$∆t$ also increases the discretization and complexity of the underlying continuous-time dynamics, which can make the learning process more difficult.\n",
    "\n",
    "We will learn a neural network dynamics model encodes the change in state that occurs as a result of executing the action $a_t$from state $s_t$ of the form:\n",
    "$$\\hat{\\Delta}_{t+1} = f_\\theta (s_t, a_t)$$\n",
    "such that\n",
    "$$ s_{t+1} =  s_t + \\hat{\\Delta}_{t+1} $$\n",
    "\n",
    "We will train $f_\\theta$ in a standard supervised learning setup, by performing gradient descent on the following objective:\n",
    "$$L(\\theta) =   \\sum_{(s_t, a_t,s_{t+1} ) \\in D}  \\lVert (s_{t+1} − s_t) − fθ(s_t, a_t)\\rVert_2^2$$\n",
    "$$L(\\theta) =   \\sum_{(s_t, a_t,s_{t+1} ) \\in D}  \\lVert (\\Delta_{t+1} − \\hat{\\Delta}_{t+1}\\rVert_2^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting training data:\n",
    "\n",
    "model_buffer is an instance of the FullBuffer class that contains the samples generated by the environment, and generate_random_dataset creates two partitions for training and validation, which are then returned by calling get_training_batch and get_valid_batch.\n",
    "1. Random Policy: to generate Date for the model\n",
    "2. Gather Rollouts and save them in buffer\n",
    "<img src=img/data_collection.png width=\"540\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy:\n",
    "    def __init__(self, env):\n",
    "        self._max_action = self._action_space_low = env.action_space.low\n",
    "        self._action_space_high = env.action_space.high\n",
    "        self._act_dim = act_dim\n",
    "        self.policy_name = \"RandomPolicy\"\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return np.random.uniform(\n",
    "            low=-self._max_action,\n",
    "            high=self._max_action,\n",
    "            size=self._act_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_rollouts(env, policy, num_rollouts, max_rollout_length, render = False):\n",
    "    dataset = ml2_utils.Dataset()\n",
    "    for _ in range(num_rollouts):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        t = 0\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = policy.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            done = done or (t >= max_rollout_length)\n",
    "            dataset.add(state, action, next_state, reward, done)\n",
    "\n",
    "            state = next_state\n",
    "            t += 1\n",
    "            \n",
    "    if render:\n",
    "        env.close()\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "num_init_random_rollouts=250\n",
    "max_rollout_length=500\n",
    "render =False\n",
    "print('Gathering random dataset')\n",
    "random_dataset = gather_rollouts(env,RandomPolicy(env),num_init_random_rollouts, max_rollout_length,render)\n",
    "print(\"The state mean: \", random_dataset.state_mean)\n",
    "print(\"The state std: \", random_dataset.action_std)\n",
    "print(\"The action mean: \", random_dataset.action_mean)\n",
    "print(\"The action std: \", random_dataset.action_std)\n",
    "print(\"thape of the random dataset: \", random_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_init_random_rollouts_valid=10\n",
    "max_rollout_length=500\n",
    "render =True\n",
    "print('Gathering random dataset')\n",
    "valid_dataset = gather_rollouts(env,RandomPolicy(env),num_init_random_rollouts_valid, max_rollout_length,render)\n",
    "print(\"The state mean: \", valid_dataset.state_mean)\n",
    "print(\"The state std: \", valid_dataset.action_std)\n",
    "print(\"The action mean: \", valid_dataset.action_mean)\n",
    "print(\"The action std: \", valid_dataset.action_std)\n",
    "print(\"thape of the random dataset: \", valid_dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model:\n",
    "We will implement a neural network dynamics model and train it using a fixed dataset consisting of rollouts collected by a random policy.\n",
    "<img src=img/5.png width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(input_layer,hidden_layers, output_layer, activation=tf.tanh, last_activation=None):\n",
    "    input_shape = (input_layer)\n",
    "    # generate input vector\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, name='model_input')\n",
    "    x = inputs\n",
    "    # generate hidden layers\n",
    "    for filters in hidden_layers:\n",
    "        x = tf.keras.layers.Dense(filters,\n",
    "                                   activation=activation)(x)\n",
    "    \n",
    "    # generate output vector\n",
    "    output = tf.keras.layers.Dense(units=output_layer,\n",
    "                                   activation=last_activation, \n",
    "                                   name='modle_output')(x)\n",
    "    # generate the model\n",
    "    dynamic = tf.keras.models.Model(inputs,\n",
    "                output,\n",
    "                name='dynamic')\n",
    "    return dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_layer = obs_dim + act_dim\n",
    "# number of units per layer\n",
    "hidden_layers = [64,64]\n",
    "output_layer = obs_dim\n",
    "# Define the model\n",
    "dynamic = mlp(input_layer,hidden_layers, output_layer)\n",
    "dynamic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(dynamic, to_file='mlp-model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training:\n",
    "Takes as input the current state, next state and compute both the actual state difference and the predicted state difference and predicted next state, and returns the loss and optimizer for training the dynamics model.\n",
    "\n",
    "1. The loss function is the mean-squared-error between the normalized state difference and normalized predicted state difference\n",
    "2. Create the optimizer by minimizing the loss using the Adam optimizer with learning rate\n",
    "3. Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "# Create the optimizer by minimizing the loss using the Adam optimizer with learning rate\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "## Compile the model\n",
    "dynamic.compile(optimizer=optimizer,\n",
    "                loss=loss_object,\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dynamics Function\n",
    "<img src=img/1.png width=\"400\">\n",
    "\n",
    "1. Normalize both the states and actions in this buffer\n",
    "2. Concatenate the normalized state and action\n",
    "3. Pass the concatenated, normalized state-action tensor through a neural network. The resulting output is the normalized predicted difference between the next state and the current state\n",
    "4. Compute the actual state difference\n",
    "5. Normalize the state difference\n",
    "6. return the normalized state difference as labels and the normalized state-action tensor as features\n",
    "\n",
    "**Note in order to produce the predicted next state you need to unnormalize the delta state prediction, and add it to the current state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_dataset(states, actions, rewards, next_states, dones):\n",
    "    # Normalize both the states and actions in this buffer\n",
    "    states_norm = ml2_utils.normalize(states, random_dataset.state_mean,   random_dataset.state_std)\n",
    "    actions_norm = ml2_utils.normalize(actions, random_dataset.action_mean,random_dataset.action_std)\n",
    "    input_layer = tf.concat([states_norm, actions_norm], axis=1)\n",
    "    # the actual state difference\n",
    "    diff = next_states - states\n",
    "    # Normalize it by using the statistics random_dataset and normalize function\n",
    "    diff_norm = ml2_utils.normalize(diff, random_dataset.delta_state_mean, random_dataset.delta_state_std)\n",
    "    yield input_layer,diff_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tf = tf.data.Dataset.from_generator(tf_dataset, \n",
    "                                            output_types =(tf.float64,tf.float64),\n",
    "                                            output_shapes = (tf.TensorShape([None,4]), tf.TensorShape([None,3])),\n",
    "                                            args = (random_dataset.list_2_np()),\n",
    "                                            ).unbatch()\n",
    "\n",
    "batch_size = 200\n",
    "batched_dataset_tf = dataset_tf.batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset_tf = tf.data.Dataset.from_generator(tf_dataset, \n",
    "                                            output_types =(tf.float64,tf.float64),\n",
    "                                            output_shapes = (tf.TensorShape([None,4]), tf.TensorShape([None,3])),\n",
    "                                            args = (valid_dataset.list_2_np())\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hist = {}\n",
    "hist[\"model\"] = dynamic.fit(batched_dataset_tf,\n",
    "                            epochs=10,\n",
    "                            validation_data= valid_dataset_tf,\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "plotter = tfdocs.plots.HistoryPlotter(metric = 'loss', smoothing_std=1)\n",
    "plotter.plot(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment:\n",
    "init_state = env.reset()\n",
    "# Define a random policy\n",
    "policy = RandomPolicy(env)\n",
    "# Evaluate the model for H-step in the future \n",
    "horizon= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a random  action sequence **on the real system**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_seq = [init_state]\n",
    "action_seq = []\n",
    "state = init_state\n",
    "for i in range(horizon):\n",
    "    action = policy.get_action(state)\n",
    "    state, reward, done , _ = env.step(action)\n",
    "    state_seq.append(state)\n",
    "    action_seq.append(action)\n",
    "\n",
    "env.close()\n",
    "# convert to numpy array\n",
    "state_seq = np.asarray(state_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same action sequence **on the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_state_seq = [init_state]\n",
    "state = init_state\n",
    "for action in action_seq:\n",
    "    # Normailize the state and the action (Prepare the state and the action space)\n",
    "    states_norm = ml2_utils.normalize(state, random_dataset.state_mean,   random_dataset.state_std)\n",
    "    actions_norm = ml2_utils.normalize(action, random_dataset.action_mean,random_dataset.action_std)\n",
    "    input_layer = tf.concat([states_norm, actions_norm], axis=0)\n",
    "    # The resulting output is the normalized predicted difference between the next state and the current state\n",
    "    pred_diff_norm = dynamic.predict(tf.expand_dims(input_layer,0))\n",
    "    # The next State\n",
    "    next_state = state + ml2_utils.unnormalize(pred_diff_norm, random_dataset.delta_state_mean,random_dataset.delta_state_std)\n",
    "    state = next_state[0]\n",
    "    pred_state_seq.append(state)\n",
    "\n",
    "# convert to numpy array\n",
    "pred_state_seq = np.asarray(pred_state_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# gripper position evaluation\n",
    "#resulting_states_list = np.rollaxis(np.array(resulting_states_list), 1)\n",
    "fig1, (ax1, ax2, ax3) = plt.subplots(figsize=(20,30), nrows=3, ncols=1)\n",
    "\n",
    "# plot the predicted state\n",
    "ax1.plot(np.arange(horizon+1), pred_state_seq[:,0], label='model prediction state 0')\n",
    "ax2.plot(np.arange(horizon+1), pred_state_seq[:,1], label='model prediction state 1')\n",
    "ax3.plot(np.arange(horizon+1), pred_state_seq[:,2], label='model prediction state 2')\n",
    "\n",
    "# plot real values\n",
    "ax1.plot(np.arange(horizon+1), state_seq[:,0], label='real state 0')\n",
    "ax2.plot(np.arange(horizon+1), state_seq[:,1], label='real state 1')\n",
    "ax3.plot(np.arange(horizon+1), state_seq[:,2], label='real state 2')\n",
    "\n",
    "# set axis lables\n",
    "for ax in (ax1, ax2, ax3):\n",
    "    ax.set_xlabel('step',fontsize='x-large')\n",
    "ax1.set_ylabel('Cos(θ)' ,fontsize='x-large')\n",
    "ax2.set_ylabel('sin(θ)' ,fontsize='x-large')\n",
    "ax3.set_ylabel('d(θ)'   ,fontsize='x-large')\n",
    "\n",
    "# plot legend\n",
    "ax1.legend(loc='best',fontsize='x-large')\n",
    "ax2.legend(loc='best',fontsize='x-large')\n",
    "ax3.legend(loc='best',fontsize='x-large')\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Selection\n",
    "Given the learned dynamics model, we now want to select and execute actions that maximize a known reward function\n",
    "$$ a^*_t = \\arg \\max_{a_t} \\sum_{t'=t}^{t+H-1} r(\\hat{s}_{t'},a_{t'})$$\n",
    "$$\\text{s.t.}\\; \\hat{s}_{t'+1} = \\hat{s}_{t'} + f_\\theta ( \\hat{s}_{t'}, a_t)$$\n",
    "\n",
    "However, solving this Equation is impractical because the learned dynamics model is imperfect, so using it to plan in such an open-loop manner will lead to accumulating errors over time and planning far into the future will become very inaccurate.\n",
    "\n",
    "We will solve this equation using sampling method (gradient-free optimization), where we will Sample $k$ random action sequences of length $H$, then we will use the model to predict the future states by taking each of these action sequences, then we will evaluate the reward with each candidate action sequence, and the last step will be to  select the best action sequence\n",
    "\n",
    "<img src=img/2.png width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Randomly sample uniformly num_random_action_selection number of action sequences, each of length mpc_horizon\n",
    "2. Starting from the input state, unroll each action sequence using your neural network dynamics model\n",
    "3. While unrolling the action sequences, keep track of the cost of each action sequence using cost_fn\n",
    "4. Find the action sequence with the lowest cost, and return the first action in that sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_random_action_selection= 3000\n",
    "mpc_horizon=10\n",
    "action_dim       = env.action_space.shape[0]\n",
    "action_space_low = env.action_space.low\n",
    "action_space_high= env.action_space.high \n",
    "actions = tf.random.uniform(\n",
    "            shape=[num_random_action_selection, mpc_horizon, action_dim],\n",
    "            minval=action_space_low,\n",
    "            maxval=action_space_high,\n",
    "            dtype=tf.float64\n",
    "        )\n",
    "print(\"The Shape of Actions: \", actions.shape)\n",
    "print(\"The first Sample: \", actions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Cost\n",
    "costs = tf.zeros(num_random_action_selection, dtype=tf.float64)\n",
    "print(\"The Shape of costs: \", costs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = env.reset()\n",
    "states = tf.stack([init_state] * num_random_action_selection)\n",
    "print(\"The Shape: \", states.shape)\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_fun(states,actions):\n",
    "    costh = states[:,0]\n",
    "    sinth = states[:,1]\n",
    "    thdot = states[:,2]\n",
    "    th = np.arctan2(sinth,costh)\n",
    "    th_normalize = (((th+np.pi) % (2*np.pi)) - np.pi)\n",
    "    action = np.clip(actions,-2.0, 2.0)[0]\n",
    "    costs = (th_normalize ** 2 + .1 * thdot ** 2 + .001 * (action ** 2))\n",
    "    return costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "planning in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = env.reset()\n",
    "states = tf.stack([init_state] * num_random_action_selection)\n",
    "for t in range(mpc_horizon):\n",
    "    states_norm = ml2_utils.normalize(states, random_dataset.state_mean,   random_dataset.state_std)\n",
    "    actions_norm = ml2_utils.normalize(actions[:, t, :], random_dataset.action_mean,random_dataset.action_std)\n",
    "    input_layer = tf.concat([states_norm,actions_norm ], axis=1)\n",
    "    # The resulting output is the normalized predicted difference between the next state and the current state\n",
    "    pred_diffs_norm = dynamic.predict(input_layer)\n",
    "    # calculate the cost\n",
    "    costs +=cost_fun(states, actions[:, t, :])\n",
    "    # The next State\n",
    "    next_states = states + ml2_utils.unnormalize(pred_diffs_norm, random_dataset.delta_state_mean,random_dataset.delta_state_std)\n",
    "    states = next_states\n",
    "# optimal sequence of actions\n",
    "action_seq  = actions[tf.argmin(costs)]\n",
    "# the first action that minimizes the cost function\n",
    "best_action = action_seq[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the optimal action sequence in the model (for evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy array\n",
    "pred_state_seq = np.asarray(pred_state_seq)\n",
    "pred_state_seq = [init_state]\n",
    "state = init_state\n",
    "for action in action_seq:\n",
    "    # Normailize the state and the action\n",
    "    state_norm = ml2_utils.normalize(state, random_dataset.state_mean,   random_dataset.state_std)\n",
    "    action_norm = ml2_utils.normalize(action, random_dataset.action_mean,random_dataset.action_std)\n",
    "    input_layer = tf.concat([state_norm, action_norm], axis=0)\n",
    "\n",
    "    # The resulting output is the normalized predicted difference between the next state and the current state\n",
    "    pred_diff_norm = dynamic.predict(tf.expand_dims(input_layer,0))\n",
    "    # The next State\n",
    "    next_state = state + ml2_utils.unnormalize(pred_diff_norm, random_dataset.delta_state_mean,random_dataset.delta_state_std)\n",
    "    state = next_state[0]\n",
    "    pred_state_seq.append(state)\n",
    "\n",
    "# convert to numpy array\n",
    "pred_state_seq = np.asarray(pred_state_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the optimal action sequence on the real system\n",
    "<img src=img/3.png width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_seq = [init_state]\n",
    "state = init_state\n",
    "for action in action_seq:\n",
    "    state, reward, done , _ = env.step(action)\n",
    "    state_seq.append(state)\n",
    "    env.render()\n",
    "    \n",
    "# convert to numpy array\n",
    "state_seq = np.asarray(state_seq)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# gripper position evaluation\n",
    "#resulting_states_list = np.rollaxis(np.array(resulting_states_list), 1)\n",
    "fig1, (ax1, ax2, ax3) = plt.subplots(figsize=(20,30), nrows=3, ncols=1)\n",
    "\n",
    "# plot the predicted state\n",
    "ax1.plot(np.arange(mpc_horizon+1), pred_state_seq[:,0], label='model prediction state 0')\n",
    "ax2.plot(np.arange(mpc_horizon+1), pred_state_seq[:,1], label='model prediction state 1')\n",
    "ax3.plot(np.arange(mpc_horizon+1), pred_state_seq[:,2], label='model prediction state 2')\n",
    "\n",
    "# plot real values\n",
    "ax1.plot(np.arange(mpc_horizon+1), state_seq[:,0], label='real state 0')\n",
    "ax2.plot(np.arange(mpc_horizon+1), state_seq[:,1], label='real state 1')\n",
    "ax3.plot(np.arange(mpc_horizon+1), state_seq[:,2], label='real state 2')\n",
    "\n",
    "# set axis lables\n",
    "for ax in (ax1, ax2, ax3):\n",
    "    ax.set_xlabel('step',fontsize='x-large')\n",
    "ax1.set_ylabel('Cos(θ)' ,fontsize='x-large')\n",
    "ax2.set_ylabel('sin(θ)' ,fontsize='x-large')\n",
    "ax3.set_ylabel('d(θ)'   ,fontsize='x-large')\n",
    "\n",
    "# plot legend\n",
    "ax1.legend(loc='best',fontsize='x-large')\n",
    "ax2.legend(loc='best',fontsize='x-large')\n",
    "ax3.legend(loc='best',fontsize='x-large')\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not Importent\n",
    "state = env.reset()\n",
    "for i in range(10):\n",
    "    actions = tf.random.uniform(\n",
    "            shape=[num_random_action_selection, mpc_horizon, action_dim],\n",
    "            minval=action_space_low,\n",
    "            maxval=action_space_high,\n",
    "            dtype=tf.float64\n",
    "        )\n",
    "    init_state = state\n",
    "    states = tf.stack([init_state] * num_random_action_selection)\n",
    "    costs = 0\n",
    "    \n",
    "    # find the optimal action sequence \n",
    "    for t in range(mpc_horizon):\n",
    "        # Normailize the state and the action\n",
    "        states_norm = ml2_utils.normalize(states, random_dataset.state_mean,   random_dataset.state_std)\n",
    "        actions_norm = ml2_utils.normalize(actions[:, t, :], random_dataset.action_mean,random_dataset.action_std)\n",
    "        input_layer = tf.concat([states_norm, actions_norm], axis=1)\n",
    "        # The resulting output is the normalized predicted difference between the next state and the current state\n",
    "        pred_diffs_norm = dynamic.predict(input_layer)\n",
    "        # calculate the cost\n",
    "        costs +=cost_fun(states, actions[:, t, :])                    \n",
    "        # The next State\n",
    "        next_states = states + ml2_utils.unnormalize(pred_diffs_norm, random_dataset.delta_state_mean,random_dataset.delta_state_std)\n",
    "        states = next_states\n",
    "    # optimal sequence of actions\n",
    "    action_seq  = actions[tf.argmin(costs)]\n",
    "    print(\"the cost of best action sequence: \", tf.reduce_min(costs))\n",
    "    # the action that minimizes the cost function\n",
    "    # run the action sequence on the real system\n",
    "    for best_action in action_seq:    \n",
    "        state, reward, done , _ = env.step(best_action)\n",
    "        env.render()\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replanning:\n",
    "<img src=img/4.png width=\"400\">\n",
    "\n",
    "1. Execute the first planned action a_t and observe the next state s_{t+1}\n",
    "2. Use model again to optimize the action sequenc a_{t+1},..., a_{t+H}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for i in range(50):\n",
    "    actions = tf.random.uniform(\n",
    "            shape=[num_random_action_selection, mpc_horizon, action_dim],\n",
    "            minval=action_space_low,\n",
    "            maxval=action_space_high,\n",
    "            dtype=tf.float64\n",
    "        )\n",
    "    init_state = state\n",
    "    states = tf.stack([init_state] * num_random_action_selection)\n",
    "    costs = 0\n",
    "    \n",
    "    # find the optimal action sequence \n",
    "    for t in range(mpc_horizon):\n",
    "        # Normailize the state and the action\n",
    "        states_norm = ml2_utils.normalize(states, random_dataset.state_mean,   random_dataset.state_std)\n",
    "        actions_norm = ml2_utils.normalize(actions[:, t, :], random_dataset.action_mean,random_dataset.action_std)\n",
    "        input_layer = tf.concat([states_norm, actions_norm], axis=1)\n",
    "        # The resulting output is the normalized predicted difference between the next state and the current state\n",
    "        pred_diffs_norm = dynamic.predict(input_layer)\n",
    "        # calculate the cost\n",
    "        costs +=cost_fun(states, actions[:, t, :])                    \n",
    "        # The next State\n",
    "        next_states = states + ml2_utils.unnormalize(pred_diffs_norm, random_dataset.delta_state_mean,random_dataset.delta_state_std)\n",
    "        states = next_states\n",
    "    #action_seq  = actions[tf.argmax(costs)]\n",
    "    print(\"the cost of best action sequence: \", tf.reduce_min(costs))\n",
    "    # the action that minimizes the cost function\n",
    "    best_action = actions[tf.argmin(costs)][0]\n",
    "    # run the actions on the real system\n",
    "    \n",
    "    state, reward, done , _ = env.step(best_action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tf2",
   "language": "python",
   "name": "venv_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
