{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e42763e",
   "metadata": {},
   "source": [
    "# Stochastic Simulation\n",
    "\n",
    "*Winter Semester 2024/25*\n",
    "\n",
    "22.11.2024\n",
    "\n",
    "Prof. Sebastian Krumscheid<br>\n",
    "Assistants: Stjepan Salatovic, Louise Kluge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5187ec",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">\n",
    "Exercise sheet 03\n",
    "</h3>\n",
    "\n",
    "---\n",
    "\n",
    "<h1 align=\"center\">\n",
    "Multivariate random variable generation and Gaussian random processes\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f636d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interact\n",
    "from scipy.stats import uniform, norm\n",
    "from scipy.integrate import quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588f46a8-3a2c-4af4-865e-cfa24a4370a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('axes', labelsize=14)     # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=12)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=12)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=14)    # legend fontsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6ec4b1",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Consider a multivariate Gaussian random variable $\\boldsymbol{X} = (X_1,X_2,\\dots,X_n)^T\\sim\\mathcal{N} (\\boldsymbol{\\mu},\\Sigma)$, with mean $\\boldsymbol{\\mu}\\in\\mathbb{R}^n$ and covariance matrix $\\Sigma\\in\\mathbb{R}^{n\\times n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09db19c",
   "metadata": {},
   "source": [
    "1. Generate a sample of $N=10^6$ independent random vectors $\\boldsymbol{X}_i$, $1\\le i\\le N$, each $\\boldsymbol{X}_i$ following a $\\mathcal{N}(\\boldsymbol{\\mu},\\Sigma)$ distribution with\n",
    "\n",
    "    \\begin{equation*}\n",
    "    \\boldsymbol{\\mu} = \\begin{pmatrix}2\\\\1 \\end{pmatrix}\n",
    "    \\quad\\text{and}\\quad\n",
    "    \\Sigma = \\begin{pmatrix} 1 & 2\\\\2 & 5\\end{pmatrix}\\;.\n",
    "    \\end{equation*}\n",
    "\n",
    "    Specifically, use the Cholesky decomposition `numpy.linalg.cholesky` to compute the factor $A$, such that $\\Sigma = A A^t$. Generate the standard normal vectors $\\boldsymbol{Y}_i \\sim \\mathcal{N}(0, I_{2 \\times 2}), 1 \\leq i \\leq N$ using `numpy.random.randn`. Then generate the $\\boldsymbol{X}_i$ as $\\boldsymbol{X}_i  = \\boldsymbol{\\mu} + A \\boldsymbol{Y}_i$. Assess the quality of the samples by  plotting a bivariate histogram using `plt.hist2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb4f86b9-6a45-44e3-96b3-1c94f7bc58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eede371",
   "metadata": {},
   "source": [
    "2. Propose a method for generating Gaussian random variables $\\boldsymbol{X}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\Sigma)$ with covariance matrix $\\Sigma = \\bigl(\\begin{smallmatrix} 1 & 2\\\\2 & 4\\end{smallmatrix} \\bigr)$ and mean $\\boldsymbol{\\mu}$ as before. Test your method by generating $N=10^6$ independent copies of the random vector and plot a bivariate histogram. Compare the outcomes with the previous point and explain the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6505d01-2f76-4c21-8616-215cd694f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6600b720",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Consider a Gaussian process $\\{X_t\\;,\\;t\\in I\\}$ on $I=[0,1]$ with\n",
    "mean function $\\mu_X\\colon I\\to \\mathbb{R}$,\n",
    "\n",
    "\\begin{equation*}\n",
    "\\large\n",
    "  \\mu_X(t) \\equiv \\mathbb{E}[X_t] = \\sin(2\\pi t)\\;,\n",
    "\\end{equation*}\n",
    "\n",
    "and covariance function $C_X\\colon I\\times I\\to  \\mathbb{R}$,\n",
    "\n",
    "\\begin{equation*}\n",
    "\\large\n",
    "  C_X(t,s) \\equiv \\mathbb{E}[(X_t-\\mu_X(t))(X_s-\\mu_X(s))] = e^{-\\lvert t-s\\rvert/\\rho}\\;,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\rho>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27315944",
   "metadata": {},
   "source": [
    "1. Generate the Gaussian process in a set of $n$ points $t_1,\\dots,t_n\\in I$. Plot the resulting point values of the random process for various values of $n$ and $\\rho$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c50347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu(t):\n",
    "    \"\"\"Mean function.\"\"\"\n",
    "    # TODO\n",
    "    return\n",
    "\n",
    "def cov(t, s, rho):\n",
    "    \"\"\"Covariance function.\"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4726f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_process(n: int, rho: float) -> np.array:\n",
    "    \"\"\"\n",
    "    Generates a Gaussian process in `n` points.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35513e62",
   "metadata": {},
   "source": [
    "2. Generate the Gaussian process for $\\rho=1/200$ on a uniform partition of $I=[0,1]$ with $n=51$ points, i.e. $t_{i}=\\frac{i-1}{n-1}$ for $i=1\\dots,n$.  Let's denote this collection of point-wise evaluations of $\\{X_t,\\,t\\in I\\}$ by $\\boldsymbol{Z}_n$.  Then generate $m=n-1=50$ additional point evaluations of the Gaussian process in new points $t_{n+1},\\dots, t_{n+m}$ by a uniform grid refinement (i.e.$t_{n+j}=\\frac{2j-1}{2(n-1)}$ for $j=1,\\dots,m=n-1$), denoted by $\\boldsymbol{Y}_m$, conditioned upon the previously generated ones $\\boldsymbol{Z}_n$. Specifically, use the results for conditioned multivariate Gaussian random variables discussed in the lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc9b4874-bcf5-489e-ac51-67349538bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_gaussian_process(n: int, rho: float) -> np.array:\n",
    "    \"\"\"\n",
    "    Refines a Gaussian process by generating new points conditioned on a previous process.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d0264",
   "metadata": {},
   "source": [
    "**Remark:** The problem is amenable to treatment using FFT and circulant embeddings, a topic that will be covered during the next course lecture. You can revisit this problem and compare the direct generation using Cholesky factorization, which has a cost of $\\mathcal{O}(n^3)$, with the cost of the FFT, which grows as $\\mathcal{O}(n\\log n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf6c0da-4044-4ac8-9560-8bfc17c992a5",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "The lecture notes introduce a Brownian bridge as a Wiener process\n",
    "$\\{W_t\\;,\\;t\\in [0,1]\\}$ conditioned upon $W_1 = b$. Derive a\n",
    "generalized Brownian bridge $\\{X_t\\;,\\;t\\in [0,1]\\}$ that is given as\n",
    "the Wiener process conditioned on $W_0 = a$ and $W_1 = b$ and generate\n",
    "realizations of this Brownian bridge at\n",
    "$0=t_0<t_1<\\dots <t_n<t_{n+1}=1$.  Specifically, carry out the\n",
    "following exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6897813-2c76-44a7-8114-ac89452a277d",
   "metadata": {},
   "source": [
    "1. Show that $$\\mu_X(t) = a + (b-a)t$$\n",
    "  and $$C_X(t,s) = \\min\\{s,t\\} - st\\;.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf377d8-2566-4c0a-9505-82c81b1f55e1",
   "metadata": {},
   "source": [
    "2. Propose and implement an iterative algorithm that generates\n",
    "  $X_{t_i}$ conditioned upon $X_{t_{i-1}}$ and\n",
    "  $X_{t_{n+1}}  = b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2180391f-ade3-44e6-8f84-2e87780a0b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_brownian_bridge(n: int, a: float, b: float, t0: float=0, T: float=1) -> np.array:\n",
    "    \"\"\"\n",
    "    Generates the Brownian bridge with start (end) in `a` (`b`) with conditioning upon the previous X_{t-1}.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f1eb00-1b94-4de8-a2e8-f44c61498ee5",
   "metadata": {},
   "source": [
    "## Exercise 4 (Optional)\n",
    "\n",
    "Let $\\xi_{-1}\\sim \\mathcal{N}(0,1)$ and denote by $Y_0(t)$ the linear function on $[0,1]$ with $Y_0(0)=0$, and $Y_0(1)=\\xi_{-1}$. For $j\\in \\mathbb{N}$ and $N=0,1,2\\dots$, let $t_j^N=2^{-N}j$ and let $Y_N(t)$ be the piecewise linear function such that \n",
    "\\begin{align}\n",
    "Y_{N+1}(t_{2j}^{2N})&=Y_{N}(t_j^N)\\\\\n",
    "Y_{N+1}(t_{2j+1}^{N+1})&=\\frac{1}{2}\\left(Y_{N}(t_j^N)+Y_{N}(t_{j+1}^N)\\right)+\\xi_{j,N},\\quad \\xi_{j,N}\\overset{\\text{iid}}{\\sim}\\mathcal{N}(0,2^{-N-2}).\n",
    "\\end{align} Here $N$ is to be understood as a \"discretization level\" of the interval $[0,1]$ on equal sub-intervals of length $2^{-N}$. This process is known as the _LÃ©vy-Ciesielski construction of a Brownian motion_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8fe84b-a483-4490-a2c0-82a731315ec0",
   "metadata": {},
   "source": [
    "1. Simulate the previous process for different values of $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d8d5fbd-3324-46ea-930d-ee7de058c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levy_ciesielski(N: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Generates a LÃ©vy-Ciesielski process with `N` refinements.\n",
    "    Returns only the last (finest) process with `2 ** N + 1` elements.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2310fe-6956-4601-a920-376055dcaa8c",
   "metadata": {},
   "source": [
    "2. Prove that for any $N\\in \\mathbb{N}$, $\\mathbb{E}[Y_N(t_j^N)]=0$ and $\\mathrm{Cov}\\left(Y_N(t_j^N),\\, Y_N(t_k^N)  \\right)=\\min\\{t_j^N,t_k^N\\},$ with $j,k=0,\\dots 2^N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e20fe0-67aa-4128-8500-e7f265f2ec7a",
   "metadata": {},
   "source": [
    "3. For any $s,t \\in [0,1]$ prove that $\\mathbb{E}[Y_N(t)] \\to 0$ and $\\text{Cov}(Y_N(t),\\, Y_N(s)) \\to \\min\\{ t,s\\}$ as $N \\to \\infty$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
