{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /home/ws/vy9905/venvs/ml_env/lib/python3.8/site-packages (1.6.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import seaborn as sns\n",
    "%pip install torchinfo\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block Structure in CNNs\n",
    "\n",
    "A Block contains multiple convolutional layers, normalization layers, activation layers and often times skip connections.\n",
    "\n",
    "\n",
    "\n",
    "Most modern CNNs contain 5 network layers, which are also called ***network stages*** to prevent confusion with convolutional layers.\n",
    "\n",
    "\n",
    "Each network stage reduces the height and widht of the feature maps and \n",
    "doubles the number of channels and contains multiple blocks.\n",
    "\n",
    "If we want to increase the size of our network, we increase the number of blocks inside our network stages. We very rarely / never increase the number of network stages.\n",
    "\n",
    "This means, that we mostly only use a single block and then stack it in the network stages as often as we want.\n",
    "\n",
    "Let's look at one block: The residual block of the most famous CNN: ResNet\n",
    "\n",
    "![](https://miro.medium.com/max/816/0*rj6SSHtBfBZXKSgC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight layers are two convolutional layers. Betwen them we have a ReLU-activation function and a batch normalization (not displayed here). We also have a skip connection (also called residual connection or shortcut) that adds the input of the block on top of the output of the two convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of different ResNets\n",
    "*    resnet18,\n",
    "*    resnet34,\n",
    "*    resnet50,\n",
    "*    resnet101,\n",
    "*    resnet152,\n",
    "*    resnext50_32x4d,\n",
    "*    resnext101_32x8d,\n",
    "*    wide_resnet50_2,\n",
    "*    wide_resnet101_2,\n",
    "\n",
    "We will look at ResNet18, ResNet34 and ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture of ResNet\n",
    "\n",
    "![](https://git.scc.kit.edu/vy9905/ml2images/-/raw/main/resnet34.PNG)\n",
    "\n",
    "\n",
    "![](https://pytorch.org/assets/images/resnet.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build a ResNet18\n",
    "\n",
    "Stages 2-5 contain two residual blocks each.\n",
    "But the second residual block reduces the resolution of the feature map but\n",
    "double the number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample=False):\n",
    "\n",
    "        # In the init we decalare the attributes of the block (mostly layers)\n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "\n",
    "        if not downsample:\n",
    "            # standard convolutional layer that keeps resolution constant\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False)\n",
    "        else:\n",
    "            # convolutional layer with stride 2. The output resolution of the feature map\n",
    "            # has half the width and half the height of the input feature map\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=2, bias=False)\n",
    "            \n",
    "            # at the end of the residual block, we add the input to the output (skip connection)\n",
    "            # but if we reduce the resolution of the output (with stride 2) the resolutions don't match\n",
    "            # so we also have to reduce the resolution of the input\n",
    "            # While reducing the resolution we also increase the number of channels\n",
    "            # so we also use out_channels here to double the number of channels\n",
    "            self.downsample_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2, bias=False)\n",
    "            self.downsample_bn = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "\n",
    "    def forward(self,input):\n",
    "        \n",
    "        # the forward method calculates the result of the block\n",
    "        conv1_out = self.conv1(input)\n",
    "        bn1_out = self.bn1(conv1_out)\n",
    "        relu1_out = F.relu(bn1_out)\n",
    "        \n",
    "        conv2_out = self.conv2(relu1_out)\n",
    "        bn2_out = self.bn2(conv2_out)\n",
    "\n",
    "        if self.downsample == True:\n",
    "            print(\"before downsampling, input had shape: \", input.shape)\n",
    "\n",
    "            # half the width and height of input feature map if the block is downsampling\n",
    "            # this also doubles the channel size\n",
    "            input = self.downsample_conv(input)\n",
    "            input = self.downsample_bn(input)\n",
    "            print(\"after downsampling, input has shape: \", input.shape)\n",
    "\n",
    "        \n",
    "        # skip connection\n",
    "        output = input + bn2_out\n",
    "        print(\"output shape of residual block after skip connection \",bn2_out.shape)\n",
    "\n",
    "        return F.relu(output)\n",
    "\n",
    "# create an example feature map with 64 channels and 56x56 resolutions\n",
    "example_input = torch.zeros(1,64,56,56)\n",
    "\n",
    "# create a residual block, that takes 64 input channels, returns 64 output channels and doesn't reduce the resolution\n",
    "res_block_without_downsample = ResidualBlock(in_channels = 64, out_channels=64, downsample=False)\n",
    "\n",
    "# feed our example input through the residul block\n",
    "output_no_downsample = res_block_without_downsample(example_input)\n",
    "print(f\"exemplary input tensor had shape {example_input.shape} and our output has shape {output_no_downsample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first block in each stage reduces height and widht of the input by half.\n",
    "It also doubles the channels of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take an input with 64 channels and ouptut feature map with 128 channels\n",
    "res_block_with_downsample = ResidualBlock(64, 128, downsample=True)\n",
    "output_downsample = res_block_with_downsample(example_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network stages 2-5 contain only residual blocks.\n",
    "Now lets look at the first stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1 (also called Stem)\n",
    "\n",
    "Convolutional layer with kernel size 7*7 and stride 2.\n",
    "Then regular maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = nn.Sequential(\n",
    "    # conv halfs width and height and creates 64 channel feature map\n",
    "    nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    # maxpool also halfs width and height, channels stay the same\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")\n",
    "\n",
    "# lets try it out\n",
    "example_input_image = torch.zeros(1,3,224,224)\n",
    "\n",
    "print(f\"if we give an input with shape {example_input_image.shape} then the stem returns an output with shape {stem(example_input_image).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Layer\n",
    "\n",
    "Takes the last feature map as input. Then calculates the average for each channel.\n",
    "Then uses one fully connected layer where the number of predicted classes is the number of neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaptive avg pool calculates the average for each channel\n",
    "# if we have feature maps with 7x7 resolution and 512 channels \n",
    "# we get the average for each 7x7 map --> 512 output values.\n",
    "\n",
    "# if we have feature maps with 25x25 resolution and 512 channels\n",
    "# we get the average for each 25x25 map --> 512 output values.\n",
    "\n",
    "# --> Convolutional Layer don't care about the input resolution but\n",
    "# Fully Connected layers do. With adaptive average pool we can input every\n",
    "# resolution because the fully connected layer always gets 512 input values.\n",
    "# --> CNN works for multiple different resolutions\n",
    "\n",
    "example_feature_map1 = torch.zeros(1,512,7,7)\n",
    "example_feature_map2 = torch.zeros(1,512,25,25)\n",
    "\n",
    "adap_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "print(f\"for the first feature map we get shape {adap_pool(example_feature_map1).shape} and the second feature map we get shape {adap_pool(example_feature_map2).shape}\")\n",
    "\n",
    "def get_classification_layer(channels):\n",
    "    return nn.Sequential(\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        nn.Flatten(),\n",
    "        # standard imagenet has 1000 classes\n",
    "        nn.Linear(in_features=channels, out_features=1000),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build the resnet\n",
    "\n",
    "![](https://pytorch.org/assets/images/resnet.png)\n",
    "\n",
    "Compare ResNet18 with ResNet34. \n",
    "Only the number of residual blocks in the network stages is different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "           \n",
    "        self.stage1 = stem\n",
    "        self.stage2 = nn.Sequential(\n",
    "            ResidualBlock(64, 64, downsample=False),\n",
    "            ResidualBlock(64, 64, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.stage3 = nn.Sequential(\n",
    "            ResidualBlock(64, 128, downsample=True),\n",
    "            ResidualBlock(128, 128, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.stage4 = nn.Sequential(\n",
    "            ResidualBlock(128, 256, downsample=True),\n",
    "            ResidualBlock(256, 256, downsample=False)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.stage5 = nn.Sequential(\n",
    "            ResidualBlock(256, 512, downsample=True),\n",
    "            ResidualBlock(512, 512, downsample=False)\n",
    "        )\n",
    "        \n",
    "        self.classification_layer = get_classification_layer(512)\n",
    "\n",
    "    def forward(self, input):\n",
    "        stage1_output = self.stage1(input)\n",
    "        stage2_output = self.stage2(stage1_output)\n",
    "        stage3_output = self.stage3(stage2_output)\n",
    "        stage4_output = self.stage4(stage3_output)\n",
    "        stage5_output = self.stage5(stage4_output)\n",
    "        classification = self.classification_layer(stage5_output)\n",
    "        return classification\n",
    "\n",
    "summary(ResNet18())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ofcourse ResNet18 is also implemented in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(torchvision.models.resnet18())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to create a ResNet34, we only need to increase the number of residual blocks for each network stage.\n",
    "from [2,2,2,2] to [3,4,6,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stage1 = stem\n",
    "        \n",
    "        self.stage2 = nn.Sequential(\n",
    "            ResidualBlock(64, 64, downsample=False),\n",
    "            ResidualBlock(64, 64, downsample=False),\n",
    "            ResidualBlock(64, 64, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.stage3 = nn.Sequential(\n",
    "            ResidualBlock(64, 128, downsample=True),\n",
    "            ResidualBlock(128, 128, downsample=False),\n",
    "            ResidualBlock(128, 128, downsample=False),\n",
    "            ResidualBlock(128, 128, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.stage4 = nn.Sequential(\n",
    "            ResidualBlock(128, 256, downsample=True),\n",
    "            ResidualBlock(256, 256, downsample=False),\n",
    "            ResidualBlock(256, 256, downsample=False),\n",
    "            ResidualBlock(256, 256, downsample=False),\n",
    "            ResidualBlock(256, 256, downsample=False),\n",
    "            ResidualBlock(256, 256, downsample=False),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.stage5 = nn.Sequential(\n",
    "            ResidualBlock(256, 512, downsample=True),\n",
    "            ResidualBlock(512, 512, downsample=False),\n",
    "            ResidualBlock(512, 512, downsample=False)\n",
    "        )\n",
    "        \n",
    "        self.classification_layer = get_classification_layer(512)\n",
    "\n",
    "    def forward(self, input):\n",
    "        stage1_output = self.stage1(input)\n",
    "        stage2_output = self.stage2(stage1_output)\n",
    "        stage3_output = self.stage3(stage2_output)\n",
    "        stage4_output = self.stage4(stage3_output)\n",
    "        stage5_output = self.stage5(stage4_output)\n",
    "        classification = self.classification_layer(stage5_output)\n",
    "        return classification\n",
    "\n",
    "summary(ResNet34())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from pytorch\n",
    "summary(torchvision.models.resnet34())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet 50\n",
    "\n",
    "ResNet50 is probably the most used CNN. But its residual block is different.\n",
    "It is called a \"bottleneck\" block.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Kai-Heinrich-3/publication/337486420/figure/fig4/AS:829046489374720@1574671494601/Standard-Residual-vs-Bottleneck-Residual-Block-He-et-al-2015.ppm)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why use a different block for deep Networks?\n",
    "\n",
    "Training a neural net requires (GPU-)memory.\n",
    "We need to store the following values:\n",
    "\n",
    "* Each parameter (weight and bias of Conv/FC layer)\n",
    "* The gradient of each parameter (after backpropagation)\n",
    "    * If Adam is used, also the adaptive learnrate for each parameter\n",
    "* All feature maps (including input image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's calculate the number of stored parameters in the fifth network stage of ResNet34\n",
    "We have 3 residual blocks with 2 convolutional layers each.\n",
    "\n",
    "Number of params in Convolutional Layer $= (w \\times h \\times c + 1) *numberFilter)$\n",
    "\n",
    "* First residual block\n",
    "    * First conv layer $= (3 \\times 3 \\times 256 + 1) *512) =1,180,160 $\n",
    "    * Second conv layer $= (3 \\times 3 \\times 512 + 1) *512) =2,359,808$\n",
    "    * Skip   conv layer $= (1 \\times 1 \\times 256 + 1) * 512) = 131,584$\n",
    "    * Total $3,671,552$ params\n",
    "* Second residual block\n",
    "    * First conv layer $= (3 \\times 3 \\times 512 + 1) *512) =2,359,808 $\n",
    "    * Second conv layer $= (3 \\times 3 \\times 512 + 1) *512) =2,359,808$\n",
    "    * Total $4,719,616$ params\n",
    "* Third residual block\n",
    "    * identical to second residual block\n",
    "\n",
    "\n",
    "Total $=3,671,552 + 4,719,616 + 4,719,616 = 13,110,784$ parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottleneck Blocks\n",
    "\n",
    "The number of parameters scales with number of input channels and number of output channels (filters).\n",
    "If we increase the number of filters the number of parameters increases dramatically.\n",
    "\n",
    "If we use $1\\times1$ convolutions, instead of $3\\times3$ convolutions, the number of needed parameters is only about ~$1/9$\n",
    "\n",
    "We use this trick to reduce the number of channels for our $3\\times3$ convolution but keep them high for $1\\times1$ convolutions. \n",
    "\n",
    "First we use one $1\\times1$ convolutional filter to reduce the number of channels. Then we filter with $3\\times3$ convolution\n",
    "on the reduced number of channels. Then we increase the number of channels back again with $1\\times1$ convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the number of stored parameters in the fith network stage of ResNet50\n",
    "\n",
    "* First bottleneck block\n",
    "\n",
    "    * First conv layer  $= (1 \\times 1 \\times 1024 + 1) *512) =524,800 $\n",
    "    \n",
    "    * Second conv layer $= (3 \\times 3 \\times 512 + 1) *512) =2,359,808 $\n",
    "    \n",
    "    * Third conv layer  $= (1 \\times 1 \\times 512 + 1) *2048) =1,050,624 $\n",
    "\n",
    "    * Skip   conv layer $= (1 \\times 1 \\times 1024 + 1) *2048) = 2,099,200$\n",
    "\n",
    "    * Total $524,800 + 2,359,808 + 2,099,200 + 1,050,624= 6,034,432$ params\n",
    "\n",
    "* Second bottleneck block\n",
    "\n",
    "    * First conv layer $= (1 \\times 1 \\times 2048 + 1) *512) =1,049,088 $\n",
    "\n",
    "    * Second conv layer $= (3 \\times 3 \\times 512 + 1) *512) =2,359,808 $\n",
    "    \n",
    "    * Third conv layer $= (1 \\times 1 \\times 512 + 1) *2048) =1,050,624 $\n",
    "\n",
    "    * Total $1,049,088 + 2,359,808 + 1,050,624 = 4,459,520$ params\n",
    "\n",
    "* Third bottleneck block\n",
    "\n",
    "    * identical to second bottleneck layer\n",
    "\n",
    "Total $= 6,034,432 + 4,459,520 + 4,459,520 =14,953,472$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's higher than our ResNet34 which uses two $3 \\times 3$ filter. But remember,\n",
    "we increased our channelsize from 512 to 2048. \n",
    "\n",
    "\n",
    "### What would happen if we used the normal residial block and increased its channelsize to 2048 in the last network stage?\n",
    "\n",
    "The first block contain\n",
    "$2,103,296 + 18,876,416 + 37,750,784 = 58.730.496$ params\n",
    "\n",
    "And in its second and third block contain  \n",
    "$37,750,784+37,750,784 = 75.501.568$ params\n",
    "\n",
    "So in total 117 million parameter which is way higher than the 15 million parameters of ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels//4, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels//4)\n",
    "\n",
    "        # decrease feature map resolution in second layer if we want to downsample with stride=2\n",
    "        self.conv2 = nn.Conv2d(out_channels//4, out_channels//4, kernel_size=3, stride=2 if downsample else 1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels//4)\n",
    "      \n",
    "        self.conv3 = nn.Conv2d(out_channels//4, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)      \n",
    "        \n",
    "        \n",
    "        if self.downsample or (in_channels != out_channels):\n",
    "                self.skip_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2 if self.downsample else 1, bias=False)\n",
    "                self.skip_bn = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        conv1_out = self.conv1(input)\n",
    "        bn1_out = self.bn1(conv1_out)\n",
    "        relu1_out = F.relu(bn1_out)\n",
    "        \n",
    "        conv2_out = self.conv2(relu1_out)\n",
    "        bn2_out = self.bn2(conv2_out)\n",
    "        relu2_out = F.relu(bn2_out)\n",
    "        \n",
    "        conv3_out = self.conv3(relu2_out)\n",
    "        bn3_out = self.bn3(conv3_out)\n",
    "\n",
    "        if self.downsample or (self.in_channels != self.out_channels):\n",
    "            input = self.skip_conv(input)\n",
    "            input = self.skip_bn(input)\n",
    "\n",
    "\n",
    "        # skip connection\n",
    "        output = input + bn3_out\n",
    "\n",
    "        return F.relu(output)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet50 is identical to ResNet34 but it uses BottleNeck blocks instead of Residual Blocks and increased channel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stage1 = stem\n",
    "        \n",
    "        self.stage2 = nn.Sequential(\n",
    "            BottleNeckBlock(64, 256, downsample=False),\n",
    "            BottleNeckBlock(256, 256, downsample=False),\n",
    "            BottleNeckBlock(256, 256, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.stage3 = nn.Sequential(\n",
    "            BottleNeckBlock(256, 512, downsample=True),\n",
    "            BottleNeckBlock(512, 512, downsample=False),\n",
    "            BottleNeckBlock(512, 512, downsample=False),\n",
    "            BottleNeckBlock(512, 512, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.stage4 = nn.Sequential(\n",
    "            BottleNeckBlock(512, 1024, downsample=True),\n",
    "            BottleNeckBlock(1024, 1024, downsample=False),\n",
    "            BottleNeckBlock(1024, 1024, downsample=False),\n",
    "            BottleNeckBlock(1024, 1024, downsample=False),\n",
    "            BottleNeckBlock(1024, 1024, downsample=False),\n",
    "            BottleNeckBlock(1024, 1024, downsample=False),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.stage5 = nn.Sequential(\n",
    "            BottleNeckBlock(1024, 2048, downsample=True),\n",
    "            BottleNeckBlock(2048, 2048, downsample=False),\n",
    "            BottleNeckBlock(2048, 2048, downsample=False)\n",
    "        )\n",
    "        \n",
    "        self.classification_layer = get_classification_layer(2048)\n",
    "\n",
    "    def forward(self, input):\n",
    "        stage1_output = self.stage1(input)\n",
    "        stage2_output = self.stage2(stage1_output)\n",
    "        stage3_output = self.stage3(stage2_output)\n",
    "        stage4_output = self.stage4(stage3_output)\n",
    "        stage5_output = self.stage5(stage4_output)\n",
    "        classification = self.classification_layer(stage5_output)\n",
    "        return classification\n",
    "\n",
    "summary(ResNet50())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ofcourse ResNet50 is already included in PyTorch and we don't have to build it ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = torchvision.models.resnet50()\n",
    "summary(resnet50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ConvNext](https://arxiv.org/pdf/2201.03545.pdf) (Released 22.03.2022)\n",
    "\n",
    "ConvNext is one of the best CNNs currently.\n",
    "But it is pretty similar to a ResNet. It still contains 5 network stages but uses different blocks\n",
    "\n",
    "![](https://git.scc.kit.edu/vy9905/ml2images/-/raw/main/convnext_architecture.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Stem, ResNet uses a 7x7 strided conv layer and a maxpool.\n",
    "ConvNext uses a 4x4 convolutional layer with stride 4 (so there is no overlap between the filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convnext_stem = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=96, kernel_size=4, stride=4),\n",
    ")\n",
    "\n",
    "example_input_image = torch.zeros(1,3,224,224)\n",
    "print(f\"the stem reduces height and width by four: input shape was  {example_input_image.shape}, output shape is {convnext_stem(example_input_image).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "\n",
    "            # depthwise convolution\n",
    "            # each filter only looks at one channel, in contrast to normal convolution\n",
    "            # massively reduces the number of params\n",
    "            # depthwise (7x7+1)*C\n",
    "            # normal (7x7xC+1)*C \n",
    "            nn.Conv2d(in_channels = channels, out_channels = channels, kernel_size=7, padding=3, groups=channels),\n",
    "            Permute([0, 2, 3, 1]), # We need to implement Permute (not that important)\n",
    "            nn.LayerNorm(channels), \n",
    "            Permute([0, 3, 1, 2]), # We need to implement Permute (not that important)\n",
    "            nn.Conv2d(in_channels = channels, out_features=4 * channels, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(in_channels=4 * channels, out_channels=channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        ## here it's a little more complicated, but not by much\n",
    "        # result = layer_scale*self.block(input)\n",
    "        # result = stochastic_depth(result)\n",
    "\n",
    "        # this here is the normal skip connection\n",
    "        result = self.block(input)\n",
    "        output = result + input\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection and Object Segmentation with CNNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## util functions\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = transforms.functional.to_pil_image(img)\n",
    "        display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection\n",
    "\n",
    "It's very computationally expensive to train an object detector so we use existing ones from PyTorch and take their trained weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object detectors from PyTorch can detect the following classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "color_palette = sns.color_palette(\"husl\",len(CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import urllib\n",
    "urllib.request.urlretrieve(\"https://git.scc.kit.edu/vy9905/ml2images/-/raw/main/eva.JPG\", \"eva.JPG\")\n",
    "\n",
    "\n",
    "orig_img = read_image(\"eva.JPG\")\n",
    "print(orig_img)\n",
    "\n",
    "show(orig_img)\n",
    "\n",
    "# create batch (with only one image)\n",
    "scaled_img = (orig_img / 255).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create our own Single-Stage Object Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need Anchor Boxes.\n",
    "For this we save the width and height of all objects in our training dataset.\n",
    "Then we create with k-means $k$ anchor boxes.\n",
    "\n",
    "The following 8 anchor boxes were created from the nuScenes dataset, which contains 1,4 million annotated objects.\n",
    "\n",
    "![](https://git.scc.kit.edu/vy9905/ml2images/-/raw/main/colored_anch_boxes.png)\n",
    "![](https://git.scc.kit.edu/vy9905/ml2images/-/raw/main/shape_anchs.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anchors():\n",
    "    # already calculated with kmeans from nuScenes\n",
    "    return torch.tensor([\n",
    "        [ 15.58690644,  33.45222749],\n",
    "        [ 38.85966844,  25.02378713],\n",
    "        [245.40705481, 269.29423133],\n",
    "        [129.14296132,  62.39221484],\n",
    "        [202.20420614, 109.16240505],\n",
    "        [ 34.61709156,  64.33043907],\n",
    "        [ 69.99003352, 166.50778584],\n",
    "        [ 76.74906765,  39.93999905]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(f\"we have {len(get_anchors())} anchors \\n {get_anchors()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetector(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.anchors = get_anchors()\n",
    "        self.resnet = ResNet50()\n",
    "\n",
    "        # use all network stages from ResNet but remove the classification layer\n",
    "        self.backbone = nn.Sequential(self.resnet.stage1, self.resnet.stage2, self.resnet.stage3, self.resnet.stage4, self.resnet.stage5)\n",
    "        \n",
    "        # how many channels are needed for object detection\n",
    "        prediction_channels = len(self.anchors) * (4 + len(CLASSES))\n",
    "        \n",
    "        # last feature map of ResNet has 2048 channels. \n",
    "        self.detection_layer = nn.Conv2d(in_channels = 2048, out_channels = prediction_channels, kernel_size = 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.backbone(x)\n",
    "        x = self.detection_layer(x)\n",
    "        return x\n",
    "\n",
    "my_OD = ObjectDetector()\n",
    "example_input_image = torch.zeros(1,3,224,224)\n",
    "object_detector_output = my_OD(example_input_image)\n",
    "object_detector_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an input image with resolution $224 \\times 224$ our backbone returns a feature map with $7\\times7$ resolution.\n",
    "Now, we have to convert this feature map into detection predictions.\n",
    "\n",
    "Each pixel predicts $k$ bounding boxes (one for each anchor box).\n",
    "A bounding box contains 4 values for it's position and size, and $num\\_classes$ values for the class prediction of the box.\n",
    "\n",
    "For $8$ anchor boxes and a final feature map resolution of $7\\times7$ pixels we predict $7*7*8 = 392$ bounding boxes. It is very unlikely that our input image contains\n",
    "this many objects, so most bounding boxes should be class \"background\".\n",
    "\n",
    "Each pixel predicts $k * (4 + num\\_classes)$ values --> so each pixel needs $k * (4 + num\\_classes)$ channels to store these values\n",
    "\n",
    "We use $8$ anchor boxes and have $91$ classes $8*(4+91) = 760$ channels\n",
    "\n",
    "**Caution**: num_classes must contain the background class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://git.scc.kit.edu/vy9905/ml2images/-/raw/main/anker_vis.PNG)\n",
    "\n",
    "Our model predicts 760 values for each pixel.\n",
    "\n",
    "For each anchor, it predicts $t_x, t_y, t_w, t_h$ and 91 classification values\n",
    "\n",
    "From these model outputs we want to calculate the $x$, $y$, $w$ and $h$ of our bounding box\n",
    "\n",
    "$x, y$ is the pixel position of the center of the bounding box\n",
    "\n",
    "$x = (c_x + \\sigma(t_x)) * 32$\n",
    "\n",
    "$y = (c_y + \\sigma(t_y)) * 32$\n",
    "\n",
    "Our final feature map has a widht and height 32 times smaller than our input image ($224 \\times 224$ --> $7 \\times 7$)\n",
    "This is caused by our 5 network stages that each reduce the width and height by half --> $5^2 = 32$\n",
    "\n",
    "We want to place the boxes on top of the input image so we need to multipy the values of our final feature with the reduction factor\n",
    "which is (always) 32\n",
    "\n",
    "\n",
    "\n",
    "We use sigmoid to force a value between (0,1)\n",
    "So the middle of the bounding box has to be inside the last feature map pixel.\n",
    "\n",
    "for $t_x = -\\infty $ we get $x = (c_x + 0)*32 = c_x*32$\n",
    "\n",
    "for $t_x = \\infty $ we get $x = (c_x + 1)*32 $\n",
    "\n",
    "for $t_x = 0 $ we get $x = (c_x + 1/2)*32$\n",
    "\n",
    "lets say $c_x = 0$ so we are at the left most position.\n",
    "For $-\\infty$ we stay at 0, for $\\infty$ the center of our bounding box is at pixel position 32\n",
    "\n",
    "if we have $c_x = 6$ so we are at the most right position of our feature map. If our model predicts $\\infty$ we get the position $6*32+32 = 224$ the most right position of our image\n",
    "\n",
    "This is identical for $y$ and $t_y$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position(width, height):\n",
    "    lin_x = torch.linspace(0, width - 1, width).repeat(height, 1).view(height * width)\n",
    "    lin_y = torch.linspace(0, height - 1, height).repeat(width, 1).t().contiguous().view(height * width)\n",
    "    return lin_x, lin_y\n",
    "\n",
    "get_position(7,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For width and height of the bounding box we use the corresponding anchor box.\n",
    "\n",
    "Each anchor box contains a width $p_w$ and height $p_h$\n",
    "We use the exponential function to force a value between $(0, \\infty)$\n",
    "\n",
    "$w = p_w * e^{t_w} $\n",
    "\n",
    "$h = p_h * e^{t_h} $\n",
    "\n",
    "for $t_w = - \\infty$ --> $w = 0$\n",
    "\n",
    "for $t_w = \\infty$ --> $w = \\infty$\n",
    "\n",
    "for $t_w = 0$ --> $w = p_w$\n",
    "\n",
    "If model predicts $0$ we use the shape of the anchor box. Because the anchor boxes are generated from the training data, this is a good estimate.\n",
    "So initially the model learns to always predict 0. Then it starts to deviate around 0 to make the bounding box better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def head(model_output):\n",
    "    model_output = model_output.detach().clone()\n",
    "    anchors = get_anchors()\n",
    "    anchor_w = anchors[:, 0].contiguous().view(8, 1)\n",
    "    anchor_h = anchors[:, 1].contiguous().view(8, 1)\n",
    "    print(\"widht of anchors \", anchor_w)\n",
    "    print(\"height of anchors \", anchor_h)\n",
    "    \n",
    "    \n",
    "    height = model_output.data.size(2)\n",
    "    width = model_output.data.size(3)\n",
    "    \n",
    "\n",
    "    lin_x, lin_y = get_position(width, height)\n",
    "    # split 760 channel into 8 anchor boxes and 95 bbox regressions/classifications \n",
    "    # merge 7,7 pixels into one dimension into 49\n",
    "\n",
    "    preds = model_output.detach().view(-1, len(anchors),\n",
    "                                           4+91,\n",
    "                                           height*width)\n",
    "\n",
    "    print(\"we reshaped our model output to \",preds.shape)\n",
    "    # sigmoid output and add x position\n",
    "    preds[:, :, 0, :].sigmoid_().add_(lin_x).mul_(32)\n",
    "    # sigmoid output and add y postion\n",
    "    preds[:, :, 1, :].sigmoid_().add_(lin_y).mul_(32)\n",
    "    # use exponential function and multiply with anchor\n",
    "    preds[:, :, 2, :].exp_().mul_(anchor_w)\n",
    "        # use exponential function and multiply with anchor\n",
    "    preds[:, :, 3, :].exp_().mul_(anchor_h)\n",
    "    \n",
    "    # use softmax for classification\n",
    "    # the index with the highest value determines the class of the \n",
    "    # bounding box\n",
    "    preds[:,:,4:96,:] = F.softmax(preds[:,:,4:96,:], dim=1)\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "m_output = head(object_detector_output)\n",
    "\n",
    "print(f\"leftmost center {torch.min(m_output[:,:,0,:])} rightmost center {torch.max(m_output[:,:,0,:])}\")\n",
    "print(f\"thinnest bounding box: {torch.min(m_output[:,:,2,:])}  widest bounding box {torch.max(m_output[:,:,2,:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Exercise in Exam?\n",
    "\n",
    "You use a standard single-stage object detector that detects the classes dog and cat.\n",
    "You use two anchor boxes with size (50,100) and (20,20).\n",
    "You want to classify a 320*320 RGB input image.\n",
    "\n",
    "Questions:\n",
    "\n",
    "1. What is the resolution of the final feature map of our object detector?\n",
    "2. How many channels does our final feature map contain?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "1. We always use 32 as reduction factor --> 320 / 32 = 10x10 resolution\n",
    "2. Formula is $k*(4+num\\_classes)$ $k$ are our 2 anchor boxes. $num\\_classes$ is $3$ (**DONT FORGET THE BACKGROUND CLASS**). $2*(4+3) = 24$ channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "You use a standard single-stage object detector that detects the classes dog, cat and background in this order.\n",
    "\n",
    "You use one anchor box with size (5,10).\n",
    "\n",
    "You want to classify a 32*32 RGB input image.\n",
    "\n",
    "Your object detector returns the following seven values: $[0, \\infty, 0, 1, 7, -2, 4]$\n",
    "\n",
    "We use a threshold of $40\\%$. \n",
    "\n",
    "1. How many bounding boxes are predicted?\n",
    "2. Where are they located?\n",
    "3. What is their width and height?\n",
    "4. What is the most likely class of the bounding box?\n",
    "5. Do we visualize the box?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seven values that the network predicts are $t_x, ~t_y, ~t_w, ~t_h, ~c_{dog}, ~c_{cat}, ~c_{background}$  \n",
    "\n",
    "Reminder $\\sigma(x) = \\frac{e^x}{1+e^x}$\n",
    "\n",
    "##### 1.\n",
    " We use only one anchor box and our output resolution is 1x1. $K*1*1 = 1$ We predict only 1 bounding box.\n",
    "\n",
    "##### 2.\n",
    "The formula is $x = (\\sigma(t_x) + c_x) * 32$. \n",
    "\n",
    "We calculate  $\\sigma(t_x) = 0.5$. $c_x = 0$ because we only have one pixel and we start counting at 0. $x= (0+0.5)*32 = 16$. \n",
    "\n",
    "Equivalent for $y = (\\sigma(\\infty)+c_y)*32 = (1+0)*32 = 32$. \n",
    "\n",
    "So our bounding box has its center at pixel position (16,32)\n",
    "\n",
    "##### 3.\n",
    "The formula is $w = p_w * e^{t_w}$\n",
    "\n",
    "$p_w$ is 5 and $t_w$ is 0. with this we calculate $w= 5*e^0 = 5$\n",
    "\n",
    "equivalent for $h.$ \n",
    "$~ p_h$ is 10 and $t_h$ is 1. With this we calculate $h = 10*e^1 = 27.18$ \n",
    "\n",
    "so our bounding box is 5 pixels wide and 27 pixels high\n",
    "\n",
    "\n",
    "##### 4.\n",
    "Just which position has the highest class besides the background position. 7 > 4 and 7 > -2. Because 7 is on the first position we predict a dog.\n",
    "\n",
    "#### 5.\n",
    "For this we calculate the softmax.\n",
    "\n",
    "$p_{dog} = \\frac{e^7}{e^7+e^{-2}+e^4} = 0.95$\n",
    "\n",
    "$p_{cat} = \\frac{e^{-2}}{e^7+e^{-2}+e^4} = 0.00$\n",
    "\n",
    "$p_{background} = \\frac{e^4}{e^7+e^{-2}+e^4} = 0.047$\n",
    "\n",
    "$p_{dog}$ is greater than 0.4, so we visualize the bounding box\n",
    "\n",
    "**Caution**: Imagine we get the values $[0.1, ~-2, ~0.2]$. Then background is the most likely class because 0.2 > 0.1 and 0.2 > -2.\n",
    "But we still visualize the bounding box with the class dog.\n",
    "$p_{dog} = \\frac{e^0.1}{e^0.1+e^{-2}+e^0.2} = 0.45$ which is higher than the threshold.\n",
    "So even if background is the most likely class it can happen, that we visualize the second highest class if its above the threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use existing Object Detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RetinaNet (very similar to our custom object detector):\n",
    "\n",
    "RetinNet uses a ResNet50 as backbone and uses their own anchor boxes. \n",
    "It was trained on the MS COCO Dataset.\n",
    "It uses a Feature Pyramid Network ![FPN](https://production-media.paperswithcode.com/methods/new_teaser_TMZlD2J.jpg) which we won't cover here. \n",
    "\n",
    "More object detectors can be found [here](https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection)\n",
    "\n",
    "(currently the best object detectors have box AP values of [63](https://paperswithcode.com/sota/object-detection-on-coco)) but they aren't available from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinanet = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_image):\n",
    "    # takes an model and feeds it our input image\n",
    "    \n",
    "    # all object detectors expect the image to have values between [0,1]\n",
    "    # a normal image has values between [0,255].\n",
    "    # if this happens, we divide by 255 and create a batch with only this image\n",
    "\n",
    "    if torch.max(input_image) > 1.0:\n",
    "        # if input image had shape [3,224,224] it now has shape [1,3,224,224]\n",
    "        input_image = (input_image / 255).unsqueeze(dim=0)\n",
    "\n",
    "    # we don't want to train the model so set it to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # put our input image on the GPU and feed it through the network\n",
    "    prediction = model(input_image.cuda())\n",
    "    \n",
    "    # we get a list for each image in our batch.\n",
    "    # because we only have one image the list contains only one element.\n",
    "    # We take it directly\n",
    "    return(prediction[0])\n",
    "\n",
    "def scale_color(inp):\n",
    "    # to get pretty colors\n",
    "    return int(inp*255) * 201231 % 255\n",
    "\n",
    "def visualize(prediction, im, threshold, verbose=True):\n",
    "    if verbose:\n",
    "        # show the original image without bounding boxes\n",
    "        show(im)\n",
    "\n",
    "    # take all class predictions that are higher than the threshold\n",
    "    class_predictions = prediction['labels'][prediction['scores'] > threshold]\n",
    "    # get their corresponding names\n",
    "    class_names = [CLASSES[i] for i in class_predictions]\n",
    "    # get their corresponding bounding boxes\n",
    "    boxes = prediction['boxes'][prediction['scores'] > threshold]\n",
    "    # get color_coded classes \n",
    "    colors =  [(color_palette[i]) for i in class_predictions]\n",
    "    colors= [tuple(map(scale_color, tup)) for tup in colors]\n",
    "\n",
    "    # draw bounding box on top of original image with colored bounding box and class text\n",
    "    bbox_img = draw_bounding_boxes(im, boxes, class_names, colors, width=5).cpu()\n",
    "    if verbose:\n",
    "        # show image with bounding boxes\n",
    "        show(bbox_img)\n",
    "    return bbox_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "detection = predict(retinanet, orig_img)\n",
    "bbox_img = visualize(detection, orig_img, threshold = threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we use this detector for autonomous driving?\n",
    "Let's take some recorded images and use the RetinaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "def nuScenesGIF():\n",
    "    fp_in = \"./nuScenes/*.jpg\"\n",
    "    fp_out = \"./nuScenes.gif\"\n",
    "    imgs = (Image.open(f).resize((854,480),Image.Resampling.LANCZOS) for f in sorted(glob.glob(fp_in)))\n",
    "    tensors = [read_image(f) for f in sorted(glob.glob(fp_in))]\n",
    "    img = next(imgs)  # extract first image from iterator\n",
    "    img.save(fp=fp_out, format='GIF', append_images=imgs,\n",
    "         save_all=True, duration=100, loop=0)\n",
    "\n",
    "    return tensors\n",
    "\n",
    "input_tensors = nuScenesGIF()\n",
    "print(f\" we have {len(input_tensors)} images each with shape {input_tensors[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create bounding boxes for this short video\n",
    "\n",
    "![](https://git.scc.kit.edu/vy9905/ml2images/-/raw/main/nuScenes.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bbox_gif(model, name):\n",
    "    gif_imgs = []\n",
    "    for i in input_tensors:\n",
    "        detection = predict(model, i)\n",
    "        bbox_img = visualize(detection, i, threshold = threshold, verbose=False)\n",
    "        pil_bbox_img = transforms.functional.to_pil_image(bbox_img).resize((480,360),Image.Resampling.LANCZOS)\n",
    "        gif_imgs.append(pil_bbox_img)\n",
    "    gif_imgs[0].save(fp=f\"{name}_bbox.gif\", format='GIF', append_images=gif_imgs[1:],\n",
    "            save_all=True, duration=200, loop=0)\n",
    "\n",
    "create_bbox_gif(retinanet, \"retina\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's also use a slightly stronger object detector FCOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcos = torchvision.models.detection.fcos_resnet50_fpn(pretrained=True).cuda()\n",
    "create_bbox_gif(fcos, \"fcos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retina on the left, FCOS to the right\n",
    "\n",
    "![Retina](https://git.scc.kit.edu/vy9905/ml2images/-/raw/main/retina_bbox.gif) | ![FCOS](https://git.scc.kit.edu/vy9905/ml2images/-/raw/main/fcos_bbox.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every prediction is done for every single image independently of all other images. We completely lose temporal information. E.g. we can't predict the speed of the objects.\n",
    "Furthermore our detector only returns the pixel positions of the bounding boxes. For automotive driving, we need the real-world coordinates\n",
    "with 3D Detections."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed820deb52ec78923d70a963f28f41a40c9a9595801e22a9d233d40f5faae55b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ml_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
