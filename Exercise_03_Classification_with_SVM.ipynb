{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Support Vector Machines (SVMs) are non-probabilistic binary linear classifiers. As they rely on labled data, they belong to the class of supervised learning models. SVMs can be used both for classification as well as regression. This exercise will focus on the classification part.\n",
    "\n",
    "The documentation for SVMs in scikit-learn can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enables inline plotting\n",
    "%matplotlib inline\n",
    "#enables inline plotting and interactivity\n",
    "#%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolor='none', edgecolor='red');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classification\n",
    "#### Task 1: Constructing linear separators (5 Minutes)\n",
    "Consider the following simple case of a classification task, in which the two classes of points are well separable.\n",
    "\n",
    "**Todo:** Modify the coefficients of the linear equations so that three lines separate the data into two classes, creating three different classification models. How did you decide on the values of $m$ and $b$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 50 samples with 2 centers and a standard deviation of 0.7\n",
    "data, labels = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.70)\n",
    "# plot the created data, with the lables as the color, the size set to 50 and the color map set to summer\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, s=50, cmap='summer');\n",
    "\n",
    "# create a equidistant spaced array from the min to the max values of the data (x)\n",
    "xfit = np.linspace(np.min(data[:, 0]), np.max(data[:, 0]))\n",
    "\n",
    "#TODO: modify the coefficients of the linear equation so that the two classes are being classified correctly\n",
    "for m, b in [(2, 1), (2, 2), (2, 3)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-')\n",
    "\n",
    "# set the axis limits to the min and max values of the data\n",
    "plt.xlim(np.min(data[:, 0]), np.max(data[:, 0]));\n",
    "plt.ylim(np.min(data[:, 1]), np.max(data[:, 1]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Maximizing Margins (5 Minutes)\n",
    "While the line in the previous task acted as a perfect classifier for the given data, the choice of $m$ and $b$ seemed arbitrary. Support vector machines offer a way to improve on this. The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw a margin of some width around each line, up to the nearest point (maximum margin classifier).\n",
    "\n",
    "**TODO:** Maximize the margin of the linear equation so that the two classes are still being classified correctly. What does a larger margin imply? Where are the support vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create 50 samples with 2 centers and a standard deviation of 0.7\n",
    "data, labels = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.70)\n",
    "# plot the created data, with the lables as the color, the size set to 50 and the color map set to summer\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, s=50, cmap='summer');\n",
    "\n",
    "# create a equidistant spaced array from the min to the max values of the data (x)\n",
    "xfit = np.linspace(np.min(data[:, 0]), np.max(data[:, 0]))\n",
    "\n",
    "#TODO: maximize the margin of the linear equation so that the two classes are still being classified correctly\n",
    "for m, b, margin in [(2, 1, 0.1), (2, 2, 0.1), (2, 3, 0.1)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-')\n",
    "    # draw a margin around the line with the \n",
    "    plt.fill_between(xfit, yfit - margin, yfit + margin, edgecolor='none', color='r', alpha=0.1)\n",
    "\n",
    "# set the axis limits to the min and max values of the data\n",
    "plt.xlim(np.min(data[:, 0]), np.max(data[:, 0]));\n",
    "plt.ylim(np.min(data[:, 1]), np.max(data[:, 1]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first SVM\n",
    "Let's see the result of an actual support vector fit to this data: we will use Scikit-Learn's support vector classifier to train an SVM model on this data. For the time being, we will use a linear kernel and set the C parameter to a very large number (we'll discuss the meaning of these in more depth momentarily).\n",
    "\n",
    "**Problem**: Classification (i.e. split the two classes\n",
    "**Solution**: Hyperplane with maximum margin\n",
    "**Reasoning**: Maximum Margin generalizes best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "# create data that is harder to split\n",
    "#data, labels = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.90)\n",
    "\n",
    "# split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.33)\n",
    "\n",
    "# create an svm classifier\n",
    "model = svm.SVC(kernel='linear', C=10)\n",
    "#model = svm.SVC(kernel='poly')\n",
    "#model = svm.SVC(kernel='rbf')\n",
    "\n",
    "# train the classifier using the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict the response for test dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# plot the training data\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=50, cmap='summer');\n",
    "# plot the text data as triangles, marker='^' \n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=50, marker='^', cmap='summer');\n",
    "# plot the decision boundary\n",
    "plot_svc_decision_function(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A binary classifier is used to classify images either being cat or non-cat.\n",
    "It classifies a total of 10 images, including 7 cat images (+) and 3 non-cat images (-).  \n",
    "Out of the 7 cat images 4 cat images are classified as cats (tp).  \n",
    "Out of the remaining 3 images 1 image is classified as a cat (fp).  \n",
    "Out of the 7 cat images 3 cat images are classified as non-cats (fn).  \n",
    "Out of the remaining 3 images 2 images are classified as non-cat (tn).  \n",
    "\n",
    "**Accuracy** is the number of correct results for all classes divided by the number of all results.  \n",
    "$\\text{accuracy} = \\frac{tp+tn}{tp+tn+fp+fn} =\\frac{4+2}{4+2+3+1} = 0.6$\n",
    "\n",
    "**Precision** is the number of correct results of a single class divided by the number of all returned results of that class.  \n",
    "$\\text{precision} = \\frac{\\text{tp}}{\\text{tp} + \\text{fp}} = \\frac{\\text{4}}{\\text{4} + \\text{1}} = 0.8$\n",
    "\n",
    "**Recall** is the number of correct results of a single class divided by the number of results that should have been returned of that class.  \n",
    "$\\text{recall} = \\frac{\\text{tp}}{\\text{tp} + \\text{fn}} = \\frac{\\text{4}}{\\text{4} + \\text{3}} \\approx 0.57$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Linear SVM: Kernel SVM\n",
    "While linearly serparable data is nice to play with, it often isn't that easy to find in reality. To overcome the problem SVMs can be combined with kernels. \n",
    "Where SVM becomes extremely powerful is when it is combined with kernels to perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. To motivate the need for kernels, let's look at some data that is not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "data, labels = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "# create an svm classifier\n",
    "model = svm.SVC(kernel='linear', C=10)\n",
    "# train the classifier\n",
    "model.fit(data, labels)\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, s=50, cmap='summer')\n",
    "# plot the decision boundary\n",
    "plot_svc_decision_function(model, plot_support=False);\n",
    "\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Transforming Data (5 Minutes)\n",
    "It is clear that no linear discrimination will ever be able to separate this data. But we might be able to transform the data into a higher dimension such that a linear separator would be sufficient.\n",
    "\n",
    "**TODO:** Choose and apply a kernel to the data so that it becomes linearily separable in a higher dimension. Why did you choose this specific kernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#TODO: apply a kernel to the data so that it becomes seperable in a higher dimension.\n",
    "r = np.sum(1)\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def plot_3D(data, labels, r, elev=10, azim=30):\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(data[:, 0], data[:, 1], r, c=labels, s=50, cmap='summer')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('r')\n",
    "    return ax\n",
    "\n",
    "def plot_3D_hyperplane(data,r,ax):\n",
    "    x = np.linspace(np.min(data[:, 0]), np.max(data[:, 0]),2)\n",
    "    y = np.linspace(np.min(data[:, 1]), np.max(data[:, 1]),2)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    r = np.ones(X.shape)*r\n",
    "    ax.plot_surface(X, Y, r, color='r', alpha=0.4);\n",
    "\n",
    "# plot the data in 3d\n",
    "ax = plot_3D(data,labels,r)\n",
    "# draw a possible hyperplane for linear separation\n",
    "plot_3D_hyperplane(data,np.average(r),ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a *RBF* kernel we see that the SVM projects the data into a higher dimensional space, so that it becomes linearly seperable again. If we project the resulting hyperplane into 2D it becomes a highly nonlinear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an svm classifier\n",
    "model = svm.SVC(kernel='linear')\n",
    "# train the classifier\n",
    "model.fit(data, labels)\n",
    "\n",
    "\n",
    "model = svm.SVC(kernel='rbf', gamma='auto');\n",
    "model.fit(data, labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data in 2d\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, s=50, cmap='summer')\n",
    "# project the resulting hyperplane to 2D\n",
    "plot_svc_decision_function(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the SVM: Softening Margins\n",
    "Once we have noise in our data, we need to tune our classificator to achieve the best compromise.\n",
    "\n",
    "$$y_{i}\\left(w^{T} x_{i}+b\\right) \\geq 1-\\xi_{i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create 50 samples with 2 centers and a standard deviation of 1.2\n",
    "data, labels = make_blobs(n_samples=50, centers=2, random_state=12, cluster_std=2)\n",
    "# split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=4)\n",
    "# plot the created data, with the lables as the color, the size set to 50 and the color map set to summer\n",
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=50, cmap='summer');\n",
    "# plot the text data as triangles, marker='^' \n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=50, marker='^', cmap='summer');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle these cases, we can soften the margin of the SVM, so that some of the points to are allowed to creep into the margin if that allows a better fit.\n",
    "\n",
    "The hyperparameter C controls the hardness of the margin, where large values for C push data out of the margin (more correct) and lower values are more benign (maximum margin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a figure 1 by 2 figure as subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 9))\n",
    "\n",
    "# create a tuples with the axis and the C values (margin values)\n",
    "for axi, C in zip(ax, [10, 0.3]):\n",
    "    model = svm.SVC(kernel='linear', C=C)\n",
    "    model.fit(X_train,y_train)\n",
    "    # predict the response for test dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "    # model Accuracy: how often is the classifier correct?\n",
    "    print(\"C = {:.2f}, Accuracy = {:.2f}\".format(C,metrics.accuracy_score(y_test, y_pred)))\n",
    "    axi.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=50, cmap='summer');\n",
    "    # plot the text data as triangles, marker='^' \n",
    "    axi.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=50, marker='^', cmap='summer');\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.set_title('C = {:.1f}'.format(C), size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Face Recognition\n",
    "As an example of support vector machines in action, let's take a look at the facial recognition problem. We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures. A fetcher for the dataset is built into Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "\n",
    "print('Number of different faces in the dataset: {}.\\n'.format(len(faces.target_names)))\n",
    "\n",
    "print('The following names are present:\\n {}\\n'.format(faces.target_names))\n",
    "\n",
    "print('The size of the images is: {}.'.format(faces.images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a figure 3 by 3 figure as subplots\n",
    "fig, ax = plt.subplots(3, 3, figsize=(8,6))\n",
    "\n",
    "# plot 9 of the faces to get an idea what we are working with\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(faces.images[i], cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[], xlabel=faces.target_names[faces.target[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image contains [62×47] or nearly 3,000 pixels. We could proceed by simply using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features; here we will use a principal component analysis (see In Depth: Principal Component Analysis) to extract 150 fundamental components to feed into our support vector machine classifier. We can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# create the principal component analysis\n",
    "pca = PCA(n_components=150, whiten=True, random_state=42)\n",
    "# create the classifier\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "# create the model combining both\n",
    "model = make_pipeline(pca, svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of testing our classifier output, we will split the data into a training and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(faces.data, faces.target, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use a grid search cross-validation to explore combinations of parameters. Here we will adjust C (which controls the margin hardness) and gamma (which controls the size of the radial basis function kernel), and determine the best model: [Grid Search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# set the grid for the grid search\n",
    "param_grid = {'svc__C': [1, 5, 10, 50],\n",
    "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "\n",
    "# create an exhaustive search over specified parameter values for a given estimator\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "\n",
    "# time the search for the best hyperparameters\n",
    "%time grid.fit(X_train, y_train);\n",
    "print(grid.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model from with the best hyperparameters\n",
    "model = grid.best_estimator_\n",
    "\n",
    "# predict the response for test dataset\n",
    "yfit = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result for the test data\n",
    "fig, ax = plt.subplots(4, 6, figsize=(16,9))\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(X_test[i].reshape(62, 47), cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1], color='black' if yfit[i] == y_test[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display some metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, yfit, target_names=faces.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# create a confusion matrix to analyze the SVMs abilities\n",
    "plt.figure(figsize=(16, 9))\n",
    "mat = confusion_matrix(y_test, yfit)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, \n",
    "            xticklabels=faces.target_names,\n",
    "            yticklabels=faces.target_names, annot_kws={\"size\": 14})\n",
    "plt.xlabel('true label', fontsize=14)\n",
    "plt.ylabel('predicted label', fontsize=14);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
