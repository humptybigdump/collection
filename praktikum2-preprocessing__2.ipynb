{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MT Praktikum 2: Text processing and Language Models\n",
    "\n",
    "In the directory `/opt/data/nc19/` you will find the raw text files from\n",
    "the News-Commentary corpus which we will use today for our\n",
    "preprocessing, as well as the language model. The corresponding source\n",
    "data you can find in  \n",
    "`/opt/data/wmt10-xlats/ref/wmt10-newssyscombtest2010-src.de.sgm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "=============\n",
    "\n",
    "Tokenization\n",
    "------------\n",
    "\n",
    "Tokenization, in brief terms, is the task of breaking down the text\n",
    "stream into discrete units, called *tokens*. Before looking at\n",
    "tokenization, let's first take a look at the data itself:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumption of the session\n",
      "I declare resumed the session of the European Parliament adjourned on Friday, 15 December 2000.\n",
      "Statements by the President\n",
      "Ladies and gentlemen, on Saturday, as you know, an earthquake struck Central America once again, with tragic consequences.\n",
      "This is an area which has already been seriously affected on a number of occasions since the beginning of the twentieth century.\n",
      "The latest, provisional, figures for victims in El Salvador are already very high.\n",
      "There are 350 people dead, 1 200 people missing, the area is completely devastated and thousands of homes have been destroyed throughout the country.\n",
      "The European Union has already shown its solidarity by sending a rescue team to the area, whilst financial assistance from the Union and Member States has been, or is in the process of being, released and I am able to inform you that some groups in the European Parliament have requested that this issue be included in the debate on topical and urgent subjects of major importance on Thursday.\n",
      "However, I should like to inform you that I have, of course, on behalf of the European Union, expressed our sincere condolences and deepest sympathy to the President of El Salvador in the tragedy which has struck his country.\n",
      "I would ask you, as a mark of respect for the victims and for the immense suffering of their families, to observe a minute' s silence.\n"
     ]
    }
   ],
   "source": [
    "head /opt/data/nc19/europarl-v9.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's manually try to extract the unique words in this corpus.\n",
    "The following command will extract a (sorted and de-duplicated) list of\n",
    "tokens, as well as an occurrence count for each of them, from the text\n",
    "file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat /opt/data/nc19/europarl-v9.en | tr ' ' '\\n' | sort | uniq -c > /tmp/europarl.hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then read the file *europarl.hist*. Try to answer the following\n",
    "questions:\n",
    "\n",
    "-   How many items (words separated by space) are there in the original\n",
    "    europarl-v9.en data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329276\n"
     ]
    }
   ],
   "source": [
    "wc -l < /tmp/europarl.hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Is each one of the items totally unique, or can you spot some\n",
    "    obvious redundancies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;31m\u001b[K      2 Example\u001b[m\u001b[K\n",
      "\u001b[01;31m\u001b[K    162 Example\u001b[m\u001b[Ks\n",
      "\u001b[01;31m\u001b[K      1 Example\u001b[m\u001b[Ks,\n",
      "\u001b[01;31m\u001b[K  11829 example\u001b[m\u001b[K\n",
      "\u001b[01;31m\u001b[K     12 example\u001b[m\u001b[K!\n",
      "\u001b[01;31m\u001b[K      4 example\u001b[m\u001b[K'\n",
      "\u001b[01;31m\u001b[K      1 example\u001b[m\u001b[K',\n",
      "\u001b[01;31m\u001b[K      1 example\u001b[m\u001b[K'.\n",
      "\u001b[01;31m\u001b[K     23 example\u001b[m\u001b[K)\n",
      "\u001b[01;31m\u001b[K     20 example\u001b[m\u001b[K),\n",
      "\u001b[01;31m\u001b[K     11 example\u001b[m\u001b[K).\n",
      "\u001b[01;31m\u001b[K      1 example\u001b[m\u001b[K)?\n",
      "\u001b[01;31m\u001b[K  16499 example\u001b[m\u001b[K,\n",
      "\u001b[01;31m\u001b[K   2328 example\u001b[m\u001b[K.\n",
      "\u001b[01;31m\u001b[K      1 example\u001b[m\u001b[K.4.If\n",
      "\u001b[01;31m\u001b[K      1 example\u001b[m\u001b[K.I\n",
      "\u001b[01;31m\u001b[K    379 example\u001b[m\u001b[K:\n",
      "\u001b[01;31m\u001b[K     46 example\u001b[m\u001b[K;\n",
      "\u001b[01;31m\u001b[K     91 example\u001b[m\u001b[K?\n",
      "\u001b[01;31m\u001b[K      1 example\u001b[m\u001b[K?!\n",
      "\u001b[01;31m\u001b[K   2263 example\u001b[m\u001b[Ks\n",
      "\u001b[01;31m\u001b[K      2 example\u001b[m\u001b[Ks)\n",
      "\u001b[01;31m\u001b[K      2 example\u001b[m\u001b[Ks),\n",
      "\u001b[01;31m\u001b[K      3 example\u001b[m\u001b[Ks).\n",
      "\u001b[01;31m\u001b[K    254 example\u001b[m\u001b[Ks,\n",
      "\u001b[01;31m\u001b[K    438 example\u001b[m\u001b[Ks.\n",
      "\u001b[01;31m\u001b[K    144 example\u001b[m\u001b[Ks:\n",
      "\u001b[01;31m\u001b[K      9 example\u001b[m\u001b[Ks;\n",
      "\u001b[01;31m\u001b[K      6 example\u001b[m\u001b[Ks?\n",
      "\u001b[01;31m\u001b[K      1 example\u001b[m\u001b[Ksas\n",
      "\u001b[01;31m\u001b[K      1 example\u001b[m\u001b[Kâ€¦\n"
     ]
    }
   ],
   "source": [
    "cat /tmp/europarl.hist | grep -i ' *[0-9]*  *example'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Does the phenomenon in question affect statistical models (such as\n",
    "    $n$-gram models) or probabilistic models such as neural language\n",
    "    models?\n",
    "  - Yes, it affects both. With more vocabulary words and less samples per word the maximum-likelihood estimation for the word embeddings, as well as the $n$-gram probabilities will yield worse estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization aims at solving the problems that we observed. For\n",
    "languages such as English and German, the tools are often implemented\n",
    "with rule-based approaches. A standard tool for such tokenization is\n",
    "`tokenizer.perl` from the `Moses` SMT project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example , ( it shows how &quot; tokenziation &quot; works ) .\n"
     ]
    }
   ],
   "source": [
    "echo 'This is an example, (it shows how \"tokenziation\" works).' |\n",
    "    tokenizer.perl -l en  2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the tool to tokenize your input file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 1\n"
     ]
    }
   ],
   "source": [
    "tokenizer.perl -l en < /opt/data/nc19/europarl-v9.en > /tmp/europarl-v9.tok.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The file name is a typical convention in the Natural Language\n",
    "Processing (NLP) community. The 'tok' suffix is just a naming\n",
    "convention, telling that tokenization is applied on top of the input\n",
    "file.\n",
    "\n",
    "Now you can try to extract to vocab again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat /tmp/europarl-v9.tok.en | tr ' ' '\\n' | sort | uniq -c > /tmp/europarl.tok.hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenized text file is always longer than the original one. By using\n",
    "the `wc` command you can verify if your command ran correctly or not.\n",
    "How many words do you now have in this vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140473\n"
     ]
    }
   ],
   "source": [
    "wc -l < /tmp/europarl.tok.hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True-Casing\n",
    "-----------\n",
    "\n",
    "When you look at the vocabulary file, you will probably find there to\n",
    "still be some duplicate words, once in upper-case form and once in\n",
    "lower-case form: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;31m\u001b[K      2 Example\u001b[m\u001b[K\n",
      "\u001b[01;31m\u001b[K  31252 example\u001b[m\u001b[K\n"
     ]
    }
   ],
   "source": [
    "cat /tmp/europarl.tok.hist | grep -i ' *[0-9]*  *example$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more we can\n",
    "reduce the number of duplication the better, so after tokenization we\n",
    "will use a true-casing tool to strip even more redundancy.  \n",
    "We apply the true-casing in a 2 step procedure:\n",
    "\n",
    "1.  train a true-casing model to get the \"true\" case of each vocabulary\n",
    "    word using  \n",
    "    `$ train-truecaser.perl --model truecase-model.en --corpus europarl-v9.tok.en`\n",
    "\n",
    "2.  apply the model to the data to convert upper-cased words at the the\n",
    "    beginning of the sentence to their respective \"true\" case:  \n",
    "    `$ truecase.perl --model truecase-model.en < europarl-v9.tok.en > europarl-v9.true.en`  \n",
    "    (it may take a few minutes to complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train-truecaser.perl --model /tmp/truecase-model.en --corpus /tmp/europarl-v9.tok.en\n",
    "truecase.perl        --model /tmp/truecase-model.en        < /tmp/europarl-v9.tok.en > /tmp/europarl-v9.true.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need the tokenized text file to train the model (Why? What\n",
    "would happen if we use the original file?).  \n",
    "\n",
    "If you check the model file\n",
    "contents, you will see it simply contains statistics about upper-case\n",
    "and lower-case occurrences for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drinking-water (10/11) Drinking-Water (1)\n",
      "forceps (1/1)\n",
      "Vals (1/1)\n",
      "Stercks (3/3)\n",
      "unwinding (6/6)\n",
      "vergine (1/1)\n",
      "legend (15/15)\n",
      "magazine (130/138) Magazine (8)\n",
      "ISD (36/36)\n",
      "EVP (4/4)\n",
      "tonnes (1954/1954)\n",
      "gradings (1/1)\n",
      "non-costly (1/1)\n",
      "self-supporting (19/19)\n",
      "weakest (521/521)\n",
      "rudely (10/10)\n",
      "TGVs (1/1)\n",
      "command (365/377) Command (12)\n",
      "mid-season (2/2)\n",
      "waking (34/34)\n",
      "V2 (1/1)\n",
      "Ã¬Ã½Ã±Ã©Ã¡ (1/1)\n",
      "Romaphobia (4/4)\n",
      "signing-on (1/1)\n",
      "welfare-promoting (1/1)\n",
      "impede (228/228)\n",
      "drei (1/1)\n",
      "Ganleys (2/2)\n",
      "heat-and-power (1/1)\n",
      "symbolized (7/7)\n",
      "hydrology (1/1)\n",
      "analgesic (6/6)\n",
      "besmirch (6/6)\n",
      "Childers (3/3)\n",
      "turning (1723/1726) Turning (3)\n",
      "avenger (1/1)\n",
      "H-0843 (1/1)\n",
      "Luang (1/1)\n",
      "soapboxes (4/4)\n",
      "implicitly (148/148)\n",
      "blood-curdling (2/2)\n",
      "Jalal-Abad (2/2)\n",
      "climate-sceptics (1/1)\n",
      "Sapir (18/18)\n",
      "people-led (1/1)\n",
      "BEF (12/12)\n",
      "consumer-led (6/6)\n",
      "opportunely (20/20)\n",
      "German-only (1/1)\n",
      "postenlargement (1/1)\n",
      "Americanization (3/3)\n",
      "peck (1/1)\n",
      "-secondly (5/5)\n",
      "dropped (467/467)\n",
      "row (263/264) Row (1)\n",
      "Brasov (3/3)\n",
      "centimetre (10/10)\n",
      "fish-based (1/1)\n",
      "substate (1/1)\n",
      "graecas (1/1)\n",
      "peanut (1/1)\n",
      "sternest (2/2)\n",
      "Nepalganj (1/1)\n",
      "organigrammes (1/1)\n",
      "Treasury (135/235) treasury (100)\n",
      "-B4-0142 (2/2)\n",
      "presence (3798/3798)\n",
      "administration-centred (1/1)\n",
      "theThird (1/1)\n",
      "Jornal (3/3)\n",
      "part-markets (2/2)\n",
      "military-industrial (19/19)\n",
      "88-word (1/1)\n",
      "Europe-America (1/1)\n",
      "profit-generating (1/1)\n",
      "Qichen (4/4)\n",
      "weapons-based (1/1)\n",
      "renting (32/32)\n",
      "Claudio (18/18)\n",
      "Globespan (1/1)\n",
      "panthers (1/1)\n",
      "Murayama (1/1)\n",
      "overhauling (32/32)\n",
      "rational (987/989) Rational (2)\n",
      "logging (220/220)\n",
      "minimalise (1/1)\n",
      "perpetrating (42/42)\n",
      "Godot (8/8)\n",
      "H-0654 (1/1)\n",
      "VHI (12/12)\n",
      "non-eligible (10/10)\n",
      "best-adapted (1/1)\n",
      "3a (47/53) 3A (6)\n",
      "Lituania (2/2)\n",
      "Prestige&apos; (1/1)\n",
      "cost-accounting (2/2)\n",
      "one-third (163/163)\n",
      "web-page (1/1)\n",
      "Serbianise (1/1)\n",
      "nomenclature (90/101) Nomenclature (11)\n",
      "eel-fishing (1/1)\n",
      "Peijs (192/192)\n",
      "de-industrialization (2/2)\n",
      "Palau (5/5)\n",
      "fluorocarbons (2/2)\n",
      "depressive (12/12)\n",
      "Keochay (2/2)\n",
      "e-TEN (1/2) E-TEN (1)\n",
      "gaffes (3/3)\n",
      "de-registration (3/3)\n",
      "Napoleon (21/21)\n",
      "C7-0011 (1/1)\n",
      "B5-0003 (2/2)\n",
      "tickets (382/382)\n",
      "encroach (71/71)\n",
      "engaging (534/539) Engaging (5)\n",
      "bricolage (2/2)\n",
      "Nord-Cotentin (1/1)\n",
      "think-tank (30/31) Think-Tank (1)\n",
      "empowered (190/190)\n",
      "Î‘5-0235 (1/1)\n",
      "EuropÃ¤ische (1/1)\n",
      "Alfonso (11/11)\n",
      "Commission-designate (23/24) Commission-Designate (1)\n",
      "direst (5/5)\n",
      "restrictive (1514/1515) Restrictive (1)\n",
      "sophistry (13/13)\n",
      "dairy-farmers (1/1)\n"
     ]
    }
   ],
   "source": [
    "head -n128 /tmp/truecase-model.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listen potato Word Peter Germany USA\n",
      "\n",
      "\u001b[01;31m\u001b[KPeter \u001b[m\u001b[K(243/250) peter (7)\n",
      "\u001b[01;31m\u001b[KUSA \u001b[m\u001b[K(3044/3044)\n",
      "\u001b[01;31m\u001b[KGermany \u001b[m\u001b[K(6193/6193)\n",
      "\u001b[01;31m\u001b[Kpotato \u001b[m\u001b[K(245/245)\n",
      "\u001b[01;31m\u001b[Klisten \u001b[m\u001b[K(2362/2367) Listen (5)\n",
      "\u001b[01;31m\u001b[Kword \u001b[m\u001b[K(4988/4996) Word (7) WORD (1)\n"
     ]
    }
   ],
   "source": [
    "echo 'Listen Potato Word Peter Germany USA' |\n",
    "    truecase.perl --model /tmp/truecase-model.en\n",
    "echo\n",
    "grep -E -i '^(Listen|Potato|Word|Peter|Germany|USA) ' /tmp/truecase-model.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the true-cased text you can now try to extract to vocab again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat /tmp/europarl-v9.true.en | tr ' ' '\\n' | sort | uniq -c > /tmp/europarl.true.hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we did everything correctly the vocabulary size should have further\n",
    "decreased.  \n",
    "What is the vocabulary size at the moment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134463\n"
     ]
    }
   ],
   "source": [
    "wc -l < /tmp/europarl.true.hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to look at the histogram file a little bit more.  \n",
    "Notice that many words share the same root and differ in suffixes or prefixes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;31m\u001b[K      1 Listen\u001b[m\u001b[K\n",
      "\u001b[01;31m\u001b[K      2 Listen\u001b[m\u001b[King\n",
      "\u001b[01;31m\u001b[K   2407 listen\u001b[m\u001b[K\n",
      "\u001b[01;31m\u001b[K   1718 listen\u001b[m\u001b[Ked\n",
      "\u001b[01;31m\u001b[K     10 listen\u001b[m\u001b[Ker\n",
      "\u001b[01;31m\u001b[K     29 listen\u001b[m\u001b[Kers\n",
      "\u001b[01;31m\u001b[K   1438 listen\u001b[m\u001b[King\n",
      "\u001b[01;31m\u001b[K     92 listen\u001b[m\u001b[Ks\n"
     ]
    }
   ],
   "source": [
    "cat /tmp/europarl.true.hist | grep -i ' *[0-9]*  *listen'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also most of the items in the vocabulary appear only once in the data (especially\n",
    "numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58704\n",
      "1334\n"
     ]
    }
   ],
   "source": [
    "cat /tmp/europarl.true.hist | grep ' *1 ' | wc -l\n",
    "cat /tmp/europarl.true.hist | grep ' *1 [0-9][0-9]*$' | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could be the problem for algorithms that learn embeddings or\n",
    "statistical/probabilistic models in general? (This is an open question,\n",
    "and there are several problems that I can remember, but in general it\n",
    "comes from the curse of dimensionality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Byte-Pair Encoding\n",
    "------------------\n",
    "\n",
    "Byte-Pair Encoding (BPE) is an algorithm that helps us automatically\n",
    "split words into smaller components. Since BPE is also a statistical\n",
    "algorithm, first we need to extract the statistics from our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword-nmt learn-bpe -s 32000 < /tmp/europarl-v9.true.en > /tmp/code.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32001 /tmp/code.en\n"
     ]
    }
   ],
   "source": [
    "wc -l /tmp/code.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to true-casing, after training the BPE codes we have to apply it\n",
    "to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword-nmt apply-bpe -c /tmp/code.en < /tmp/europarl-v9.true.en > /tmp/europarl-v9.bpe.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can check the vocabulary size once more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat /tmp/europarl-v9.bpe.en | tr ' ' '\\n' | sort | uniq -c > /tmp/europarl.bpe.hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31520\n",
      "\n",
      "\u001b[01;31m\u001b[K   2407 listen\u001b[m\u001b[K\n",
      "\u001b[01;31m\u001b[K     39 listen\u001b[m\u001b[K@@\n",
      "\u001b[01;31m\u001b[K   1719 listen\u001b[m\u001b[Ked\n",
      "\u001b[01;31m\u001b[K   1438 listen\u001b[m\u001b[King\n",
      "\u001b[01;31m\u001b[K     92 listen\u001b[m\u001b[Ks\n",
      "\n",
      "listen@@ ers an@@ da@@ very@@ long@@ word\n"
     ]
    }
   ],
   "source": [
    "wc -l < /tmp/europarl.bpe.hist\n",
    "echo\n",
    "cat /tmp/europarl.bpe.hist | grep -i ' *[0-9]*  *listen'\n",
    "echo\n",
    "echo 'listeners andaverylongword' | subword-nmt apply-bpe -c /tmp/code.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Language Model\n",
    "==========================\n",
    "\n",
    "In the following we will use n-gram language modeling to look at domain,\n",
    "an important consideration when training NMT models.  \n",
    "All of the data are provided at `/opt/data/lmdom/`.  \n",
    "To train an order-n language model use\n",
    "```bash\n",
    "lmplz -o {order} < {training_data} > {lm_name.arpa}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the perplexity of a dataset using an arpa file use\n",
    "```bash\n",
    "python3 perp.py {lm_name.arpa} {text_data}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   First, train a bigram ($n=2$) LM on the English UDHR. Use this model\n",
    "    on `dev` data. What is the perplexity?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /opt/data/lmdom/english.udhr\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 1778 types 627\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:7524 2:216371812761\n",
      "Statistics:\n",
      "1 627 D1=0.786408 D2=1.03486 D3+=1.95146\n",
      "2 1341 D1=0.8418 D2=1.35127 D3+=1.19614\n",
      "Memory estimate for binary LM:\n",
      "type    kB\n",
      "probing 39 assuming -p 1.5\n",
      "probing 41 assuming -r models -p 1.5\n",
      "trie    21 without quantization\n",
      "trie    18 assuming -q 8 -b 8 quantization \n",
      "trie    21 assuming -a 22 array pointer compression\n",
      "trie    18 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:7524 2:21456\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:7524 2:21456\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:211474696 kB\tVmRSS:6212 kB\tRSSMax:57650144 kB\tuser:2.4\tsys:12.42\tCPU:14.8237\treal:14.8265\n"
     ]
    }
   ],
   "source": [
    "lmplz -o 2 < /opt/data/lmdom/english.udhr > /tmp/udhr.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /tmp/udhr.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "613.303224701251\n"
     ]
    }
   ],
   "source": [
    "perp.py /tmp/udhr.arpa /opt/data/lmdom/dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Now, train a bigram LM on the wikipedia data. Use this model to\n",
    "    calculate the perplexity of the `dev` data. What is the perplexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /opt/data/lmdom/wiki.en.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 42681165 types 1114998\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:13379976 2:216358436864\n",
      "Statistics:\n",
      "1 1114998 D1=0.731948 D2=1.03082 D3+=1.2829\n",
      "2 9551015 D1=0.754579 D2=1.0721 D3+=1.31276\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 191 assuming -p 1.5\n",
      "probing 195 assuming -r models -p 1.5\n",
      "trie     84 without quantization\n",
      "trie     58 assuming -q 8 -b 8 quantization \n",
      "trie     84 assuming -a 22 array pointer compression\n",
      "trie     58 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:13379976 2:152816240\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:13379976 2:152816240\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:211475712 kB\tVmRSS:35688 kB\tRSSMax:57803088 kB\tuser:17.224\tsys:14.504\tCPU:31.7307\treal:31.7894\n"
     ]
    }
   ],
   "source": [
    "lmplz -o 2 < /opt/data/lmdom/wiki.en.txt > /tmp/wiki.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /tmp/wiki.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "2455.2919557159653\n"
     ]
    }
   ],
   "source": [
    "perp.py /tmp/wiki.arpa /opt/data/lmdom/dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Finally, train a bigram LM on the novel chapter. Use this model to\n",
    "    calculate the perplexity of the `dev` data. What is the perplexity?    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /opt/data/lmdom/hpchapter1.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 5722 types 1251\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:15012 2:216371789824\n",
      "Statistics:\n",
      "1 1251 D1=0.637427 D2=1.27739 D3+=1.75624\n",
      "2 4003 D1=0.808964 D2=1.09147 D3+=1.53722\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 102 assuming -p 1.5\n",
      "probing 107 assuming -r models -p 1.5\n",
      "trie     49 without quantization\n",
      "trie     39 assuming -q 8 -b 8 quantization \n",
      "trie     49 assuming -a 22 array pointer compression\n",
      "trie     39 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:15012 2:64048\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:15012 2:64048\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:211474716 kB\tVmRSS:6540 kB\tRSSMax:57652808 kB\tuser:2.344\tsys:12.452\tCPU:14.8\treal:14.8007\n"
     ]
    }
   ],
   "source": [
    "lmplz -o 2 < /opt/data/lmdom/hpchapter1.txt > /tmp/hpchapter1.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /tmp/hpchapter1.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "516.76552634914\n"
     ]
    }
   ],
   "source": [
    "perp.py /tmp/hpchapter1.arpa /opt/data/lmdom/dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   How was the perplexity different with each of the models? Look at\n",
    "    the first few lines of each training dataset, and the `dev` data.\n",
    "    Why might this be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Data Set|dev Perplexity|\n",
    "|--------|--------------|\n",
    "|UHDR | 613.3 |\n",
    "|Wikipedia | 2455.3 |\n",
    "|Novel Chapter | 516.8 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal Declaration of Human Rights\n",
      "Preamble\n",
      "Whereas recognition of the inherent dignity and of the equal and inalienable rights of all members of the human family is the foundation of freedom, justice and peace in the world,\n",
      "Whereas disregard and contempt for human rights have resulted in barbarous acts which have outraged the conscience of mankind, and the advent of a world in which human beings shall enjoy freedom of speech and belief and freedom from fear and want has been proclaimed as the highest aspiration of the common people,\n",
      "Whereas it is essential, if man is not to be compelled to have recourse, as a last resort, to rebellion against tyranny and oppression, that human rights should be protected by the rule of law,\n",
      "Whereas it is essential to promote the development of friendly relations between nations,\n",
      "Whereas the peoples of the United Nations have in the Charter reaffirmed their faith in fundamental human rights, in the dignity and worth of the human person and in the equal rights of men and women and have determined to promote social progress and better standards of life in larger freedom,\n",
      "Whereas Member States have pledged themselves to achieve, in cooperation with the United Nations, the promotion of universal respect for and observance of human rights and fundamental freedoms,\n",
      "Whereas a common understanding of these rights and freedoms is of the greatest importance for the full realization of this pledge,\n",
      "Now, therefore,\n"
     ]
    }
   ],
   "source": [
    "head /opt/data/lmdom/english.udhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Handog ng Pilipino sa Mundo \" ( lit. \" The Gift of the Filipinos to the World \" ) is a 1986 song recorded in Filipino by a supergroup composed of 15 Filipino artists. The song became the anthem of the bloodless People Power Revolution. The lyrics of the song are inscribed on a wall of Our Lady of EDSA Shrine , the center of the revolution. Songwriter Jim Paredes wrote the song in three minutes , with no revisions , using the success of the 1986 EDSA People Power Revolution as his inspiration. After finishing the composition , he sent it to WEA Records , who at that time is compiling an album of patriotic songs. The song eventually became its carrier single. [ 1 ] [ 2 ] A music video was also made for the song. Paredes then invited artists who were involved with the EDSA Revolution. Kris Aquino , then a teenager , also appeared in the music video. National heroes since the Spanish period like Jose Rizal and Andres Bonifacio , prominent anti-Marcos figures and scenes from the revolution were also featured. [ 2 ] As part of the 25th Anniversary of the EDSA Revolution , broadcast television network ABS-CBN created a cover version of the song , with slightly different arrangements. Featured artists are Martin Nievera , Gary Valenciano , Zsa Zsa Padilla , Vina Morales , Erik Santos , Piolo Pascual , Christian Bautista , Toni Gonzaga , Sam Milby , Juris Fernandez , Aiza Seguerra , Yeng Constantino and Jovit Baldivino. \n"
     ]
    }
   ],
   "source": [
    "head -1 /opt/data/lmdom/wiki.en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. and Mrs. Dursley , of number four , Privet Drive , were proud to say that they were perfectly normal , thank you very much . They were the last people you &apos;d expect to be involved in anything strange or mysterious , because they just didn &apos;t hold with such nonsense .\n",
      "\n",
      "Mr. Dursley was the director of a firm called Grunnings , which made drills . He was a big , beefy man with hardly any neck , although he did have a very large mustache . Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck , which came in very useful as she spent so much of her time craning over garden fences , spying on the neighbors . The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere .\n",
      "\n",
      "The Dursleys had everything they wanted , but they also had a secret , and their greatest fear was that somebody would discover it . They didn &apos;t think they could bear it if anyone found out about the Potters . Mrs. Potter was Mrs. Dursley &apos;s sister , but they hadn &apos;t met for several years ; in fact , Mrs. Dursley pretended she didn &apos;t have a sister , because her sister and her good-for-nothing husband were as unDursleyish as it was possible to be . The Dursleys shuddered to think what the neighbors would say if the Potters arrived in the street . The Dursleys knew that the Potters had a small son , too , but they had never even seen him . This boy was another good reason for keeping the Potters away ; they didn &apos;t want Dudley mixing with a child like that .\n",
      "\n",
      "When Mr. and Mrs. Dursley woke up on the dull , gray Tuesday our story starts , there was nothing about the cloudy sky outside to suggest that strange and mysterious things would soon be happening all over the country . Mr. Dursley hummed as he picked out his most boring tie for work , and Mrs. Dursley gossiped away happily as she wrestled a screaming Dudley into his high chair .\n",
      "\n",
      "None of them noticed a large , tawny owl flutter past the window .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head /opt/data/lmdom/hpchapter1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Long-Expected Party\n",
      "When Mr. Bilbo Baggins of Bag End announced that he would shortly be\n",
      "celebrating his eleventy-first birthday with a party of special\n",
      "magnificence , there was much talk and excitement in Hobbiton .\n",
      "Bilbo was very rich and very peculiar , and had been the\n",
      "wonder of the Shire for sixty years , ever since his remarkable\n",
      "disappearance and unexpected return . The riches he had brought back\n",
      "from his travels had now become a local legend , and it was popularly\n",
      "believed , whatever the old folk might say , that the Hill at Bag End\n",
      "was full of tunnels stuffed with treasure . And if that was not enough\n"
     ]
    }
   ],
   "source": [
    "head /opt/data/lmdom/dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **UHDR** = Universal Declaration of Human Rights\n",
    "  - formal text\n",
    "  - \"[legaleze](https://www.merriam-webster.com/dictionary/legalese)\"\n",
    "- Wikipedia: each sentence is a paragraph from a Wikipedia article\n",
    "- the **dev** data turns out to also be a novel chapter\n",
    "  - same domain as the **hpchapter1**\n",
    "  - lowest perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   How do you think this would change if you used a larger order\n",
    "    (e.g. 4) LM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591.2512898622641\n"
     ]
    }
   ],
   "source": [
    "lmplz -o 4 < /opt/data/lmdom/english.udhr > /tmp/udhr_o4.arpa 2> /dev/null\n",
    "perp.py /tmp/udhr_o4.arpa /opt/data/lmdom/dev 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1952.3559672598092\n"
     ]
    }
   ],
   "source": [
    "lmplz -o 4 < /opt/data/lmdom/wiki.en.txt > /tmp/wiki_o4.arpa 2> /dev/null\n",
    "perp.py /tmp/wiki_o4.arpa /opt/data/lmdom/dev 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503.7123702342283\n"
     ]
    }
   ],
   "source": [
    "lmplz -o 4 < /opt/data/lmdom/hpchapter1.txt > /tmp/hp_o4.arpa 2> /dev/null\n",
    "perp.py /tmp/hp_o4.arpa /opt/data/lmdom/dev 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams can be good models, but only if the test corpus looks like\n",
    "the training corpus. In reality, it often does not. We need to come\n",
    "up with adaptation and smoothing methods to account for this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Perplexity is often used as intrinsic evaluation for language\n",
    "    models. Let's think about what these values mean more closely.\n",
    "    Suppose a 'sentence' consists of random digits. What is the\n",
    "    perplexity of this sentence, if our model assigns probability\n",
    "    $p=1/10$ to each digit?  \n",
    "  - intuitive explanation of perplexity:\n",
    "    - on average, for each output word in the sequence, how many different vocabulary words does the model have to consider?\n",
    "  - uniform distribution (e.g. $p_w = \\frac{1}{|V|} \\text{ for } w \\in V$) yields the worst-case perplexity:\n",
    "    - the model has to consider every single vocabulary word $w$ at every step (since all words are equally probable)\n",
    "    - perplexity becomes $|V|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Consider now a natural language sentence. What is the maximum\n",
    "    perplexity of a sentence with 10 tokens? With 100 tokens?\n",
    "  - since perplexity doesn't differentiate between sequences of natural language words and sequences of digits the above explanation holds true\n",
    "  - maximum perplexity is the vocabulary size $|V|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Let's return to our language models. Pick one of the three datasets,\n",
    "    and train trigram and 4-gram LMs as well. Evaluate the perplexity of\n",
    "    the `dev` data. How is it different between the bigram, trigram, and\n",
    "    4-gram models? Why might this be?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   One major problem with models is generalization. If we have a bigram\n",
    "    we have never seen before in `dev`, our model will produce a\n",
    "    probability of 0 for the sentence and we can't compute the\n",
    "    perplexity (can't divide by 0!). Not good  \n",
    "    In order to do something about this, people typically use smoothing\n",
    "    methods. The simplest is called add-one or Laplace smoothing. This\n",
    "    is as simple as it sounds: we increment the counts of all seen word\n",
    "    types (unique) by 1, and the vocabulary by the same amount (size of\n",
    "    the vocabulary, number of unique words)  \n",
    "    Now, there is a small probability allocated for unknown words:\n",
    "    unseen n-grams have $\\frac{1}{N+V}$ instead of 0!\n",
    "    $$\\mathcal{P}_{Laplace}(w_i) = \\frac{count_i + 1}{N + V}$$\n",
    "\n",
    "    A basic bigram language model has been coded for you in python,\n",
    "    `bigram_lm.py`. Use this script to train a bigram LM on\n",
    "    hpchapter1.txt. It will print the model entropy.   \n",
    "    In this last exercise, modify this script to use add-one smoothing.\n",
    "    Now train a bigram LM. How has the entropy changed?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entropy of the bigram model for this file is: 3.052 bits.\n"
     ]
    }
   ],
   "source": [
    "bigram_lm.py /opt/data/lmdom/hpchapter1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File bigram_lm.py is read-only; trying to patch anyway\n",
      "patching file /tmp/bigram_lm.py (read from bigram_lm.py)\n"
     ]
    }
   ],
   "source": [
    "# patch the file and write the patched file to /tmp/bigram_lm.py\n",
    "patch -d /usr/local/bin -o /tmp/bigram_lm.py << EOF\n",
    "--- /usr/local/bin/bigram_lm.py\t2021-07-05 12:54:44.000000000 +0000\n",
    "+++ /tmp/bigram_lm.py\t2021-07-30 11:24:14.340705608 +0000\n",
    "@@ -39,8 +39,8 @@\n",
    "         bigramsplit = k.split(\"_\")\n",
    "         hist = bigramsplit[0]\n",
    "         if hist==j:\n",
    "-            numer = bigramfreqs[k]\n",
    "-            denom = bigramhcs[j]\n",
    "+            numer = bigramfreqs[k]+1\n",
    "+            denom = bigramhcs[j]+bigramnum\n",
    "             frac = numer/denom\n",
    "             y = math.log(frac,2)\n",
    "             z = numer*y\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entropy of the bigram model for this file is: 18.98 bits.\n"
     ]
    }
   ],
   "source": [
    "/tmp/bigram_lm.py /opt/data/lmdom/hpchapter1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a lot of probability mass is now assigned to all of the unseen bigrams\n",
    "- thus the seen bigrams have lower probability\n",
    "- proper English sentences become much less probable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
