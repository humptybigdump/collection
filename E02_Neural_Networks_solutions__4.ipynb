{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cover.png\" style=\"width:800px;height:500px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PATH=/Library/TeX/texbin:$PATH\n",
    "#!pip install nbconvert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second ML1 exercise. We will build our first neural network, which will have a hidden layer. We will also see how to train a neural network for various purposes. We will recognise the difference between several activation functions. \n",
    "\n",
    "**You will learn how to:**\n",
    "- Implement a 2-class classification neural network with a single hidden layer\n",
    "- Use units with a non-linear activation function, such as tanh \n",
    "- Compute the cross entropy loss \n",
    "- Implement forward and backward propagation\n",
    "\n",
    "**Notation**:\n",
    "- Superscript $[l]$ denotes a quantity associated with the $l^{th}$ layer. \n",
    "    - Example: $a^{[L]}$ is the $L^{th}$ layer activation. $W^{[L]}$ and $b^{[L]}$ are the $L^{th}$ layer parameters.\n",
    "- Superscript $(i)$ denotes a quantity associated with the $i^{th}$ example. \n",
    "    - Example: $x^{(i)}$ is the $i^{th}$ training example.\n",
    "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
    "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the $l^{th}$ layer's activations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook we will train a Neural Network with a single hidden layer.\n",
    "\n",
    "**Here is our neural network**:\n",
    "<img src=\"NN.png\" style=\"width:800px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input layer\n",
    "Each node in the input layer refers to each feature in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layers\n",
    "The next layers after the input layer where all the magic happens. The hidden layer takes the input layer and applies a non-linear activation function to it. Mathematically, we can represent the function of the hidden layer as follows:\n",
    "$$f(x) =  \\sigma(Wx + b) $$\n",
    "Where $\\sigma $ refers to the non-linear activation function\n",
    "\n",
    "To keep things simple, we will only use one hidden layer in our model for this notebook. Increasing the number of hidden layers tends to increase the model complexity and training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer\n",
    "These are the output our network arrives at after it performs its calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "When designing the neural network model architecture, we also need to decide what activation functions to use for each layer. Activation functions have an important role to play in neural networks. You can think of activation functions as transformers in neural networks; they take an input value, transform the input value, and pass the transformed value to the next layer. The purpose of the activation function is to introduce non-linearity into the output of a neuron.\n",
    "\n",
    "**Why do we need Non-linear activation functions?**  \n",
    "A neural network without an activation function is essentially just a linear regression model. The activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries:\n",
    "first we will import all the packages that are required for this exercise. \n",
    "- [numpy](www.numpy.org) is the main package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs in Python.\n",
    "- np.random.seed(1) is used to keep all the random function calls consistent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Linear \n",
    "You can interpret this as no activation function. We usually use it in the output layer in regression networks as we don't need any transformation to the outputs.\n",
    "$$f(x) = x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def der_linear(x):\n",
    "    return np.ones_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the function \n",
    "x  = np.linspace(-10, 10, 100)\n",
    "f  = linear(x)\n",
    "df = der_linear(x)\n",
    "plt.plot(x, f, label=\"linear\") \n",
    "plt.plot(x, df, label=\"derivative\")\n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"linear(X)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Sigmoid\n",
    "The Sigmoid Activation Function simply takes a value and squashes it between 0 and 1. You can interpret it as the probability of an output prediction. Therefore, we usually use it in the output layer in binary classification networks. Besides that, we sometimes use it in hidden layers. However, it should be noted that the sigmoid function is monotonic but its derivative is not. Hence, the neural network may get stuck at a suboptimal solution.\n",
    "$$f(x) = \\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "Its derivative \n",
    "$$f'(x) = \\sigma(x)*(1-\\sigma(x)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def der_sigmoid(x):\n",
    "    return sigmoid(x)* (1- sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the function \n",
    "x  = np.linspace(-10, 10, 100)\n",
    "f  = sigmoid(x)\n",
    "df = der_sigmoid(x)\n",
    "plt.plot(x, f, label=\"sigmoid\") \n",
    "plt.plot(x, df, label=\"derivative\")\n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"Sigmoid(X)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When we will use Sigmoid: ####\n",
    "* If you want output value between 0 to 1 use sigmoid at output layer neuron only\n",
    "* When you are doing binary classification problem use sigmoid\n",
    "\n",
    "#### Problem with sigmoid: ####\n",
    "* When the x value is small or big the slope is zero, then there is no learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Hyperbolic Tangent\n",
    "The tanh function is just another possible functions that can be used as a nonlinear activation function between layers of a neural network. It actually shares a few things in common with the sigmoid activation function. They both look very similar. But while a sigmoid function will map input values to be between 0 and 1, Tanh will map values to be between -1 and 1.\n",
    "\n",
    "$$f(x) = tanh(x) = \\frac {sinh(x)}{cosh(x)} = \\frac {e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "Its derivative \n",
    "$$f'(x) = 1 - tanh(x)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def der_tanh(x):\n",
    "    return 1- tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the function \n",
    "x  = np.linspace(-10, 10, 100)\n",
    "f  = tanh(x)\n",
    "df = der_tanh(x)\n",
    "plt.plot(x, f, label=\"hyperbolic\") \n",
    "plt.plot(x, df, label=\"derivative\")\n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"tanh(x)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When we will use tanh: ####\n",
    "* Usually used in hidden layers of a neural network. It helps in centering the data by bringing mean close to 0. This makes learning for the next layer much easier (Optimization is easier in this method). \n",
    "\n",
    "#### Problem with tanh: ####\n",
    "* Vanishing gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. ReLU\n",
    "Today, ReLU is the most popular choice of activation function for DNNs, and it has become a default choice for activation functions. Its range is from 0 to infinity, and both the function itself and its derivative are monotonic. One drawback of the ReLU function is the inability to appropriately map the negative part of the input where all negative inputs are transformed to zero. To fix the \"dying negative\" problem in ReLU, *Leaky ReLU* was invented to introduce a small slope in the negative part.\n",
    "$$f(x) = \\max(0,x)$$\n",
    "\n",
    "The derivative of the ReLU is :\n",
    "\n",
    "$$f'(x) = \\begin{cases} 1 & \\mbox{if } x > 0 \\\\ 0 & x <= 0 \\end{cases}$$\n",
    "\n",
    "**Instructions**:\n",
    "- You will Compare a vector x with sclar 0 and return a new array containing the element-wise maxima \n",
    "    - Use: `numpy.maximum(x, 0)`.\n",
    "- You will Compare vector x with sclar 0 and return a new array containing either ones or zeros\n",
    "    - Use: `np.where(x <= 0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x) :\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def der_relu(x):\n",
    "    i     = np.where(x <= 0)\n",
    "    df    = np.ones_like(x)\n",
    "    df[i] = 0 \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the function \n",
    "x = np.linspace(-10, 10, 100)\n",
    "f = relu(x)\n",
    "df = der_relu(x)\n",
    "plt.plot(x, f,  label=\"relu\")\n",
    "plt.plot(x, df, label=\"derivative\")\n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"ReLU(X)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem with ReLU: ####\n",
    "* The only issue is that the derivative is not defined at $x = 0$, which we can overcome by assigning the derivative to $0$ at $x = 0$. However, this means that for $x <= 0$ the gradient is zero and again can’t learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Leaky ReLU\n",
    "Leaky ReLU is an improved version of the ReLU function. ReLU function, the gradient is 0 for x<0, which made the neurons die for activations in that region, a leaky ReLU will instead have a small negative slope (of 0.01, or so). \n",
    "$$f(x) = max(0.01x, x)$$\n",
    "\n",
    "The derivative of the leaky ReLU is :\n",
    "\n",
    "$$f'(x) = \\begin{cases} 1 & \\mbox{if } x > 0 \\\\ 0.01 & x <= 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x) :\n",
    "    return np.maximum(x, 0.01*x)\n",
    "\n",
    "def der_leaky_relu(x):\n",
    "    i     = np.where(x <= 0)\n",
    "    df    = np.ones_like(x)\n",
    "    df[i] = 0.01 \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the function \n",
    "x = np.linspace(-10, 10, 100)\n",
    "f = leaky_relu(x)\n",
    "df = der_leaky_relu(x)\n",
    "plt.plot(x, f,  label=\"leaky_relu\")\n",
    "plt.plot(x, df, label=\"derivative\")\n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"Leaky_ReLU(x)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Softmax\n",
    "Logistic Regression, softmax is a generalized logistic function used for multiclass classification. Hence, we use it in the output layer in multiclass classification networks.\n",
    "\n",
    "#### Properties of Softmax Function ####\n",
    "\n",
    "1. The calculated probabilities will be in the range of 0 to 1.\n",
    "2. The sum of all the probabilities is equals to 1.\n",
    "\n",
    "#### Softmax Function Usage ####\n",
    "1. Used in multiple classification logistic regression model.\n",
    "2. In building neural networks softmax functions used in different layer level and multilayer perceptrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([2.1, -0.2 , -1.2])\n",
    "y_probas = softmax(x)\n",
    "print('Probabilities:\\\\n', y_probas)\n",
    "print('The sum of Probabilities: ', np.sum(y_probas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps involved in the implementation of a neural network:\n",
    "The general methodology to build a Neural Network is to:\n",
    "1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Forward propagation\n",
    "    - Compute loss\n",
    "    - Backward propagation to get the gradients\n",
    "    - Update parameters (gradient descent, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "On a feedforward neural network, we have a set of input features and some random weights. Forward propagation is all steps from input to prediction:\n",
    "\n",
    "**Mathematically**:   \n",
    "For one example $x^{(i)}$  \n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$ \n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } \\hat{y}^{(i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n",
    "<img src=\"NN.png\" style=\"width:800px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation:\n",
    "During backpropagation, we calculate the error between predicted output and target output (the loss) $\\mathcal{L}$ as follows: \n",
    "$$ \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(\\hat{y}^{(i)}) - (1-y^{(i)} )  \\log(1-\\hat{y}^{(i)})$$\n",
    "\n",
    "* It says that if you make wrong prediction, loss(error) becomes big.\n",
    "    * **Example:** our real image is **sign one** and its label is 1 $(y = 1)$, then we make prediction $\\hat{y} = 1$. When we put y and y_head into loss(error) equation the result is 0. We make correct prediction therefore our loss is 0. However, if we make wrong prediction like $\\hat{y}= 0$, loss(error) is infinity.\n",
    "\n",
    "After that, the cost function is summation of loss function. Each input creates loss function. Cost function is summation of loss functions that is created by each input image.\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "Then we will use an algorithm (gradient descent) to update the weight values to reduce the weights.\n",
    "\n",
    "\n",
    "#### What is Gradient Descent? ####\n",
    "Gradient Descent is an Optimization algorithm that operates iteratively to find the optimal values for its parameters. It takes into account, user-defined learning rate, and initial parameter values.\n",
    "Working: (Iterative)\n",
    "1. Start with initial values.\n",
    "2. Calculate cost.\n",
    "3. Update values using the update function.\n",
    "4. Returns minimized cost for our cost function\n",
    "\n",
    "<img src=\"GD.jpg\" style=\"width:350px;height:250px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain rule ####\n",
    "<img src=\"BP.png\" style=\"width:900px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mathematically**: \n",
    "The goal of backpropagation is to change the weights and biases of the network to reduce loss; in other words, we tune the network parameters to improve its predictions. We want to get $(dW^{[2]},db^{[2]},dW^{[1]},db^{[1]})$.\n",
    "\n",
    "$$ dW^{[2]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[2]}} = \\frac{\\partial \\mathcal{L} }{\\partial a^{[2]}} \\frac{\\partial a^{[2]} }{\\partial z^{[2]}}\\frac{\\partial z^{[2]} }{\\partial W^{[2]}}\\tag{7}$$\n",
    "$$ db^{[2]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[2]}} = \\frac{\\partial \\mathcal{L} }{\\partial a^{[2]}} \\frac{\\partial a^{[2]} }{\\partial z^{[2]}}\\frac{\\partial z^{[2]} }{\\partial b^{[2]}}\\tag{8}$$\n",
    "$$dW^{[1]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[1]}} =  \\frac{\\partial \\mathcal{L} }{\\partial a^{[2]}} \\frac{\\partial a^{[2]} }{\\partial z^{[2]}}\\frac{\\partial z^{[2]} }{\\partial a^{[1]}}\\frac{\\partial a^{[1]} }{\\partial z^{[1]}}\\frac{\\partial z^{[1]} }{\\partial W^{[1]}} \\tag{9}$$\n",
    "$$db^{[1]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[1]}} =  \\frac{\\partial \\mathcal{L} }{\\partial a^{[2]}} \\frac{\\partial a^{[2]} }{\\partial z^{[2]}}\\frac{\\partial z^{[2]} }{\\partial a^{[1]}}\\frac{\\partial a^{[1]} }{\\partial z^{[1]}}\\frac{\\partial z^{[1]} }{\\partial b^{[1]}} \\tag{10}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Initialization of Backpropagation**  \n",
    "To backpropagate through this network, we know that the output is, $a^{[2]} = \\sigma(z^{[2]})$. Your code thus needs to compute $da^{[2]}= \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}}$.\n",
    "$$da^{[2]} = \\frac{\\partial \\mathcal{L} }{\\partial a^{[2]}} =  - (\\frac{y }{a^{[2]}} - \\frac{1-y }{1-a^{[2]}})\\tag{11}$$\n",
    "then,\n",
    "$$dz^{[2]} = \\frac{\\partial \\mathcal{L} }{\\partial z^{[2]}}   = \\frac{\\partial \\mathcal{L} }{\\partial a^{[2]}}  \\frac{\\partial a^{[2]} }{\\partial z^{[2]}}  = da^{[2]} \\sigma'(z^{[2]}) \\tag{12}$$\n",
    "\n",
    "We can now calculate the way to update the parameters $W^{[2]}$ and $b^{[2]}$ to reduce the weight\n",
    "$$  dW^{[2]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[2]}} =  \\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}} \\frac{\\partial z^{[2]} }{\\partial W^{[2]}}= dz^{[2]} a^{[1] T} \\tag{13}  $$\n",
    "$$ db^{[2]} =  \\frac{\\partial \\mathcal{L} }{\\partial b^{[2]}} =  \\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}} \\frac{\\partial z^{[2]} }{\\partial b^{[2]}}= dz^{[2]} \\tag{14}  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute $da^{[1]}= \\frac{\\partial \\mathcal{L}}{\\partial a^{[1]}}$.\n",
    "$$da^{[1]} = \\frac{\\partial \\mathcal{L} }{\\partial a^{[1]}} = \\frac{\\partial\\mathcal{L}}{\\partial z^{[2]}} \\frac{\\partial z^{[2]} }{\\partial a^{[1]}} = W^{[2] T} dz^{[2]} \\tag{15}$$\n",
    "then,\n",
    "$$dz^{[1]} = \\frac{\\partial \\mathcal{L} }{\\partial z^{[1]}}   = \\frac{\\partial \\mathcal{L} }{\\partial a^{[1]}}  \\frac{\\partial a^{[1]} }{\\partial z^{[1]}} = da^{[1]}  \\tanh'(z^{[1]}) \\tag{16} $$\n",
    "To update $W^{[1]},b^{[1]}$ we can use the  equations below\n",
    "$$ \\frac{\\partial \\mathcal{L} }{\\partial W^{[1]}} =  \\frac{\\partial \\mathcal{L}}{\\partial z^{[1]}} \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}= dz^{[1]} x^{T} \\tag{17}  $$\n",
    "$$ \\frac{\\partial \\mathcal{L} }{\\partial b^{[1]}} =  \\frac{\\partial \\mathcal{L}}{\\partial z^{[1]}} \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}= dz^{[1]} \\tag{18}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Parameters\n",
    "\n",
    "In this section you will update the parameters of the model, using gradient descent: \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{19}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{20}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $l \\in \\{1,2\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "In this practical sesion we will start with implementing a shallow network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data:\n",
    "We will be using pandas to load the CSV data to a pandas data frame.\n",
    "### Overview the Data Set\n",
    "For this exercise, we will use the \"Sign Language Digit Data Set\". In this data set, there are 2062 sign language digit images. You know that the numbers are from 0 to 9, so there are ten unique characters. To start the exercise: We will use only the symbols 0 and 1 for simplicity. In data, sign zero is between the indices 204 and 408. The number of zeros is 205. The sign one is also between the indices 822 and 1027. The number with the first sign is 206. Therefore, we will use 205 samples from each class (labels).\n",
    "\n",
    "**Note:** 205 samples are tiny for deep learning. But since it is a tutorial, it is not that important.\n",
    "\n",
    "\n",
    "Lets prepare our X and Y arrays. X is image array (zero and one signs) and Y is label array (0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data set\n",
    "x_l = np.load('./X.npy')\n",
    "Y_l = np.load('./Y.npy')\n",
    "img_size = 64\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(x_l[260].reshape(img_size, img_size),cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(x_l[900].reshape(img_size, img_size),cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we will do to preprocess the dataset:**\n",
    "* Choose our labels (classes) that are sign zero and sign one\n",
    "* Create and flatten train and test sets\n",
    "* Our final inputs(images) and outputs(labels or classes) should look like this:\n",
    "\n",
    "<img src=\"example.png\" style=\"width:500px;height:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an image array, we will concatenate null character and one character arrays (to create a small dataset). Then we will create a label array 0 for images with zero sign and 1 for images with one sign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join a sequence of arrays along an row axis.\n",
    "X = np.concatenate((x_l[204:409], x_l[822:1027] ), axis=0) # from 0 to 204 is zero sign and from 205 to 410 is one sign \n",
    "z = np.zeros(205)\n",
    "o = np.ones(205)\n",
    "Y = np.concatenate((z, o), axis=0).reshape(X.shape[0],1)\n",
    "print(\"X shape: \" , X.shape)\n",
    "print(\"Y shape: \" , Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the X is (410, 64, 64). 410 means that we have 410 images (zero and one signs). 64 means that our image size is 64x64 (64x64 pixels). The shape of the Y is (410,1). 410 means that we have 410 labels (0 and 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split X and Y into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then lets create x_train, y_train, x_test, y_test arrays\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "number_of_train = X_train.shape[0]\n",
    "number_of_test = X_test.shape[0]\n",
    "print(\"number of samples in training data set: \" , number_of_train)\n",
    "print(\"number of samples in test data set: \" , number_of_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `test_size` = percentage of test size.\n",
    "* `random_state` = use same seed while randomizing. It means that if we call train_test_split repeatedly, it always * creates same train and test distribution because we have same random_state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 3 dimensional input array (X) so we need to make it flatten (2D) in order to use as input for our first deep learning model. Our label array (Y) is already flatten(2D) so we leave it like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flatten = X_train.reshape(number_of_train,X_train.shape[1]*X_train.shape[2])\n",
    "X_test_flatten = X_test .reshape(number_of_test,X_test.shape[1]*X_test.shape[2])\n",
    "print(\"X train flatten\",X_train_flatten.shape)\n",
    "print(\"X test flatten\",X_test_flatten.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have 62 images and each image has 4096 pixels in image test array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train_flatten.T\n",
    "x_test = X_test_flatten.T\n",
    "y_train = Y_train.T\n",
    "y_test = Y_test.T\n",
    "print(\"x train: \",x_train.shape)\n",
    "print(\"x test: \",x_test.shape)\n",
    "print(\"y train: \",y_train.shape)\n",
    "print(\"y test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model's parameters:\n",
    "We will begin to use random weights that we will optimize using backward propagation. The number of parameters we have to initialize is \n",
    "* $n_x$ -- size of the input layer\n",
    "* $n_h$ -- size of the hidden layer\n",
    "* $n_y$ -- size of the output layer\n",
    "\n",
    "**Exercise**: Implement the function `initialize_parameters()` to be used to initialize parameters for a two layer model.\n",
    "\n",
    "**Remember** that when we compute $W X + b$ in python, it carries out broadcasting. For example, if: \n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{10}$$\n",
    "\n",
    "Then $WX + b$ will be:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{11}  $$\n",
    "\n",
    "\n",
    "**Instructions**:\n",
    "- Make sure your parameters' sizes are right. Refer to the neural network figure above if needed.\n",
    "- You will initialize the weights matrices with random values. \n",
    "    - Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b).\n",
    "- You will initialize the bias vectors as zeros. \n",
    "    - Use: `np.zeros((a,b))` to initialize a matrix of shape (a,b) with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_dims, init_method =\"random\"):\n",
    "    np.random.seed(1)\n",
    "    n_x, n_h, n_y = layers_dims\n",
    "    if init_method == \"random\":\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "        b1 = np.zeros(shape=(n_h, 1))\n",
    "        W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "        b2 = np.zeros(shape=(n_y, 1))\n",
    "        ### END CODE HERE ###\n",
    "    elif init_method == \"xavier\":\n",
    "        W1 = np.random.randn(n_h,n_x)*np.sqrt(1/n_h)\n",
    "        b1 = np.zeros(shape=(n_h, 1))\n",
    "        W2 = np.random.randn(n_y,n_h)\n",
    "        b2 = np.zeros(shape=(n_y, 1))\n",
    "    elif init_method == \"zeros\":\n",
    "        W1 = np.zeros(shape=(n_h, n_x))\n",
    "        b1 = np.zeros(shape=(n_h, 1))\n",
    "        W2 = np.zeros(shape=(n_y, n_h))\n",
    "        b2 = np.zeros(shape=(n_y, 1))\n",
    "    \n",
    "    # Test if the shape correct is\n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x=3 \n",
    "n_h=2 \n",
    "n_y=1\n",
    "layers_dims = [n_x,n_h,n_y]\n",
    "parameters = initialize_parameters(layers_dims,init_method = \"random\")\n",
    "print(\"W1 = \\\\n\" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \\\\n\" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \\\\n\" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \\\\n\" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(layers_dims,init_method = \"xavier\")\n",
    "print(\"W1 = \\\\n\" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \\\\n\" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \\\\n\" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \\\\n\" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Forward propagation :\n",
    "**Exercise:**\n",
    "Forward propagation is all steps from input to prediction, use the equations (1)-(5) and the help functions `tanh` and `sigmoid` to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    z1 = np.dot(W1, X) + b1\n",
    "    a1 = tanh(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Test if the shape correct is\n",
    "    assert(a2.shape == (1, X.shape[1]))\n",
    "    cache = {\"z1\": z1,\n",
    "             \"a1\": a1,\n",
    "             \"W1\": W1,\n",
    "             \"b1\": b1,\n",
    "             \"z2\": z2,\n",
    "             \"a2\": a2,\n",
    "             \"W2\": W2,\n",
    "             \"b2\": b2}    \n",
    "    return a2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Loss function:\n",
    "**Exercise:**\n",
    "Implement the loss function with equation 6, this function takes the output of `forward_propagation` and the true label as input and returns the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(a2, Y):\n",
    "    m = Y.shape[1]\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    logprobs = np.multiply(-np.log(a2),Y) + np.multiply(-np.log(1 - a2), 1 - Y)\n",
    "    loss = 1./m * np.nansum(logprobs)\n",
    "    ### END CODE HERE ###\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Backpropagation :\n",
    "**Exercise:**\n",
    "Backpropagation is all steps from cost calculation to calculation of the gradient, use the equations (11)-(18) and the help functions `der_tanh` and `der_sigmoid` to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, cache):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    # Retrieve each variables from the dictionary \"cache\"\n",
    "    z1 = cache['z1']\n",
    "    a1 = cache['a1']\n",
    "    W1 = cache['W1']\n",
    "    b1 = cache['b1']\n",
    "    z2 = cache['z2']\n",
    "    a2 = cache['a2']\n",
    "    W2 = cache['W2']\n",
    "    b2 = cache['b2']\n",
    "    ### START CODE HERE ### (8 line of code) \n",
    "    # Initializing the backpropagation ()\n",
    "    da2 =  - (np.divide(Y, a2) - np.divide(1 - Y, 1 - a2))\n",
    "    \n",
    "    # 2th layer (SIGMOID -> LINEAR) gradients\n",
    "    dz2 = np.multiply(da2,der_sigmoid(z2)) # da2 * sigmoid'    \n",
    "    \n",
    "    #  Implement the linear portion of backward propagation for a single layer (approx. 2 lines)\n",
    "    dW2 = 1/m * np.dot(dz2, a1.T)\n",
    "    db2 = 1/m * np.sum(dz2, axis=1, keepdims = True)\n",
    "    \n",
    "    # LINEAR z2 -> activation a2\n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    \n",
    "    # 1th layer (tanh -> LINEAR) gradients.\n",
    "    dz1 = np.multiply(da1, der_tanh(z1))\n",
    "    \n",
    "    # Implement the linear portion of backward propagation for a single layer (approx. 2 lines)\n",
    "    dW1 = 1/m * np.dot(dz1, X.T)\n",
    "    db1 = 1/m * np.sum(dz1, axis=1, keepdims = True)\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Test if the shape correct is\n",
    "    assert (da2.shape == a2.shape)\n",
    "    assert (dz2.shape == z2.shape)\n",
    "    assert (dW2.shape == W2.shape)\n",
    "    assert (db2.shape == b2.shape)\n",
    "    assert (da1.shape == a1.shape)\n",
    "    assert (dz1.shape == z1.shape)\n",
    "    assert (dW1.shape == W1.shape)\n",
    "    assert (db1.shape == b1.shape)\n",
    "    gradients = {\"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).\n",
    "\n",
    "**General gradient descent rule**: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter.\n",
    "\n",
    "**Illustration**: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    n = len(parameters) // 2 # number of layers in the neural networks\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for k in range(n):\n",
    "        parameters[\"W\" + str(k+1)] = parameters[\"W\" + str(k+1)] - learning_rate * grads[\"dW\" + str(k+1)]\n",
    "        parameters[\"b\" + str(k+1)] = parameters[\"b\" + str(k+1)] - learning_rate * grads[\"db\" + str(k+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "Now we want to train our model and combine all functions to first calculate the forward propagation, then estimate the loss function and the gradient, and finally update the parameters to reduce the loss. update the parameters using the gradient decay.\n",
    "\n",
    "**Exercise:** Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "def optimize(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"random\"):\n",
    "    \n",
    "    grads      = {}\n",
    "    costs      = [] # to keep track of the loss\n",
    "    accuracies = [] # to keep track of the accuracy\n",
    "    m = X.shape[1] # number of examples\n",
    "    layers_dims = [X.shape[0], 10, 1]\n",
    "    \n",
    "    # Initialize parameters dictionary.\n",
    "    if initialization == \"zeros\":\n",
    "        parameters = initialize_parameters(layers_dims, init_method =\"zeros\")\n",
    "    elif initialization == \"random\":\n",
    "        parameters = initialize_parameters(layers_dims, init_method =\"random\")\n",
    "    elif initialization == \"xavier\":\n",
    "        parameters = initialize_parameters(layers_dims, init_method =\"xavier\")\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        a2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Loss\n",
    "        cost = compute_loss(a2, Y)\n",
    "\n",
    "        # Backward propagation.\n",
    "        grads = backward_propagation(X, Y, cache)\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the loss every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "            p = predict(X, parameters)\n",
    "            accuracy = np.mean(p[0,:] == Y[0,:])\n",
    "            print(\"Accuracy after iteration {}: {}\".format(i, accuracy))\n",
    "            costs.append(cost)\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "    # plot the loss, accuracy\n",
    "    fig1, (ax1, ax2) = plt.subplots(figsize=(10,12), nrows=2, ncols=1)\n",
    "    ax1.plot(costs)\n",
    "    ax1.set_ylabel('cost')\n",
    "    ax1.set_xlabel('iterations (per hundreds)')\n",
    "    ax1.set_title(\"Learning rate =\" + str(learning_rate))\n",
    "    ax2.plot(accuracies)\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.set_xlabel('iterations (per hundreds)')\n",
    "    ax2.set_title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = optimize(x_train, y_train, learning_rate = 0.01, num_iterations = 10000, print_cost = True, initialization = \"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Predictions\n",
    "\n",
    "Now is the time to use our model to predict by building predict() function.\n",
    "Use forward propagation to predict results.\n",
    "\n",
    "**Reminder**: predictions = $y_{prediction} = \\mathbb 1 \\text{{activation > 0.5}} = \\begin{cases}\n",
    "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$  \n",
    "    \n",
    "As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: ```X_new = (X > threshold)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    # Prediction for one image or for a batch of images  \n",
    "    m = X.shape[1] # batch of images      \n",
    "    p = np.zeros((1,m), dtype = np.int)\n",
    "    \n",
    "    # Forward propagation\n",
    "    a2, caches = forward_propagation(X, parameters)\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, a2.shape[1]):\n",
    "        if a2[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "            \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test[0].reshape(img_size, img_size),cmap='gray')\n",
    "plt.show()\n",
    "real_image = X_test[0]\n",
    "prep_image = np.expand_dims(x_test[:,0],axis=1)\n",
    "p = predict(prep_image, parameters) \n",
    "print(\"the model prediction {}\".format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = predict(x_test, parameters)\n",
    "accuracy = np.mean(p[0,:] == y_test[0,:])\n",
    "print(\"Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Choice of learning rate\n",
    "\n",
    "**Reminder**:\n",
    "The learning rate $\\alpha$  determines how rapidly we update the parameters. If the learning rate is too large we may \"overshoot\" the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That's why it is crucial to use a well-tuned learning rate.\n",
    "\n",
    "Let's compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the `learning_rates` variable to contain, and see what happens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Varying the hidden layer size ####\n",
    "In the example above we picked a hidden layer size of 3. Let’s now get a sense of how varying the hidden layer size affects the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Preventing overfitting in neural networks ####\n",
    "\n",
    "#### Dropout #### \n",
    "\n",
    "means ignoring a certain set of hidden nodes during the learning phase of a neural network. And those hidden nodes are chosen randomly given a specified probability. In the forward pass during a training iteration, the randomly selected nodes are temporarily not used in calculating the loss; in the backward pass, the randomly selected nodes are not updated temporarily.\n",
    "\n",
    "#### Early stopping ####\n",
    "As the name implies, training a network with early stopping will end if the model performance doesn't improve for a certain number of iterations. The model performance is measured on a validation set that is different from the training set, in order to assess how well it generalizes. During training, if the performance degrades after several (let's say 50) iterations, it means the model is overfitting and not able to generalize well anymore. Hence, stopping the learning early in this case helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Instead of batch gradient descent, use minibatch gradient descent (more info) to train the network. Minibatch gradient descent typically performs better in practice.\n",
    "2. We used a fixed learning rate $\\epsilon$ for gradient descent. Implement an annealing schedule for the gradient descent learning rate (more info).\n",
    "3. We used a $\\tanh$ activation function for our hidden layer. Experiment with other activation functions (some are mentioned above). Note that changing the activation function also means changing the backpropagation derivative.\n",
    "4. Extend the network from two to three classes. You will need to generate an appropriate dataset for this.\n",
    "5. Extend the network to four layers. Experiment with the layer size. Adding another hidden layer means you will need to adjust both the forward propagation as well as the backpropagation code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://www.geeksforgeeks.org/activation-functions/\n",
    "2. https://medium.com/@omkar.nallagoni/activation-functions-with-derivative-and-python-code-sigmoid-vs-tanh-vs-relu-44d23915c1f4\n",
    "3. https://maelfabien.github.io/deeplearning/act/#linear-activation\n",
    "4. Deep Learning Specialization on Coursera\n",
    "5. https://github.com/Kulbear/deep-learning-coursera\n",
    "6. https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_SA",
   "language": "python",
   "name": "venv_sa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
