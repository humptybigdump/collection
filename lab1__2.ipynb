{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b41de7e3-a554-43ab-9f50-1b61c1eccc65",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Obtaining sources\n",
    "\n",
    "Today we are going to talk how to obtain sources for digital history projects. Generally it could be done in two ways, either via already existing Internet Archives or on Your own while using Web Scraping and platforms' API (application programming interface)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e3052-a7a4-46e1-9b76-98317b98e31a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Internet archives\n",
    "\n",
    "As discussed during last meeting it is possible to use open-access Internet Archives (links on Ilias). Such data are usually delivered without any 'methodological' description how there were obtained or without any source code. Note that there are certain standards (e.g. Dublin Core) how to 'describe' databases using specific metadata. Although they function in different formats depending on the source type (html, json, png etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f896e6c-81ae-48b6-bc5e-54eebe4f5bdb",
   "metadata": {},
   "source": [
    "## Web scrapping\n",
    "\n",
    "Web Scraping is a type of 'Data Scraping'. In other words it is a technique that extracts data from other program (in our case platform or website)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09671c9-39ba-43ca-8986-c8f4a82b765c",
   "metadata": {},
   "source": [
    "## Automatization of data archiving\n",
    "\n",
    "We are going to use two methods in automated web scraping: API (recommended by platform owners) and automated browser scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f342b68-97a6-4ceb-a378-32a4c992dabb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827cd0fd-0120-47b3-890b-7bfeee89d570",
   "metadata": {},
   "source": [
    "An application programming interface (API) allows communication between computers or between computer programs (in contrast to a user interface, which connects a computer to a person).  It is not intended to be used directly by a person (the end user) other than a computer programmer who is incorporating it into the software. API form is always standarized and dedscribed in API documentation (specification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3920a2d5-122e-4922-b187-ff01fe0f2a93",
   "metadata": {},
   "source": [
    "In our example we utilize Wikipedia API to scrap some articles from Wikipedia. We use (Wikipedia-API)[https://pypi.org/project/Wikipedia-API/] Python module. In order to install the module, open terminal and run command:\n",
    "\n",
    "```bash\n",
    "pip install wikipedia-api\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb99112-a467-46d1-9032-2a681f52b713",
   "metadata": {},
   "source": [
    "Now you are able to use this module in notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3979cc7-9668-433b-9d2a-cb99088b81f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee031ba7-9818-49b4-9b52-18b1e99af4c5",
   "metadata": {},
   "source": [
    "Firstly you need to initialize `Wikipedia` object (this is the 'thing' making API request for us), which takes language as argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c8adf-080e-40eb-a986-6ffd11ce9252",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = wikipediaapi.Wikipedia(language=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a9eacc-1682-4a76-97fc-5d67a1cb2888",
   "metadata": {},
   "source": [
    "Let's get the first article, which title is `\"Python (programming language)\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92c7782-69e9-4342-b74f-8918266ee7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = wiki.page(\"Python (programming language)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197db1ed-2632-41c4-80cd-a1abbee72df9",
   "metadata": {},
   "source": [
    "Now all the data is stored in variable `page`. Let's try to extract the URL of the article to compare with original website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f526595-53be-48a2-a16a-2bcd1932e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page.fullurl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7245a2b-7d08-44ba-9598-f09a1d1b0f14",
   "metadata": {},
   "source": [
    "You can open it (watch for all characters in link). To get the full text of article, use attribute `text` (function `print` is used here for more human-friendly output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af01c59b-49d3-43bd-9779-765c21811346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ebf725-a4be-4f8f-8248-7b37054c8f33",
   "metadata": {},
   "source": [
    "You can also extract parts of text, for example summary of the article (attribute: `summary`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57faee2-43de-4088-9e77-9293ba851eab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(page.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d488a0-dd62-4c8c-b1e8-17439502d377",
   "metadata": {},
   "source": [
    "or list of sections (attribure: `sections`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b260e-d61d-4480-9745-0ec6ef4b2841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "page.sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca072c1-2d95-4ead-af4f-5c35a7c71e07",
   "metadata": {
    "tags": []
   },
   "source": [
    "or section by title (method `section_by_title` taking title as argument):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4de258-8d00-4580-89b5-1d01457538b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "page.section_by_title(\"History\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dede13-345b-4246-86ba-798e86b05949",
   "metadata": {},
   "source": [
    "Moreover you can obtain additional information about the article, such as list of pages in other languages (attribute: `langlinks`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84de9ba-28e3-45fb-a909-cca95a24ca77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "page.langlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32196250-7783-45db-8cec-33dd39958924",
   "metadata": {},
   "source": [
    "For example to get article in german:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781fc006-f232-49a1-9d04-78004e98c0d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(page.langlinks[\"de\"].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9271630-7834-458d-a9b3-86ea9bfe1ecb",
   "metadata": {},
   "source": [
    "To get list of categories, use attribute `categories`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca04664-a703-4550-b8fb-b3028e9a0815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "page.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce611e58-fb7a-487e-9176-1f32c8641629",
   "metadata": {},
   "source": [
    "You can also extract all pages refered in links in article, using `links`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b9fc8-5a15-4ae1-8128-ee7d1bcff92c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "page.links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca1127-2c1c-4f99-8358-ece0cd668c56",
   "metadata": {},
   "source": [
    "and back references, using `backlinks`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d5bb5-f882-4695-a227-cde121fdf6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "page.backlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2b8943-8dee-4e35-9099-6d74d1d43cdd",
   "metadata": {},
   "source": [
    "### Parsing websites\n",
    "\n",
    "While different APIs have their limitations (e.g. time or querry limit) it is possible to somehow overcome such problem. One approach is to download the source of website and parse its content to extract interesting data. In the simplest case website consists of just plain HTML. However, nowadays a lot of websites are dynamic, which means they contain information that changes, depending on the viewer, the time of the day, the time zone, the viewer's native language, and other factors. To obtain desired data you usually have to send specific request and preprocess huge distionaries with massive amount of noise.\n",
    "On the higher level, the automated browsers (e.g. Selenium) can be utilized to simplify interacting with webpage. It imitates user behaviour like opening links or filling in data, which can be programmed and repeated over and over to collect necessary amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c026d99-b199-4a84-b4bb-c1caf177d829",
   "metadata": {},
   "source": [
    "#### YouTube scrapping - using raw requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e191679c-ee96-4dc3-af1f-e7a1fa5e6dc5",
   "metadata": {},
   "source": [
    "To perform YouTube scrapping we will use our dedicated module written in Python. To install the module, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c4ac8ca-b752-4e2b-9a0c-ea5fa9316852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://gitlab.com/digital-history1/youtube-scrapper.git@main\n",
      "  Cloning https://gitlab.com/digital-history1/youtube-scrapper.git (to revision main) to /tmp/pip-req-build-02wkg6pc\n",
      "  Running command git clone --filter=blob:none --quiet https://gitlab.com/digital-history1/youtube-scrapper.git /tmp/pip-req-build-02wkg6pc\n",
      "  Resolved https://gitlab.com/digital-history1/youtube-scrapper.git to commit 4be059ca0f2f64fae6bf31d989acad0fab7300ee\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4==4.11.1 in /usr/local/lib/python3.10/site-packages (from youtube-scrapper==0.1.0) (4.11.1)\n",
      "Requirement already satisfied: requests==2.27.1 in /usr/local/lib/python3.10/site-packages (from youtube-scrapper==0.1.0) (2.27.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/site-packages (from beautifulsoup4==4.11.1->youtube-scrapper==0.1.0) (2.3.2.post1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests==2.27.1->youtube-scrapper==0.1.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests==2.27.1->youtube-scrapper==0.1.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/site-packages (from requests==2.27.1->youtube-scrapper==0.1.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests==2.27.1->youtube-scrapper==0.1.0) (3.3)\n",
      "Building wheels for collected packages: youtube-scrapper\n",
      "  Building wheel for youtube-scrapper (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for youtube-scrapper: filename=youtube_scrapper-0.1.0-py3-none-any.whl size=7460 sha256=676e693be1a44e25d50a7b5c35c60d40588d322719fb4d64a8c864105de7983e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6skg433n/wheels/eb/21/28/f7a100af56ceeaadfc1f79e6ebda71d084a407ec9e4d951139\n",
      "Successfully built youtube-scrapper\n",
      "Installing collected packages: youtube-scrapper\n",
      "Successfully installed youtube-scrapper-0.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://gitlab.com/digital-history1/youtube-scrapper.git@main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcf0396-7982-4fa9-9c48-2cd4a28745ba",
   "metadata": {},
   "source": [
    "*The package is still under development, so we appreciate any feedback and issue reports: https://gitlab.com/digital-history1/youtube-scrapper/-/issues .*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04742b9-c865-4434-a53b-7c45a82d15f1",
   "metadata": {},
   "source": [
    "Currently the package consists of two main obiects: `Video` and`Channel`. The former allows you to collect metadata of the video by its ID (unique part of URL, identifying the video). Using the latter, you can scrap all the videos from given channel. We begin with importing module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d4df751-fa1d-42f5-9baf-38bb00b57b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import youtube_scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6917d2d2-fcd0-4377-9600-3aca7a6cb69e",
   "metadata": {},
   "source": [
    "Consider the following URL: `https://www.youtube.com/watch?v=wmgyXK84TR0`. The string apearing after `watch?v=` is the video ID. To obtain metadata of the video, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a23ab-adaa-4893-a053-83202f90ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = youtube_scrapper.Video(\"wmgyXK84TR0\")\n",
    "v.get_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a00f9a-aa09-4c08-838f-bc307c0edd0f",
   "metadata": {},
   "source": [
    "Now you have access to several, self-explenatory attributes, including:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb25072-fd7e-4b49-ad66-025236ccf100",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945846b3-b9af-47ad-b90d-e4b8df9d9193",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a5e0d-a238-4ee1-bb52-4597e8eb3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.channel_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0ccd7-db6a-4029-8437-ab774b55f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103c7c7-417e-4fb4-a5c7-ad5f9723814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c31b9ce-1f65-4360-b281-0e65ab10a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.thumbnail  # URL to thumbnail of the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4087ba9-ac41-4e43-b074-cebdde3ce321",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.duration  # in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6068f775-b0fb-44aa-b23c-74fb6c094fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(v.published_at.date())  # v.published_at is datatime object, so you need this trick for pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cccb7d-3113-4c7b-9093-946ca0e8072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.view_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78361a4d-1eac-4756-8996-db206b7cec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.like_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418196d1-0333-4df6-8708-3baa2a2263ae",
   "metadata": {},
   "source": [
    "We obtained some interesting information about the video, however repeating this for multiple videos would be annoying. That is where `Channel` object becomes handy. Use the following snippet of code to download metadata of all videos from given channel and save it to `python_simplified_metadata.json` file (more about file formats later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746e37a-4018-4dd0-a1bc-2b3e391c75b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = youtube_scrapper.Channel.get_by_title(\"PythonSimplified\")\n",
    "c.dump_all_videos_to_json(\"python_simplified_metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b87f2c-1cdb-428d-8e17-2747ab839def",
   "metadata": {},
   "source": [
    "Now you can check out the output by opening file `python_simplified_metadata.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a5e305-0667-4ed5-86c0-0ccda7f7814c",
   "metadata": {},
   "source": [
    "#### Twitter scrapping - using Selenium\n",
    "\n",
    "Altough Twitter has it's own API (https://developer.twitter.com/en/docs/twitter-api) it allows only to make 100 000 requests in 24 hours, scraping tweets for last 7 days. With Tweepy, one of the well-built Python library for Twitter, you can get as much as 3 200 last tweets. Moreover you have to register a developer account on Twitter to receive necessary authentication keys for your app in order to communicate with API.\n",
    "\n",
    "There is also a new possibility from Twitter where you can acquire for a 'academic account' with most of the limits stripped but, for now, this solution is not such popular as webscrapping with Selenium.\n",
    "\n",
    "That's when automated browser scrapping with Selenium comes handy.\n",
    "\n",
    "https://betterprogramming.pub/twitter-scrapers-are-all-broken-what-should-we-do-62a7349bfca6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a08fb62-87af-44bf-b230-677c0708522b",
   "metadata": {},
   "source": [
    "##### Selenium installation guide\n",
    "\n",
    "Let's start with installing Selenium library for Python:\n",
    "\n",
    "https://selenium-python.readthedocs.io/installation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34fd9a0-bcba-46f9-b8ea-2b5d3b488ddd",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Scweet installation guide\n",
    "\n",
    "We are going to use Scweet library for Python: https://github.com/Altimis/Scweet\n",
    "\n",
    "In general it allows us to bypass the spoken limitations. Bear in mind that such automatization might be banned by the platform if too many requests are sent. That is why most of scripts uses time delays between the actions that limits the number of request to be made e.g. in 1 second breaks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ced3ea7-b118-4530-bf38-66e16689e234",
   "metadata": {},
   "source": [
    "Note : You must have Chrome installed on your system. Run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31713755-121d-4a46-a833-e743fc4401fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Scweet==1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b30cbb7-b2bb-4560-847d-6cd03093a0f6",
   "metadata": {},
   "source": [
    "As simple as that ;)\n",
    "\n",
    "We need to restart our kernel to go on.\n",
    "\n",
    "Open a new notebook and lets start coding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843da5d-f952-4974-83ef-b65abf8d05c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scweet.scweet import scrape\n",
    "from Scweet.user import get_user_information, get_users_following, get_users_followers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efba5874-e2dd-404d-9578-462b8bcddf68",
   "metadata": {},
   "source": [
    "Ok we have installed Scweet and imported all the necessary methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100ebf5-58a1-41c6-b1e2-17e202db07c7",
   "metadata": {},
   "source": [
    "Let's analyze README.md file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a6693-257f-4e60-81a5-d3aa5c0b6f10",
   "metadata": {},
   "source": [
    "Scweet output is stored in `csv` file (comma separated value) and contains the following attributes:\n",
    "\n",
    ">    'UserScreenName' :  \n",
    ">    'UserName' : UserName  \n",
    ">    'Timestamp' : timestamp of the tweet  \n",
    ">    'Text' : tweet text  \n",
    ">    'Embedded_text' : embedded text written above the tweet. This can be an image, a video or even another tweet if the tweet in question is a reply  \n",
    ">    'Emojis' : emojis in the tweet  \n",
    ">    'Comments' : number of comments  \n",
    ">    'Likes' : number of likes  \n",
    ">    'Retweets' : number of retweets  \n",
    ">    'Image link' : link of the image in the tweet  \n",
    ">    'Tweet URL' : tweet URL  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d3020-5ec5-4810-86f9-915154614c5d",
   "metadata": {},
   "source": [
    "Note that Twitter loves changing its front-end so output data might differ in future and the code will require some update (you can always check 'issue' in a library repository (on either gitlab or github) for any problems and solutions.\n",
    "For now we found the following bugs:\n",
    "\n",
    "1. 'text' and 'embedded_text' are mixed - in text field you can only see the user name but in embedded text you will find everything what is needed (plus stats from metadata).\n",
    "1. 'likes' and 'retweets' columns are mutually mixed.\n",
    "1. unfortunately tweets with video are doubles (see: https://github.com/Altimis/Scweet/issues/126)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6613d3-ebf7-4fa8-91d1-3744db565f48",
   "metadata": {},
   "source": [
    "Start with very basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26ac2c92-3b9e-4c3f-8c77-95ac59b072b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape\u001b[49m(\n\u001b[1;32m      2\u001b[0m     since\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2022-05-09\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     until\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2022-05-18\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     from_account\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdonaldtusk\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      6\u001b[0m     headless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     display_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     save_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     proxy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     resume\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     filter_replies\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m     proximity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scrape' is not defined"
     ]
    }
   ],
   "source": [
    "data = scrape(\n",
    "    since=\"2022-05-09\",\n",
    "    until=\"2022-05-18\",\n",
    "    from_account=\"donaldtusk\",\n",
    "    interval=1,\n",
    "    headless=True,\n",
    "    display_type=\"Top\",\n",
    "    save_images=False,\n",
    "    proxy = None,\n",
    "    save_dir = \"outputs\",\n",
    "    resume=False,\n",
    "    filter_replies=True,\n",
    "    proximity=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784dbbca-7b9f-4f8a-9e70-8a41050e8086",
   "metadata": {},
   "source": [
    "As you can see this method takes a bunch of arguments (variables entered by user) to run. In general try to edit those variables that interest you (dates, usernames...). There is whole list of optional arguments:\n",
    "```\n",
    "  -h, --help            show this help message and exit\n",
    "  --words WORDS         Words to search for. they should be separated by \"//\" : Cat//Dog.\n",
    "  --from_account FROM_ACCOUNT\n",
    "                        Tweets posted by \"from_account\" account.\n",
    "  --to_account TO_ACCOUNT\n",
    "                        Tweets posted in response to \"to_account\" account.\n",
    "  --mention_account MENTION_ACCOUNT\n",
    "                        Tweets that mention \"mention_account\" account.         \n",
    "  --hashtag HASHTAG\n",
    "                        Tweets containing #hashtag\n",
    "  --until UNTIL         End date for search query. example : %Y-%m-%d.\n",
    "  --since SINCE\n",
    "                        Start date for search query. example : %Y-%m-%d.\n",
    "  --interval INTERVAL   Interval days between each start date and end date for\n",
    "                        search queries. example : 5.\n",
    "  --lang LANG           Tweets language. Example : \"en\" for english and \"fr\"\n",
    "                        for french.\n",
    "  --headless HEADLESS   Headless webdrives or not. True or False\n",
    "  --limit LIMIT         Limit tweets to be scraped.\n",
    "  --display_type DISPLAY_TYPE\n",
    "                        Display type of Twitter page : Latest or Top tweets\n",
    "  --resume RESUME       Resume the last scraping. specify the csv file path.\n",
    "  --proxy PROXY         Proxy server\n",
    "  --proximity PROXIMITY Proximity\n",
    "  --geocode GEOCODE     Geographical location coordinates to center the\n",
    "                        search (), radius. No compatible with proximity\n",
    "  --minreplies MINREPLIES\n",
    "                        Min. number of replies to the tweet\n",
    "  --minlikes MINLIKES   Min. number of likes to the tweet\n",
    "  --minretweets MINRETWEETS\n",
    "                        Min. number of retweets to the tweet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8739e02-fa1f-4a72-9904-3c6a0c32a27d",
   "metadata": {},
   "source": [
    "This one allows to scrape by `hashtags` in proximity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf6ba0-c050-45bf-a677-c9fad40abfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scrape(\n",
    "    hashtag=\"covid19\",\n",
    "    since=\"2020-04-01\",\n",
    "    until=\"2020-04-15\",\n",
    "    from_account = None,\n",
    "    interval=1,\n",
    "    headless=True,\n",
    "    display_type=\"Top\",\n",
    "    save_images=False,\n",
    "    proxy = None,\n",
    "    save_dir = 'outputs',\n",
    "    resume=False,\n",
    "    filter_replies=True,\n",
    "    proximity=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6227e463-ef44-422f-9d34-c3796b0d70c9",
   "metadata": {},
   "source": [
    "If interested you may try scraping with different words but bear in mind as it is to general and broad approach you might receive a lot of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b044d9-af70-4f38-8995-8c0b1e361607",
   "metadata": {},
   "source": [
    "In this example provided from the repository code searches for tweets in proximity of 200 km from Alicante (Spain) with the words `bitcoin` and `ethereum`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa7f617-1627-4137-a3fd-e0e8b2649859",
   "metadata": {},
   "source": [
    "data = scrape(\n",
    "    words=[\"bitcoin\", \"ethereum\"],\n",
    "    since=\"2021-10-01\",\n",
    "    until=\"2021-10-05\",\n",
    "    from_account = None,\n",
    "    interval=1,\n",
    "    headless=False,\n",
    "    display_type=\"Top\",\n",
    "    save_images=False,\n",
    "    lang=\"en\",\n",
    "    resume=False,\n",
    "    filter_replies=False,\n",
    "    proximity=False,\n",
    "    geocode=\"38.3452,-0.481006,200km\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32654c6-7943-4b06-ac0b-62b8bded4ac8",
   "metadata": {},
   "source": [
    "You will find your results in 'outputs' directory titled under the name of the account scraped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab64cca9-9851-407c-ae35-56ab884d1a8a",
   "metadata": {},
   "source": [
    "## Data storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abefe1d7-918e-464c-96ac-fb1dbe7ccd90",
   "metadata": {},
   "source": [
    "The next step (after scrapping and data preprocessing) is storing data. Without proper data storage we are vulnerable to ineffective accesing data and even losing it (e.g. as an efect of hardware failure or ransomware attack). Besides choosing right format, one should consider performing regular backups of the data. \n",
    "\n",
    "The easiest way (sufficient for the begining) is to store data in files. Other, more advanced solutions such as databases, requires dedicated software and specialized knowlegde, which is beyond the scope of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba0309-6379-449c-a6f6-d1b5274f6e80",
   "metadata": {},
   "source": [
    "### File formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81e2d4-57f0-4b64-a9a0-6572f20c58fc",
   "metadata": {},
   "source": [
    "#### TXT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa918d1-eda3-44a4-8731-f65188522e1a",
   "metadata": {},
   "source": [
    "Storing data in plain text files is the easiest way, although structuralization of data is impossible. This solution can be utilized in case of storing long texts (for example wikipedia article). \n",
    "Plain text files does not contain any information about text formatting (in contrast to e.g. DOCX files), so they are space efficient, independent of the platform and easy to further process. Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73bc535-5a7b-4366-9e80-44310f7406ae",
   "metadata": {},
   "source": [
    "`plain_text_file.txt`:\n",
    "```\n",
    "This is the plain text file containing text, some text and even more text. The next lines are just generic content.\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Sit amet mauris commodo quis imperdiet. Nisi est sit amet facilisis magna etiam. Nec ultrices dui sapien eget mi proin sed. Erat imperdiet sed euismod nisi porta. Sollicitudin nibh sit amet commodo nulla facilisi nullam vehicula ipsum. Nam aliquam sem et tortor consequat id porta. Et malesuada fames ac turpis egestas integer eget. Tincidunt eget nullam non nisi est sit amet. Pharetra pharetra massa massa ultricies mi quis hendrerit dolor magna. Sed arcu non odio euismod. Pretium quam vulputate dignissim suspendisse in est. Ullamcorper velit sed ullamcorper morbi tincidunt ornare massa. Nunc faucibus a pellentesque sit amet porttitor.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2fab85-163e-460a-8d92-0c7e3dc2c74b",
   "metadata": {},
   "source": [
    "To save text to plain text file, the following code may be utilized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ec1378f-4827-4485-ac5f-3bb631b4aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"This is the plain text file containing text, some text and even more text. The next lines are just generic content. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Sit amet mauris commodo quis imperdiet. Nisi est sit amet facilisis magna etiam. Nec ultrices dui sapien eget mi proin sed. Erat imperdiet sed euismod nisi porta. Sollicitudin nibh sit amet commodo nulla facilisi nullam vehicula ipsum. Nam aliquam sem et tortor consequat id porta. Et malesuada fames ac turpis egestas integer eget. Tincidunt eget nullam non nisi est sit amet. Pharetra pharetra massa massa ultricies mi quis hendrerit dolor magna. Sed arcu non odio euismod. Pretium quam vulputate dignissim suspendisse in est. Ullamcorper velit sed ullamcorper morbi tincidunt ornare massa. Nunc faucibus a pellentesque sit amet porttitor.\"\n",
    "\n",
    "with open(\"plain_text_file.txt\", \"w\") as file:\n",
    "    file.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbebbb08-fbbb-4c4e-a158-201ed0f07225",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0706dcf-e889-48c5-9184-5fc20fc6acde",
   "metadata": {},
   "source": [
    "Comma-separated values is another text file format, that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas. This file format is very similar to Excel sheet. This format is suitable, when stored data can be arranged in plain table. Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa88e24-d74a-4ea8-97ab-b6d99aad5c61",
   "metadata": {},
   "source": [
    "`comma_separated_data.csv`:\n",
    "```csv\n",
    "name,gender,height,shoe_size,birthday\n",
    "Alice,f,160,36,May 12\n",
    "Bob,m,190,47,January 2\n",
    "Chris,m,173,40,July 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09755d1-2002-4377-aba3-067e782fd736",
   "metadata": {},
   "source": [
    "To operate on CSV files, the Pandas module can be utilized (Pandas Dataframes cooperates well with csv files):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2127a52-ff02-4a97-86d1-9b4c7f74dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# preparing example data\n",
    "raw_data = {\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Chris\"],\n",
    "    \"gender\": [\"f\", \"m\", \"m\"],\n",
    "    \"height\": [160, 190, 173],\n",
    "    \"shoe_size\": [36, 47, 40],\n",
    "    \"birthday\": [\"May 12\", \"January 2\", \"July 5\"]\n",
    "}\n",
    "data = pandas.DataFrame(raw_data)\n",
    "\n",
    "# actual saving to file\n",
    "data.to_csv(\"comma_separated_data2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad3c3c-b21a-4ec1-a742-161785800f3d",
   "metadata": {},
   "source": [
    "#### JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a38e8-d657-4fc1-a04b-8b9e6f166991",
   "metadata": {},
   "source": [
    "JavaScript Object Notation is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute-value pairs and arrays (or other serializable values). It allows storing more complex data structures and often is utilized to store some configuration of webpages and applications. All string fields (keys or values) should be surrounded by quotation marks. Objects inside curly brackets `{}` are dictionaries (contains key-value pairs connected with colon `:`, each pair is separated by comma `,`) and objects inside square brackets `[]` are lists (series of single values, separated by commas `,`). Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a7551-34e0-44b5-b2b6-b95e6a409062",
   "metadata": {},
   "source": [
    "`video_metadata.json`:\n",
    "```json\n",
    "  {\n",
    "    \"id\": \"ZR7_D1V3zD0\",\n",
    "    \"title\": \"Network Chuck\",\n",
    "    \"author\": \"NetworkChuck\",\n",
    "    \"description\": \"This channel is dedicated to all things networking with a side of servers. Subscribe to see tutorials on Cisco Switches, ASAs, Routers, Voice, CUCM....etc. Basically, a lot of Cisco stuff.\",\n",
    "    \"keywords\": [\n",
    "      \"Cisco Systems Inc. (Business Operation)\",\n",
    "      \"cisco\",\n",
    "      \"cisco asa\",\n",
    "      \"cisco switch\",\n",
    "      \"nexus\",\n",
    "      \"Computer Network (Industry)\",\n",
    "      \"cisco nexus\",\n",
    "      \"cisco router\",\n",
    "      \"routers\",\n",
    "      \"router\",\n",
    "      \"catalyst\",\n",
    "      \"cisco catalyst\",\n",
    "      \"call manager\",\n",
    "      \"cucm\",\n",
    "      \"cucm 9\",\n",
    "      \"cisco unified communications\",\n",
    "      \"voice\",\n",
    "      \"ccna\",\n",
    "      \"ccna voice\",\n",
    "      \"CCNA (Field Of Study)\",\n",
    "      \"network engineer\",\n",
    "      \"network admin\",\n",
    "      \"Network Administrator (Profession)\",\n",
    "      \"voice admin\",\n",
    "      \"voice engineer\"\n",
    "    ],\n",
    "    \"thumbnail\": \"https://i.ytimg.com/vi/ZR7_D1V3zD0/maxresdefault.jpg\",\n",
    "    \"published_at\": \"2014-10-04\",\n",
    "    \"channel_id\": \"UC9x0AN7BWHpCDHSm9NiJFJQ\",\n",
    "    \"duration\": 158,\n",
    "    \"view_count\": 68767,\n",
    "    \"like_count\": 3175\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22a2af-ba7b-413d-bf7e-392070475781",
   "metadata": {},
   "source": [
    "To save data to json file, the json module can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe051c9c-ef40-4da7-96d0-d5122479b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data =   {\n",
    "    \"id\": \"ZR7_D1V3zD0\",\n",
    "    \"title\": \"Network Chuck\",\n",
    "    \"author\": \"NetworkChuck\",\n",
    "    \"description\": \"This channel is dedicated to all things networking with a side of servers. Subscribe to see tutorials on Cisco Switches, ASAs, Routers, Voice, CUCM....etc. Basically, a lot of Cisco stuff.\",\n",
    "    \"keywords\": [\n",
    "      \"Cisco Systems Inc. (Business Operation)\",\n",
    "      \"cisco\",\n",
    "      \"cisco asa\",\n",
    "      \"cisco switch\",\n",
    "      \"nexus\",\n",
    "      \"Computer Network (Industry)\",\n",
    "      \"cisco nexus\",\n",
    "      \"cisco router\",\n",
    "      \"routers\",\n",
    "      \"router\",\n",
    "      \"catalyst\",\n",
    "      \"cisco catalyst\",\n",
    "      \"call manager\",\n",
    "      \"cucm\",\n",
    "      \"cucm 9\",\n",
    "      \"cisco unified communications\",\n",
    "      \"voice\",\n",
    "      \"ccna\",\n",
    "      \"ccna voice\",\n",
    "      \"CCNA (Field Of Study)\",\n",
    "      \"network engineer\",\n",
    "      \"network admin\",\n",
    "      \"Network Administrator (Profession)\",\n",
    "      \"voice admin\",\n",
    "      \"voice engineer\"\n",
    "    ],\n",
    "    \"thumbnail\": \"https://i.ytimg.com/vi/ZR7_D1V3zD0/maxresdefault.jpg\",\n",
    "    \"published_at\": \"2014-10-04\",\n",
    "    \"channel_id\": \"UC9x0AN7BWHpCDHSm9NiJFJQ\",\n",
    "    \"duration\": 158,\n",
    "    \"view_count\": 68767,\n",
    "    \"like_count\": 3175\n",
    "  }\n",
    "with open(\"video_metadata.json\", 'w') as file:\n",
    "    json.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c69357-6e51-423c-a209-4274b451e15a",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. For each article of:\n",
    "   * `Angela_Merkel`\n",
    "   * `Cicero`\n",
    "   * `Battle_of_Thermopylae`\n",
    "   * `Battle_of_Waterloo`\n",
    "   \n",
    "   download the whole text and save in separate plain text files.\n",
    "1. For each channel of:\n",
    "   * `KingsandGenerals`\n",
    "   * `BeyondScience`\n",
    "   * `HistoriaCivilis`\n",
    "   * `HistoryBuffsLondon`\n",
    "   \n",
    "   scrape metadata of all videos and save in separate json files.\n",
    "1. For each Twitter user of:\n",
    "   * `BorisJohnson`\n",
    "   * `vonderleyen`\n",
    "   * `elonmusk`\n",
    "   * `Pontifex`\n",
    "   \n",
    "   scrape tweets from 1.04.2022 to 30.04.2022 and save in separate csv files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389a0b92-674f-47d7-889d-f8595596d3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
