{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7VjPEYWwV81"
   },
   "source": [
    "Consider the world shown below. An agent can move from cell to cell using the displayed actions (arrows). The reward for an action is equal to the number on the arrows. \n",
    "\n",
    "Assume that the **optimal policy** has been learned and it is a deterministic MDP. \n",
    "\n",
    "Assign the state values of this strategy to the corresponding states $s_i, \\forall i âˆˆ\\{0,\\ldots,8\\}$. The discount factor is $\\gamma=0.9$, and the results should be rounded to integers.\n",
    "\n",
    "Note: The states $s_3$ and $s_8$ are terminal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"5f.png\"\n",
    "     alt=\"World\"\n",
    "     width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization:\n",
    "\n",
    "\n",
    "$\\hat{V}_{1}^*(s_0) =0$    \n",
    "$\\hat{V}_{1}^*(s_1) =0$     \n",
    "$\\hat{V}_{1}^*(s_2) =0$    \n",
    "$\\hat{V}_{1}^*(s_3) =0$   \n",
    "$\\hat{V}_{1}^*(s_4) =0$   \n",
    "$\\hat{V}_{1}^*(s_5) =0$   \n",
    "$\\hat{V}_{1}^*(s_6) =0$  \n",
    "$\\hat{V}_{1}^*(s_7) =0$      \n",
    "$\\hat{V}_{1}^*(s_8) =0$   \n",
    "\n",
    "$a_{greedy} = \\{?,?,?,?,?,?,?,?\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "i_Cne7YLFgl2"
   },
   "source": [
    "## the first iteration K=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcydAFVu-lIG"
   },
   "source": [
    "### State $s_0$\n",
    "\n",
    "$$ \\hat{V}_{(1)}^*(s_0) = \\max \\{R(s_0,a_{up}) + \\gamma  \\hat{V}_{(0)}^*(s_3),\\quad R(s_0,a_{right}) + \\gamma  \\hat{V}_{(0)}^*(s_1)\\}$$\n",
    "\n",
    "$$ \\hat{V}_{(1)}^*(s_0) = \\max  \\{-30, 0\\} = 0 $$\n",
    "\n",
    "$a_{greedy} = \\{a_{right},?,?,?,?,?,?,?\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AduEoAnFBt-9"
   },
   "source": [
    "### State $s_1$\n",
    "\n",
    "$$  \\hat{V}_{(1)}^*(s_1) = \\max \\{R(s_1,a_{left}) + \\gamma  \\hat{V}_{(0)}^*(s_0), \\quad R(s_1,a_{right}) + \\gamma  \\hat{V}_{(0)}^*(s_2),\\quad R(s_1,a_{up}) + \\gamma  \\hat{V}_{(0)}^*(s_4)\\} $$\n",
    "\n",
    "$$ \\hat{V}_{(1)}^*(s_1) = \\max  \\{0, 0, -30\\} = 0 $$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a_{greedy} = \\{a_{right},[a_{left},a_{right}],?,?,?,?,?,?\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xCa3dYjZDHGe"
   },
   "source": [
    "### State $s_2$\n",
    "\n",
    "$$ \\hat{V}_{(1)}^*(s_2) =  R(s_2,a_{up}) + \\gamma  \\hat{V}_{(0)}^*(s_5)$$\n",
    "\n",
    "$$ \\hat{V}_{(1)}^*(s_2) = 0 $$\n",
    "\n",
    "$a_{greedy} = \\{a_{right},[a_{left},a_{right}],a_{up},?,?,?,?,?\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "acQXySQKAIEu"
   },
   "source": [
    "### State $s_3$\n",
    "\n",
    "$$ \\hat{V}_{(1)}^*(s_3) = 0 $$\n",
    "\n",
    "$a_{greedy} = \\{a_{right},[a_{left},a_{right}],a_{up}, [\\:],?,?,?,?\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3t7km1LuDVQn"
   },
   "source": [
    "### State $s_4$\n",
    "\n",
    "$$ \\hat{V}_{(1)}^*(s_4) = \\max \\{R(s_4,a_{left}) + \\gamma  \\hat{V}_{(0)}^*(s_3),\\quad\n",
    "  R(s_4,a_{right}) + \\gamma  \\hat{V}_{(0)}^*(s_5),\\quad\n",
    "  R(s_4,a_{up}) +  \\gamma  \\hat{V}_{(0)}^*(s_7)\\} $$\n",
    "\n",
    "$$ \\hat{V}_{(1)}^*(s_4) = \\max  \\{-40, 0, 10\\} = 10 $$\n",
    "\n",
    "$a_{greedy} = \\{a_{right},[a_{left},a_{right}],a_{up}, [\\:],a_{up},?,?,?\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eeRRGHTBDzmu"
   },
   "source": [
    "### State $s_5$\n",
    "\n",
    "$$ \\hat{V}_{(1)}^*(s_5) = \\max \\{R(s_5,a_{left}) +  \\gamma  \\hat{V}_0^*(s_4),\\quad\n",
    "  R(s_5,a_{down}) + p(s_2 | s_5,a_{down}) \\gamma  \\hat{V}_0^*(s_2)\\,\\quad\n",
    "  R(s_5,a_{up}) + p(s_8 | s_5,a_{up}) \\gamma  \\hat{V}_0^*(s_8)\\} $$\n",
    "\n",
    "$$ \\hat{V}_{(1)}^*(s_5) = \\max  \\{0, 0, 80\\} = 80 $$\n",
    "\n",
    "$\\pi_{greedy} = \\{a_{right},[a_{left},a_{right}],a_{up}, [\\:],a_{up},a_{up},?,?\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hdsPxAnrBGno"
   },
   "source": [
    "### State $s_6$\n",
    "\n",
    "$$ \\hat{V}_{(1)}^*(s_6) = \\max \\{R(s_6,a_{down}) +\\gamma  \\hat{V}_{(0)}^*(s_3),\\quad R(s_6,a_{right}) + \\gamma  \\hat{V}_{(0)}^*(s_7)\\}$$\n",
    "\n",
    "$$ \\hat{V}_{(1)}^*(s_6) = \\max  \\{-20, -10\\} = -10 $$\n",
    "\n",
    "$\\pi_{greedy} = \\{a_{right},[a_{left},a_{right}],a_{up}, [\\:],a_{up},a_{up},a_{right},?,?\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5xxDOmydBt4Q"
   },
   "source": [
    "### State $s_7$\n",
    "\n",
    "$$ \\hat{V}_{(1)}^*(s_7) =  R(s_7,a_{right}) + \\gamma  \\hat{V}_{(0)}^*(s_8)$$\n",
    "\n",
    "$$ \\hat{V}_{(1)}^*(s_7) = 100 $$\n",
    "\n",
    "$\\pi_{greedy} = \\{a_{right},[a_{left},a_{right}],a_{up}, [\\:],a_{up},a_{up},a_{right},a_{right} , ?\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RrT2UPWZBt7l"
   },
   "source": [
    "### State $s_8$\n",
    "\n",
    "$$ \\hat{V}_{(1)}^*(s_8) = 0 $$\n",
    "\n",
    "$\\pi_{greedy} = \\{a_{right},[a_{left},a_{right}],a_{up}, [\\:],a_{up},a_{up},a_{right},a_{right} , [\\:]\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2vpxWCJFklF"
   },
   "source": [
    "## the second iteration $K=2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "U6TsJMIRKjwL"
   },
   "source": [
    "### State $s_0$\n",
    "\n",
    "$$ \\hat{V}_{(2)}^*(s_0) = \\max \\{R(s_0,a_{up}) + p(s_3 | s_0,a_{up}) \\gamma  \\hat{V}_{(1)}^*(s_3),\\quad R(s_0,a_{right}) + p(s_1 | s_0,a_{right}) \\gamma  \\hat{V}_{(1)}^*(s_1)\\}$$\n",
    "\n",
    "$$ \\hat{V}_{(2)}^*(s_0) = \\max  \\{-30 + 0.9*0, 0 + 0.9* 0\\} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDeiYtdEHyPV"
   },
   "source": [
    "### State $s_1$\n",
    "\n",
    "$$ \\hat{V}_{(2)}^*(s_1) = \\max \\{\\\\R(s_1,a_{left}) + p(s_0 | s_1,a_{left}) \\gamma  \\hat{V}_{(1)}^*(s_0),\\\\\n",
    "  R(s_1,a_{right}) + p(s_2 | s_1,a_{right}) \\gamma  \\hat{V}_{(1)}^*(s_2)\\,\\\\R(s_1,a_{up}) + p(s_4 | s_1,a_{up}) \\gamma  \\hat{V}_{(1)}^*(s_4)\\} $$\n",
    "\n",
    "$$ \\hat{V}_{(2)}^*(s_1) = \\max  \\{-30 + 0.9*0\\quad,\\quad 0 + 0.9*0\\quad,\\quad -30 + 0.9*10 \\} = 0 $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXlY8Yi_IgXH"
   },
   "source": [
    "### State $s_2$\n",
    "\n",
    "$$ \\hat{V}_{(2)}^*(s_2) =  R(s_2,a_{up}) + p(s_5 | s_2,a_{up}) \\gamma  \\hat{V}_{(1)}^*(s_5)$$\n",
    "\n",
    "$$ \\hat{V}_{(2)}^*(s_2) = 0 + 0.9*80 = 72 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxTzVxZwI6Zk"
   },
   "source": [
    "### State $s_4$\n",
    "\n",
    "$$ \\hat{V}_{(2)}^*(s_4) = \\max \\{\\\\R(s_4,a_{left}) + p(s_3 | s_4,a_{left}) \\gamma  \\hat{V}_{(1)}^*(s_3),\\\\\n",
    "  R(s_4,a_{right}) + p(s_5 | s_4,a_{right}) \\gamma  \\hat{V}_{(1)}^*(s_5)\\,\n",
    "  \\\\R(s_4,a_{up}) + p(s_7 | s_4,a_{up}) \\gamma  \\hat{V}_{(1)}^*(s_7)\\} $$\n",
    "\n",
    "$$ \\hat{V}_{(2)}^*(s_4) = \\max  \\{-40 + 0.9 * 0\\quad,\\quad 0 + 0.9 * 80 \\quad,\\quad 10  + 0.9 * 100\\} = 100 $$\n",
    "\n",
    "It will not change any more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOBWYjEvGU8Y"
   },
   "source": [
    "### State $s_5$\n",
    "\n",
    "$$ \\hat{V}_{(2)}^*(s_5) = \\max \\{\\\\R(s_5,a_{left}) + p(s_4 | s_5,a_{left}) \\gamma  \\hat{V}_{(1)}^*(s_4),\\\\\n",
    "  R(s_5,a_{down}) + p(s_2 | s_5,a_{down}) \\gamma  \\hat{V}_{(1)}^*(s_2)\\,\n",
    "  \\\\R(s_5,a_{up}) + p(s_8 | s_5,a_{up}) \\gamma  \\hat{V}_{(1)}^*(s_8)\\} $$\n",
    "\n",
    "$$ \\hat{V}_{(2)}^*(s_5) = \\max  \\{0 + 0.9*10\\quad,\\quad 0 + 0.9*0\\quad,\\quad 80 + 0.9*0\\} = 80 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xT3wszFdFpBd"
   },
   "source": [
    "### State $s_6$\n",
    "\n",
    "$$ \\hat{V}_{(2)}^*(s_6) = \\max \\{\\\\R(s_6,a_{down}) + p(s_3 | s_6,a_{down}) \\gamma  \\hat{V}_{(1)}^*(s_3),\\\\\n",
    "  R(s_6,a_{right}) + p(s_7 | s_6,a_{right}) \\gamma  \\hat{V}_{(1)}^*(s_7)\\}$$\n",
    "\n",
    "$$ \\hat{V}_{(2)}^*(s_6) = \\max  \\{-20 + 0.9*0\\quad,\\quad -10 + 0.9*100\\} = 80 $$\n",
    "\n",
    "It will not change any more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-R7JlSbKXzx"
   },
   "source": [
    "The Value for states $\\{s_3, s_4, s_6,s_7, s_8\\}$ will not change any more\n",
    "\n",
    "\n",
    "$ \\hat{V}_{(2)}^*(s_0)=  0$    \n",
    "$ \\hat{V}_{(2)}^*(s_1) = 0$     \n",
    "$ \\hat{V}_{(2)}^*(s_2) = 72$    \n",
    "$ \\hat{V}_{(2)}^*(s_3) = 0$   \n",
    "$ \\hat{V}_{(2)}^*(s_4) = 100$   \n",
    "$ \\hat{V}_{(2)}^*(s_5) = 80$   \n",
    "$ \\hat{V}_{(2)}^*(s_6) = 80$  \n",
    "$ \\hat{V}_{(2)}^*(s_7) = 100$      \n",
    "$ \\hat{V}_{(2)}^*(s_8) = 0$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLtBo3JwKfiX"
   },
   "source": [
    "## the third iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sod6RmCALCfK"
   },
   "source": [
    "### State $s_0$\n",
    "\n",
    "We can calculate the value of $s_0$, but we know its value depends on the value of  $s_1$ will change again. Therefore we don't need to figure it now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gmgz9YHqLwiI"
   },
   "source": [
    "### State $s_1$\n",
    "\n",
    "We can calculate the value of $s_1$, but we know its value depends on the values of $s_2, s_4$. \n",
    "$s_2$  will change again. Therefore we don't need to figure it now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ejtj-e2OMAgc"
   },
   "source": [
    "### State $s_2$\n",
    "\n",
    "We can calculate the value of $s_1$, but we know its value depends on the value of $s_5$. $s_5$  will change again. Therefore we don't need to figure it now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbAn6Y-TMKHx"
   },
   "source": [
    "### State $s_5$\n",
    "\n",
    "$$ \\hat{V}_{(3)}^*(s_5) = \\max \\{\\\\R(s_5,a_{left}) + p(s_4 | s_5,a_{left}) \\gamma  \\hat{V}_{(2)}^*(s_4),\\\\R(s_5,a_{up}) + p(s_8 | s_5,a_{up}) \\gamma  \\hat{V}_{(2)}^*(s_8)\\} $$\n",
    "\n",
    "$$ \\hat{V}_{(3)}^*(s_5) = \\max  \\{0 + 0.9*100'  80 + 0.9*0\\} = 90 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Value for states $\\{s_3, s_4, s_5, s_6,s_7, s_8\\}$ will not change any more\n",
    "\n",
    "\n",
    "$ \\hat{V}_{(2)}^*(s_0)=  0$    \n",
    "$ \\hat{V}_{(2)}^*(s_1) = 0$     \n",
    "$ \\hat{V}_{(2)}^*(s_2) = 72$    \n",
    "$ \\hat{V}_{(3)}^*(s_3) = 0$   \n",
    "$ \\hat{V}_{(3)}^*(s_4) = 100$   \n",
    "$ \\hat{V}_{(3)}^*(s_5) = 90$   \n",
    "$ \\hat{V}_{(3)}^*(s_6) = 80$  \n",
    "$ \\hat{V}_{(3)}^*(s_7) = 100$      \n",
    "$ \\hat{V}_{(3)}^*(s_8) = 0$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnlxX5QANLOR"
   },
   "source": [
    "## the fourth iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhlSfUYuNYoe"
   },
   "source": [
    "### State $s_2$\n",
    "\n",
    "$$ \\hat{V}_{(4)}^*(s_2) =  R(s_2,a_{up}) + p(s_5 | s_2,a_{up}) \\gamma  \\hat{V}_{(3)}^*(s_5)$$\n",
    "\n",
    "$$ \\hat{V}_{(4)}^*(s_2) = 0 + 0.9*90 = 81 $$\n",
    "\n",
    "The Value for states $\\{s_2,s_3, s_4, s_5, s_6,s_7,s_8\\}$ will not change any more\n",
    "\n",
    "\n",
    "$ \\hat{V}_{(2)}^*(s_0)=  0$    \n",
    "$ \\hat{V}_{(2)}^*(s_1) = 0$     \n",
    "$ \\hat{V}_{(4)}^*(s_2) = 81$    \n",
    "$ \\hat{V}_{(4)}^*(s_3) = 0$   \n",
    "$ \\hat{V}_{(4)}^*(s_4) = 100$   \n",
    "$ \\hat{V}_{(4)}^*(s_5) = 90$   \n",
    "$ \\hat{V}_{(4)}^*(s_6) = 80$  \n",
    "$ \\hat{V}_{(4)}^*(s_7) = 100$      \n",
    "$ \\hat{V}_{(4)}^*(s_8) = 0$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZk19cmLNkQG"
   },
   "source": [
    "## the fifth iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-3cWtczNsFr"
   },
   "source": [
    "### State $s_1$\n",
    "\n",
    "$$ \\hat{V}_{(5)}^*(s_1) = \\max \\{\\\\R(s_1,a_{right}) + p(s_2 | s_1,a_{right}) \\gamma  \\hat{V}_{(4)}^*(s_2)\\,\\\\R(s_1,a_{up}) + p(s_4 | s_1,a_{up}) \\gamma  \\hat{V}_{(4)}^*(s_4)\\} $$\n",
    "\n",
    "$$ \\hat{V}_{(5)}^*(s_1) = \\max  \\{ 0 + 0.9*81\\quad,\\quad -30 + 0.9*100 \\} = 72.9 = 73 $$\n",
    "\n",
    "The Value for states $\\{s_1,s_2,s_3, s_4, s_5, s_6,s_7,s_8\\}$ will not change any more\n",
    "\n",
    "$ \\hat{V}_{(2)}^*(s_0)=  0$    \n",
    "$ \\hat{V}_{(5)}^*(s_1) = 73$     \n",
    "$ \\hat{V}_{(5)}^*(s_2) = 81$    \n",
    "$ \\hat{V}_{(5)}^*(s_3) = 0$   \n",
    "$ \\hat{V}_{(5)}^*(s_4) = 100$   \n",
    "$ \\hat{V}_{(5)}^*(s_5) = 90$   \n",
    "$ \\hat{V}_{(5)}^*(s_6) = 80$  \n",
    "$ \\hat{V}_{(5)}^*(s_7) = 100$      \n",
    "$ \\hat{V}_{(5)}^*(s_8) = 0$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiSc4VdKOdVb"
   },
   "source": [
    "## the sixth iteration\n",
    "\n",
    "### State $s_0$\n",
    "\n",
    "$$ \\hat{V}_{(6)}^*(s_0) = R(s_0,a_{right}) + p(s_1 | s_0,a_{right}) \\gamma  \\hat{V}_{(5)}^*(s_1)$$\n",
    "\n",
    "$$ \\hat{V}_{(6)}^*(s_0) =  0 + 0.9* 73 = 65.7 = 66 $$\n",
    "\n",
    "\n",
    "$ \\hat{V}_{(6)}^*(s_0)=  66$    \n",
    "$ \\hat{V}_{(6)}^*(s_1) = 73$     \n",
    "$ \\hat{V}_{(6)}^*(s_2) = 81$    \n",
    "$ \\hat{V}_{(6)}^*(s_3) = 0$   \n",
    "$ \\hat{V}_{(6)}^*(s_4) = 100$   \n",
    "$ \\hat{V}_{(6)}^*(s_5) = 90$   \n",
    "$ \\hat{V}_{(6)}^*(s_6) = 80$  \n",
    "$ \\hat{V}_{(6)}^*(s_7) = 100$      \n",
    "$ \\hat{V}_{(6)}^*(s_8) = 0$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzGJBKH_PeqK"
   },
   "source": [
    "what is the optimal policy given that we start form $s_0$?\n",
    "\n",
    "$$s_0 \\rightarrow s_1 \\rightarrow s_2 \\rightarrow s_5 \\rightarrow s_4\\rightarrow s_7 \\rightarrow s_8 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2\n",
    "\n",
    "<img src=\"5f.png\"\n",
    "     alt=\"World\"\n",
    "     width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Strat from the the terminal states $s_8, s_3$\n",
    "\n",
    "### States $s_3, s_8$\n",
    "\n",
    "$$V^*(s_3) = R(s_3,a_i) = 0   $$ \n",
    "$$V^*(s_8) = R(s_8,a_i) = 0   $$ \n",
    "Where \n",
    "\n",
    "$ a_i = \\{\\}$ No actions can be performed there. These values of these states will not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go from the goal back to the first states\n",
    "\n",
    "### State $s_7$\n",
    "\n",
    "$$V^*(s_7) =  R(s_7,a_{right}) + p(s_8 | s_7,a_{right}) \\gamma V^*(s_8)$$\n",
    "\n",
    "$$V^*(s_7) = 100 $$\n",
    "\n",
    "These value will not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State $s_6$\n",
    "\n",
    "$$V^*(s_6) = R(s_6,a_{right}) + p(s_7 | s_6,a_{right}) \\gamma V^*(s_7)$$\n",
    "\n",
    "$$V^*(s_6) = -10 + 0.9*100 = 80 $$\n",
    "\n",
    "This value will not change any more. It is clear that we will not use the action in the direction of s3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From $s_5$ and $s_4$, there are two paths to the destination $s_8$. Maybe We will need to do two iterations here to calculate the values of these states. \n",
    "\n",
    "\n",
    "### State $s_5$\n",
    "\n",
    "$$ \\hat{V}_{(1)}^*(s_5) = \\max \\{\\\\R(s_5,a_{left}) + p(s_4 | s_5,a_{left}) \\gamma  \\hat{V}_{(0)}^*(s_4),\\\\R(s_5,a_{up}) + p(s_8 | s_5,a_{up}) \\gamma V^*(s_8)\\} $$\n",
    "\n",
    "$$ \\hat{V}_{(1)}^*(s_5) = \\max  \\{0 + 0.9*0, 80 + 0.9*0\\} = 80 $$\n",
    "\n",
    "We do not need to consider the action in the direction of  s2. \n",
    "\n",
    "\n",
    "### State $s_4$\n",
    "\n",
    "$$ \\hat{V}_{(2)}^*(s_4) = \\max \\{\\\\R(s_4,a_{right}) + p(s_5 | s_4,a_{right}) \\gamma  \\hat{V}_{(1)}^*(s_5)\\,\n",
    "  \\\\R(s_4,a_{up}) + p(s_7 | s_4,a_{up}) \\gamma V^*(s_7)\\} $$\n",
    "\n",
    "$$ \\hat{V}_{(2)}^*(s_4) = \\max  \\{ 0 + 0.9 * 80 \\quad,\\quad 10  + 0.9 * 100\\} = 100 $$\n",
    "\n",
    "We do not need to use the action in the direction of s3. \n",
    "\n",
    "We will recalculate the value of $s_5$ because the state $s_4$ is better than we thought. If the new value of $s_5$ is bigger than $\\frac{100}{.9}$, then we need another iteration to calculate $V^*(s_4)$.\n",
    "\n",
    "\n",
    "\n",
    "### State $s_5$\n",
    "\n",
    "$$ \\hat{V}_{(3)}^*(s_5) = \\max \\{\\\\R(s_5,a_{left}) + p(s_4 | s_5,a_{left}) \\gamma  \\hat{V}_{(2)}^*(s_4),\\\\R(s_5,a_{up}) + p(s_8 | s_5,a_{up}) \\gamma V^*(s_8)\\} $$\n",
    "\n",
    "$$ \\hat{V}_{(3)}^*(s_5) = \\max  \\{0 + 0.9*100, 80 + 0.9*0\\} = 90 $$\n",
    "\n",
    "\n",
    "The values $V^*(s_5)$ and $V^*(s_4)$  will not change any more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State $s_2$\n",
    "\n",
    "The value $s_2$ depends only on the value of $s_5$\n",
    "\n",
    "$$V^*(s_2) =  R(s_2,a_{up}) + p(s_5 | s_2,a_{up}) \\gamma V^*(s_5)$$\n",
    "\n",
    "$$V^*(s_2) = 0 + 0.9*90 = 81 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State $s_1$\n",
    "\n",
    "\n",
    "$$V^*(s_1) = \\max \\{\\\\R(s_1,a_{right}) + p(s_2 | s_1,a_{right}) \\gamma V^*(s_2)\\,\\\\R(s_1,a_{up}) + p(s_4 | s_1,a_{up}) \\gamma V^*(s_4)\\} $$\n",
    "\n",
    "$$V^*(s_1) = \\max  \\{ 0 + 0.9*81\\quad,\\quad -30 + 0.9*100 \\} = 72.9 = 73 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State $s_0$\n",
    "\n",
    "$$V^*(s_0) = R(s_0,a_{right}) + p(s_1 | s_0,a_{right}) \\gamma V^*(s_1)$$\n",
    "\n",
    "$$V^*(s_0) =  0 + 0.9* 73 = 65.7 = 66 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Reinforcement Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
