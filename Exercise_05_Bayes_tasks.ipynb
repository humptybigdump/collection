{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rLnsHouAk8rG"
   },
   "source": [
    "## Introduction:\n",
    "\n",
    "In this notebook, we will learn about the naive Bayes classifier, which relates the probability an instance belongs to a given target class to the prior probability of each feature and the joint probability of the given set of features given the target class. By making an implicit assumption of conditional independence, this joint probability can be simplified into a product of individual conditional probabilities for each feature. Using this with Bayes theorem, we get the final probabilistic classification, given a set of features. This algorithm can be quick to apply, once the model is trained, and can often provide satisfactory results, which can be used on its own, or as a comparative benchmark for more advanced techniques.\n",
    "\n",
    "We will first explore the probability concepts behind this classifier. Next, the Naive Bayes classification algorithm will be introduced by using the Iris data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KxKEFMW9lnO2"
   },
   "source": [
    "## Classification\n",
    "* **Input** : Measurements $x^{(1)} ,\\ldots, x^{(n)}$ in an input space $\\mathcal{X} \\in \\mathbb{R}^d $. Each measurement $x^{(i)} $ consists of $d$ features.\n",
    "\n",
    "* **Output** : The discrete output space $\\mathcal{Y}$ is composed of $K$ possible classes : \n",
    "  * $ \\mathcal{Y}=  \\{ 0 , 1 \\}$ is called binary classification. \n",
    "  * $ \\mathcal{Y} = \\{ 1,\\ldots, K \\}$ is called multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PYrwpG5yQIhp"
   },
   "source": [
    "## The Bayes Classifier:\n",
    "\n",
    "In Bayesian classification, we're interested in finding the probability of a label $Y=y$ given some observed features X, which we can write as $P(Y=y~|~X=x)$.\n",
    "Bayes's theorem tells us how to express this in terms of quantities we can compute more directly:\n",
    "\n",
    "$$\n",
    "P(Y=y~|~X=x) = \\frac{P(X=x~|~Y=y)P(Y=y)}{P({\\rm X=x})}\n",
    "$$\n",
    "\n",
    "Under the assumption $( X , Y ) \\overset{\\text{iid}}{\\sim} \\mathcal{P}$ , the optimal classifier is\n",
    "$$f^* ( x ) := \\arg \\max_{ y \\in \\mathcal{Y}  }P(Y = y | X = x ) $$\n",
    "\n",
    "From Bayes rule we equivalently have\n",
    "$$ f^* ( x ) = \\arg \\max_{ y \\in \\mathcal{Y}  } P(X = x  | Y = y )P(Y = y )$$\n",
    "\n",
    "where:\n",
    "* $P ( X = x | Y = y )$ is called the class conditional distribution of $X$ .\n",
    "* $P ( Y = y )$ is called the class prior .\n",
    "* In practice we don‚Äôt know either of these, so we approximate them.\n",
    "\n",
    "**Goal:** Find the label y from Y with the greatest probability given the observed data ùê∑. This is the maximum a posteriori (MAP) hypothesis\n",
    "\n",
    "All we need now is some model by which we can compute $P(X=x~|~Y=y_i)$ for each label in $Y$.\n",
    "Such a model is called a *generative model* because it specifies the hypothetical random process that generates the data.\n",
    "\n",
    "Specifying this generative model for each label is the main piece of the training of such a Bayesian classifier.\n",
    "The general version of such a training step is a very difficult task, but we can make it simpler through the use of some simplifying assumptions about the form of this model.\n",
    "\n",
    "**Question:** Under the assumption, MAP can be simplified to the maximum likelihood (ML) hypothesis\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aMn2ReKy2DEy"
   },
   "source": [
    "## NAIVE BAYES\n",
    "\n",
    "We have to define $P( X = x | Y = y )$ . The \"naive\" in \"naive Bayes\" comes in: if we make very naive assumptions about the generative model for each label, we can find a rough approximation of the generative model for each class, and then proceed with the Bayesian classification. \n",
    "\n",
    "**Simplifying assumption:** Naive Bayes is a Bayes classifier that makes the assumption\n",
    "$P ( X = x | Y = y ) = \\prod_{j=1}^d P( x_j | Y = y )$ .\n",
    "\n",
    "i.e., it treats the dimensions of $X$ as conditionally independent given $ y$.\n",
    "\n",
    "**Question:** For which of the following data sets can we use the Naive Bayes assumption?\n",
    "\n",
    "<img src=\"https://matplotlib.org/3.1.1/_images/sphx_glr_confidence_ellipse_001.png\" style=\"width:600px;height:200px;\">\n",
    "\n",
    "One approach to computing the conditional probabilities was demonstrated in the Lecture. A problem with using the counts from a training data set is that the entire process becomes completely deterministic, which means that the same result occurs every time we run the algorithm.  \n",
    "In some cases, this would not be a problem, but in general, we want to account for the fact that any data we have collected (or sampled) is merely a subset of the entire parent population that we wish to quantify. If we were to collect a second data set (or sample), we likely would produce different results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-x8Kcvi9qmbu"
   },
   "source": [
    "## Naive Bayes: Classification\n",
    "\n",
    "When applying the naive Bayes algorithm, we must choose a specific classifier that assumes the features are sampled from a relevant distribution. \n",
    "\n",
    "In the `scikit learn` library, we can employ the naive Bayes algorithm by creating one of three estimators, which are all in the `naive_bayes` module:\n",
    "\n",
    "* [Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html): the features are assumed to follow a normal distribution.\n",
    "* [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html): the features are assumed to follow a multinomial distribution.\n",
    "* [Bernoulli Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html): the features are assumed to follow a binomial distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries:\n",
    "first we will import all the packages that are required for this exercise. \n",
    "- [numpy](www.numpy.org) is the main package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) and [seaborn](https://seaborn.pydata.org/introduction.html) are libraries to plot graphs in Python.\n",
    "- np.random.seed(1) is used to keep all the random function calls consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [9,6]\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPiKT_Km2ytd"
   },
   "source": [
    "## Gaussian Naive Bayes\n",
    "In this classifier, the assumption is that data from each label is drawn from a simple Gaussian distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lGGDy_k8fTne"
   },
   "source": [
    "### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uwFWo1lbnTh-"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
    "plt.title('Training Data ', size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generated from two Gaussian distribution.\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
    "ax.set_title('Naive Bayes Model', size=14)\n",
    "\n",
    "xlim = (-8, 8)\n",
    "ylim = (-15, 5)\n",
    "\n",
    "xg = np.linspace(xlim[0], xlim[1], 60)\n",
    "yg = np.linspace(ylim[0], ylim[1], 40)\n",
    "xx, yy = np.meshgrid(xg, yg)\n",
    "Xgrid = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "for label, color in enumerate(['red', 'blue']):\n",
    "    mask = (y == label)\n",
    "    mu, std = X[mask].mean(0), X[mask].std(0)\n",
    "    P = np.exp(-0.5 * (Xgrid - mu) ** 2 / std ** 2).prod(1)\n",
    "    Pm = np.ma.masked_array(P, P < 0.03)\n",
    "    ax.pcolorfast(xg, yg, Pm.reshape(xx.shape), alpha=0.5,\n",
    "                  cmap=color.title() + 's')\n",
    "    ax.contour(xx, yy, P.reshape(xx.shape),\n",
    "               levels=[0.01, 0.1, 0.5, 0.9],\n",
    "               colors=color, alpha=0.2)\n",
    "    \n",
    "ax.set(xlim=xlim, ylim=ylim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7AnrhAyrtFv"
   },
   "source": [
    "The Gaussian naive Bayes estimator takes one hyperparameter: `priors`, which are the prior probabilities of the different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-L-x5LmbN-Ds"
   },
   "source": [
    "### Task 1:\n",
    "Deine a [Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) and  fit it according to X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XCkuk-A-niF6"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c1p8RQwuDsCJ"
   },
   "source": [
    "#### Naive Bayes: Decision Surface\n",
    "\n",
    "We now compute and display the decision surface for the naive Bayes classifier. \n",
    "* First, we create a mesh (or grid of points in two-dimensions) that spans the features. \n",
    "* Next, we fit a naive Bayes classifier to the training data in these two dimensions, before applying this model to the test data and the two-dimensional mesh. \n",
    "\n",
    "The resulting figure shows the non-linear nature of this classifier, especially when using the GaussianNB estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ALg1g5I-DuaE"
   },
   "outputs": [],
   "source": [
    "# Construct mesh grid data\n",
    "x_1lim = (-8, 8)\n",
    "x_2lim = (-15, 5)\n",
    "x_1g = np.linspace(x_1lim[0], x_1lim[1], 60)\n",
    "x_2g = np.linspace(x_2lim[0], x_2lim[1], 40)\n",
    "xx, yy = np.meshgrid(x_1g, x_2g)\n",
    "Xgrid = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "# Predict for mesh grid\n",
    "z = model.predict(Xgrid)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(Xgrid[:, 0], Xgrid[:, 1], c=z, s=40, cmap='RdBu',alpha=0.2)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
    "ax.set_title('Naive Bayes Model', size=14)\n",
    "ax.set(xlim=x_1lim, ylim=x_2lim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qn_0RiWYEZKj"
   },
   "source": [
    "We see a slightly curved boundary in the classifications‚Äîin general, the boundary in Gaussian naive Bayes is quadratic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-GV4t7oDEfK6"
   },
   "source": [
    "\n",
    "A nice piece of this Bayesian formalism is that it naturally allows for probabilistic classification, which we can compute using the predict_proba method. Now let's generate some new data and predict the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tsx7lgzHFCsI"
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "Xnew = [-6, -14] + [14, 18] * rng.rand(80, 2)\n",
    "\n",
    "ynew = model.predict(Xnew)\n",
    "print(\"ynew\",ynew[-10:].round(2))\n",
    "\n",
    "yprob = model.predict_proba(Xnew)\n",
    "print(\"yprob\",yprob[-10:].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAbmZwwvDOe0"
   },
   "source": [
    "The columns give the posterior probabilities of the first and second label, respectively.\n",
    "\n",
    "Of course, the final classification will only be as good as the model assumptions that lead to it, which is why Gaussian naive Bayes often does not produce very good results. \n",
    "\n",
    "Still, in many cases‚Äîespecially as the number of features becomes large‚Äîthis assumption is not detrimental enough to prevent Gaussian naive Bayes from being a useful method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgTzVtOgrLcd"
   },
   "source": [
    "## Multinomial Naive Bayes:\n",
    "\n",
    "Another useful example is multinomial naive Bayes, where the features are assumed to be generated from a simple multinomial distribution. \n",
    "\n",
    "The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive Bayes is most appropriate for features that represent counts or count rates.\n",
    "\n",
    "The idea is precisely the same as before, except that instead of modeling the data distribution with the best-fit Gaussian, we model the data distribuiton with a best-fit multinomial distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJYWf6DEfsF7"
   },
   "source": [
    "### Example 2: Classifying Text\n",
    " One place where multinomial naive Bayes is often used is in text classification, where the features are related to word counts or frequencies within the documents to be classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n8RC37ldEV-X"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups()\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgX2OV_H9hwy"
   },
   "source": [
    "We take only a subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AR3OeqelFDeB"
   },
   "outputs": [],
   "source": [
    "categories = ['talk.religion.misc', 'soc.religion.christian',\n",
    "              'sci.space', 'comp.graphics']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktRii8glFIz3"
   },
   "outputs": [],
   "source": [
    "print(train.data[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfaSoXnLI0gi"
   },
   "source": [
    "\n",
    "### Text Features\n",
    "\n",
    "Another common need in feature engineering is to convert text to a set of representative numerical values. One of the simplest methods of encoding data is by word counts: you take each snippet of text, count the occurrences of each word within it, and put the results in a table.\n",
    "\n",
    "For example, consider the following set of three phrases:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rh3dpRExFgLC"
   },
   "outputs": [],
   "source": [
    "sample = ['problem of evil',\n",
    "          'evil queen',\n",
    "          'horizon problem',\n",
    "         'of and a',\n",
    "         'of the a',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8n6dpF8JIb5"
   },
   "source": [
    "While doing this by hand would be possible, the tedium can be avoided by using Scikit-Learn's CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1547378777423,
     "user": {
      "displayName": "karam daaboul",
      "photoUrl": "https://lh5.googleusercontent.com/-pIJAbmNh3Dg/AAAAAAAAAAI/AAAAAAAACqQ/EH7noOZ8y3U/s64/photo.jpg",
      "userId": "10489733265214992802"
     },
     "user_tz": -60
    },
    "id": "AW1gSTOYrX6R",
    "outputId": "46bd3e82-3891-43fb-f5d2-3c54f9bf1ff7"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(sample)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8X0xTSu4JVdC"
   },
   "source": [
    "It is easier to inspect if we convert this to a DataFrame with labeled columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTFd91wACI5T"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HbzrlEgwJ1_z"
   },
   "source": [
    "The raw word counts lead to features which put too much weight on words that appear very frequently, and this can be sub-optimal in some classification algorithms. One approach to fix this is known as term frequency-inverse document frequency (TF‚ÄìIDF).which weights the word counts by a measure of how often they appear in the documents but offset them by the number of times the word appears in the entire dataset. This offset helps remove the importance from really common words  like \"the\" or \"a\". The syntax for computing these features is similar to the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1547378781878,
     "user": {
      "displayName": "karam daaboul",
      "photoUrl": "https://lh5.googleusercontent.com/-pIJAbmNh3Dg/AAAAAAAAAAI/AAAAAAAACqQ/EH7noOZ8y3U/s64/photo.jpg",
      "userId": "10489733265214992802"
     },
     "user_tz": -60
    },
    "id": "zmMVv62lKDT5",
    "outputId": "24c8dcc4-780d-484c-c590-a684f8ebfde6"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(sample)\n",
    "pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-RA7cDryYvvl"
   },
   "source": [
    "### Example 2 (Continue)\n",
    "Create a pipeline that attaches it to a multinomial naive Bayes classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLRAuvY3CKMp"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SSNrzseY024s"
   },
   "source": [
    "### Task 2:\n",
    "With this pipeline, you can apply the model to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kuJA7IlCYyzf"
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model.fit(train.data, train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvk7MB655d9M"
   },
   "source": [
    "Next use the model to predict labels for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0uflkiGl5dL9"
   },
   "outputs": [],
   "source": [
    "labels = model.predict(test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mXlX5SIv-6iC"
   },
   "source": [
    "Now that we have predicted the labels for the test data, we can evaluate them to learn about the performance of the estimator. For example, here is the confusion matrix between the true and predicted labels for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RVCGT6-TY0nd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(test.target, labels)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=train.target_names, yticklabels=train.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CfFez5J_07Do"
   },
   "source": [
    "Evidently, even this very simple classifier can successfully separate space talk from computer talk, but it gets confused between talk about religion and talk about Christianity. This is perhaps an expected area of confusion!\n",
    "\n",
    "The very cool thing here is that we now have the tools to determine the category for any string, using the `predict()` method of this pipeline. Here's a quick utility function that will return the prediction for a single string:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T4KmDDHBhwcw"
   },
   "outputs": [],
   "source": [
    "def predict_category(s, train=train, model=model):\n",
    "    pred = model.predict([s])\n",
    "    return train.target_names[pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kI-MPkxP1Csh"
   },
   "outputs": [],
   "source": [
    "predict_category('sending a payload to the ISS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LYW_qYcf1Jjq"
   },
   "outputs": [],
   "source": [
    "predict_category('determining the screen resolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rTs-6IblThJ0"
   },
   "source": [
    "## Conclusion:\n",
    "This algorithm can be quick to apply, once the model is trained, and can often provide reasonable results, which can be used on its own, or as a comparative benchmark for more advanced techniques.  It  has very few (if any) tunable parameters.\n",
    "\n",
    "As the dimension of a dataset grows, it is much less likely for any two points to be found close together (after all, they must be close in every single dimension to be close overall). This means that clusters in high dimensions tend to be more separated, on average, than clusters in low dimensions, assuming the new dimensions actually add information. For this reason, simplistic classifiers like naive Bayes tend to work as well or better than more complicated classifiers as the dimensionality grows: once you have enough data, even a simple model can be very powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baysian Network \n",
    "\n",
    "A Bayesian network is a probabilistic model represented by a direct acyclic graph $G = {V, E}$, where the vertices are random variables $X_i$, and the edges determine a conditional dependence among them.\n",
    "<img src=\"https://imgur.com/eL7d94r.png\" style=\"width:300px;height:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avhuWoxQTNh7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def X1_sample(p=0.35):\n",
    "    return np.random.binomial(1, p)\n",
    "\n",
    "def X2_sample(p=0.65):\n",
    "    return np.random.binomial(1, p)\n",
    "\n",
    "def X3_sample(x1, x2, p1=0.75, p2=0.4):\n",
    "    if x1 == 1 and x2 == 1:\n",
    "        return np.random.binomial(1, p1)\n",
    "    else:\n",
    "        return np.random.binomial(1, p2)\n",
    "\n",
    "def X4_sample(x3, p1=0.65, p2=0.5):\n",
    "    if x3 == 1:\n",
    "        return np.random.binomial(1, p1)\n",
    "    else:\n",
    "        return np.random.binomial(1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "Nsamples = 5000\n",
    "\n",
    "S = np.zeros((N, Nsamples))\n",
    "Fsamples = {}\n",
    "\n",
    "for t in range(Nsamples):\n",
    "    x1 = X1_sample()\n",
    "    x2 = X2_sample()\n",
    "    x3 = X3_sample(x1, x2)\n",
    "    x4 = X4_sample(x3)\n",
    "\n",
    "    sample = (x1, x2, x3, x4)\n",
    "\n",
    "    if sample in Fsamples:\n",
    "        Fsamples[sample] += 1\n",
    "    else:\n",
    "        Fsamples[sample] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the sampling is complete, it's possible to extract the full joint probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.array(list(Fsamples.keys()), dtype=np.bool_)\n",
    "probabilities = np.array(list(Fsamples.values()), dtype=np.float64) / Nsamples\n",
    "\n",
    "for i in range(len(samples)):\n",
    "    print('P{} = {}'.format(samples[i], probabilities[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also query the model. For example, we could be interested in $P(X_4=True)$. We can do this by looking for all the elements where $X_4=True$, and summing up the relative probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4t = np.argwhere(samples[:, 3]==True)\n",
    "print(np.sum(probabilities[p4t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bayes_tasks.ipynb",
   "provenance": [
    {
     "file_id": "1OE4Qkpfq90ZsUSTJdzKf6qchu-lRoXlI",
     "timestamp": 1547380225640
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "venv_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
