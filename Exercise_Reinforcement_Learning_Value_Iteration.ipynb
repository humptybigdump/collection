{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7VjPEYWwV81"
   },
   "source": [
    "Consider the world shown below. An agent can move from cell to cell using the displayed actions (arrows). The reward for an action is equal to the number on the arrows. \n",
    "\n",
    "Assume that it is a **deterministic** MDP. \n",
    "\n",
    "Conduct **value iteration** to iteratively compute the state values for all states $s_i, \\forall i âˆˆ\\{0,\\ldots,8\\}$. The **discount factor** is $\\gamma=0.9$, and the results should be rounded to integers.\n",
    "\n",
    "Note: The states $s_3$ and $s_8$ are terminal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap: Value Iteration**\n",
    "\n",
    "Value iteration combines the policy evaluation and improvement step to iteratively update the state value function estimates according to\n",
    "\n",
    "$$ V_{k+1}(s) = \\max_{a \\in \\mathcal{A}} \\left\\{r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} p(s'|s,a) V_{k}(s')\\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"grid.png\"\n",
    "     alt=\"Grid World\"\n",
    "     width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization:\n",
    "\n",
    "\n",
    "$ V_{0}(s_0) =0$  \n",
    "$ V_{0}(s_1) =0$  \n",
    "$ V_{0}(s_2) =0$  \n",
    "$ V_{0}(s_3) =0$  \n",
    "$ V_{0}(s_4) =0$  \n",
    "$ V_{0}(s_5) =0$  \n",
    "$ V_{0}(s_6) =0$  \n",
    "$ V_{0}(s_7) =0$  \n",
    "$ V_{0}(s_8) =0$  \n",
    "\n",
    "$\\pi_{greedy} = [?,?,?,?,?,?,?,?]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "i_Cne7YLFgl2"
   },
   "source": [
    "## First iteration (k=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcydAFVu-lIG"
   },
   "source": [
    "### State $s_0$\n",
    "\n",
    "$$ V_{1}(s_0) = \\max \\{r(s_0,a_{up}) + \\gamma  V_{0}(s_3),\\quad r(s_0,a_{right}) + \\gamma  V_{0}(s_1)\\}$$\n",
    "\n",
    "$$ V_{1}(s_0) = \\max  \\{-30, 0\\} = 0 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},?,?,?,?,?,?,?]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AduEoAnFBt-9"
   },
   "source": [
    "### State $s_1$\n",
    "\n",
    "$$  V_{1}(s_1) = \\max \\{r(s_1,a_{left}) + \\gamma  V_{0}(s_0), \\quad r(s_1,a_{right}) + \\gamma  V_{0}(s_2),\\quad r(s_1,a_{up}) + \\gamma  V_{0}(s_4)\\} $$\n",
    "\n",
    "$$ V_{1}(s_1) = \\max  \\{0, 0, -30\\} = 0 $$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pi_{greedy} = [a_{right},\\{a_{left},a_{right}\\},?,?,?,?,?,?]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xCa3dYjZDHGe"
   },
   "source": [
    "### State $s_2$\n",
    "\n",
    "$$ V_{1}(s_2) =  r(s_2,a_{up}) + \\gamma  V_{0}(s_5)$$\n",
    "\n",
    "$$ V_{1}(s_2) = 0 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},\\{a_{left},a_{right}\\},a_{up},?,?,?,?,?]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "acQXySQKAIEu"
   },
   "source": [
    "### State $s_3$\n",
    "\n",
    "$$ V_{1}(s_3) = 0 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},\\{a_{left},a_{right}\\},a_{up}, \\{\\},?,?,?,?]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3t7km1LuDVQn"
   },
   "source": [
    "### State $s_4$\n",
    "\n",
    "$$ V_{1}(s_4) = \\max \\{r(s_4,a_{left}) + \\gamma  V_{0}(s_3),\\quad\n",
    "  r(s_4,a_{right}) + \\gamma  V_{0}(s_5),\\quad\n",
    "  r(s_4,a_{up}) +  \\gamma  V_{0}(s_7)\\} $$\n",
    "\n",
    "$$ V_{1}(s_4) = \\max  \\{-40, 0, 10\\} = 10 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},\\{a_{left},a_{right}\\},a_{up}, \\{\\},a_{up},?,?,?]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eeRRGHTBDzmu"
   },
   "source": [
    "### State $s_5$\n",
    "\n",
    "$$ V_{1}(s_5) = \\max \\{r(s_5,a_{left}) +  \\gamma  V_0(s_4),\\quad\n",
    "  r(s_5,a_{down}) + \\gamma  V_0(s_2)\\,\\quad\n",
    "  r(s_5,a_{up}) + \\gamma  V_0(s_8)\\} $$\n",
    "\n",
    "$$ V_{1}(s_5) = \\max  \\{0, 0, 80\\} = 80 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},\\{a_{left},a_{right}\\},a_{up}, \\{\\},a_{up},a_{up},?,?]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hdsPxAnrBGno"
   },
   "source": [
    "### State $s_6$\n",
    "\n",
    "$$ V_{1}(s_6) = \\max \\{r(s_6,a_{down}) +\\gamma  V_{0}(s_3),\\quad r(s_6,a_{right}) + \\gamma  V_{0}(s_7)\\}$$\n",
    "\n",
    "$$ V_{1}(s_6) = \\max  \\{-20, -10\\} = -10 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},\\{a_{left},a_{right}\\},a_{up}, \\{\\},a_{up},a_{up},a_{right},?,?]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5xxDOmydBt4Q"
   },
   "source": [
    "### State $s_7$\n",
    "\n",
    "$$ V_{1}(s_7) =  r(s_7,a_{right}) + \\gamma  V_{0}(s_8)$$\n",
    "\n",
    "$$ V_{1}(s_7) = 100 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},\\{a_{left},a_{right}\\},a_{up}, \\{\\},a_{up},a_{up},a_{right},a_{right} , ?]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RrT2UPWZBt7l"
   },
   "source": [
    "### State $s_8$\n",
    "\n",
    "$$ V_{1}(s_8) = 0 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},\\{a_{left},a_{right}\\},a_{up}, \\{\\},a_{up},a_{up},a_{right},a_{right} , \\{\\}]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2vpxWCJFklF"
   },
   "source": [
    "## Second iteration (k=2)\n",
    "\n",
    "$V_1 = \\{0,0,0,0,\\textcolor{red}{10},\\textcolor{red}{80},\\textcolor{red}{-10},\\textcolor{red}{100},0\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "U6TsJMIRKjwL"
   },
   "source": [
    "### State $s_0$\n",
    "\n",
    "$$ V_{2}(s_0) = \\max \\{r(s_0,a_{up}) + \\gamma  V_{1}(s_3),\\quad r(s_0,a_{right}) + \\gamma  V_{1}(s_1)\\}$$\n",
    "\n",
    "$$ V_{2}(s_0) = \\max  \\{-30 + 0.9*0,\\quad 0 + 0.9* 0\\} = 0 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},\\{a_{left},a_{right}\\},a_{up}, \\{\\},a_{up},a_{up},a_{right},a_{right} , \\{\\}]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DDeiYtdEHyPV"
   },
   "source": [
    "### State $s_1$\n",
    "\n",
    "$$ V_{2}(s_1) = \\max \\{r(s_1,a_{left}) +  \\gamma  V_{1}(s_0),\\quad\n",
    "  r(s_1,a_{right}) +  \\gamma  V_{1}(s_2),\\quad\n",
    "  r(s_1,a_{up}) +  \\gamma  V_{1}(s_4)\\} $$\n",
    "\n",
    "$$ V_{2}(s_1) = \\max  \\{0 + 0.9*0, \\quad 0 + 0.9*0, \\quad -30 + 0.9*10 \\} = 0 $$ \n",
    "\n",
    "$\\pi_{greedy} = [a_{right},\\{a_{left},a_{right}\\},a_{up}, \\{\\},a_{up},a_{up},a_{right},a_{right} , \\{\\}]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FXlY8Yi_IgXH"
   },
   "source": [
    "### State $s_2$\n",
    "\n",
    "$$ V_{2}(s_2) =  r(s_2,a_{up}) + \\gamma  V_{1}(s_5)$$\n",
    "\n",
    "$$ V_{2}(s_2) = 0 + 0.9*80 = 72 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},\\{a_{left},a_{right}\\},a_{up}, \\{\\},a_{up},a_{up},a_{right},a_{right} , \\{\\}]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sxTzVxZwI6Zk"
   },
   "source": [
    "### State $s_4$\n",
    "\n",
    "$$ V_{2}(s_4) = \\max \\{r(s_4,a_{left}) +  \\gamma  V_{1}(s_3),\\quad\n",
    "  r(s_4,a_{right}) +  \\gamma  V_{1}(s_5),\\quad\n",
    "  r(s_4,a_{up}) +  \\gamma  V_{1}(s_7)\\} $$\n",
    "\n",
    "$$ V_{2}(s_4) = \\max  \\{-40 + 0.9 * 0,\\quad 0 + 0.9 * 80, \\quad 10  + 0.9 * 100\\} = 100 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},\\{a_{left},a_{right}\\},a_{up}, \\{\\},a_{up},a_{up},a_{right},a_{right} , \\{\\}]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NOBWYjEvGU8Y"
   },
   "source": [
    "### State $s_5$ \n",
    "\n",
    "$$ V_{2}(s_5) = \\max \\{r(s_5,a_{left}) + \\gamma  V_{1}(s_4),\\quad\n",
    "  r(s_5,a_{down}) + \\gamma  V_{1}(s_2),\\quad\n",
    "  r(s_5,a_{up}) + \\gamma  V_{1}(s_8)\\} $$\n",
    "\n",
    "$$ V_{2}(s_5) = \\max  \\{0 + 0.9*10,\\quad 0 + 0.9*0,\\quad 80 + 0.9*0\\} = 80 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},\\{a_{left},a_{right}\\},a_{up}, \\{\\},a_{up},a_{up},a_{right},a_{right} , \\{\\}]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xT3wszFdFpBd"
   },
   "source": [
    "### State $s_6$\n",
    "\n",
    "$$ V_{2}(s_6) = \\max \\{r(s_6,a_{down}) + \\gamma  V_{2}(s_3),\\quad r(s_6,a_{right}) + \\gamma  V_{2}(s_7)\\}$$\n",
    "\n",
    "$$ V_{2}(s_6) = \\max  \\{-20 + 0.9*0,\\quad -10 + 0.9*100\\} = 80 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},\\{a_{left},a_{right}\\},a_{up}, \\{\\},a_{up},a_{up},a_{right},a_{right} , \\{\\}]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State $s_7$\n",
    "\n",
    "$$ V_{2}(s_7) =  r(s_7,a_{right}) + \\gamma  V_{1}(s_8)$$\n",
    "\n",
    "$$ V_{2}(s_7) = 100 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},\\{a_{left},a_{right}\\},a_{up}, \\{\\},a_{up},a_{up},a_{right},a_{right} , \\{\\}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLtBo3JwKfiX"
   },
   "source": [
    "## Third iteration (k=3)\n",
    "\n",
    "$V_2 = \\{0,0,\\textcolor{red}{72},0,\\textcolor{red}{100},80,\\textcolor{red}{80},100,0\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sod6RmCALCfK"
   },
   "source": [
    "### State $s_0$\n",
    "\n",
    "\n",
    "$$ V_{3}(s_0) = \\max \\{r(s_0,a_{up}) + \\gamma  V_{2}(s_3),\\quad r(s_0,a_{right}) + \\gamma  V_{2}(s_1)\\}$$\n",
    "\n",
    "$$ V_{3}(s_0) = \\max  \\{-30 + 0.9*0,\\quad 0 + 0.9* 0\\} = 0 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},\\{a_{left},a_{right}\\},a_{up}, \\{\\},a_{up},a_{up},a_{right},a_{right} , \\{\\}]$\n",
    "\n",
    "We can calculate the value of $s_0$, but we know its value depends on the value of  $s_1$ which will change again. Therefore we don't need to figure it out now. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Gmgz9YHqLwiI"
   },
   "source": [
    "### State $s_1$\n",
    "\n",
    "$$ V_{3}(s_1) = \\max \\{r(s_1,a_{left}) +  \\gamma  V_{2}(s_0),\\quad\n",
    "  r(s_1,a_{right}) +  \\gamma  V_{2}(s_2),\\quad\n",
    "  r(s_1,a_{up}) +  \\gamma  V_{2}(s_4)\\} $$\n",
    "\n",
    "$$ V_{3}(s_1) = \\max  \\{0 + 0.9*0, \\quad 0 + 0.9*0, \\quad -30 + 0.9*100 \\} = 60 $$ \n",
    "\n",
    "$\\pi_{greedy} = [a_{right},\\textcolor{red}{a_{up}},a_{up}, \\{\\},a_{up},a_{up},a_{right},a_{right} , \\{\\}]$\n",
    "\n",
    "We can calculate the value of $s_1$, but we know its value depends on the value of  $s_2$ which will change again. Therefore we don't need to figure it out now. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ejtj-e2OMAgc"
   },
   "source": [
    "### State $s_2$\n",
    "\n",
    "$$ V_{3}(s_2) =  r(s_2,a_{up}) + \\gamma  V_{2}(s_5)$$\n",
    "\n",
    "$$ V_{3}(s_2) = 0 + 0.9*80 = 72 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},a_{up},a_{up}, \\{\\},a_{up},a_{up},a_{right},a_{right} , \\{\\}]$\n",
    "\n",
    "We can calculate the value of $s_2$, but we know its value depends on the value of  $s_5$ which will change again. Therefore we don't need to figure it out now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State $s_4$\n",
    "\n",
    "Since we can only reach states $s_3$, $s_5$ and $s_7$ from state $s_4$, and $V(s_3)$, $V(s_5)$ and $V(s_7)$ have not been changed in the last iteration, we know that $V(s_4)$ will not change in this iteration."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TbAn6Y-TMKHx"
   },
   "source": [
    "### State $s_5$ \n",
    "\n",
    "$$ V_{3}(s_5) = \\max \\{r(s_5,a_{left}) + \\gamma  V_{2}(s_4),\\quad\n",
    "  r(s_5,a_{down}) + \\gamma  V_{2}(s_2),\\quad\n",
    "  r(s_5,a_{up}) + \\gamma  V_{2}(s_8)\\} $$\n",
    "\n",
    "$$ V_{3}(s_5) = \\max  \\{0 + 0.9*100,\\quad 0 + 0.9*72,\\quad 80 + 0.9*0\\} = 90 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},a_{up},a_{up}, \\{\\},a_{up},\\textcolor{red}{a_{left}},a_{right},a_{right} , \\{\\}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States $s_6$ and $s_7$\n",
    "\n",
    "Since the V-values for the successor states for $s_6$ and $s_7$ have not been changed in the last iteration, we know that $V(s_6)$ and $V(s_7)$ will not change in this iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnlxX5QANLOR"
   },
   "source": [
    "## Fourth iteration (k=4)\n",
    "\n",
    "$V_3 = \\{0,\\textcolor{red}{60},72,0,100,\\textcolor{red}{90},80,100,0\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State $s_0$\n",
    "\n",
    "$$ V_{4}(s_0) = \\max \\{r(s_0,a_{up}) + \\gamma  V_{3}(s_3),\\quad r(s_0,a_{right}) + \\gamma  V_{3}(s_1)\\}$$\n",
    "\n",
    "$$ V_{4}(s_0) = \\max  \\{-30 + 0.9*0,\\quad 0 + 0.9* 60\\} = 54 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},a_{up},a_{up}, \\{\\},a_{up},a_{left},a_{right},a_{right} , \\{\\}]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jhlSfUYuNYoe"
   },
   "source": [
    "### State $s_2$\n",
    "\n",
    "$$ V_{4}(s_2) =  r(s_2,a_{up}) + \\gamma  V_{3}(s_5)$$\n",
    "\n",
    "$$ V_{4}(s_2) = 0 + 0.9*90 = 81 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},a_{up},a_{up}, \\{\\},a_{up},a_{left},a_{right},a_{right} , \\{\\}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State $s_4$\n",
    "\n",
    "$$ V_{4}(s_4) = \\max \\{r(s_4,a_{left}) +  \\gamma  V_{3}(s_3),\\quad\n",
    "  r(s_4,a_{right}) +  \\gamma  V_{3}(s_5),\\quad\n",
    "  r(s_4,a_{up}) +  \\gamma  V_{3}(s_7)\\} $$\n",
    "\n",
    "$$ V_{4}(s_4) = \\max  \\{-40 + 0.9 * 0,\\quad 0 + 0.9 * 90,\\quad 10  + 0.9 * 100\\} = 100 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},a_{up},a_{up}, \\{\\},a_{up},a_{left},a_{right},a_{right} , \\{\\}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZk19cmLNkQG"
   },
   "source": [
    "## Fifth iteration (k=5)\n",
    "\n",
    "$V_4 = \\{\\textcolor{red}{54},60,\\textcolor{red}{81},0,100,90,80,100,0\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4-3cWtczNsFr"
   },
   "source": [
    "### State $s_1$\n",
    "\n",
    "$$ V_{5}(s_1) = \\max \\{r(s_1,a_{left}) +  \\gamma  V_{4}(s_0),\\quad\n",
    "  r(s_1,a_{right}) +  \\gamma  V_{4}(s_2),\\quad\n",
    "  r(s_1,a_{up}) +  \\gamma  V_{4}(s_4)\\} $$\n",
    "\n",
    "$$ V_{5}(s_1) = \\max  \\{0 + 0.9*0, \\quad 0 + 0.9*81, \\quad -30 + 0.9*100 \\} = 73 $$ \n",
    "\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},\\textcolor{red}{a_{right}},a_{up}, \\{\\},a_{up},a_{left},a_{right},a_{right} , \\{\\}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State $s_5$ \n",
    "\n",
    "$$ V_{5}(s_5) = \\max \\{r(s_5,a_{left}) + \\gamma  V_{4}(s_4),\\quad\n",
    "  r(s_5,a_{down}) + \\gamma  V_{4}(s_2),\\quad\n",
    "  r(s_5,a_{up}) + \\gamma  V_{4}(s_8)\\} $$\n",
    "\n",
    "$$ V_{5}(s_5) = \\max  \\{0 + 0.9*100,\\quad 0 + 0.9*81,\\quad 80 + 0.9*0\\} = 90 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},a_{right},a_{up}, \\{\\},a_{up},a_{left},a_{right},a_{right} , \\{\\}]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uiSc4VdKOdVb"
   },
   "source": [
    "## Sixth iteration (k=6)\n",
    "\n",
    "$V_5 = \\{54,\\textcolor{red}{73},81,0,100,90,80,100,0\\}$\n",
    "\n",
    "### State $s_0$\n",
    "\n",
    "$$ V_{6}(s_0) = \\max \\{r(s_0,a_{up}) + \\gamma  V_{5}(s_3),\\quad r(s_0,a_{right}) + \\gamma  V_{5}(s_1)\\}$$\n",
    "\n",
    "$$ V_{6}(s_0) = \\max  \\{-30 + 0.9*0,\\quad 0 + 0.9* 73\\} = 66 $$\n",
    "\n",
    "$\\pi_{greedy} = [a_{right},a_{right},a_{up}, \\{\\},a_{up},a_{left},a_{right},a_{right} , \\{\\}]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal state value function \n",
    "\n",
    "\n",
    "$ V_{6}^*(s_0)=  66$    \n",
    "$ V_{6}^*(s_1) = 73$     \n",
    "$ V_{6}^*(s_2) = 81$    \n",
    "$ V_{6}^*(s_3) = 0$   \n",
    "$ V_{6}^*(s_4) = 100$   \n",
    "$ V_{6}^*(s_5) = 90$   \n",
    "$ V_{6}^*(s_6) = 80$  \n",
    "$ V_{6}^*(s_7) = 100$      \n",
    "$ V_{6}^*(s_8) = 0$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzGJBKH_PeqK"
   },
   "source": [
    "What is the optimal policy given that we start form $s_0$?\n",
    "\n",
    "$$s_0 \\rightarrow s_1 \\rightarrow s_2 \\rightarrow s_5 \\rightarrow s_4\\rightarrow s_7 \\rightarrow s_8 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2\n",
    "\n",
    "<img src=\"grid.png\"\n",
    "     alt=\"Grid World\"\n",
    "     width=\"600\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from the the terminal states $s_3, s_8$\n",
    "\n",
    "### States $s_3, s_8$\n",
    "\n",
    "$$V^*(s_3) = r(s_3,a_i) = 0   $$ \n",
    "$$V^*(s_8) = r(s_8,a_i) = 0   $$ \n",
    "\n",
    "No actions can be performed there. The values of these states will not change."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From which state can you reach the final state with an action?\n",
    "\n",
    "### State $s_7$\n",
    "\n",
    "$$V^*(s_7) =  r(s_7,a_{right}) + \\gamma V^*(s_8)$$\n",
    "\n",
    "$$V^*(s_7) = 100 $$\n",
    "\n",
    "This value will not change."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State $s_6$\n",
    "\n",
    "It is clear that we will not use the action in the direction of $s_3$.\n",
    "\n",
    "$$V^*(s_6) = r(s_6,a_{right}) +  \\gamma V^*(s_7)$$\n",
    "\n",
    "$$V^*(s_6) = -10 + 0.9*100 = 80 $$\n",
    "\n",
    "This value will not change any more."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From $s_5$ and $s_4$, there are two paths to the destination $s_8$. Maybe we will need to do two iterations here to calculate the values for these states. \n",
    "\n",
    "\n",
    "### State $s_5$\n",
    "\n",
    "We do not need to consider the action in the direction of $s_2$.\n",
    "\n",
    "$$ V_{2}(s_5) = \\max \\{r(s_5,a_{left}) + \\gamma  V_{0}(s_4), \\quad r(s_5,a_{up}) + \\gamma V^*(s_8)\\} $$\n",
    "\n",
    "$$ V_{2}(s_5) = \\max  \\{0 + 0.9*0, \\quad 80 + 0.9*0\\} = 80 $$\n",
    "\n",
    "\n",
    "### State $s_4$\n",
    "\n",
    "It is clear that we will not use the action in the direction of $s_3$.\n",
    "\n",
    "$$ V_{2}(s_4) = \\max \\{r(s_4,a_{right}) + \\gamma  V_{2}(s_5)\\,\n",
    "  \\quad r(s_4,a_{up}) +  \\gamma V^*(s_7))\\} $$\n",
    "\n",
    "$$ V_{2}(s_4) = \\max  \\{ 0 + 0.9 * 80,\\quad 10  + 0.9 * 100\\} = 100 $$\n",
    "\n",
    "We will recalculate the value of $s_5$ because the state $s_4$ is better than we thought. If the new value of $s_5$ is bigger than $\\frac{100}{0.9} \\approx 111$, then we need another iteration to calculate $V(s_4)$.\n",
    "\n",
    "\n",
    "### State $s_5$\n",
    "\n",
    "$$ V_{3}(s_5) = \\max \\{r(s_5,a_{left}) +  \\gamma  V_{2}(s_4), \\quad r(s_5,a_{up}) +  \\gamma V^*(s_8)\\} $$\n",
    "\n",
    "$$ V_{3}(s_5) = \\max  \\{0 + 0.9*100,\\quad 80 + 0.9*0\\} = 90 $$\n",
    "\n",
    "\n",
    "The values $V^*(s_5)$ and $V^*(s_4)$  will not change any more."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State $s_2$\n",
    "\n",
    "The value of $s_2$ depends only on the value of $s_5$\n",
    "\n",
    "$$V^*(s_2) =  r(s_2,a_{up}) +  \\gamma V^*(s_5)$$\n",
    "\n",
    "$$V^*(s_2) = 0 + 0.9*90 = 81 $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State $s_1$\n",
    "\n",
    "\n",
    "$$V^*(s_1) = \\max \\{r(s_1,a_{right}) +  \\gamma V^*(s_2), \\quad r(s_1,a_{up}) +  \\gamma V^*(s_4)\\} $$\n",
    "\n",
    "$$V^*(s_1) = \\max  \\{ 0 + 0.9*81, \\quad -30 + 0.9*100 \\} = 72.9 = 73 $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State $s_0$\n",
    "\n",
    "$$V^*(s_0) = r(s_0,a_{right}) +\\gamma V^*(s_1)$$\n",
    "\n",
    "$$V^*(s_0) =  0 + 0.9* 73 = 65.7 = 66 $$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Reinforcement Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
