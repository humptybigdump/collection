{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tc5f11x8bt5v"
   },
   "source": [
    "## Local Setup\n",
    "\n",
    "If you prefer to work locally, see the following instructions for setting up Python in a virtual environment. You can then ignore the instruction in \"Colab Setup\".\n",
    "\n",
    "Tips:\n",
    "- As this exercise could require substantial computation, we recommend using GPU for training. If you do not have GPU on local machine, please consider using Colab instead\n",
    "- The D4RL and mujoco-py that we will use in this exercise are unfortunately quite tricky to install, please conisder to use Colab instead\n",
    "- If you still want to run the notebook locally, here is an installation guide for reference. Notice: the setup instruction is tested with Ubuntu 22.04.\n",
    "\n",
    "If you haven't yet, create a [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) environment using:\n",
    "```\n",
    "conda create --name rl_hw5 python=3.8\n",
    "conda activate rl_hw5\n",
    "```\n",
    "\n",
    "Install the required conda packages:\n",
    "```\n",
    "conda install -c conda-forge glew\n",
    "conda install -c conda-forge mesalib\n",
    "conda install -c anaconda mesa-libgl-cos6-x86_64  \n",
    "conda install -c menpo glfw3  \n",
    "```\n",
    "\n",
    "Download [mujoco 2.1.0](https://github.com/google-deepmind/mujoco/releases/tag/2.1.0) and place it under your `$HOME/.mujoco/` , unzip the `tar.gz` file and rename the folder to `mujoco210`. The folder structure should look like following:\n",
    "```\n",
    "[user] cd mujoco210\n",
    "[user] pwd\n",
    "/home/(username)/.mujoco/mujoco210\n",
    "[user] ls\n",
    "bin  include  model  sample  THIRD_PARTY_NOTICES\n",
    "```\n",
    "Now we can try to install the [D4RL](https://github.com/Farama-Foundation/D4RL) module, you could either follow the instruction guidance from the official repository or the following guides:\n",
    "```\n",
    "pip install git+https://github.com/tinkoff-ai/d4rl@master#egg=d4rl\n",
    "\n",
    "pip install patchelf\n",
    "pip install 'cython<3'\n",
    "conda env config vars set LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
    "```\n",
    "Reactivate the conda environment to make the environmental variable setting take effect:\n",
    "```\n",
    "conda deactivate\n",
    "conda activate rl_hw5\n",
    "```\n",
    "Test if the d4rl works, open a python interactive console, run the following codes:\n",
    "You can safely ignore several warnings, as long as you can import the module successfully.\n",
    "```\n",
    "import d4rl\n",
    "import d4rl.gym_mujoco\n",
    "import gym\n",
    "env = gym.make(\"halfcheetah-medium-expert-v2\")\n",
    "```\n",
    "\n",
    "Install Torch using conda **or** pip, so run e.g.:\n",
    "```\n",
    "conda install pytorch pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "```\n",
    "**or**\n",
    "```\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "```\n",
    "For this exercise, you require a CUDA-enabled GPU, as training an image-based model on the CPU takes a very long time.\n",
    "Visit [the installation page](https://pytorch.org/get-started/locally/) to see the options available for different CUDA versions.\n",
    "The remaining dependencies can be installed with pip:\n",
    "```\n",
    "pip install matplotlib numpy tqdm ipykernel notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDeFmr_MdEVd"
   },
   "source": [
    "## Colab Setup\n",
    "\n",
    "Google Colab provides you with a temporary environment for python programming.\n",
    "While this conveniently works on any platform and internally handles dependency issues and such, it also requires you to set up the environment from scratch every time.\n",
    "The \"Colab Setup\" section below will be part of **every** exercise and contains utility that is needed before getting started.\n",
    "\n",
    "**IMPORTANT**: For this exercise, you require a GPU runtime environment, as training AWAC agent on the CPU takes a very long time.\n",
    "To do this, select \"Change runtime type\" from the context menu in the top right corner (next to the **Connect** button), and select **T4 GPU**.\n",
    "\n",
    "There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window).\n",
    "Any changes you make to the Jupyter notebook itself should be saved to your Google Drive.\n",
    "We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout.\n",
    "However, you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1uFyGBElg0-",
    "outputId": "e7324c38-a28d-4a31-8587-2ee7354794dd"
   },
   "outputs": [],
   "source": [
    "#Include this at the top of your colab code\n",
    "import os\n",
    "if not os.path.exists('.mujoco_setup_complete'):\n",
    "  # Get the prereqs\n",
    "  !apt-get -qq update\n",
    "  !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
    "  # Get Mujoco\n",
    "  !mkdir ~/.mujoco\n",
    "  !wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
    "  !tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n",
    "  !rm mujoco.tar.gz\n",
    "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "  !echo 'export LD_LIBRARY_PATH=HOME/.mujoco/mujoco210/bin' >> ~/.bashrc\n",
    "  !echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> ~/.bashrc\n",
    "  # THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n",
    "  !echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n",
    "  !ldconfig\n",
    "  # Install Mujoco-py\n",
    "  !pip3 install -U 'mujoco-py<2.2,>=2.1'\n",
    "  !pip install 'cython<3'\n",
    "  # run once\n",
    "  !touch .mujoco_setup_complete\n",
    "\n",
    "try:\n",
    "  if _mujoco_run_once:\n",
    "    pass\n",
    "except NameError:\n",
    "  _mujoco_run_once = False\n",
    "if not _mujoco_run_once:\n",
    "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "  try:\n",
    "    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/root/.mujoco/mujoco210/bin'\n",
    "    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/usr/lib/nvidia'\n",
    "  except KeyError:\n",
    "    os.environ['LD_LIBRARY_PATH']='/root/.mujoco/mujoco210/bin'\n",
    "  try:\n",
    "    os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "  except KeyError:\n",
    "    os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "  # presetup so we don't see output on first env initialization\n",
    "  # !pip install 'cython<3'\n",
    "  import mujoco_py\n",
    "  _mujoco_run_once = True\n",
    "  print(\"Successfully install mujoco_py, congratulations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SK5Ojh1sE0Ya",
    "outputId": "2fbb4ecc-e6fa-4113-8749-45bfc4018330"
   },
   "outputs": [],
   "source": [
    "%pip install -f https://download.pytorch.org/whl/torch_stable.html \\\n",
    "                einops\\\n",
    "                gym \\\n",
    "                protobuf\\\n",
    "                git+https://github.com/tinkoff-ai/d4rl@master#egg=d4rl\n",
    "\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpLQRUB5d1vS"
   },
   "source": [
    "# Exercise 5: Imitation Learning and Offline Reinforcement Learning\n",
    "\n",
    "In this homework, we are going to implement **self-attention**, **behavioral cloning (BC)** and **advantage weighted actor-critic (AWAC)** algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WURmQQW9-XR"
   },
   "source": [
    "## Part 1: Self-Attention (4pts)\n",
    "In this exercise, you will implement a function to compute the most fundamental variant of self-attention, known as the scaled dot-product attention. This mechanism is a cornerstone in many state-of-the-art models, particularly in natural language processing and imitation learning.\n",
    "\n",
    "Your task is to fill in a function that computes the scaled dot-product attention of input vectors. The input to your function will be three matrices: Queries (Q), Keys (K), and Values (V), each with a shape of $(n, d_k)$, where $n$ represents the number of embeddings and $d_k$ represents the embedding dimension.\n",
    "\n",
    "The scaled dot-product attention is mathematically represented as:\n",
    "$$\n",
    "\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{Q K^T}{\\sqrt{d_k}})V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bb_nOEu99fE",
    "outputId": "2f44f426-8fd7-41d1-fcea-fa41444957e9"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32\n",
    "\n",
    "def self_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Implement the self-attention mechanism.\n",
    "\n",
    "    Parameters:\n",
    "    Q (torch.Tensor): Query matrix\n",
    "    K (torch.Tensor): Key matrix\n",
    "    V (torch.Tensor): Value matrix\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Output of self-attention mechanism\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Calculate attention scores\n",
    "    # Step 2: Apply softmax to get attention weights\n",
    "    # Step 3: Calculate the output as weighted sum of values\n",
    "\n",
    "    ## TODO ##\n",
    "\n",
    "    ## END ##\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "for _ in tqdm(range(2000)):\n",
    "    inputs = torch.randn(10, 20).to(DEVICE).to(dtype)\n",
    "    d_k = inputs.shape[-1]\n",
    "\n",
    "    ### Initialize the weights matrix for Q, K, V\n",
    "    W_q = torch.randn(d_k, d_k).to(DEVICE).to(dtype)\n",
    "    W_k = torch.randn(d_k, d_k).to(DEVICE).to(dtype)\n",
    "    W_v = torch.randn(d_k, d_k).to(DEVICE).to(dtype)\n",
    "\n",
    "    ### Calculate the Q, K, V from embeddings\n",
    "    Q = torch.matmul(inputs, W_q)\n",
    "    K = torch.matmul(inputs, W_k)\n",
    "    V = torch.matmul(inputs, W_v)\n",
    "\n",
    "    # Call your self_attention function\n",
    "    output_vectors = self_attention(Q, K, V)\n",
    "\n",
    "    # Verify the results\n",
    "    self_attention_builtin = F.scaled_dot_product_attention(Q, K, V)\n",
    "    assert torch.allclose(self_attention_builtin, output_vectors, atol=1e-4),\"\\n The implementation seems to be incorrect!\"\n",
    "\n",
    "print(\"\\n The implementation seems to be correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLzkYNDvs0gQ"
   },
   "source": [
    "## Imports and Utilities for Part 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQoDRPrPezRt"
   },
   "source": [
    "**Hint** :After execute the following block, there could be some warnings regarding 'flow', 'carla' and OpenGL, feel free to ignore them â˜º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VoC69JIHc4k",
    "outputId": "aeb489ad-4fd3-48b1-d92c-fbf16e3edeca"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import d4rl\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtJdpssqHL2L"
   },
   "outputs": [],
   "source": [
    "def update_plot(iterations, rewards, algorithm='AWAC'):\n",
    "    \"\"\"\n",
    "    Update the training progress plot with new iterations and rewards.\n",
    "    Designed to work in environments like Google Colab.\n",
    "\n",
    "    Args:\n",
    "    iterations (list or array): A list or array of iteration numbers.\n",
    "    rewards (list or array): A list or array of rewards (normalized scores in\n",
    "    this exercise) corresponding to the iterations.\n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(iterations, rewards, label=f'{algorithm}')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Normalized Score')\n",
    "    plt.title(f'Learning Curve of the {algorithm} Algorithm')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def set_seed(\n",
    "    seed: int, env: Optional[gym.Env] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Sets a fixed random seed for various modules and the environment (if provided)\n",
    "    to ensure reproducibility.\n",
    "\n",
    "    Parameters:\n",
    "    - seed (int): The random seed to set.\n",
    "    - env (Optional[gym.Env]): The gym environment to set the seed for.\n",
    "    Defaults to None.\n",
    "\n",
    "    Description:\n",
    "    - If an environment is provided, its seed and action space seed are set to\n",
    "    ensure consistent results in the environment.\n",
    "    - Sets the PYTHONHASHSEED environment variable, which controls the randomness\n",
    "    of Python's hash-based operations.\n",
    "    - Initializes the random seeds for NumPy, Python's built-in random module,\n",
    "    and PyTorch to ensure consistent random number generation across these libraries.\n",
    "    \"\"\"\n",
    "    if env is not None:\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def wrap_env(\n",
    "    env: gym.Env,\n",
    "    state_mean: Union[np.ndarray, float] = 0.0,\n",
    "    state_std: Union[np.ndarray, float] = 1.0,\n",
    ") -> gym.Env:\n",
    "    \"\"\"\n",
    "    Wraps a gym environment to normalize its states.\n",
    "\n",
    "    Parameters:\n",
    "    - env (gym.Env): The gym environment to wrap.\n",
    "    - state_mean (Union[np.ndarray, float]): The mean used for state normalization, can be a scalar or an array. Defaults to 0.0.\n",
    "    - state_std (Union[np.ndarray, float]): The standard deviation used for state normalization, can be a scalar or an array. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "    - gym.Env: The wrapped gym environment with state normalization.\n",
    "\n",
    "    Description:\n",
    "    - Normalizes the states of the environment by subtracting the mean and dividing by the standard deviation.\n",
    "    - Utilizes the TransformObservation wrapper from gym.wrappers to apply the normalization.\n",
    "    \"\"\"\n",
    "    def normalize_state(state):\n",
    "        return (state - state_mean) / state_std\n",
    "\n",
    "    env = gym.wrappers.TransformObservation(env, normalize_state)\n",
    "    return env\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_policy(\n",
    "        env: gym.Env, policy: torch.nn.Module, num_episodes: int = 10,\n",
    "        seed: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Evaluates a policy in the given environment over a specified number of episodes.\n",
    "\n",
    "    Parameters:\n",
    "    - env (gym.Env): The gym environment to evaluate the policy in.\n",
    "    - policy (torch.nn.Module): The policy model to evaluate.\n",
    "    - num_episodes (int): The number of episodes to run the evaluation for. Defaults to 10.\n",
    "    - seed (int): The random seed for environment reproducibility. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: An array of cumulative rewards received in each episode.\n",
    "\n",
    "    Description:\n",
    "    - Sets the seed for the environment for consistent episode generation.\n",
    "    - Puts the policy in evaluation mode to disable any training-specific operations like dropout.\n",
    "    - Runs the policy for a specified number of episodes, collecting total rewards per episode.\n",
    "    - Switches the policy back to training mode after evaluation.\n",
    "    - Uses PyTorch's no_grad context manager to disable gradient calculations, improving performance.\n",
    "    \"\"\"\n",
    "    env.seed(seed)\n",
    "    policy.eval()\n",
    "    episode_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0.0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.act(state, device=DEVICE)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "        episode_rewards.append(episode_reward)\n",
    "    policy.train()\n",
    "    return np.array(episode_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYFSXCkj76jI"
   },
   "source": [
    "**Policy Network (Actor)**\n",
    "\n",
    "The following neural network is trained to represent the policy `p(a|s)`, the forward function return action **sampled** from predict distribution, along with its log-probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sl814oPbHqmc"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_dim: int,\n",
    "        min_log_std: float = -20.0,\n",
    "        max_log_std: float = 2.0,\n",
    "        min_action: float = -1.0,\n",
    "        max_action: float = 1.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        An actor class that defines a policy network for reinforcement learning.\n",
    "\n",
    "        Parameters:\n",
    "        - state_dim (int): Dimension of the state space.\n",
    "        - action_dim (int): Dimension of the action space.\n",
    "        - hidden_dim (int): Dimension of the hidden layers in the neural network.\n",
    "        - min_log_std (float): Minimum logarithm of the standard deviation for the action distribution. Defaults to -20.0.\n",
    "        - max_log_std (float): Maximum logarithm of the standard deviation for the action distribution. Defaults to 2.0.\n",
    "        - min_action (float): Minimum action value. Defaults to -1.0.\n",
    "        - max_action (float): Maximum action value. Defaults to 1.0.\n",
    "\n",
    "        Description:\n",
    "        - Initializes a policy network using a multilayer perceptron (MLP) with ReLU activations.\n",
    "        - The network predicts the mean of the action distribution.\n",
    "        - The standard deviation of the action distribution is parameterized as a learnable parameter, clamped between specified min and max log values.\n",
    "        - The action range is bounded between specified min and max values.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._mlp = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "        )\n",
    "        self._log_std = nn.Parameter(torch.zeros(action_dim, dtype=torch.float32))\n",
    "        self._min_log_std = min_log_std\n",
    "        self._max_log_std = max_log_std\n",
    "        self._min_action = min_action\n",
    "        self._max_action = max_action\n",
    "\n",
    "    def _get_policy(self, state: torch.Tensor) -> torch.distributions.Distribution:\n",
    "        \"\"\"\n",
    "        Computes the policy distribution for a given state.\n",
    "\n",
    "        Parameters:\n",
    "        - state (torch.Tensor): The state tensor.\n",
    "\n",
    "        Returns:\n",
    "        - torch.distributions.Distribution: The policy distribution as a Normal distribution.\n",
    "\n",
    "        Description:\n",
    "        - Passes the state through the MLP to get the mean of the action distribution.\n",
    "        - Clamps the logarithm of the standard deviation within the specified range.\n",
    "        - Constructs a Normal distribution with the computed mean and standard deviation.\n",
    "        \"\"\"\n",
    "        mean = self._mlp(state)\n",
    "        log_std = self._log_std.clamp(self._min_log_std, self._max_log_std)\n",
    "        policy = torch.distributions.Normal(mean, log_std.exp())\n",
    "        return policy\n",
    "\n",
    "    def log_prob(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the log probability of a given action under the policy for a given state.\n",
    "\n",
    "        Parameters:\n",
    "        - state (torch.Tensor): The state tensor.\n",
    "        - action (torch.Tensor): The action tensor.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The log probability of the action under the policy.\n",
    "\n",
    "        Description:\n",
    "        - Retrieves the policy distribution for the given state.\n",
    "        - Calculates the log probability of the given action under this distribution.\n",
    "        - Sums the log probabilities across the action dimensions.\n",
    "        \"\"\"\n",
    "        policy = self._get_policy(state)\n",
    "        log_prob = policy.log_prob(action).sum(-1, keepdim=True)\n",
    "        return log_prob\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the policy network.\n",
    "\n",
    "        Parameters:\n",
    "        - state (torch.Tensor): The state tensor.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple[torch.Tensor, torch.Tensor]: A tuple of the action tensor and its log probability.\n",
    "\n",
    "        Description:\n",
    "        - Gets the policy distribution for the given state.\n",
    "        - Samples an action from this distribution using the reparameterization trick.\n",
    "        - Clamps the action within the specified range.\n",
    "        - Computes the log probability of the sampled action.\n",
    "        \"\"\"\n",
    "        policy = self._get_policy(state)\n",
    "        action = policy.rsample()\n",
    "        action.clamp_(self._min_action, self._max_action)\n",
    "        log_prob = policy.log_prob(action).sum(-1, keepdim=True)\n",
    "        return action, log_prob\n",
    "\n",
    "    def act(self, state: np.ndarray, device: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Determines the action to take for a given state.\n",
    "\n",
    "        Parameters:\n",
    "        - state (np.ndarray): The state array.\n",
    "        - device (str): The device to perform computations on.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The action array.\n",
    "\n",
    "        Description:\n",
    "        - Converts the state to a PyTorch tensor and sends it to the specified device.\n",
    "        - Computes the policy distribution for the given state.\n",
    "        - If the network is in training mode, samples an action from the distribution; otherwise, uses the mean of the distribution.\n",
    "        - Converts the action back to a NumPy array and returns it.\n",
    "        \"\"\"\n",
    "        state_t = torch.tensor(state[None], dtype=torch.float32, device=device)\n",
    "        policy = self._get_policy(state_t)\n",
    "        if self._mlp.training:\n",
    "            action_t = policy.sample()\n",
    "        else:\n",
    "            action_t = policy.mean\n",
    "        action = action_t[0].cpu().numpy()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seQ0szUa-JLA"
   },
   "source": [
    "**Deep Q-Network (Critic)**\n",
    "\n",
    "The following critic network is trained to represent the Q-function, the input is the state `s` and action `a`, and the output is Q value for this state-action pair.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rC5-bLwDHx5j"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_dim: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the Critic model, which is a neural network used for value estimation in reinforcement learning.\n",
    "\n",
    "        The Critic model evaluates the quality of actions taken in a given state. It is used to guide the training\n",
    "        of the Actor model by providing feedback on the expected returns of the actions it selects.\n",
    "\n",
    "        Parameters:\n",
    "        - state_dim (int): The dimensionality of the state space.\n",
    "        - action_dim (int): The dimensionality of the action space.\n",
    "        - hidden_dim (int): The number of neurons in each hidden layer.\n",
    "\n",
    "        The Critic network takes both state and action as inputs and outputs a Q-value representing the expected return\n",
    "        of taking the given action in the given state. The network consists of a Multi-Layer Perceptron (MLP) with ReLU\n",
    "        activations.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._mlp = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass through the Critic network.\n",
    "\n",
    "        Parameters:\n",
    "        - state (torch.Tensor): The state tensor for which the Q-value is to be evaluated.\n",
    "        - action (torch.Tensor): The action tensor for which the Q-value is to be evaluated.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The Q-value of the given state-action pair.\n",
    "\n",
    "        This method concatenates the state and action tensors and passes them through the MLP to output\n",
    "        a Q-value. This value represents the Critic's estimate of the expected return for taking the given\n",
    "        action in the given state.\n",
    "        \"\"\"\n",
    "        q_value = self._mlp(torch.cat([state, action], dim=-1))\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtGiZx9Hx5yS"
   },
   "source": [
    "**Replay Buffer**\n",
    "\n",
    "The Replay Buffer stores transitions (state, action, reward, next state, done). These stored transitions are used to train the agent by sampling random mini-batches, enabling the agent to learn from past experiences (replay). Different from the online replay buffer that used in online RL, in this exercise, we will use a fixed offline replay buffer that loaded from an offline collected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GV0r5nDoH2HI"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        buffer_size: int,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a Replay Buffer for storing experiences in reinforcement learning.\n",
    "\n",
    "        Parameters:\n",
    "        - state_dim (int): Dimensionality of the state space.\n",
    "        - action_dim (int): Dimensionality of the action space.\n",
    "        - buffer_size (int): The maximum number of transitions the buffer can hold.\n",
    "        - device (str): The device on which the tensors are stored. Default is 'cpu'.\n",
    "\n",
    "        The buffer initializes tensors to store states, actions, rewards, next states, and done flags.\n",
    "        Each tensor has a size determined by the buffer_size and the respective dimensionality of its content.\n",
    "        \"\"\"\n",
    "        self._buffer_size = buffer_size\n",
    "        self._pointer = 0\n",
    "        self._size = 0\n",
    "\n",
    "        self._states = torch.zeros(\n",
    "            (buffer_size, state_dim), dtype=torch.float32, device=device\n",
    "        )\n",
    "        self._actions = torch.zeros(\n",
    "            (buffer_size, action_dim), dtype=torch.float32, device=device\n",
    "        )\n",
    "        self._rewards = torch.zeros((buffer_size, 1), dtype=torch.float32, device=device)\n",
    "        self._next_states = torch.zeros(\n",
    "            (buffer_size, state_dim), dtype=torch.float32, device=device\n",
    "        )\n",
    "        self._dones = torch.zeros((buffer_size, 1), dtype=torch.float32, device=device)\n",
    "        self._device = device\n",
    "\n",
    "    def _to_tensor(self, data: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Converts numpy array data to a PyTorch tensor.\n",
    "\n",
    "        Parameters:\n",
    "        - data (np.ndarray): Data to be converted.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The corresponding tensor on the specified device.\n",
    "\n",
    "        This method is used to convert numpy arrays to PyTorch tensors, ensuring that all data in the\n",
    "        buffer is in tensor format for efficient processing and batched learning.\n",
    "        \"\"\"\n",
    "        return torch.tensor(data, dtype=torch.float32, device=self._device)\n",
    "\n",
    "    def load_d4rl_dataset(self, data: Dict[str, np.ndarray]):\n",
    "        \"\"\"\n",
    "        Loads a dataset from D4RL (Datasets for Deep Data-Driven Reinforcement Learning) into the buffer.\n",
    "\n",
    "        Parameters:\n",
    "        - data (Dict[str, np.ndarray]): The dataset to be loaded, typically containing 'observations',\n",
    "          'actions', 'rewards', 'next_observations', and 'terminals'.\n",
    "\n",
    "        This method loads the entire dataset into the buffer, assuming the buffer is empty and large enough\n",
    "        to hold the dataset. It raises an error if the buffer is non-empty or smaller than the dataset.\n",
    "        \"\"\"\n",
    "        if self._size != 0:\n",
    "            raise ValueError(\"Trying to load data into non-empty replay buffer\")\n",
    "        n_transitions = data[\"observations\"].shape[0]\n",
    "        if n_transitions > self._buffer_size:\n",
    "            raise ValueError(\n",
    "                \"Replay buffer is smaller than the dataset you are trying to load!\"\n",
    "            )\n",
    "        self._states[:n_transitions] = self._to_tensor(data[\"observations\"])\n",
    "        self._actions[:n_transitions] = self._to_tensor(data[\"actions\"])\n",
    "        self._rewards[:n_transitions] = self._to_tensor(data[\"rewards\"][..., None])\n",
    "        self._next_states[:n_transitions] = self._to_tensor(data[\"next_observations\"])\n",
    "        self._dones[:n_transitions] = self._to_tensor(data[\"terminals\"][..., None])\n",
    "        self._size += n_transitions\n",
    "        self._pointer = min(self._size, n_transitions)\n",
    "\n",
    "        print(f\"Dataset size: {n_transitions}\")\n",
    "\n",
    "    def sample(self, batch_size: int) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Samples a mini-batch of experiences from the buffer.\n",
    "\n",
    "        Parameters:\n",
    "        - batch_size (int): The size of the mini-batch to sample.\n",
    "\n",
    "        Returns:\n",
    "        - List[torch.Tensor]: A list containing tensors of states, actions, rewards, next states, and dones.\n",
    "\n",
    "        Randomly samples a batch of experiences from the buffer. This method is used during the training\n",
    "        process to provide the agent with a diverse set of experiences from which to learn, breaking the\n",
    "        correlation present in sequential experiences.\n",
    "        \"\"\"\n",
    "        indices = np.random.randint(0, min(self._size, self._pointer), size=batch_size)\n",
    "        states = self._states[indices]\n",
    "        actions = self._actions[indices]\n",
    "        rewards = self._rewards[indices]\n",
    "        next_states = self._next_states[indices]\n",
    "        dones = self._dones[indices]\n",
    "        return [states, actions, rewards, next_states, dones]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOcXCK7q1IVL"
   },
   "source": [
    "**Setup environmental hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20AvC6J2IGTP"
   },
   "outputs": [],
   "source": [
    "ENV_NAME = \"halfcheetah-medium-expert-v2\"\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pp5PV1qc1W4Z"
   },
   "source": [
    "## Part 2: Behavioral Cloning (BC) (3pts)\n",
    "**Behavioral Cloning (BC)** is a classic imitation learning method where a model, typically a neural network, is trained to \"cloning\" the behavioral of an expert from expert demonstrations.\n",
    "\n",
    "1. **Data Collection:**\n",
    "   - Collect a dataset $\\mathcal{D}$ of state-action pairs $\\{(s_i, a_i)\\}$ from expert demonstrations.\n",
    "\n",
    "2. **Initialize:**\n",
    "   - Initialize the policy network $\\pi_\\theta(a|s)$ with parameters $\\theta$.\n",
    "\n",
    "3. **Training Loop:**\n",
    "   - **For each epoch** or until convergence:\n",
    "     - Shuffle the dataset $\\mathcal{D}$.\n",
    "     - **For each mini-batch** $\\mathcal{B}$ in $\\mathcal{D}$:\n",
    "       - Extract mini-batch of state-action pairs $(s, a)$.\n",
    "       - Compute the log-likelihood loss: $$ L(\\theta) = -\\frac{1}{|\\mathcal{B}|} \\sum_{(s, a) \\in B} \\log \\pi_\\theta(a|s) $$\n",
    "       - Update the parameters $\\theta$ of the policy network using gradient descent to minimize the loss.\n",
    "\n",
    "Tip:\n",
    "- There are multiple choices of the loss function in behavioral cloning, e.g., Mean Square Error (MSE) and Cross Entropy (CE). In this exercise, we focus on maximizing the log-likelihood as it is more straightforward to compare with AWAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_Bc66bULP_3"
   },
   "outputs": [],
   "source": [
    "class BehavioralCloning:\n",
    "    \"\"\"\n",
    "    A Behavioral Cloning class for training an agent using expert demonstrations.\n",
    "\n",
    "    Attributes:\n",
    "        _actor (nn.Module): The policy network that outputs actions given states.\n",
    "        _actor_optimizer (torch.optim.Optimizer): Optimizer for training the actor.\n",
    "        _device (str): The device (e.g., 'cpu' or 'cuda') on which to perform computations.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 actor: nn.Module,\n",
    "                 actor_optimizer: torch.optim.Optimizer,\n",
    "                 device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Initializes the BehavioralCloning class with an actor network, its optimizer, and the device.\n",
    "\n",
    "        Args:\n",
    "            actor (nn.Module): The policy network.\n",
    "            actor_optimizer (torch.optim.Optimizer): Optimizer for the actor.\n",
    "            device (str, optional): Computation device, 'cpu' by default.\n",
    "        \"\"\"\n",
    "        self._actor = actor\n",
    "        self._actor_optimizer = actor_optimizer\n",
    "        self._device = device\n",
    "\n",
    "    def _actor_loss(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the loss for the actor based on the log likelihood of the actions given the states.\n",
    "\n",
    "        Args:\n",
    "            states (torch.Tensor): The input states.\n",
    "            actions (torch.Tensor): The expert actions corresponding to the states.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed loss for the actor.\n",
    "        \"\"\"\n",
    "        ## TODO ##\n",
    "\n",
    "        ## END ##\n",
    "        return loss\n",
    "\n",
    "    def _update_actor(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a single optimization step for the actor.\n",
    "\n",
    "        Args:\n",
    "            states (torch.Tensor): States from the replay buffer.\n",
    "            actions (torch.Tensor): Corresponding actions from the replay buffer.\n",
    "\n",
    "        Returns:\n",
    "            float: The loss value after the optimization step.\n",
    "        \"\"\"\n",
    "        self._actor_optimizer.zero_grad()\n",
    "        loss = self._actor_loss(states, actions)\n",
    "        loss.backward()\n",
    "        self._actor_optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def update(self,\n",
    "              replay_buffer: ReplayBuffer,\n",
    "              batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Updates the actor using a batch of data from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            replay_buffer (ReplayBuffer): The replay buffer to sample experiences from.\n",
    "            batch_size (int): The number of samples to draw from the replay buffer.\n",
    "\n",
    "        Returns:\n",
    "            float: The loss value from the actor update.\n",
    "        \"\"\"\n",
    "        states, actions, _, _, _ = replay_buffer.sample(batch_size)\n",
    "        actor_loss = self._update_actor(states, actions)\n",
    "        return actor_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UU7nDLHLTTF"
   },
   "outputs": [],
   "source": [
    "def bc_train():\n",
    "    env = gym.make(ENV_NAME)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    dataset = d4rl.qlearning_dataset(env)\n",
    "\n",
    "    state_mean = dataset['observations'].mean(axis=0)\n",
    "    state_std = dataset['observations'].std(axis=0)\n",
    "\n",
    "    # Normalize the observations in the dataset\n",
    "    dataset['observations'] = (dataset['observations'] - state_mean) / state_std\n",
    "\n",
    "    # Wrap the environment to apply state normalization during training\n",
    "    env = wrap_env(env, state_mean=state_mean, state_std=state_std)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        buffer_size=2_000_000,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    replay_buffer.load_d4rl_dataset(dataset)\n",
    "    set_seed(SEED, env)\n",
    "\n",
    "    actor_kwargs = {\n",
    "        \"state_dim\": state_dim,\n",
    "        \"action_dim\": action_dim,\n",
    "        \"hidden_dim\": 256,\n",
    "    }\n",
    "\n",
    "    actor = Actor(**actor_kwargs)\n",
    "    actor.to(DEVICE)\n",
    "    actor_optimizer = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
    "\n",
    "    bc = BehavioralCloning(actor=actor,\n",
    "                           actor_optimizer=actor_optimizer,\n",
    "                           device=DEVICE)\n",
    "\n",
    "    eval_score_list = [ ]\n",
    "    iteration_list = [ ]\n",
    "    for t in trange(60000, ncols=100):\n",
    "        actor_loss = bc.update(replay_buffer, batch_size=256)\n",
    "        if (t + 1) % 1000 == 0:\n",
    "            eval_scores = evaluate_policy(env, actor, num_episodes=10)\n",
    "            normalized_eval_scores = env.get_normalized_score(eval_scores)\n",
    "            print(f\"Eval_scores: {eval_scores.mean():.2f} +/- {eval_scores.std():.2f}\")\n",
    "            print(f\"Normalized eval_scores: {normalized_eval_scores.mean():.2f} +/- {normalized_eval_scores.std():.2f}\")\n",
    "            print(f\"Actor loss: {actor_loss:.4f}\")\n",
    "            eval_score_list.append(normalized_eval_scores.mean())\n",
    "            iteration_list.append(t+1)\n",
    "            update_plot(iteration_list, eval_score_list, 'BC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNBloKb93fHp"
   },
   "source": [
    "**Run the following block to train your BC agent!**\n",
    "\n",
    "Hint:\n",
    "- the BC agent with default hyperparameters should be able to achieve normalized scores >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "ryOjKIFILWm9",
    "outputId": "fc6840b4-7afe-4051-cfdc-dde1f220d392"
   },
   "outputs": [],
   "source": [
    "bc_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Al_z6CZHAZ3g"
   },
   "source": [
    "## Part 3: Advantage-Weighted Actor-Critic (AWAC) (8pts)\n",
    "In this part, we will implement the AWAC algorithm with double critic, which is similar to the technique used in Soft Actor-Critic(SAC) and TD3. The algorithm looks as follow. The AWAC algorithm can be used both as online or offline algorithm, in this exercise we focus only on its offline variant.\n",
    "\n",
    "1. **Initialize:**\n",
    "   - Initialize actor network $\\pi_\\theta(a|s)$ with parameters $\\theta$.\n",
    "   - Initialize two critic networks $Q_{\\phi_1}(s, a)$ and $Q_{\\phi_2}(s, a)$ with parameters $\\phi_1$ and $\\phi_2$.\n",
    "   - Initialize target critic networks $Q_{\\phi'_1}$ and $Q_{\\phi'_2}$ with parameters $\\phi'_1 \\leftarrow \\phi_1$ and $\\phi'_2 \\leftarrow \\phi_2$.\n",
    "   - Initialize replay buffer $\\mathcal{D}$.\n",
    "\n",
    "2. **For each iteration:**\n",
    "   - **For each gradient step:**\n",
    "     - Sample a batch of transitions $(s, a, r, s', d)$ from $\\mathcal{D}$.\n",
    "     - Compute target values using the smaller Q value of the two target critics: $$ y = r + \\gamma (1 - d) \\min_{i=1,2} Q_{\\phi'_i}(s', a') $$\n",
    "     - Update each critic by minimizing the loss: $$ L(\\phi_i) = \\frac{1}{|B|} \\sum_{(s, a, r, s', d) \\in B} (Q_{\\phi_i}(s, a) - y)^2 \\quad \\text{for } i=1,2 $$\n",
    "     - Compute the advantage for a given state action pair (s, a):\n",
    "     \\begin{equation}\n",
    "     A(s,a)=\\min_{i=1,2}Q_{\\phi_i}(s,a) - \\mathbb{E}_{a^\\pi \\sim \\pi_{\\theta}(a|s)}[\\min_{i=1,2}Q_{\\phi_i}(s,a^\\pi)]\n",
    "     \\end{equation}\n",
    "     \n",
    "     **Important Hint** : here we use the expectation of Q function to represent the $V(s)$. In theory, we need multiple samples to get better estimation, however, only **one sample** usually works fine in practice.\n",
    "     - Compute advantage-weighted policy gradient: $$ \\nabla_\\theta J(\\theta) = \\frac{1}{|B|} \\sum_{(s, a) \\in B} \\nabla_\\theta \\log \\pi_\\theta(a|s) \\exp \\left( \\frac{1}{\\lambda} A(s,a) \\right)$$\n",
    "     - Update actor network by gradient ascent: $$ \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta) $$\n",
    "     - Softly update target critic networks: $$ \\phi'_i \\leftarrow \\tau \\phi_i + (1 - \\tau) \\phi'_i \\quad \\text{for } i=1,2 $$\n",
    "\n",
    "3. **Repeat** until convergence or maximum number of iterations reached.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHMqXO-oH57l"
   },
   "outputs": [],
   "source": [
    "class AdvantageWeightActorCritic:\n",
    "    def __init__(\n",
    "        self,\n",
    "        actor: nn.Module,\n",
    "        actor_optimizer: torch.optim.Optimizer,\n",
    "        critic_1: nn.Module,\n",
    "        critic_1_optimizer: torch.optim.Optimizer,\n",
    "        critic_2: nn.Module,\n",
    "        critic_2_optimizer: torch.optim.Optimizer,\n",
    "        gamma: float = 0.99,\n",
    "        tau: float = 0.005,\n",
    "        awac_lambda: float = 1.0,\n",
    "        exp_adv_max: float = 100.0,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the Advantage Weighted Actor-Critic (AWAC) agent.\n",
    "\n",
    "        AWAC combines off-policy actor-critic methods with an advantage-weighted behavioral cloning loss to stabilize\n",
    "        and improve the training of deep reinforcement learning policies.\n",
    "\n",
    "        Parameters:\n",
    "        - actor (nn.Module): The actor network.\n",
    "        - actor_optimizer (torch.optim.Optimizer): Optimizer for the actor network.\n",
    "        - critic_1 (nn.Module): The first critic network.\n",
    "        - critic_1_optimizer (torch.optim.Optimizer): Optimizer for the first critic network.\n",
    "        - critic_2 (nn.Module): The second critic network.\n",
    "        - critic_2_optimizer (torch.optim.Optimizer): Optimizer for the second critic network.\n",
    "        - gamma (float): Discount factor for future rewards. Default is 0.99.\n",
    "        - tau (float): Soft update rate for the target networks. Default is 0.005.\n",
    "        - awac_lambda (float): Scaling factor for advantage weights in the actor loss. Default is 1.0.\n",
    "        - exp_adv_max (float): Maximum limit for exponentiated advantages. Default is 100.0.\n",
    "        - device (str): The device on which to perform computations. Default is 'cpu'.\n",
    "\n",
    "        This implementation uses two critic networks to estimate the value function, along with their target networks,\n",
    "        which are updated using a soft update strategy. The actor's policy is updated using an advantage-weighted\n",
    "        loss to encourage actions that lead to higher returns than currently estimated.\n",
    "        \"\"\"\n",
    "        self._actor = actor\n",
    "        self._actor_optimizer = actor_optimizer\n",
    "\n",
    "        self._critic_1 = critic_1\n",
    "        self._critic_1_optimizer = critic_1_optimizer\n",
    "        self._target_critic_1 = deepcopy(critic_1)\n",
    "\n",
    "        self._critic_2 = critic_2\n",
    "        self._critic_2_optimizer = critic_2_optimizer\n",
    "        self._target_critic_2 = deepcopy(critic_2)\n",
    "\n",
    "        self._gamma = gamma\n",
    "        self._tau = tau\n",
    "        self._awac_lambda = awac_lambda\n",
    "        self._exp_adv_max = exp_adv_max\n",
    "\n",
    "        self._device = device\n",
    "\n",
    "    def _actor_loss(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the loss for the actor network.\n",
    "\n",
    "        The loss is calculated using an advantage-weighted behavioral cloning approach. It encourages the actor to\n",
    "        take actions that lead to higher returns compared to the current policy's estimate.\n",
    "\n",
    "        Parameters:\n",
    "        - states (torch.Tensor): The batch of states.\n",
    "        - actions (torch.Tensor): The batch of actions.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The computed loss for the actor network.\n",
    "\n",
    "        The advantage is calculated as the difference between the Q-values from the critics for the actual\n",
    "        actions and the estimated V-values from the critics for the actions sampled by the actor. These advantages\n",
    "        are then scaled and used to weight the log probabilities of the actions in the actor's policy.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            pi_actions, _ = self._actor(states)\n",
    "\n",
    "            \"\"\"\n",
    "            Calculate advantages and the weights\n",
    "              Hints:\n",
    "                1. V(s) can be represented using expectation of Q(s,a).\n",
    "                   In practice, one sample could be enough\n",
    "                2. self._actor() return action samples from the actor\n",
    "                3. Weights should not contain gradient\n",
    "            \"\"\"\n",
    "\n",
    "            ## TODO ##\n",
    "\n",
    "            ## END ##\n",
    "\n",
    "            weights = torch.clamp(weights, max=self._exp_adv_max)\n",
    "\n",
    "        ### Implement the AWAC loss ###\n",
    "        \"\"\"\n",
    "        Hint: be careful about the sign of loss, we are trying to maximize the\n",
    "        objective.\n",
    "        \"\"\"\n",
    "        ## TODO ##\n",
    "\n",
    "        ## END ##\n",
    "        return loss\n",
    "\n",
    "    def _critic_loss(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"\n",
    "        Computes the loss for the critic networks.\n",
    "\n",
    "        The loss is the Mean Squared Error (MSE) between the critics' Q-value estimates and the target Q-values,\n",
    "        which are computed using the Bellman equation.\n",
    "\n",
    "        Parameters:\n",
    "        - states, actions, rewards, next_states, dones: Tensors representing the batch of transitions.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The computed loss for the critic networks.\n",
    "\n",
    "        The target Q-values are calculated using the minimum Q-value of the two target critics for the next state\n",
    "        and action pair, adjusted by the reward and discount factor.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Hints:\n",
    "          1. next_action could be compute by calling self._actor(next_states)\n",
    "          2. targets should not contain gradient\n",
    "          3. we have two critics, so we need to calculate the loss for both and add them up\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Compute the next actions\n",
    "\n",
    "        # Step 2: Calculate the targets\n",
    "\n",
    "        # Step 3: Compute the critic loss with targets\n",
    "\n",
    "\n",
    "        ## TODO ##\n",
    "\n",
    "        ## END ##\n",
    "\n",
    "        loss = critic_1_loss + critic_2_loss\n",
    "        return loss\n",
    "\n",
    "    def _update_target_network(self, target_network: nn.Module, network: nn.Module):\n",
    "        \"\"\"\n",
    "        Performs a soft update on the target network.\n",
    "\n",
    "        Parameters:\n",
    "        - target_network (nn.Module): The target network to be updated.\n",
    "        - network (nn.Module): The current network whose parameters are used for the update.\n",
    "\n",
    "        This method updates the target network's parameters by blending them with the parameters from the\n",
    "        current network, controlled by the tau parameter. This soft update helps in stabilizing training by\n",
    "        providing a slowly changing target for the critics.\n",
    "        \"\"\"\n",
    "        for target_param, param in zip(target_network.parameters(), network.parameters()):\n",
    "            target_param.data.copy_(self._tau * param.data + (1.0 - self._tau) * target_param.data)\n",
    "\n",
    "    def _update_actor(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Updates the actor network by minimizing its loss.\n",
    "\n",
    "        Parameters:\n",
    "        - states (torch.Tensor): The batch of states.\n",
    "        - actions (torch.Tensor): The batch of actions.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The actor loss after the update.\n",
    "\n",
    "        This method performs a gradient descent step to update the actor's parameters in order to minimize\n",
    "        the advantage-weighted loss.\n",
    "        \"\"\"\n",
    "        self._actor_optimizer.zero_grad()\n",
    "        loss = self._actor_loss(states, actions)\n",
    "        loss.backward()\n",
    "        self._actor_optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def _update_critics(\n",
    "        self,\n",
    "        states: torch.Tensor,\n",
    "        actions: torch.Tensor,\n",
    "        rewards: torch.Tensor,\n",
    "        next_states: torch.Tensor,\n",
    "        dones: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Updates the critic networks by minimizing their loss.\n",
    "\n",
    "        Parameters:\n",
    "        - states, actions, rewards, next_states, dones: Tensors representing the batch of transitions.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The critic loss after the update.\n",
    "\n",
    "        This method performs a gradient descent step to update the critic networks' parameters in order to minimize\n",
    "        the MSE loss between their Q-value estimates and the target Q-values.\n",
    "        \"\"\"\n",
    "        self._critic_1_optimizer.zero_grad()\n",
    "        self._critic_2_optimizer.zero_grad()\n",
    "        loss = self._critic_loss(states, actions, rewards, next_states, dones)\n",
    "        loss.backward()\n",
    "        self._critic_1_optimizer.step()\n",
    "        self._critic_2_optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        replay_buffer: ReplayBuffer,\n",
    "        batch_size: int,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Performs a training update on both the actor and critic networks.\n",
    "\n",
    "        Parameters:\n",
    "        - replay_buffer (ReplayBuffer): The replay buffer to sample transitions from.\n",
    "        - batch_size (int): The number of transitions to sample for the update.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple[torch.Tensor, torch.Tensor]: The actor and critic losses after the update.\n",
    "\n",
    "        This method samples a batch of transitions from the replay buffer and uses them to update both the actor\n",
    "        and critic networks. It also updates the target critic networks using the soft update method.\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "        critic_loss = self._update_critics(states, actions, rewards, next_states, dones)\n",
    "        actor_loss = self._update_actor(states, actions)\n",
    "        self._update_target_network(self._target_critic_1, self._critic_1)\n",
    "        self._update_target_network(self._target_critic_2, self._critic_2)\n",
    "        return actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yMi31FAIAD2"
   },
   "outputs": [],
   "source": [
    "def awac_train():\n",
    "\n",
    "    # Environment setup and seeding\n",
    "    env = gym.make(ENV_NAME)\n",
    "    set_seed(SEED, env)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    dataset = d4rl.qlearning_dataset(env)\n",
    "\n",
    "    state_mean = dataset['observations'].mean(axis=0)\n",
    "    state_std = dataset['observations'].std(axis=0)\n",
    "\n",
    "    dataset['observations'] = (dataset['observations'] - state_mean) / state_std\n",
    "    dataset['next_observations'] = (dataset['next_observations'] - state_mean) / state_std\n",
    "\n",
    "    env = wrap_env(env, state_mean=state_mean, state_std=state_std)\n",
    "\n",
    "    # Initialize replay buffer\n",
    "    replay_buffer = ReplayBuffer(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        buffer_size=2_000_000,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    replay_buffer.load_d4rl_dataset(dataset)\n",
    "\n",
    "    actor_critic_kwargs = {\n",
    "        \"state_dim\": state_dim,\n",
    "        \"action_dim\": action_dim,\n",
    "        \"hidden_dim\": 256,\n",
    "    }\n",
    "\n",
    "    actor = Actor(**actor_critic_kwargs)\n",
    "    actor.to(DEVICE)\n",
    "    actor_optimizer = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
    "\n",
    "    critic_1 = Critic(**actor_critic_kwargs)\n",
    "    critic_1.to(DEVICE)\n",
    "    critic_1_optimizer = torch.optim.Adam(critic_1.parameters(), lr=3e-4)\n",
    "\n",
    "    critic_2 = Critic(**actor_critic_kwargs)\n",
    "    critic_2.to(DEVICE)\n",
    "    critic_2_optimizer = torch.optim.Adam(critic_2.parameters(), lr=3e-4)\n",
    "\n",
    "    awac = AdvantageWeightActorCritic(\n",
    "        actor=actor,\n",
    "        actor_optimizer=actor_optimizer,\n",
    "        critic_1=critic_1,\n",
    "        critic_1_optimizer=critic_1_optimizer,\n",
    "        critic_2=critic_2,\n",
    "        critic_2_optimizer=critic_2_optimizer,\n",
    "        gamma=0.99,\n",
    "        tau=0.005,\n",
    "        awac_lambda=1.0,\n",
    "        exp_adv_max=100.0,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    eval_score_list = []\n",
    "    iteration_list = []\n",
    "    for t in trange(60000, ncols=100):\n",
    "        actor_loss, critic_loss = awac.update(replay_buffer, batch_size=1024)\n",
    "        if (t + 1) % 1000 == 0:\n",
    "            eval_scores = evaluate_policy(env, actor, num_episodes=10)\n",
    "            normalized_eval_scores = env.get_normalized_score(eval_scores)\n",
    "            print(f\"Eval_scores: {eval_scores.mean():.2f} +/- {eval_scores.std():.2f}\")\n",
    "            print(f\"Normalized eval_scores: {normalized_eval_scores.mean():.2f} +/- {normalized_eval_scores.std():.2f}\")\n",
    "            print(f\"Actor loss: {actor_loss:.4f}, critic loss: {critic_loss:.4f}\")\n",
    "            eval_score_list.append(normalized_eval_scores.mean())\n",
    "            iteration_list.append(t+1)\n",
    "            update_plot(iteration_list, eval_score_list, 'AWAC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1F44xgiJzPJq"
   },
   "source": [
    "**Run the following block to train the awac agent!**\n",
    "\n",
    "Hints:\n",
    "- the AWAC agent with default hyperparameters should be able to achieve normalized scores >= 0.8\n",
    "- There could be a drop of performance at the beginning of training. You should be able to observe significant performance improvement after 20000 iterations.\n",
    "- It takes 10~12 minutes to finish training on T4-GPU runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "-Of4WHk_INYA",
    "outputId": "fd2a2d07-a014-4168-c872-ad603408cafe"
   },
   "outputs": [],
   "source": [
    "awac_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXOtT7AK8utY"
   },
   "source": [
    "## Self-test questions (optional)\n",
    "Why AWAC can achieve better performance than BC?\n",
    "\n",
    ".## TODO ##\n",
    "\n",
    "What are the challenges of applying offline RL in the real world? ï¼ˆOpen-ended)\n",
    "\n",
    ".## TODO ##"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
