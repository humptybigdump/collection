{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298c9604",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6553bce0f133730e38895f7a5000d0c5",
     "grade": false,
     "grade_id": "cell-b6f4b35e0e97b36d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Grundlagen der Künstlichen Intelligenz - Wintersemester 2024/25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f241d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "970475cffb22a4d6bd4b434ddabea4e4",
     "grade": false,
     "grade_id": "cell-9a8d7a352449aaa7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Übung 3: Neuronale Netzwerke und Backpropagation\n",
    "\n",
    "---\n",
    "\n",
    "> 'Grundlagen der künstlichen Intelligenz' im Wintersemester 2024/2025\n",
    ">\n",
    "> - T.T.-Prof. Benjamin Schäfer, benjamin.schaefer@kit.edu\n",
    "> - Prof. Gerhard Neumann, gerhard.neumann@kit.edu\n",
    "\n",
    "---\n",
    "\n",
    "### Übungsteam\n",
    "\n",
    "- Sebastian Pütz, sebastian.puetz@kit.edu\n",
    "- Ulrich Oberhofer, ulrich.oberhofer@kit.edu\n",
    "- Philipp Dahlinger, philipp.dahlinger@kit.edu\n",
    "- Nicolas Schreiber, nicolas.schreiber@kit.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d511317",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73ce18950a9553ef7306776da6baaba8",
     "grade": false,
     "grade_id": "cell-9cfa94b8c00d9bb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Gruppenabgabe\n",
    "\n",
    "Die Übungsblätter können in Gruppen von bis zu **3 Studierenden** abgegeben werden. **Jede Person aus der Gruppe muss die finale Version der Abgabe über Ilias hochladen**, es genügt nicht, dass nur eine Person aus der Gruppe dies tut. Es ist prinzipiell möglich, im Laufe des Semesters sich einer neuen Gruppe anzuschließen, sollte sich die eigene Gruppe vorzeitig auflösen. Generell muss jede Gruppe ihre eigene Lösung hochladen, wir werden die Abgaben auf Duplikate überprüfen.\n",
    "\n",
    "Die Gruppen werden automatisch erfasst, **gebt deshalb die u-Kürzel eurer Gruppenmitglieder in die folgende Zelle ein.** Falls eure Gruppe nur aus 2 Studierenden besteht, oder ihr alleine abgibt, lasst die verbleibenden Felder frei. Hier ein Beispiel für eine Gruppe bestehend aus uabcd und uefgh:\n",
    "\n",
    "_U-Kürzel der Gruppenmitglieder:_\n",
    "\n",
    "_Mitglied 1: uabcd_\n",
    "\n",
    "_Mitglied 2: uefgh_\n",
    "\n",
    "_Mitglied 3:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e95584",
   "metadata": {},
   "source": [
    "U-Kürzel der Gruppenmitglieder:\n",
    "\n",
    "Mitglied 1:\n",
    "\n",
    "Mitglied 2:\n",
    "\n",
    "Mitglied 3:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb46b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f41124650ee96a7102c789946d280b14",
     "grade": false,
     "grade_id": "cell-7a83b59c56b8a5df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Auto-grading\n",
    "\n",
    "Wir nutzen ein auto-grading System, welches eure abgegebenen Jupyter Notebooks automatisch analysiert und über\n",
    "hidden Tests auf Richtigkeit prüft. Über diese Tests werden die Punkte bestimmt, die ihr für das Übungsblatt erhaltet.\n",
    "\n",
    "Damit das auto-grading reibungslos funktioniert bitte folgende Dinge beachten:\n",
    "\n",
    "- Vor dem Abgeben eines Notebooks bitte testen, dass alles von vorne bis hinten ohne Fehler durchläuft.\n",
    "- Zellen, welche mit \"### DO NOT CHANGE ###\" markiert sind dürfen weder gelöscht noch bearbeitet werden\n",
    "- Eure Lösung muss in die richtige Zelle (markiert mit \"# YOUR CODE HERE\") eingetragen werden.\n",
    "    - (dabei natürlich den NotImplementedError löschen!)\n",
    "- Es gibt potentiell scheinbar leere Zellen, die auch mit \"### DO NOT CHANGE ###\" markiert sind. Auch diese dürfen nicht bearbeitet oder gelöscht werden.\n",
    "    - Falls dies doch gemacht wird, dann wird das automatische Grading nicht funktionieren und ihr erhaltet keine Punkte.\n",
    "    - Wir werden hier strikt handeln und keine Ausnahmen machen, falls jemand doch Zellen verändert, die eindeutig als readonly markiert sind!\n",
    "- Die Jupyter Notebooks haben inline Tests (für euch sichtbar), welche euer Ergebnis auf grobe Richtigkeit überprüfen.\n",
    "    - Diese sind primär für euch, um Fehler zu erkennen und zu korrigieren.\n",
    "    - Die inline Tests, die ihr im Notebook sehen könnt, sind allerdings nicht die Tests welche für das Grading verwendet werden!\n",
    "    - Die inline Tests sind eine notwendige Bedingung, um beim Grading der Aufgabe Punkte zu erhalten!\n",
    "\n",
    "# **WICHTIG** Abgabe des Notebooks\n",
    "- Bitte das Jupyter Notebook mit dem ursprünglichen Dateinamen ins Ilias hochladen (\"ex_03_neural_networks.ipynb\")\n",
    "- Bitte Jupyter Notebook und handgeschriebene PDF einzeln hochladen, nicht als ZIP.\n",
    "- Bitte darauf achten, dass die Jupyter Notebook Zell-Metadaten erhalten bleiben. Das ist eigentlich immer der Fall,\n",
    "in wenigen Fällen gab es hier jedoch Probleme. Um auf Nummer Sicher zu gehen bitte das Notebook vor der Abgabe ein Mal\n",
    "in einem normalen Texteditor öffnen und nach \"nbgrader\" suchen. Wenn hier dann keine entsprechenden JSON-Einträge auftauchen\n",
    "dann sind leider die Metadaten verloren gegangen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2699e9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "522619e05dc9c3f78b2e933a15a49298",
     "grade": false,
     "grade_id": "cell-c1eb32bbbc84bf86",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "import os\n",
    "import gzip\n",
    "from typing import Tuple, List\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485f1147",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc672c69eaf86e24a558baa59b564996",
     "grade": false,
     "grade_id": "cell-1713fa78a1e0be9c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In dieser Übung werden wir ein einfaches, vollständig verbundenes neuronales Netz (fully-connected neural network) mit Hilfe von NumPy selbst implementieren. Der Fokus liegt dabei auf der Implementierung der Backpropagation, einem zentralen Bestandteil des Trainings neuronaler Netze.\n",
    "\n",
    "Anschließend wirst du dein Netzwerk auf den MNIST-Datensatz anwenden. Dieser Datensatz besteht aus 28x28 Pixel großen Graustufenbildern handgeschriebener Ziffern, die jeweils mit einem passenden Label versehen sind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e53a902",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2119603b545c8123500a32b79f87d37c",
     "grade": false,
     "grade_id": "cell-b2fb71caa65ef88a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Zunächst laden wir den MNIST-Datensatz herunter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc62bcf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f148b5e10a720d8c9b7fbdb1e8e0651",
     "grade": false,
     "grade_id": "cell-e625d1b00d63e31e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# define utility functions to load the mnist dataset\n",
    "\n",
    "def load_mnist_images(filename: str) -> np.ndarray:\n",
    "    \"\"\"Load mnist images from the given gzip file.\"\"\"\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        data = data.reshape(-1, 28, 28)\n",
    "    return data\n",
    "\n",
    "def load_mnist_labels(filename: str) -> np.ndarray:\n",
    "    \"\"\"Load mnist labels from the given gzip file.\"\"\"\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    return data\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce7689f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53f67ee4ef6fb1c62d511182a3ace09d",
     "grade": false,
     "grade_id": "cell-ab129f96854d7d8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# Ensure MNIST dataset is downloaded\n",
    "data = torchvision.datasets.MNIST('./', download=True)\n",
    "\n",
    "# Define file paths for MNIST dataset\n",
    "files = {\n",
    "    \"train_images\": \"MNIST/raw/train-images-idx3-ubyte.gz\",\n",
    "    \"train_labels\": \"MNIST/raw/train-labels-idx1-ubyte.gz\",\n",
    "    \"test_images\": \"MNIST/raw/t10k-images-idx3-ubyte.gz\",\n",
    "    \"test_labels\": \"MNIST/raw/t10k-labels-idx1-ubyte.gz\"\n",
    "}\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_images = load_mnist_images(files[\"train_images\"]).reshape(-1, 28 * 28)  # Flatten images\n",
    "train_labels = load_mnist_labels(files[\"train_labels\"])\n",
    "test_images = load_mnist_images(files[\"test_images\"]).reshape(-1, 28 * 28)  # Flatten images\n",
    "test_labels = load_mnist_labels(files[\"test_labels\"])\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = np.eye(10)[train_labels]\n",
    "test_labels = np.eye(10)[test_labels]\n",
    "\n",
    "# Display dataset shapes\n",
    "print(f\"Training images shape: {train_images.shape}\")\n",
    "print(f\"Training labels shape: {train_labels.shape}\")\n",
    "print(f\"Testing images shape: {test_images.shape}\")\n",
    "print(f\"Testing labels shape: {test_labels.shape}\")\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c5db1a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28e9e4ba0a6dd3da86a0decc15d766fa",
     "grade": false,
     "grade_id": "cell-d3b3b67f474b5634",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Als nächstes lassen wir für jedes Label ein Beispiel aus dem Datensatz anzeigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525daa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert images back to 28x28 shape for displaying\n",
    "train_images_display = train_images.reshape(-1, 28, 28)\n",
    "\n",
    "# Find one example for each label\n",
    "unique_labels = np.unique(np.argmax(train_labels, axis=1)) # np.unique returns sorted array\n",
    "n_labels = len(unique_labels)\n",
    "\n",
    "# Plot one example for each label\n",
    "fig, axes = plt.subplots(1, n_labels, figsize=(n_labels * 2, 2))\n",
    "for ax, label in zip(axes, unique_labels):\n",
    "    idx = np.argmax(train_labels, axis=1) == label # Find the images with the current label\n",
    "    ax.imshow(train_images_display[idx][0], cmap='gray') # Plot the first matching image\n",
    "    ax.set_title(f\"Label: {label}\") # Add title with the label\n",
    "    ax.axis('off') # Turn off axis to only show the images\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be7e18",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19e4b55326f8aceed02c85a25489e6c2",
     "grade": false,
     "grade_id": "cell-c50ced238ea362d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Es ist üblich, die Ipnput-daten von neuronalen Netzen zu skalieren. Dies sorgt dafür, dass die verschiedenen Features auf derselben Skala liegen, was die Konvergenz des Trainings verbessert (Stichwort: ill-conditioned Hessian).\n",
    "\n",
    "Eine häufig genutzte Methode ist die Standardisierung, bei der für jedes Merkmal der Mittelwert subtrahiert und durch die Standardabweichung geteilt wird (“standard scaler”). Da wir hier jedoch mit 2D-Bildern arbeiten, ist es wenig sinnvoll, jedes Feature/jeden Pixel separat zu normalisieren. Stattdessen skalieren wir die Werte, indem wir sie einfach durch 255.0 teilen (Der Datensatz enthält 8-bit Bilder: 0=schwarz und 255=weiß). So liegen alle Pixelwerte im Bereich $[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0cf02e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08ef10ff573a89a31c14ba02daf98173",
     "grade": false,
     "grade_id": "cell-f0dacbc1ad4ebd95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# Scale images to [0, 1] range\n",
    "train_images = train_images / 255.0 \n",
    "test_images = test_images / 255.0   \n",
    "\n",
    "# Confirm scaling\n",
    "print(f\"Scaled training images range: [{train_images.min()}, {train_images.max()}]\")\n",
    "print(f\"Scaled testing images range: [{test_images.min()}, {test_images.max()}]\")\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7959c34",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ca11617a993010d6f44b13deaa8b951",
     "grade": false,
     "grade_id": "cell-52c466f86b208764",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Aufgabe 2.1 - Implementierung des DataLoaders (1 Punkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff8803",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ea0f777e66eb04cb0e7d486fe02c87c",
     "grade": false,
     "grade_id": "cell-5910c689218f3431",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Nun implementieren wir einen sogenannten `DataLoader`. Dieser hat die Aufgabe, unsere NumPy-Arrays in Batches für das Training aufzuteilen.\n",
    "Später werden wir sehen, dass wir über unseren `DataLoader`einfach iterieren können, um in jeder Iteration eine neue Batch von Daten und den zugehörigen Labels zu erhalten.\n",
    "\n",
    "Vervollständigt den folgenden DataLoader an der markierten Stelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737909c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a859cc1cb28720a4d69c20982dab6c24",
     "grade": false,
     "grade_id": "cell-b0373287eb18824f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    A DataLoader-like object for iterating over arrays and returning batches of data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            *arrays, \n",
    "            batch_size=32, \n",
    "            shuffle=False,\n",
    "            drop_last=True\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize a DataLoader.\n",
    "\n",
    "        *arrays (np.ndarrays): Arrays to store. Must have the same length @ dim 0.\n",
    "        batch_size (int): Batch size to use when iterating\n",
    "        shuffle (bool): If True, shuffle the data *in-place* whenever an\n",
    "            iterator is created out of this object.\n",
    "        drop_last (bool): If True, drop the last incomplete batch if the dataset size is not \n",
    "            divisible by the batch size. If False, the last batch will include remaining samples.\n",
    "        \"\"\"\n",
    "\n",
    "        # Ensure all arrays have the same number of samples\n",
    "        if not all(array.shape[0] == arrays[0].shape[0] for array in arrays):\n",
    "            raise ValueError(\"All input arrays must have the same number of samples (size along dimension 0).\")\n",
    "\n",
    "        self.arrays = arrays\n",
    "        self.dataset_len = self.arrays[0].shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        # Calculate the number of batches\n",
    "        self.n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n",
    "        if not self.drop_last and remainder > 0:\n",
    "            self.n_batches += 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Create an iterator for the DataLoader.\n",
    "        If shuffle is True, shuffles the data before returning the iterator.\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            indices = np.random.permutation(self.dataset_len)\n",
    "            self.arrays = tuple(array[indices] for array in self.arrays)\n",
    "        self.i = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "        Return the next batch of data.\n",
    "        \"\"\"\n",
    "        if self.i >= self.dataset_len or (\n",
    "            self.drop_last and (self.dataset_len - self.i) < self.batch_size\n",
    "        ):\n",
    "            raise StopIteration\n",
    "        \n",
    "        # Hier wollen wir ein Tupel zurückgeben, wobei jeder Eintrag eine Batch des entsprechenden Arrays in self.arrays ist.\n",
    "        # Wenn wir beispielsweise \"x\" und \"y\" als Arrays an unseren DataLoader im Konstruktor übergeben haben,\n",
    "        # soll das Tupel am Ende eine Batch (basierend auf der aktuellen Position self.i) von x und eine Batch von y enthalten.\n",
    "        # Vergiss am Ende nicht, self.i zu updaten.\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of batches.\n",
    "        \"\"\"\n",
    "        return self.n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e161a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b81eb2ebe3e2c759876d6ea83ce0420",
     "grade": true,
     "grade_id": "ex_1_data_loader",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: ex_1_data_loader - possible points: 1\n",
    "\n",
    "# Self-check\n",
    "train_dataloader = DataLoader(\n",
    "    np.array([5.0, 3.0, 2.0, 1.0, 5.0]), \n",
    "    np.array([4.0, 5.0, 2.0, 3.0, 4.0]), \n",
    "    batch_size=1, \n",
    "    shuffle=False, \n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# Check number of batches\n",
    "assert len(train_dataloader) == 5, \"Number of batches should be 5\"\n",
    "# Check the first batch\n",
    "assert np.all(np.isclose(\n",
    "    next(iter(train_dataloader)), \n",
    "    (np.array([5.0]), np.array([4.0]))\n",
    ")), \"First batch does not match expected output\"\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8853610b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dac5665ff645e797cb21f9dd6aa1adf0",
     "grade": false,
     "grade_id": "cell-06f589226be84411",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Aufgabe 2.2 - Softmax und Ableitungen für die Gradientenberechnung (1+1+1 Punkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6bc633",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b3ab400dd2f89d1486d4e491af2ce43",
     "grade": false,
     "grade_id": "cell-16a7af6aa69bfcb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Als nächstes müssen wir die Backpropagation (und den forward pass) implementieren.\n",
    "Dazu benötigen wir zunächst etwas Vorarbeit:\n",
    "\n",
    "Vervollständige in der folgenden Zelle die letzten drei Funktionen. Nutze dafür die Ergebnisse vom handschriftlichen Teil dieser Übung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c45d6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "502503686bc000413c2b3b72d860caf0",
     "grade": false,
     "grade_id": "cell-bc6d810895b868df",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the sigmoid activation function.\n",
    "    Args:\n",
    "        z: Input array with arbitrary dimensions.\n",
    "    Returns:\n",
    "        The sigmoid function applied to each entry in the input array.\n",
    "        Returns an array with the same dimensions as the input.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the softmax activation function.\n",
    "    Args:\n",
    "        z: Input array with dimensions (Number of classes, Number of samples in batch).\n",
    "    Returns:\n",
    "        The softmax of the input with dimensions (Number of classes, Number of samples in batch).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def sigmoid_derivative(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the derivative of the sigmoid activation function (see exercise sheet equation 4).\n",
    "    Args:\n",
    "        z: Input array with arbitrary dimensions.\n",
    "    Returns:\n",
    "        The derivative of the sigmoid function for each entry in the input array.\n",
    "        Returns an array with the same dimensions as the input.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def dL_dz(a: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the derivative of the loss with respect to z for a sample y.\n",
    "    This represents the \"delta\" of the last layer in backpropagation (see exercise sheet equation 9).\n",
    "    Args:\n",
    "        a (np.ndarray): Predicted output (activation of the last layer) usually of shape (Number of samples in batch, Number of classes).\n",
    "        y (np.ndarray): True label for the sample usually of shape (Number of samples in batch, Number of classes).\n",
    "    Returns:\n",
    "        Derivative of the loss with respect to z of same shape as a and y.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7cc9e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b8292d22c06c96086ebdad1b5a84601",
     "grade": false,
     "grade_id": "cell-d0daba15f0cf4c1e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# Self-check\n",
    "assert np.all(np.isclose(np.sum(softmax(np.array([[6, 5, 4], [3, 2, 1]])), axis=0), np.array([1.0, 1.0, 1.0]))), \"Softmax does not sum to 1 for each sample\"\n",
    "assert np.all(np.isclose(sigmoid_derivative(np.array([0, 0])), np.array([0.25, 0.25]))), \"Sigmoid derivative is not correct for z=0\"\n",
    "assert np.all(np.isclose(dL_dz(np.array([[0.5, 0.5]]), np.array([[1, 0]])), np.array([[-0.5, 0.5]]))), \"dL_dz is not correct\"\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed76ac13",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a74e4adb22400921c0fad8bdd1ab7448",
     "grade": true,
     "grade_id": "ex_2_1_softmax",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4684f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0fcd025479b2e668dc97cc2d2999e82b",
     "grade": true,
     "grade_id": "ex_2_2_sg_derivate",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12095b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8738e9cd3e5017c8ed4550ef4adb41cb",
     "grade": true,
     "grade_id": "ex_2_3_dl_dz",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce41d24a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f958c4d0e3dc5af3c393f209ba5f2e5",
     "grade": false,
     "grade_id": "cell-44508bfad8b66a01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Aufgabe 2.3 - Cross-Entropy Loss und Accuracy (1+1 Punkte)\n",
    "\n",
    "Außerdem benötigen wir Funktionen, um die Accuracy und den Loss zu berechnen. Implementiert dafür die beiden Funktionen `loss` und `accuracy` in der folgenden `Network` Klasse. Die Funktionen sollen den mean cross-entropy loss und die mean accuracy gemittelt über alle Samples in der Batch zurückgeben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a321b077",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00dfe3d722f7f81fea6429fb5679c907",
     "grade": false,
     "grade_id": "cell-ac3a36e391e47af3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Aufgabe 2.4 - Implementierung des forward und backward pass (2+3 Punkte)\n",
    "\n",
    "Nun implementieren wir den forward pass und die backpropagation. Hierbei helfen wieder die Formeln, welche wir im schriftlichen Teil der Übung hergeleitet haben.\n",
    "Implementiert den Code für den forward pass und den backward pass in der folgenden Klasse (siehe die markierten Stellen)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe63376",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3971b2804bdcd860b74807e6bc9cbdce",
     "grade": false,
     "grade_id": "cell-7aecc4299a5e5162",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Bei der Implementierung ist es sehr wichtig, die Dimensionen der Arrays in den verschidenen Schritten zu berücksichtigen. Um den Überblick zu behalten und Fehler zu vermeiden, haben wir die Dimensionen der Arrays in den Kommentaren notiert. Hierbei wurden die folgenden Variablennamen verwendet:\n",
    "\n",
    "- `N` - Anzahl der Samples in der Batch\n",
    "- `C` - Anzahl der Klassen (10 für den MNIST-Datensatz)\n",
    "- `D` - Anzahl der Input-Features (Anzahl der Pixel in einem Bild; 28x28=784 für den MNIST-Datensatz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2437f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2d82c1d3dea430e98005aa8b8ed8e09",
     "grade": false,
     "grade_id": "cell-7f8da47c9e67bf98",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"\n",
    "    A simple feedforward neural network implementation in NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self, structure: List[int], batch_size: int, learning_rate: float):\n",
    "        \"\"\"\n",
    "        Initialize the network.\n",
    "\n",
    "        Args:\n",
    "            structure (List[int]): List of layer sizes.\n",
    "            batch_size (int): Batch size for training.\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "        \"\"\"\n",
    "        self.structure = structure\n",
    "        self.num_layers = len(structure) \n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.Ws = [np.random.randn(prev_l, l) for prev_l, l in zip(structure[:-1], structure[1:])] # shape: (Neurons in previous layer, Neurons in current layer)\n",
    "        self.Bs = [np.random.randn(l, 1) for l in structure[1:]] # shape: (Neurons in current layer, 1)\n",
    "\n",
    "        # Initialize gradients\n",
    "        self.dL_dWs = [np.zeros_like(W) for W in self.Ws] # Gradient of loss with respect to weights (same shape as weights)\n",
    "        self.dL_dBs = [np.zeros_like(B) for B in self.Bs] # Gradient of loss with respect to biases (same shape as biases)\n",
    "\n",
    "        # Initialize intermediate values for forward/backward passes\n",
    "        self.Zs = [np.zeros((layer_size, batch_size)) for layer_size in structure[1:]]  # Exclude input layer\n",
    "        self.As = [np.zeros((layer_size, batch_size)) for layer_size in structure]  # Include input layer to include input data as activation (As[0])\n",
    "\n",
    "    def loss(self, y: np.ndarray, y_predicted: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Computes the categorical cross-entropy loss.\n",
    "\n",
    "        Args:\n",
    "            y (np.ndarray): One-hot encoded true labels of shape (N,C).\n",
    "            y_predicted (np.ndarray): Predicted probabilities of shape (N,C).\n",
    "\n",
    "        Returns:\n",
    "            float: Loss value.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def accuracy(self, y: np.ndarray, y_predicted: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Computes the accuracy.\n",
    "\n",
    "        Args:\n",
    "            y (np.ndarray): One-hot encoded true labels of shape (N,C).\n",
    "            y_predicted (np.ndarray): Predicted probabilities of shape (N,C).\n",
    "\n",
    "        Returns:\n",
    "            float: Accuracy value.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Input data of shape (N, D).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Output predictions of shape (N, C).\n",
    "        \"\"\"\n",
    "        x = x.T  # Transpose for matrix multiplication (N,D) -> (D,N)\n",
    "\n",
    "        self.As[0] = x  # The first \"activation\" is just the input\n",
    "\n",
    "        for i, (W, B) in enumerate(zip(self.Ws, self.Bs)):\n",
    "\n",
    "            current_input = self.As[i]  # Use stored activations for input to the next layer\n",
    "\n",
    "            # Berechne hier die Werte für z und a des aktuellen Layers:\n",
    "            # 1. Berechne z, den gewichteten Summenvektor für das aktuelle Layer, basierend auf den Gewichten W,\n",
    "            #    den Biases B und den Eingaben (current_input).\n",
    "            # 2. Bestimme die Aktivierung a des aktuellen Layers:\n",
    "            #    - Verwende die Sigmoid-Aktivierungsfunktion für alle versteckten Layer.\n",
    "            #    - Verwende die Softmax-Aktivierungsfunktion für das letzte Layer. \n",
    "            #    - Nutze die Sigmoid und Softmax-funktionen aus Aufgabe 2.2\n",
    "            # \n",
    "            # Stelle sicher, dass die Shapes der Variablen z und a korrekt sind:\n",
    "            #    - z sollte die Shape (Neuronen im aktuellen Layer, N) haben\n",
    "            #    - a sollte dieselbe Shape wie z haben\n",
    "\n",
    "\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            self.Zs[i] = z\n",
    "            self.As[i + 1] = a  # Store activations for the next layer\n",
    "\n",
    "        return a.T # Transpose back to (N,C)\n",
    "\n",
    "    def forward_backward(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform a forward and backward pass for a batch of samples.\n",
    "        After calling this function, self.dL_dWs will contain the gradient of the loss function with respect \n",
    "        to the weights of each layer, averaged over the batch\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Input data of shape (N, D).\n",
    "            y (np.ndarray): One-hot encoded labels of shape (N, C).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Output predictions of shape (N, C).\n",
    "        \"\"\"\n",
    "\n",
    "        #####  Forward pass #####\n",
    "\n",
    "        # In the forward pass, we have to store the intermediate values for all layers and all samples,\n",
    "        # in order to be able to reuse them during the backward pass.\n",
    "\n",
    "        y_predicted = self.forward(x)\n",
    "\n",
    "        #####  Backward pass #####\n",
    "        x = x.T # (N, D) -> (D, N) for easier matrix multiplication\n",
    "        y = y.T # (N, C) -> (C, N) for easier matrix multiplication\n",
    "        H = self.num_layers - 1\n",
    "\n",
    "        for L in range(H, 0, -1): # Iterate over all layers, starting from the last layer (H) down to the first layer (1)\n",
    "\n",
    "            # Da die Indizes in Python bei 0 beginnen, müssen wir darauf achten, dass wir die Indizes für die Layer richtig verwenden.\n",
    "            # Für self.As stimmt der Index mit dem Layer überein, d.h. self.As[0] enthält die Eingabe, self.As[1] die Aktivierung von Layer 1, etc.\n",
    "            # Für self.Ws, self.Bs und self.Zs haben wir jedoch den Index um 1 verschoben, d.h. self.Zs enthält den Pre-Aktivierungswert von Layer L in self.Zs[L - 1].\n",
    "            # Das gleiche gilt für self.dL_dWs und self.dL_dBs, die den Gradienten bezüglich der Gewichte und Biases enthalten und in dieser Funktion berechnet werden. \n",
    "            #\n",
    "            # 1. Berechne delta für das aktuelle Layer (siehe Übungsblatt):\n",
    "            #    - Im letzten Layer (L == H) berechnest du delta mithilfe von dL_dz(), welches die Differenz \n",
    "            #      zwischen der vorhergesagten Ausgabe (self.As[L]) und den Labels y liefert.\n",
    "            #    - Für alle anderen Layer berechnest du delta mithilfe von sigmoid_derivative(self.Zs[L - 1])\n",
    "            #      und der Matrixmultiplikation mit den Gewichten des nächsten Layers (self.Ws[L])\n",
    "            # \n",
    "            # 2. Aktualisiere die Gradienten:\n",
    "            #    - self.dL_dWs[L - 1]: Der Gradient bezüglich der Gewichte im aktuellen Layer.\n",
    "            #      - Berechne den Gradient, indem du die letzten Aktivierungen (self.As[L - 1]) mit delta multiplizierst (siehe Übungsblatt Gleichung 7)\n",
    "            #      - Mittel die Ergebnisse über die Batch-Dimension\n",
    "            #    - self.dL_dBs[L - 1]: Der Gradient bezüglich der Biases im aktuellen Layer (siehe Übungsblatt Gleichung 8)\n",
    "            #      - Berechne den Gradient als Mittelwert von delta über die Batch-Dimension\n",
    "            # \n",
    "            # Hinweise:\n",
    "            # - Verwende die gespeicherten Werte in self.As und self.Zs für die Aktivierungen und Pre-Aktivierungen.\n",
    "            # - delta kann jeweils überschrieben werden\n",
    "            # - Achte darauf, dass delta, self.dL_dWs[L - 1] und self.dL_dBs[L - 1] die korrekten Shapes haben:\n",
    "            #   - delta hat die Shape (Neuronen im aktuellen Layer, N)\n",
    "            #   - self.dL_dWs[L - 1] hat die Shape (Neuronen im vorherigen Layer, Neuronen im aktuellen Layer)\n",
    "            #   - self.dL_dBs[L - 1] hat die Shape (Neuronen im aktuellen Layer, 1)\n",
    "\n",
    "\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        return y_predicted\n",
    "\n",
    "\n",
    "    def gradient_descent(self, x: np.ndarray, y: np.ndarray) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Perform a gradient descent step.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Input data of shape (N, D).\n",
    "            y (np.ndarray): One-hot encoded labels of shape (N, C).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float]: Loss and accuracy for the batch.\n",
    "        \"\"\"\n",
    "\n",
    "        y_predicted = self.forward_backward(x, y) # Process batch\n",
    "        loss = self.loss(y, y_predicted)\n",
    "        accuracy = self.accuracy(y, y_predicted)\n",
    "\n",
    "        # Update weights and biases\n",
    "        for i in range(len(self.Ws)):\n",
    "            self.Ws[i] -= self.learning_rate * self.dL_dWs[i]\n",
    "            self.Bs[i] -= self.learning_rate * self.dL_dBs[i]\n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "    def test(self, test_dataloader) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the network on a test dataset.\n",
    "\n",
    "        Args:\n",
    "            test_dataloader: DataLoader object for the test dataset.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float]: Average loss and accuracy on the test dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "\n",
    "        for x, y in test_dataloader:\n",
    "            y_predicted = self.forward(x)\n",
    "            total_loss += self.loss(y, y_predicted)\n",
    "            total_accuracy += self.accuracy(y, y_predicted)\n",
    "\n",
    "        avg_loss = total_loss / len(test_dataloader)\n",
    "        avg_accuracy = total_accuracy / len(test_dataloader)\n",
    "\n",
    "        return avg_loss, avg_accuracy\n",
    "\n",
    "    def fit(self, train_dataloader, test_dataloader, epochs: int, test_loss_frequency: int = 1):\n",
    "        \"\"\"\n",
    "        Train the network.\n",
    "\n",
    "        Args:\n",
    "            train_dataloader: DataLoader object for the training dataset.\n",
    "            test_dataloader: DataLoader object for the test dataset.\n",
    "            epochs (int): Number of epochs.\n",
    "            test_loss_frequency (int): Frequency for testing the network.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[float], List[float]]: Training losses and accuracies over epochs.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "\n",
    "        last_test_loss = None\n",
    "        last_test_accuracy = None\n",
    "        \n",
    "        # Train the network\n",
    "        for epoch in (pbar := tqdm(range(epochs))): # Did you know about the walrus operator := ?\n",
    "            total_loss = 0.0\n",
    "            total_accuracy = 0.0\n",
    "\n",
    "            # Iterate over all batches\n",
    "            for x, y in train_dataloader:\n",
    "                loss, accuracy = self.gradient_descent(x, y) # Performs a forward and backward pass and updates weights\n",
    "\n",
    "                total_loss += loss\n",
    "                total_accuracy += accuracy\n",
    "\n",
    "            avg_loss = total_loss / len(train_dataloader)\n",
    "            avg_accuracy = total_accuracy / len(train_dataloader)\n",
    "\n",
    "            if (epoch + 1) % test_loss_frequency == 0:\n",
    "                last_test_loss, last_test_accuracy = self.test(test_dataloader)\n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, Loss: {avg_loss:.3f}, Accuracy: {avg_accuracy:.3f}; Test Loss: {last_test_loss:.3f}, Test Accuracy: {last_test_accuracy:.3f}\")\n",
    "\n",
    "            losses.append(avg_loss)\n",
    "            accuracies.append(avg_accuracy)\n",
    "\n",
    "        return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d018a78",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4aa2ae6687acf19cf4daf1ddc7f6b47a",
     "grade": false,
     "grade_id": "cell-f4c36cd8dbe4282e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# self-check\n",
    "network = Network([784, 128, 64, 10], batch_size=32, learning_rate=0.01)\n",
    "\n",
    "# check accuracy and cross-entropy\n",
    "y = np.array([[0, 1], [1, 0], [1, 0], [0, 1]]) # here we have 4 samples and 2 classes instead of 10\n",
    "y_predicted = np.array([[0.1, 0.9], [0.8, 0.2], [0.7, 0.3], [0.2, 0.8]])\n",
    "\n",
    "assert np.isclose(network.accuracy(y, y_predicted), 1.0), \"Accuracy should be 1\"\n",
    "assert np.isclose(network.loss(y, y_predicted), 0.2, atol=0.05), \"Loss calculation is incorrect\"\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4411dd16",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb9feac240a905d8f2651504ca2a097a",
     "grade": true,
     "grade_id": "ex_3_1_acc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d855d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f90b332f111532ee9fdc92a0a9e17f05",
     "grade": true,
     "grade_id": "ex_3_2_cross_entropy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ddd5c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "980be79f4740c854c1f6a46d472d9551",
     "grade": true,
     "grade_id": "ex_4_1_forward",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be39db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc0321b7c1058eb76b5077ee7700ed1c",
     "grade": true,
     "grade_id": "ex_4_2_backward",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4356681",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68a9446a5dc0b5b509e57476895ec7f7",
     "grade": false,
     "grade_id": "cell-f12069d97c749cc0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Wie ihr sehen könnt, benötigt es tatsächlich nicht viele Zeilen Code, um Backpropagation selbst zu implementieren!\n",
    "\n",
    "Da das Training mit NumPy jedoch recht zeitintensiv ist (insbesondere, für 300+ Studierenden) und das unser Auto-Grading erschweren würde, endet der bewertete Teil des Notebooks an dieser Stelle.\n",
    "\n",
    "Im folgenden Abschnitt könnt ihr versuchen das Netzwerk selbst auf die MNIST-Daten fitten und so überprüfen ob eure Implementierung funktioniert. Eure Aufgabe ist es, geeignete Hyperparameter wie `epochs`, `learning_rate` und `batch_size` zu finden, um die Performance des Modells zu maximieren.\n",
    "Was ist die höchste Accuracy, die ihr erzielen könnt?\n",
    "\n",
    "Zusätzlich könnt ihr hier die Auswirkungen von Overfitting auszuprobieren:\n",
    "- Verwende dazu einen DataLoader mit deutlich weniger Trainingsbildern.\n",
    "- Fitte ein vergleichsweise großes Modell auf diesen reduzierten Datensatz.\n",
    "\n",
    "Wie verhalten sich der Trainings- und Testfehler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f0cc4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ca83030fe4a8f2b9e27d9d92f545d9a",
     "grade": false,
     "grade_id": "cell-245085f202abf7a0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# First, let's define some training hyperparameters:\n",
    "\n",
    "epochs: int = None\n",
    "learning_rate: float = None\n",
    "batch_size: int = None\n",
    "network_structure = [] # [train_images.shape[1], ..., 10] # Hier kannst du auch mit verschiedenen hidden layer sizes experimentieren\n",
    "\n",
    "# Wird nicht bewertet!\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Construct dataloaders for the training and test sets\n",
    "# These dataloaders enable easy iteration over batches of data during training and evaluation.\n",
    "train_dataloader = DataLoader(\n",
    "    train_images, \n",
    "    train_labels, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,  # Shuffle training data \n",
    "    drop_last=True  # Drop incomplete batches\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_images, \n",
    "    test_labels, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,  # Do not shuffle test data\n",
    "    drop_last=True  # Drop incomplete batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb72838a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cf877fa02bef78276599df6a7459812",
     "grade": false,
     "grade_id": "cell-612c78e671df8453",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# Calculate the total number of parameters in the network\n",
    "n_params = 0  # Initialize the parameter counter\n",
    "\n",
    "# Iterate through each pair of consecutive layers to compute parameters\n",
    "for input_size, output_size in zip(network_structure[:-1], network_structure[1:]):\n",
    "    n_params += (input_size + 1) * output_size  # Add weights and biases for each layer\n",
    "\n",
    "print(f\"Total number of parameters in the network: {n_params}\")\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0a728",
   "metadata": {},
   "outputs": [],
   "source": [
    "grading_env_var = os.getenv('NBGRADER_EXECUTION', None)\n",
    "is_grading = (grading_env_var == \"autograde\" or grading_env_var == \"validate\")\n",
    "\n",
    "# Falls ihr hier etwas ändert, bitte darauf achten, dass diese If-Abfrage nicht entfernt wird.\n",
    "# Ansonsten wird das auto-grading zu lange dauern und abbrechen.\n",
    "if not is_grading:\n",
    "    network = Network(network_structure, batch_size, learning_rate)\n",
    "    losses, accuracies = network.fit(train_dataloader, test_dataloader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dee271",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_grading:\n",
    "\n",
    "    # Plot the loss curve:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses, label=\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the accuracy curve:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(accuracies, label=\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143cfbb8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8fb70c8bc0a818c787f7ca8ea74b859d",
     "grade": false,
     "grade_id": "cell-57b3840583807cb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Abschließende Bemerkungen\n",
    "Wir hoffen, dass dir diese Übung einen guten Einblick geben konnte, wie neuronale Netze tatsächlich funktionieren. Der Workflow, den wir hier gezeigt haben, ähnelt stark dem, den du bei realen Machine-Learning-Problemen verwenden würdest – mit dem Unterschied, dass du in der Praxis auf leistungsfähige Libraries wie PyTorch oder TensorFlow zurückgreifen würdest, anstatt alles selbst zu implementieren.\n",
    "\n",
    "Hier sind einige weitere Ideen, was du mit diesem Code machen kannst, wenn du mehr lernen und tiefer eintauchen möchtest:\n",
    "\n",
    "- Was ist die höchste Genauigkeit, die du mit unserer Implementierung erreichen kannst?\n",
    "    - Um schneller zu besseren Ergebnissen zu kommen, könntest du Optimierungsverfahren wie den Adam-Optimizer oder SGD mit Momentum implementieren. Diese Methoden beschleunigen die Konvergenz und können dir helfen, noch bessere Resultate zu erzielen.\n",
    "- Ersetze NumPy durch PyTorch-Tensoren, behalte aber unseren Backpropagation-Code bei (verwende nicht PyTorchs Auto-Differenzierung).\n",
    "    - Meistens reicht es, `np` durch `torch` zu ersetzen.\n",
    "    - Verarbeite eine Batch auf der GPU, indem du den PyTorch Tensor und die Gewichte auf die GPU verschiebst. Führe den Forward- und Backward-Pass sowie den Gradient-Descent-Schritt auf der GPU durch.  Auch die weights und deren Ableitungen und auch den gradient descent step solltest du mit Tensoren auf der GPU ausführen. Ist das schneller als vorher?\n",
    "- Vergleiche mit einem äquivalenten Code, in dem du unser Modell mit typischen PyTorch-Layern implementierst und nun auch PyTorchs Autodifferenzierung für das Training verwendest. Welcher Code ist schneller? Pytorch oder unserer?\n",
    "- Fully-connected neural networks sind nicht ideal für die Verarbeitung von Bildern, wie wir in der Vorlesung sehen (werden).\n",
    "    - Kannst du den Code so erweitern, dass er auch Convolutional Layer unterstützt? Wie sehen hier die Ableitungen aus?\n",
    "- Implementiere eine Funktion, mit der du per Maus Ziffern zeichnen kannst, um sie dann durch dein Modell klassifizieren zu lassen. Folgender Code könnte hier hilfreich sein: https://gist.github.com/korakot/8409b3feec20f159d8a50b0a811d3bca\n",
    "\n",
    "Viel Spaß beim Experimentieren!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "gki_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
