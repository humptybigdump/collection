{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Science and AI for Energy Systems** \n",
    "\n",
    "Karlsruhe Institute of Technology\n",
    "\n",
    "Institute of Automation and Applied Informatics\n",
    "\n",
    "Summer Term 2024\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise XII: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Use the updated docker image with version 1.0.1 (or arm-1.0.1) from this exercise onwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T08:06:43.126781Z",
     "start_time": "2024-07-15T08:06:43.119045Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# import gym\n",
    "# from gym import spaces\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem XII.3 (programming) - Implementing Deep Reinforcement Learning Methods in A Building Energy Management Problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming task, we will work on a building installed with photovoltaic (PV) panels and batteries (storage). Data to be used here is named as `building_battery_data.csv`. This data is recorded from a house within this project [SonyCSL](https://www.sonycsl.co.jp/tokyo/daisuke/14820/). Below is a figure showing the configuration of the building.\n",
    "<img src=\"house.png\" alt=\"Local Image\" width=\"800\">\n",
    "The energy flow of this building can be simplified as\n",
    "<img src=\"flow.png\" alt=\"Local Image\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**(a) Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T08:02:57.665658Z",
     "start_time": "2024-07-15T08:02:57.559358Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('building_energy_data.csv')\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T08:02:58.432618Z",
     "start_time": "2024-07-15T08:02:57.675196Z"
    }
   },
   "outputs": [],
   "source": [
    "T = int( len(data) - 1/24 ) # 345 days of data\n",
    "data = data[:T]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (12, 10))\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "charge_discharge_power = data['charge_discharge_power'] # W\n",
    "rsoc                   = data['rsoc'] # % \n",
    "pvc_charge_power       = data['pvc_charge_power'] # W\n",
    "battery_current        = data['battery_current'] # A(DC)\n",
    "p2                     = data['p2'] # W\n",
    "ups_output_power       = data['ups_output_power']\n",
    "\n",
    "battery_voltage = 52. # for overall battery voltage\n",
    "\n",
    "# 2019-01-01 one day data\n",
    "charge_discharge_power_plot = ax.plot(charge_discharge_power[0:24], 'm*-', label = \"char_dischar\")\n",
    "rosc_plot                   = ax2.plot(rsoc[0:24], 'go-', label = \"rsoc\")\n",
    "pvc_charge_power_plot       = ax.plot(pvc_charge_power[0:24], 'r*-', label = \"pvc_char\")\n",
    "battery_current_plot        = ax2.plot(battery_current[0:24], 'k.-', label = \"battery_current\")\n",
    "p2_plot                     = ax.plot(p2[0:24], 'b*-', label = \"p2\")\n",
    "ups_output_power_plot       = ax.plot(ups_output_power[0:24], 'y*-', label = \"ups_output_power\")\n",
    "\n",
    "\n",
    "# Show all label in one box\n",
    "plots = charge_discharge_power_plot + rosc_plot + pvc_charge_power_plot + battery_current_plot + ups_output_power_plot + p2_plot\n",
    "labels = [plot.get_label() for plot in plots]\n",
    "ax.legend(plots, labels, loc = 0, fontsize=14)\n",
    "\n",
    "ax.set_xlabel(\"hours of a day\", fontsize=14)\n",
    "ax.set_xticks(range(0, 25, 1))\n",
    "ax.set_ylabel(\"Power (W)\", fontsize=14)\n",
    "ax2.set_ylabel(\"A (DC) / %\", fontsize=14)\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State concatenate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T08:02:58.473409Z",
     "start_time": "2024-07-15T08:02:58.441676Z"
    }
   },
   "outputs": [],
   "source": [
    "pv = data[['pvc_charge_power']].values\n",
    "load = data[['ups_output_power']].values\n",
    "# ups_output_power + p1 -  p2\n",
    "p2 = data[['p2']].values\n",
    "\n",
    "x = np.concatenate([pv, load, p2], axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Q-Network Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T08:02:58.501398Z",
     "start_time": "2024-07-15T08:02:58.476964Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQNNet():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, learning_rate):\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        \n",
    "        # state_size = (3, ) \n",
    "        input = Input(shape = self.state_size)\n",
    "\n",
    "        x = Dense(64, activation = \"relu\", \n",
    "                  kernel_initializer = glorot_uniform(seed = 42))(input)\n",
    "        x = Dense(256, activation = \"relu\",\n",
    "                  kernel_initializer = glorot_uniform(seed = 42))(x)\n",
    "\n",
    "        output = Dense(self.action_size, activation = \"linear\", \n",
    "                  kernel_initializer = glorot_uniform(seed = 42))(x)\n",
    "\n",
    "        model = Model(inputs = [input], outputs = [output])\n",
    "        model.compile(loss = \"mse\", optimizer = Adam(lr = self.learning_rate))\n",
    "        model.summary()\n",
    "\n",
    "        return model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T08:02:58.533044Z",
     "start_time": "2024-07-15T08:02:58.508917Z"
    }
   },
   "outputs": [],
   "source": [
    "# A tree based array containing priority of each experience for fast sampling\n",
    "# This SumTree code is a modified version and the original code is from:\n",
    "# https://github.com/jaromiru/AI-blog/blob/master/SumTree.py\n",
    "\n",
    "class SumTree():\n",
    "            \n",
    "    \"\"\"\n",
    "    __init__ - create data array storing experience and a tree based array storing priority\n",
    "    add - store new experience in data array and update tree with new priority\n",
    "    update - update tree and propagate the change through the tree\n",
    "    get_leaf - find the final nodes with a given priority value\n",
    "    \n",
    "    store data with its priority in the tree.\n",
    "    \"\"\"\n",
    "\n",
    "    data_pointer = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "          \n",
    "        \"\"\"\n",
    "        capacity - Number of final nodes containing experience, for all priority values\n",
    "        data - array containing experience (with pointers to Python objects), for all transitions\n",
    "        tree - a tree shape array containing priority of each experience\n",
    "\n",
    "        tree index:\n",
    "            0       -> storing priority sum\n",
    "           / \\\n",
    "          1   2\n",
    "         / \\ / \\\n",
    "        3  4 5  6   -> storing priority for transitions\n",
    "        \n",
    "        Array type for storing:\n",
    "        [0, 1, 2, 3, 4, 5, 6]\n",
    "        \"\"\"\n",
    "        \n",
    "        self.capacity = capacity        \n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype = object)\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        \n",
    "        # Start from first leaf node of the most bottom layer\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "\n",
    "        self.data[self.data_pointer] = data # Update data frame\n",
    "        self.update(tree_index, priority) # Update priority\n",
    "\n",
    "        # Overwrite if exceed memory capacity\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:\n",
    "            self.data_pointer = 0\n",
    "\n",
    "    def update(self, tree_index, priority):\n",
    "\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index] \n",
    "        self.tree[tree_index] = priority\n",
    "\n",
    "        # Propagate the change through tree\n",
    "        while tree_index != 0:  # this method is faster than the recursive loop in the reference code\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "\n",
    "    def get_leaf(self, v):\n",
    "\n",
    "        parent_index = 0\n",
    "\n",
    "        while True:  # while loop is faster than the method in the reference code\n",
    "            left_child_index = 2 * parent_index + 1  # this leaf's left and right kids\n",
    "            right_child_index = left_child_index + 1\n",
    "            # Downward search, always search for a higher priority node till the last layer\n",
    "            if left_child_index >= len(self.tree): # reach the bottom, end search\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            else:    # downward search, always search for a higher priority node\n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "\n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        # tree leaf index, priority, experience\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T08:02:58.578122Z",
     "start_time": "2024-07-15T08:02:58.546354Z"
    }
   },
   "outputs": [],
   "source": [
    "class Memory():  # stored as (s, a, r, s_) in SumTree\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    __init__ - create SumTree memory\n",
    "    store - assign priority to new experience and store with SumTree.add & SumTree.update\n",
    "    sample - uniformly sample from the range between 0 and total priority and \n",
    "           retrieve the leaf index, priority and experience with SumTree.get_leaf\n",
    "    batch_update - update the priority of experience after training with SumTree.update\n",
    "\n",
    "    PER_e - Hyperparameter that avoid experiences having 0 probability of being taken\n",
    "    PER_a - Hyperparameter that allows tradeoff between taking only experience with \n",
    "          high priority and sampling randomly (0 - pure uniform randomness, 1 -\n",
    "          select experiences with the highest priority), convert the importance of TD error to priority\n",
    "    PER_b - Importance-Sampling (IS), from initial value increasing to 1, control how much beta\n",
    "          IS affect learning\n",
    "    \"\"\"\n",
    "  \n",
    "    PER_e = 0.01 \n",
    "    PER_a = 0.6\n",
    "    PER_b = 0.4\n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    absolute_error_upper = 1.  # Clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def store(self, experience):\n",
    "        \n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "\n",
    "        # If the max priority = 0, this experience will never have a chance to be selected\n",
    "        # So a minimum priority is assigned\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "\n",
    "        self.tree.add(max_priority, experience)  # set the max priority for new priority\n",
    "\n",
    "    def sample(self, n):\n",
    "        \n",
    "        \"\"\"\n",
    "        First, to sample a minibatch of k size, the range [0, priority_total] is\n",
    "        divided into k ranges. A value is uniformly sampled from each range. Search \n",
    "        in the sumtree, the experience where priority score correspond to sample \n",
    "        values are retrieved from. Calculate IS weights for each minibatch element\n",
    "        \"\"\"\n",
    "\n",
    "        b_idx = np.empty((n, ), dtype=np.int32)\n",
    "        b_memory = [] # np.empty((n, self.tree.data[0].size))        \n",
    "        b_ISWeights =  np.empty((n, 1))\n",
    "\n",
    "        priority_segment = self.tree.tree[0] / n   \n",
    "\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling]) # max = 1\n",
    "\n",
    "        prob_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.tree[0] # for later calculate ISweight\n",
    "        max_weight = (prob_min * n) ** (-self.PER_b)\n",
    "\n",
    "        for i in range(n):\n",
    "            a = priority_segment * i\n",
    "            b = priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            prob = priority / self.tree.tree[0]\n",
    "            b_ISWeights[i, 0] = (prob * n) ** (-self.PER_b) / max_weight               \n",
    "            b_idx[i]= index\n",
    "            b_memory.append([data])\n",
    "\n",
    "        return b_idx, b_memory, b_ISWeights\n",
    "\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        # To avoid 0 probability\n",
    "        abs_errors += self.PER_e # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Battery Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**(b) Set up the environment**\n",
    "\n",
    "The RSOC and battery current have the following relationship:\n",
    "\\begin{equation} \n",
    "RSOC_{t+1} = \\left\\{\\begin{matrix}\n",
    " RSOC_{t+1} + (K_d \\times a_t - Decay) \\times timestep &  \\text{if} a_t<0, \\text{discharge}\\\\\n",
    " RSOC_{t+1} + (K_c \\times a_t - Decay) \\times timestep &  \\text{if} a_t>0, \\text{charge}\\\\\n",
    " RSOC_{t+1} - Decay \\times timestep &  \\text{if} a_t=0, \\text{idle},\\\\\n",
    "\\end{matrix}\\right.\n",
    "\\end{equation}\n",
    "where $K_d$ and $K_c$ is the discharge and charge coefficients, Decay is the battery natural decay. In addition, from the above sketch of the energy flow, we can set the reward function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reward = -p2\\_sim\n",
    "    * battery\\_charge\\_power = battery\\_voltage $\\times$ action\n",
    "    * p2\\_sim = battery\\_charge\\_power + load - pvc\\_charge\\_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T08:02:58.600854Z",
     "start_time": "2024-07-15T08:02:58.583509Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the Battery environment\n",
    "class BatteryEnv():\n",
    "    \n",
    "    def __init__(self, action_size):\n",
    "        \"\"\"\n",
    "        coeff_d - discharge coefficient\n",
    "        coeff_c - charge coefficient\n",
    "\n",
    "        wear cost is not considered at this moment\n",
    "        actions space is 3, where\n",
    "        a = -1, battery discharge\n",
    "        a = 0,  battery in idle\n",
    "        a = 1,  battery charge\n",
    "        \"\"\"\n",
    "        self.action_set      = np.linspace(-35, 35, num=action_size, endpoint=True)\n",
    "        self.initial_rsoc    = 30.\n",
    "        self.coeff_c         = 0.02\n",
    "        self.coeff_d         = 0.02\n",
    "        self.decay           = 0.0018\n",
    "    \n",
    "    # def _get_state(self):\n",
    "    #     return self.data.iloc[self.current_step].values    \n",
    "    # \n",
    "    # def reset(self):\n",
    "    #     self.current_step = 0\n",
    "    #     self.state = self._get_state()\n",
    "    #     return self.state\n",
    "\n",
    "    def step(self, state, action, timestep):\n",
    "        current_pv   = state[0]\n",
    "        current_load = state[1]\n",
    "        current_p2   = state[2]\n",
    "        current_rsoc = state[3]\n",
    "\n",
    "        # rsoc\n",
    "        if self.action_set[action] < 0: #== -1:   # discharge\n",
    "            next_rsoc = current_rsoc + (self.coeff_d * self.action_set[action] - self.decay) * timestep\n",
    "            next_rsoc = np.maximum(next_rsoc, 20.)\n",
    "\n",
    "        elif self.action_set[action] > 0: #== 1:   # charge\n",
    "            next_rsoc = current_rsoc + (self.coeff_c * self.action_set[action] - self.decay) * timestep\n",
    "            next_rsoc = np.minimum(next_rsoc, 100.)\n",
    "\n",
    "        else:  # idle\n",
    "            next_rsoc = current_rsoc - self.decay * timestep\n",
    "            next_rsoc = np.maximum(next_rsoc, 20.)\n",
    "            \n",
    "        next_rsoc = np.array([next_rsoc])\n",
    "\n",
    "        battery_charge_power = battery_voltage * self.action_set[action] # battery_output\n",
    "        cost = - current_pv + battery_charge_power + current_p2 # house_useage\n",
    "        p2_sim = current_pv - battery_charge_power\n",
    "        \n",
    "        # reward function, bill price: fixed\n",
    "        reward =  np.minimum(- cost, 0.)\n",
    "\n",
    "        return next_rsoc, reward , p2_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T08:02:58.622931Z",
     "start_time": "2024-07-15T08:02:58.610857Z"
    }
   },
   "outputs": [],
   "source": [
    "# DQN hyperparameters\n",
    "state_size = (4, ) # pv_power, consumption, p2, rsoc\n",
    "action_size = 11\n",
    "learning_rate = 0.01\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "gamma = 0.95     \n",
    "\n",
    "# Exploration hyperparameters for epsilon greedy strategy\n",
    "explore_start = 1.0 # exploration probability at start\n",
    "explore_stop = 0.01 # minimum exploration probability \n",
    "decay_rate = 0.001 # exponential decay rate for exploration prob\n",
    "\n",
    "# Memory hyperparameters\n",
    "pretrain_length = 10000 # # of experiences stored in Memory during initialization\n",
    "memory_size = 10000 # # of experiences Memory can keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T08:02:59.502988Z",
     "start_time": "2024-07-15T08:02:58.629431Z"
    }
   },
   "outputs": [],
   "source": [
    "battery = BatteryEnv(action_size = action_size)\n",
    "                  \n",
    "memory = Memory(memory_size)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Memory initialization\n",
    "RSOC              = np.array([battery.initial_rsoc])\n",
    "day               = 0\n",
    "quarter_hour      = 0\n",
    "done              = False\n",
    "timestep = 15.\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "\n",
    "    state = np.concatenate((x[day * 24 + quarter_hour, :], RSOC), axis=-1)\n",
    "    action = np.random.randint(0, action_size)\n",
    "  \n",
    "    # Compute the reward and new state based on the selected action\n",
    "    # next_rsoc, reward\n",
    "    next_rsoc, reward, p2_sim = battery.step(state, action, timestep)\n",
    "#     print('next_rsoc: ', next_rsoc, 'reward: ', reward)\n",
    "\n",
    "    # Store the experience in memory\n",
    "    if quarter_hour < 24 - 1:\n",
    "        quarter_hour += 1      \n",
    "        next_state = np.concatenate((x[day * 24 + quarter_hour, :], next_rsoc), axis = -1)\n",
    "    else:\n",
    "        done = True\n",
    "        day += 1\n",
    "        quarter_hour = 0\n",
    "        if day < len(x) / 24:\n",
    "            next_state = np.concatenate((x[day * 24 + quarter_hour, :], next_rsoc), axis = -1)\n",
    "        else:\n",
    "            break\n",
    "      \n",
    "    RSOC = next_rsoc\n",
    "    experience = state, action, reward, next_state, done\n",
    "    memory.store(experience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**(c) Implement DRL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Q-Network (DQN) Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T08:04:39.871037Z",
     "start_time": "2024-07-15T08:02:59.505880Z"
    }
   },
   "outputs": [],
   "source": [
    "DQN = DQNNet(state_size = state_size, \n",
    "             action_size = action_size, \n",
    "             learning_rate = learning_rate)\n",
    "\n",
    "decay_step = 0 # Decay rate for ϵ-greedy policy\n",
    "RSOC = np.array([battery.initial_rsoc])\n",
    "day = 0\n",
    "quarter_hour = 0\n",
    "done = False\n",
    "timestep = 15.\n",
    "quarter_hour_rewards = []\n",
    "day_mean_rewards = []\n",
    "\n",
    "while day < len(x) / 24:   \n",
    "  \n",
    "    state = np.concatenate((x[day * 24 + quarter_hour, :], RSOC), axis = -1)\n",
    "\n",
    "    # ϵ-greedy policy        \n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        action = np.random.randint(0, action_size)\n",
    "    else:\n",
    "        action = np.argmax(DQN.model.predict(np.expand_dims(state, axis = 0)))\n",
    "\n",
    "    # Compute the reward and new state based on the selected action\n",
    "    next_RSOC, reward, p2_sim= battery.step(state, action, timestep)     \n",
    "    #   print('next_rsoc: ', next_RSOC, 'reward: ', reward)\n",
    "\n",
    "    quarter_hour_rewards.append(reward)\n",
    "  \n",
    "    # Store the experience in memory\n",
    "    if quarter_hour < 24 - 1:\n",
    "        quarter_hour += 1\n",
    "        next_state = np.concatenate((x[day * 24 + quarter_hour, :], next_RSOC), axis = -1)\n",
    "    else:\n",
    "        done = True\n",
    "        day += 1\n",
    "        quarter_hour = 0\n",
    "        if day < len(x) / 24:\n",
    "            next_state = np.concatenate((x[day * 24 + quarter_hour, :], next_RSOC), axis = -1)\n",
    "        else:\n",
    "            break\n",
    "        mean_reward = np.mean(quarter_hour_rewards)\n",
    "        day_mean_rewards.append(mean_reward)\n",
    "        quarter_hour_rewards = []\n",
    "        print(\"Day: {}\".format(day),\n",
    "              \"Mean reward: {:.2f}\".format(mean_reward),\n",
    "              \"Training loss: {:.2f}\".format(loss),\n",
    "              \"Explore P: {:.2f} \\n\".format(explore_probability))  \n",
    "\n",
    "    RSOC = next_RSOC \n",
    "    experience = state, action, reward, next_state, done\n",
    "    memory.store(experience)\n",
    "    decay_step +=1\n",
    "\n",
    "    # DQN training            \n",
    "    tree_idx, batch, ISWeights_mb = memory.sample(batch_size) # Obtain random mini-batch from memory\n",
    "    \n",
    "    states_mb = np.array([each[0][0] for each in batch])\n",
    "    actions_mb = np.array([each[0][1] for each in batch])\n",
    "    rewards_mb = np.array([each[0][2] for each in batch]) \n",
    "    next_states_mb = np.array([each[0][3] for each in batch])\n",
    "    dones_mb = np.array([each[0][4] for each in batch])\n",
    "\n",
    "    targets_mb = DQN.model.predict(states_mb)\n",
    "    \n",
    "#     print('s_mb:',states_mb, 'a_mb:', actions_mb, 'r_mb:', rewards_mb)\n",
    "\n",
    "    # Update those targets at which actions are taken\n",
    "    target_batch = []\n",
    "    q_next_state = DQN.model.predict(next_states_mb)\n",
    "    for i in range(0, len(batch)):  \n",
    "        action = np.argmax(q_next_state[i])\n",
    "        if dones_mb[i] == 1:\n",
    "            target_batch.append(rewards_mb[i])\n",
    "        else:\n",
    "            target = rewards_mb[i] + gamma * q_next_state[i][action]\n",
    "            target_batch.append(rewards_mb[i])\n",
    "\n",
    "    # Replace the original with the updated targets\n",
    "    one_hot = np.zeros((len(batch), action_size))\n",
    "    one_hot[np.arange(len(batch)), actions_mb] = 1\n",
    "    targets_mb = targets_mb.astype(\"float64\")\n",
    "    target_batch = np.array([each for each in target_batch]).astype(\"float64\")\n",
    "    np.place(targets_mb, one_hot > 0, target_batch)\n",
    "\n",
    "    loss = DQN.model.train_on_batch(states_mb, targets_mb, sample_weight = ISWeights_mb.ravel())\n",
    "\n",
    "    # Update priority\n",
    "    absolute_errors = []\n",
    "    predicts_mb = DQN.model.predict(states_mb)\n",
    "    for i in range(0, len(batch)):\n",
    "        absolute_errors.append(np.abs(predicts_mb[i][actions_mb[i]] - targets_mb[i][actions_mb[i]]))\n",
    "    absolute_errors = np.array(absolute_errors)\n",
    "\n",
    "    tree_idx = np.array([int(each) for each in tree_idx])\n",
    "    memory.batch_update(tree_idx, absolute_errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**(d) Plot the average reward**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T08:04:40.263363Z",
     "start_time": "2024-07-15T08:04:39.874059Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (12,4))\n",
    "\n",
    "ax.plot(day_mean_rewards, \"b-\", label = \"reward\")\n",
    "\n",
    "ax.set_xlabel(\"days of Year 2019\", fontsize=14)\n",
    "ax.set_ylabel(\"Average reward\", fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "ax.legend(loc ='lower right', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Prioritized_Replay_DQN]",
   "language": "python",
   "name": "conda-env-Prioritized_Replay_DQN-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
