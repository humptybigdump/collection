{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Setup\n",
    "\n",
    "If you prefer to work locally, see the following instructions for setting up Python in a virtual environment.\n",
    "You can then ignore the instructions in \"Colab Setup\".\n",
    "\n",
    "If you haven't yet, create a [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) environment using:\n",
    "```\n",
    "conda create --name rl_exercises\n",
    "conda activate rl_exercises\n",
    "```\n",
    "The remaining dependencies can be installed with pip:\n",
    "```\n",
    "pip install matplotlib pillow numpy ipykernel \"gymnasium[toy-text]\"\n",
    "```\n",
    "\n",
    "Even if you are running the Jupyter notebook locally, please run the code cell in **Colab Setup**, because it defines some global variables required later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup\n",
    "\n",
    "Google Colab provides you with a temporary environment for python programming.\n",
    "While this conveniently works on any platform and internally handles dependency issues and such, it also requires you to set up the environment from scratch every time.\n",
    "The \"Colab Setup\" section below will be part of **every** exercise and contains utility that is needed before getting started.\n",
    "\n",
    "There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window).\n",
    "Any changes you make to the Jupyter notebook itself should be saved to your Google Drive.\n",
    "We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout.\n",
    "However, you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Your work will be stored in a folder called `rl_ws23` by default to prevent Colab \n",
    "instance timeouts from deleting your edits.\n",
    "We do this by mounting your google drive on the virtual machine created in this colab \n",
    "session. For this, you will likely need to sign in to your Google account and allow\n",
    "access to your Google Drive files.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    COLAB = True\n",
    "except ImportError:\n",
    "    COLAB = False\n",
    "\n",
    "# Create paths in your google drive\n",
    "if COLAB:\n",
    "    DATA_ROOT = Path(\"/content/gdrive/My Drive/rl_ws24\")\n",
    "    DATA_ROOT.mkdir(exist_ok=True)\n",
    "else:\n",
    "    DATA_ROOT = Path.cwd() / \"rl_ws24\"\n",
    "\n",
    "# Install **python** packages\n",
    "if COLAB:\n",
    "    %pip install matplotlib pillow numpy \"gymnasium[toy-text]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Tabular Reinforcement Learning\n",
    "\n",
    "In this homework, we will implement basic planning and reinforcement learning algorithms.\n",
    "We will look at Policy Iteration and Value Iteration, as well as tabular Q-Learning.\n",
    "The algorithms will be evaluated on a gridworld task from OpenAI gym.\n",
    "\n",
    "All homeworks are self-contained.\n",
    "They can be completed in their respective notebooks.\n",
    "Please fill in any missing code or answer any questions that are marked with `##TODO##` statements.\n",
    "Questions not marked with `##TODO##` are self-test questions and do **not** need to be answered for points.\n",
    "To edit and re-run code, you can simply edit and restart the code cells below.\n",
    "When you are finished, you will need to submit the notebook as well as all saved figures (see exercises) as a zip file via Ilias.\n",
    "\n",
    "We start by importing all the necessary python modules and defining some helper functions which you do not need to change.\n",
    "Still, make sure you are aware of what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "# specify the path to save the recordings of this run to.\n",
    "DATA_PATH = DATA_ROOT / \"exercise_1\" / time.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "def save_figure(fig: plt.Figure, save_name: str) -> None:\n",
    "    \"\"\"Saves a figure into your google drive folder or local directory\"\"\"\n",
    "    DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    path = DATA_PATH / (save_name + \".png\")\n",
    "    fig.savefig(str(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym  # gymnasium is successor to gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "\n",
    "\n",
    "def measure_policy_success(\n",
    "    env: FrozenLakeEnv,\n",
    "    pi: np.ndarray,\n",
    "    n_eval: int,\n",
    "    render: bool = False,\n",
    "    fps: float | None = None,\n",
    ") -> float:\n",
    "    \"\"\"Evaluate a policy on an environment and return success rate\n",
    "\n",
    "    :param env: FrozenLake env\n",
    "    :param pi: a policy that dictates the deterministic action to take in each state [n_states]\n",
    "    :param render: render the trajectory?\n",
    "    :return: The mean success rate of the given policy\n",
    "    \"\"\"\n",
    "\n",
    "    successes = []\n",
    "    for _ in range(n_eval):\n",
    "        state, info = env.reset()\n",
    "        if render:\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(Image.fromarray(env.render()))\n",
    "            if fps is not None:\n",
    "                time.sleep(1 / fps)\n",
    "        for _ in range(100):\n",
    "            action = int(pi[state])\n",
    "\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            if render:\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(Image.fromarray(env.render()))\n",
    "                if fps is not None:\n",
    "                    time.sleep(1 / fps)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                successes.append(reward)\n",
    "                break\n",
    "\n",
    "    return np.mean(successes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FrozenLake Environment\n",
    "First, let's have a look at the problem we are solving. The agent controls the movement of a character in a grid world.\n",
    "Some tiles of the grid are walkable, and others lead to the agent falling into the water.\n",
    "Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction.\n",
    "The agent is rewarded for finding a walkable path to a goal tile.\n",
    "The episode ends when you reach the goal or fall in a hole.\n",
    "You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "Actions are encoded as integers 0 = left, 1 = down, 2 = right and 3 = up.\n",
    "The states are counted from 0 to $N_{states}$.\n",
    "\n",
    "![The FrozenLake Environment](https://gymnasium.farama.org/_images/frozen_lake.gif)\n",
    "\n",
    "See https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    "### Hint\n",
    "Like all DiscreteEnvs in Gym, FrozenLake has a property `P` which is a dictionary of lists, where\n",
    "```\n",
    "P[s][a] == [(transition probability, next state, reward, done), ...]\n",
    "```\n",
    "for a state $s$ and an action $a$.\n",
    "In FrozenLake, the player will move in intended direction with probability of $1/3$ else will move in either perpendicular direction with equal probability of $1/3$ in both directions.\n",
    "In non-terminal states, `len(P[s][a]) == 3`.\n",
    "This environment is very similar to the one shown in the Optimal Decision Making lecture.\n",
    "\n",
    "Execute the next cell to see what a randomly initialized policy does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env: FrozenLakeEnv = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", render_mode=\"rgb_array\")\n",
    "env.reset(seed=SEED)\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n\n",
    "pi_random = np.random.default_rng(SEED).integers(n_actions, size=(n_states,))\n",
    "\n",
    "success_rate = measure_policy_success(env, pi_random, n_eval=3, render=True, fps=30 if not COLAB else None)\n",
    "\n",
    "print(f\"Random policy success rate={success_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TASK 1: Policy Iteration**\n",
    "The first algorithm we will implement is called Policy Iteration.\n",
    "It consists of an inner and an outer loop.\n",
    "The inner loop is called Policy Evaluation and computes state values for the _current policy_.\n",
    "For every state, it averages the expected returns of all possible actions in that state, weighted by the current policy's action probabilities.\n",
    "The outer loop is called Policy Improvement and it uses the current policy's value function to return an improved policy.\n",
    "It does this by choosing an action for each state that maximizes the expected value of the next state.\n",
    "In Policy Iteration, we first initialize a policy and value function randomly, and then iteratively run Policy Evaluation and Policy Improvement until convergence.\n",
    "\n",
    "The **pseudocode** looks as follows:\n",
    "\n",
    "---\n",
    "- **Initialize** $V_{(0)}^{\\pi_0}(s)$ randomly for all $s$, $\\pi_0 \\leftarrow$ uniform, $k = 0$\n",
    "\n",
    "- **Repeat** for $i=1, 2, \\dots$\n",
    "\n",
    "    - **Policy Evaluation:**\n",
    "\n",
    "        - **Initialize** $V_{(0)}^{\\pi_{i}}(s) \\leftarrow V_{(k)}^{\\pi_{i-1}}(s)$ (i.e. initialize the value function of the new policy with the converged value function of the old policy)\n",
    "\n",
    "        - **Repeat** for $k=1, 2, \\dots$\n",
    "\n",
    "            \\begin{equation*}\n",
    "                V_{(k)}^{\\pi_{i}}(s) \\leftarrow \\sum_a \\pi(a \\mid s) \\sum_{s'} P(s' \\mid s,a) \\Big( r(s,a,s') + \\overline{d} \\gamma \\, V_{(k - 1)}^{\\pi_{i}}(s') \\Big)\n",
    "            \\end{equation*}\n",
    "\n",
    "        - **Until convergence**\n",
    "\n",
    "    - **Policy Improvement:**\n",
    "\n",
    "        \\begin{align*}\n",
    "            \\pi_{i}(a \\mid s) \\leftarrow \\begin{cases}\n",
    "                1, & \\text{if } a = \\underset{a'}{\\arg \\max} \\sum_{s'} P(s' \\mid s,a') \\Big( r(s,a',s') + \\overline{d} \\gamma \\, V^{\\pi_i}(s') \\Big)\\\\\n",
    "                0 & \\text{else}\n",
    "            \\end{cases}\n",
    "        \\end{align*}\n",
    "\n",
    "- **Until convergence**\n",
    "---\n",
    "\n",
    "Because the reward depends on the next state and not just the action (i.e. $r=r(s,a,s')$), we must slightly update the equations for policy iteration and value iteration seen in the lecture.\n",
    "\n",
    "Also note the addition of $\\overline{d}$ to the formulas from the lecture.\n",
    "The expected value of some transition is the transition reward plus the (discounted) value of the next state.\n",
    "However, some transitions result in termination (e.g. falling through the ice).\n",
    "For these transitions, the trajectory is over and we cannot use the (discounted) value of the next state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 (4 Points)\n",
    "\n",
    "In this first task you have to implement the core algorithm of **policy evaluation** (2 points) and **policy improvement** (2 points), both marked by `##TODO##`.\n",
    "For convenience in this discrete environment, we maintain both an array called `pi_prob` for the _action probabilities_ and an array called `pi` that just contains the deterministic action that the policy takes in each state.\n",
    "\n",
    "After you have finished implementing the corresponding functions, execute the code cell.\n",
    "Your algorithm should be able to solve the task with around ~65% success rate.\n",
    "\n",
    "Note: the dynamics of the environment are available to you through the `mdp` variable.\n",
    "If you index this with the state and the action (i.e. `mdp[state][action]`), you are given a list of possible transitions, where each transition is a tuple of the form `(transition_prob: float, next_state: int, reward: float, done: bool)`.\n",
    "In other words, each transition has an associated probability, reward, and next state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(\n",
    "    mdp: dict[int, dict[int, list[tuple[float, int, float, bool]]]],\n",
    "    pi_prob: np.ndarray,\n",
    "    previous_value: np.ndarray,\n",
    "    gamma: float,\n",
    "    max_iters: int = 10000,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Perform the Policy Evaluation step given a policy pi and an environment.\n",
    "\n",
    "    :param mdp: an MDP as a list of transitions for each state-action pair\n",
    "    :param pi_prob: Action probabilities [num_states, num_actions]\n",
    "    :param previous_value: Initial value function [num_states]\n",
    "    :return: value function of the provided policy [num_states]\n",
    "    \"\"\"\n",
    "    n_states, n_actions = pi_prob.shape\n",
    "    for _ in range(max_iters):\n",
    "        value = np.zeros_like(previous_value)\n",
    "        ## SOLUTION ##\n",
    "        for state in range(n_states):\n",
    "            # loop over all actions the policy can take\n",
    "            for action, action_prob in enumerate(pi_prob[state]):\n",
    "                # transitions is a list of tuple representing each possible outcome of this\n",
    "                # action in this state\n",
    "                transitions = mdp[state][action]\n",
    "                # compute expected return from taking this action in this state, where the\n",
    "                # expectation is over the various possible transitions\n",
    "                q_s_a = sum(\n",
    "                    transition_prob\n",
    "                    * (reward + (not done) * gamma * previous_value[next_state])\n",
    "                    for transition_prob, next_state, reward, done in transitions\n",
    "                )\n",
    "                # accumulate value of this state resulting from this action and the\n",
    "                # probability of the policy choosing it\n",
    "                value[state] += action_prob * q_s_a\n",
    "        ## END ##\n",
    "\n",
    "        # run policy evaluation until convergence\n",
    "        if np.allclose(value, previous_value):\n",
    "            break\n",
    "\n",
    "        # save current value estimate for next iteration\n",
    "        previous_value = np.copy(value)\n",
    "\n",
    "    return value\n",
    "\n",
    "def policy_improvement(\n",
    "    mdp: dict[int, dict[int, list[tuple[float, int, float, bool]]]],\n",
    "    value: np.ndarray,\n",
    "    gamma: float,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Perform the Policy Improvement step given a value function.\n",
    "\n",
    "    :param mdp: an MDP as a list of transitions for each state-action pair\n",
    "    :param value: Value function of a policy [num_states]\n",
    "    :return: New policy [num_states] and distribution over action probabilities [num_states, num_actions]\n",
    "    \"\"\"\n",
    "    n_states, n_actions = len(mdp), len(mdp[0])\n",
    "    # initialize policy\n",
    "    # contains the actual actions\n",
    "    pi = np.zeros(shape=n_states, dtype=np.int64)\n",
    "    # contains the action probabilities for each state\n",
    "    pi_prob = np.zeros(shape=(n_states, n_actions))\n",
    "\n",
    "    ## SOLUTION ##\n",
    "    for state in range(n_states):\n",
    "        # list of q values for current state s\n",
    "        qs = []\n",
    "        for action in range(n_actions):\n",
    "            # transitions is a list of tuple representing each possible outcome of this\n",
    "            # action in this state\n",
    "            transitions = mdp[state][action]\n",
    "            # compute expected return from taking this action in this state, where the\n",
    "            # expectation is over the various possible transitions\n",
    "            q_s_a = sum(\n",
    "                transition_prob * (reward + (not done) * gamma * value[next_state])\n",
    "                for transition_prob, next_state, reward, done in transitions\n",
    "            )\n",
    "            qs.append(q_s_a)\n",
    "\n",
    "        # new policy action is the one with highest q\n",
    "        pi[state] = np.argmax(qs)\n",
    "        # set probability of best action to 1.0, all others 0.0\n",
    "        pi_prob[state, pi[state]] = 1\n",
    "    ## END ##\n",
    "\n",
    "    return pi, pi_prob\n",
    "\n",
    "def policy_iteration(\n",
    "    env: FrozenLakeEnv,\n",
    "    gamma: float,\n",
    "    n_iters: int,\n",
    "    max_iters_policy_eval: int = 10000,\n",
    "    random_init: bool = True,\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Policy Iteration algorithm\n",
    "\n",
    "    :param env: a FrozenLakeEnv environment\n",
    "    :param gamma: discount factor\n",
    "    :param n_iter: number of loop iterations\n",
    "    :param max_iters_policy_eval: maximum iterations of policy evaluation for each loop iteration\n",
    "    :param random_init: initialize value function randomly or as all zeros?\n",
    "    :return: Figure containing final value function and learning curves\n",
    "    \"\"\"\n",
    "\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    mdp = env.unwrapped.P\n",
    "    rng = np.random.default_rng(seed=SEED)\n",
    "    if random_init:\n",
    "        value = rng.normal(loc=0, scale=0.5, size=(n_states,))\n",
    "    else:\n",
    "        value = np.zeros(shape=n_states)\n",
    "    pi_prob = (\n",
    "        np.ones(shape=(n_states, n_actions)) / n_actions\n",
    "    )  # contains the action probabilities for each state\n",
    "    pi = np.zeros(shape=n_states, dtype=np.int64)  # contains the actual actions\n",
    "\n",
    "    # setup plotting\n",
    "    fig, (ax_value, ax_success, ax_change) = plt.subplots(1, 3, figsize=(12,5))\n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "    grid = (nrow, ncol) = (env.unwrapped.nrow, env.unwrapped.ncol)\n",
    "    image_value = ax_value.imshow(value.reshape(grid), vmin=0.0, vmax=0.9)\n",
    "    fig.colorbar(image_value)\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            desc = env.unwrapped.desc[i, j].decode()\n",
    "            ax_value.text(j, i, desc, ha=\"center\", va=\"center\", color=\"w\")\n",
    "    ax_success.set(xlabel=\"Iterations\", ylabel=\"Success Rate\")\n",
    "    ax_success.set_xlim(0, n_iters)\n",
    "    ax_success.set_ylim(0.0, 1.0)\n",
    "    line_success, *_ = ax_success.plot([], [])\n",
    "    ax_change.set(xlabel=\"Iterations\", ylabel=\"Mean Delta\")\n",
    "    ax_change.set_xlim(0, n_iters)\n",
    "    ax_change.set_ylim(1e-7, 1.0)\n",
    "    ax_change.set_yscale(\"log\")\n",
    "    line_change, *_ = ax_change.plot([], [])\n",
    "\n",
    "    env.reset(seed=int(rng.integers(2**32)))\n",
    "\n",
    "    success_rates = []\n",
    "    mean_delta_values = []\n",
    "    for iter in range(n_iters):\n",
    "        previous_value = np.copy(value)\n",
    "\n",
    "        # run policy evaluation\n",
    "        value = policy_evaluation(mdp, pi_prob, value, gamma, max_iters_policy_eval)\n",
    "\n",
    "        # run policy improvement\n",
    "        pi, pi_prob = policy_improvement(mdp, value, gamma)\n",
    "\n",
    "        # evaluate policy success rate\n",
    "        success_rate = measure_policy_success(env, pi, n_eval=100)\n",
    "        success_rates.append(success_rate)\n",
    "\n",
    "        mean_delta_value = np.abs(value - previous_value).mean()\n",
    "        mean_delta_values.append(mean_delta_value)\n",
    "\n",
    "        # update plots\n",
    "        display.clear_output(wait=True)\n",
    "        image_value.set_data(value.reshape(grid))\n",
    "        line_success.set_xdata(np.arange(len(success_rates)) + 1)\n",
    "        line_success.set_ydata(success_rates)\n",
    "        line_change.set_xdata(np.arange(len(mean_delta_values)) + 1)\n",
    "        line_change.set_ydata(mean_delta_values)\n",
    "        display.display(fig)\n",
    "\n",
    "    # clear duplicate plot\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Run Policy Iteration\n",
    "fig = policy_iteration(\n",
    "    env,\n",
    "    gamma=0.99,\n",
    "    n_iters=10,\n",
    "    max_iters_policy_eval=10000,\n",
    "    random_init=True,\n",
    ")\n",
    "save_figure(fig, \"policy_iteration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the hyperparameters for this algorithm if you wish.\n",
    "\n",
    "In this implementation, we stop policy evaluation when we reach convergence or after at most `max_iters_policy_eval` iterations.\n",
    "Set `max_iters_policy_eval` to $1$, meaning that we only update the values once before using it to improve the policy again.\n",
    "How much do you need to increase `n_iters` to reach the same level of success as before?\n",
    "\n",
    "Answer: Convergence is a little slower with `max_iters_policy_eval=1`, but with `n_iters=20` (instead of $10$), the algorithm converges as before.\n",
    "\n",
    "What happens as you increase or decrease `gamma`, the discount factor?\n",
    "How much can you _decrease_ the `gamma` before the success rate starts to suffer?\n",
    "\n",
    "Answer: Increasing the $\\gamma$ (e.g. `gamma=0.999` or `gamma=0.9999`) generally increases the value of the average state, while decreasing it (e.g. `gamma=0.9` or `gamma=0.8`) generally decreases the values, but overall doesn't greatly change the success rate of the policy.\n",
    "In fact, you can reduce $\\gamma$ to $0.001$ and the policy reaches around $40\\%$ success rate.\n",
    "For achieving success, the values themselves don't matter, only the relative differences between possible next states.\n",
    "\n",
    "Set `gamma` to $1.0$.\n",
    "Are the final values for the holes (marked \"H\") $0$, or something else?\n",
    "If they aren't $0$, perhaps you forgot to take into account terminal states (`done`) during policy evaluation.\n",
    "\n",
    "Answer: Terminal states have transitions that deterministically stay in that state, and give no rewards.\n",
    "However, if the value of the next state is not ignored on terminal transitions (e.g. by multiplying by `done`), then these values only change because they are multiplied by $\\gamma$ at each iteration.\n",
    "If $\\gamma$ is $1.0$, they stay frozen after initialization.\n",
    "\n",
    "Be sure to set to the values back to their original values before generating the figures for submission!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TASK 2: Value Iteration**\n",
    "Next, we will have a look at Value Iteration.\n",
    "It is very similar to Policy Iteration, except the sum over the policy $\\pi(a \\mid s)$ has been replaced by a max over actions.\n",
    "In other words, we no longer consider the current policy during the evaluation step, and instead simply assume that the current policy is always optimal with respect to the current value function.\n",
    "The **pseudocode** looks as follows\n",
    "\n",
    "---\n",
    "- **Initialize** $V_{(0)}^\\ast(s)$ randomly for all $s$\n",
    "\n",
    "- **Repeat** for $i=1, 2, \\dots$\n",
    "\n",
    "    \\begin{equation*}\n",
    "        V^\\ast_{(i)}(s) \\leftarrow \\underset{a}{\\max} \\sum_{s'} P(s' \\mid s,a) \\Big( r(s,a,s') + \\overline{d} \\gamma \\, V^\\ast_{(i-1)}(s') \\Big)\n",
    "    \\end{equation*}\n",
    "\n",
    "- **Until convergence**\n",
    "---\n",
    "\n",
    "Again, note the addition of $\\overline{d}$ to the formula from the lecture, to account for terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 (4 Points)\n",
    "In this task you have to implement the core algorithm of **value iteration** (4 points), marked by `##TODO##`.\n",
    "Since the policy is implicitly defined as the optimal policy for the current value function, you must also compute the updated policy as part of value iteration.\n",
    "\n",
    "After you have finished implementing the corresponding functions, execute the code cell.\n",
    "Your algorithm should be able to solve the task with around ~65% success rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(\n",
    "    env: FrozenLakeEnv,\n",
    "    gamma: float,\n",
    "    n_iters: int,\n",
    "    random_init: bool = True,\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Value Iteration algorithm\n",
    "\n",
    "    :param env: a FrozenLakeEnv environment\n",
    "    :param gamma: discount factor\n",
    "    :param n_iter: number of loop iterations\n",
    "    :param random_init: initialize value function randomly or as all zeros?\n",
    "    :return: Figure containing final value function and learning curves\n",
    "    \"\"\"\n",
    "\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    mdp = env.unwrapped.P\n",
    "    rng = np.random.default_rng(seed=SEED)\n",
    "    if random_init:\n",
    "        value = rng.normal(loc=0, scale=0.5, size=(n_states,))\n",
    "    else:\n",
    "        value = np.zeros(shape=n_states)\n",
    "    pi = np.zeros(shape=n_states, dtype=np.int64)  # contains the actual actions\n",
    "\n",
    "    # setup plotting\n",
    "    fig, (ax_value, ax_success, ax_change) = plt.subplots(1, 3, figsize=(12,5))\n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "    grid = (nrow, ncol) = (env.unwrapped.nrow, env.unwrapped.ncol)\n",
    "    image_value = ax_value.imshow(value.reshape(grid), vmin=0.0, vmax=0.9)\n",
    "    fig.colorbar(image_value)\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            desc = env.unwrapped.desc[i, j].decode()\n",
    "            ax_value.text(j, i, desc, ha=\"center\", va=\"center\", color=\"w\")\n",
    "    ax_success.set(xlabel=\"Iterations\", ylabel=\"Success Rate\")\n",
    "    ax_success.set_xlim(0, n_iters)\n",
    "    ax_success.set_ylim(0.0, 1.0)\n",
    "    line_success, *_ = ax_success.plot([], [])\n",
    "    ax_change.set(xlabel=\"Iterations\", ylabel=\"Mean Delta\")\n",
    "    ax_change.set_xlim(0, n_iters)\n",
    "    ax_change.set_ylim(1e-7, 1.0)\n",
    "    ax_change.set_yscale(\"log\")\n",
    "    line_change, *_ = ax_change.plot([], [])\n",
    "\n",
    "    env.reset(seed=int(rng.integers(2**32)))\n",
    "\n",
    "    success_rates = []\n",
    "    mean_delta_values = []\n",
    "    for iter in range(0, n_iters):\n",
    "        previous_value = np.copy(value)\n",
    "\n",
    "        ## SOLUTION ##\n",
    "        for state in range(n_states):  # loop over all states\n",
    "            qs = []  # list for q values in current state\n",
    "            for action in range(n_actions):\n",
    "                transition = mdp[state][action]\n",
    "                q_s_a = sum(\n",
    "                    transition_prob\n",
    "                    * (reward + (not done) * gamma * previous_value[next_state])\n",
    "                    for transition_prob, next_state, reward, done in transition\n",
    "                )\n",
    "                qs.append(q_s_a)\n",
    "\n",
    "            # optimal action is the one with the highest q\n",
    "            pi[state] = np.argmax(qs)\n",
    "            # value of state is the q of the best action\n",
    "            value[state] = np.max(qs)\n",
    "        ## END ##\n",
    "\n",
    "        # Evaluate policy success rate\n",
    "        success_rate = measure_policy_success(env, pi, n_eval=100)\n",
    "        success_rates.append(success_rate)\n",
    "\n",
    "        mean_delta_value = np.abs(value - previous_value).mean()\n",
    "        mean_delta_values.append(mean_delta_value)\n",
    "\n",
    "        # update plots\n",
    "        display.clear_output(wait=True)\n",
    "        image_value.set_data(value.reshape(grid))\n",
    "        line_success.set_xdata(np.arange(len(success_rates)) + 1)\n",
    "        line_success.set_ydata(success_rates)\n",
    "        line_change.set_xdata(np.arange(len(mean_delta_values)) + 1)\n",
    "        line_change.set_ydata(mean_delta_values)\n",
    "        display.display(fig)\n",
    "\n",
    "    # clear duplicate plot\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Run Value Iteration\n",
    "fig = value_iteration(env, gamma=0.99, n_iters=20, random_init=True)\n",
    "save_figure(fig, \"value_iteration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize a value function randomly and then run $20$ iterations of value iteration.\n",
    "How is this different from running $20$ iterations of policy iteration (the previous task) with `max_iters_policy_eval` set to $1$?\n",
    "Which one converges faster?\n",
    "\n",
    "Answer: Value iteration **simultaneously** computes the optimal policy and the next value function **in a single step**.\n",
    "In contrast, policy iteration (with `max_iters_policy_eval=1`) first updates the value function using the **old** policy, and only then updates the policy.\n",
    "The difference is highlighted by the first iteration of each algorithm, where policy iteration sets the value function equal to the mean Q over all actions (because the policy is initialized uniformly), whereas value iteration sets the value function equal to the Q of the best action in each state.\n",
    "\n",
    "The heat map of the value function for policy iteration is noticably lighter (i.e. higher values) than the value function for value iteration.\n",
    "Is there a reason why policy iteration converges to higher state values than value iteration, or can this discrepancy be resolved?\n",
    "\n",
    "Answer: Value iteration just takes somewhat longer to converge.\n",
    "Set `n_iters=200` and you will see that the heat maps become almost identical.\n",
    "Since the optimal value function is unique, and both of these algorithms converge to the optimal value function (and optimal policy), they must converge to the same value function.\n",
    "\n",
    "Play with the hyperparameters for this algorithm if you wish.\n",
    "\n",
    "What happens if the value function is initialized uniformly to $0$ instead of randomly (set `random_init=False`)?\n",
    "Why does convergence slow down?\n",
    "\n",
    "Answer: There is actually nothing fancy going on here, it's sort of a quirk of value iteration and strongly depends on the scale of the Gaussian used to initialize the value function randomly.\n",
    "If the value function is initialized from $\\sim N(0, 0.5)$, there is a good chance that some states are initialized with a value near $0.5$, which is near the average value over all states of the optimal value function (in this case).\n",
    "These states then propagate this higher value to all nearby states, such that most of the states quickly approach a value near $0.5$, which is the optimal value.\n",
    "There are (on expectation) an equal number of states that are initialized to values near $-0.5$, which are very far from the optimal value, but these states do not propagate their values to neighboring states.\n",
    "This is because the update function of value iteration only takes the best action (and therefore only the best neighboring states) into account.\n",
    "If you want to test this, try changing `scale=0.5` to `scale=0.01`, and it behaves very similar to initializing with $0$.\n",
    "In contrast, random initialization with `scale=2.0` also slows down convergence.\n",
    "\n",
    "Be sure to set to the values back to their original values before generating the figures for submission!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TASK 3: Q-Learning**\n",
    "In Policy Iteration and Value Iteration, we assume that we have knowledge about the problem's underlying dynamics (the dictionary of transitions) and the reward function.\n",
    "This information is usually not available in practice.\n",
    "Instead, we must solve the problem using only the trajectories we have already seen, and ensure we collect sufficient new trajectories through exploration.\n",
    "One common and effective algorithm for solving such problems is Q-Learning.\n",
    "\n",
    "The **pseudocode** look as follows:\n",
    "\n",
    "---\n",
    "- **Initialize** $Q_{(0)}(s, a)$ randomly for all $s$ and $a$\n",
    "\n",
    "- **Repeat** for $i=1, 2, \\dots$\n",
    "    - sample an action $a$ using the exploration strategy and get the next state $s'$ and associated reward $r$\n",
    "\n",
    "    \\begin{align*}\n",
    "        \\delta &= r(s, a) + \\overline{d} \\gamma \\underset{a'}{\\max} Q_{(i-1)}(s', a') - Q_{(i-1)}(s, a) \\\\\n",
    "        Q_{(i)}(s, a) &= Q_{(i-1)}(s, a) + \\alpha \\delta\n",
    "    \\end{align*}\n",
    "    \n",
    "    - If $s'$ is terminal: reset environment and sample new initial state $s'$\n",
    "    - Set $ s \\leftarrow s' $\n",
    "\n",
    "- **Until convergence**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 (4 Points)\n",
    "Finally, you have to implement the core algorithm of **QLearning** (4 points), marked by `##TODO##`.\n",
    "Just like in the pseudocode above, you should first compute the TD-Error $\\delta$ and then update the Q function.\n",
    "\n",
    "After you have finished implementing the corresponding functions, execute the code cell.\n",
    "If implemented correctly, your algorithm should be able to solve the task with something between 60% and 80% success rate, although the variability is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(\n",
    "    env: FrozenLakeEnv,\n",
    "    alpha_init: float,\n",
    "    gamma: float,\n",
    "    n_steps: int,\n",
    "    t_decay: int | float,\n",
    "    random_init: bool = True,\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Q-Learning algorithm\n",
    "\n",
    "    :param env: a FrozenLakeEnv environment\n",
    "    :param alpha: initial learning rate (decays over time)\n",
    "    :param gamma: discount factor\n",
    "    :param n_steps: total number of steps the agent takes in the environment\n",
    "    :param t_decay: hyperparameter that controls the rate of exploration decay\n",
    "    :param random_init: initialize Q function randomly or as all zeros?\n",
    "    :return: Figure containing final value function and learning curve\n",
    "    \"\"\"\n",
    "    eval_every = n_steps // 200\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    if random_init:\n",
    "        q = rng.normal(loc=0, scale=0.5, size=(n_states, n_actions))\n",
    "    else:\n",
    "        q = np.zeros((n_states, n_actions))\n",
    "    value = np.max(q, axis=1)\n",
    "\n",
    "    # setup plotting\n",
    "    fig, (ax_value, ax_success) = plt.subplots(1, 2, figsize=(12,5))\n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "    grid = (nrow, ncol) = (env.unwrapped.nrow, env.unwrapped.ncol)\n",
    "    image_value = ax_value.imshow(value.reshape(grid), vmin=0.0, vmax=0.9)\n",
    "    fig.colorbar(image_value)\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            desc = env.unwrapped.desc[i, j].decode()\n",
    "            ax_value.text(j, i, desc, ha=\"center\", va=\"center\", color=\"w\")\n",
    "    ax_success.set(xlabel=\"# Steps\", ylabel=\"Success Rate\")\n",
    "    ax_success.set_xlim(0, n_steps)\n",
    "    ax_success.set_ylim(0.0, 1.0)\n",
    "    line_success, *_ = ax_success.plot([], [])\n",
    "\n",
    "    state, info = env.reset(seed=int(rng.integers(2**32)))\n",
    "\n",
    "    ts = []\n",
    "    success_rates = []\n",
    "    for t in range(n_steps):\n",
    "        epsilon = max(0.01, 1 - t / t_decay)\n",
    "        alpha = max(0.01, alpha_init * (1 - t / n_steps))\n",
    "\n",
    "        ## SOLUTION ##\n",
    "        # select action using epsilon-greedy method\n",
    "        if rng.random() < epsilon:\n",
    "            # return a random action\n",
    "            action: int = rng.integers(n_actions)\n",
    "        else:\n",
    "            # return the greedy action, i.e. the one with the highest q\n",
    "            action: int = int(np.argmax(q[state]))\n",
    "        ## END ##\n",
    "\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        ## SOLUTION ##\n",
    "        delta = reward - q[state, action] + (not terminated) * gamma * np.max(q[next_state])\n",
    "        q[state, action] = q[state, action] + alpha * delta\n",
    "        ## END ##\n",
    "\n",
    "        if terminated or truncated:\n",
    "            state, info = env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "        # evaluate policy success rate\n",
    "        if t % eval_every == 0:\n",
    "            value = np.max(q, axis=1)\n",
    "            pi = np.argmax(q, axis=1)\n",
    "\n",
    "            ts.append(t)\n",
    "            success_rate = measure_policy_success(env, pi, n_eval=100)\n",
    "            success_rates.append(success_rate)\n",
    "\n",
    "            # update plots\n",
    "            display.clear_output(wait=True)\n",
    "            image_value.set_data(value.reshape(grid))\n",
    "            line_success.set_xdata(ts)\n",
    "            line_success.set_ydata(success_rates)\n",
    "            display.display(fig)\n",
    "\n",
    "    # clear duplicate plot\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Run QLearning\n",
    "small_env: FrozenLakeEnv = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", render_mode=\"rgb_array\")\n",
    "fig = q_learning(\n",
    "    small_env,\n",
    "    alpha_init=0.2,  # initial learning rate (decays over time)\n",
    "    gamma=0.99,  # discount factor\n",
    "    n_steps=40000,  # total number of steps the agent takes in the environment\n",
    "    t_decay=20000,  # controls the rate of exploration decay\n",
    "    random_init=False,  # initialize Q function randomly or with 0?\n",
    ")\n",
    "save_figure(fig, \"q_learning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you think that the success rate of Q-learning varies more during learning than it does with policy iteration or value iteration?\n",
    "\n",
    "Answer: Q-learning does not have access to the environment dynamics, making the learning problem much harder.\n",
    "As a result, there is no guarantee of monotonic improvement.\n",
    "In contrast, policy and value iteration are guaranteed to improve on each iteration (or at least not be worse), and the variance in the measured success rate is only due to the stochasticity in the environment.\n",
    "\n",
    "When we measure success, we set the policy to be just the argmax over all actions in each state.\n",
    "Why does this approach hurt exploration, especially during the initial stages of training?\n",
    "\n",
    "Answer: If we initialize the Q-function to all $0$ instead of randomly and then set the policy to be the argmax action for each state, the argmax just returns the first element.\n",
    "Instead of taking a random action when there is a tie, the policy will simply take the \"first\" action in the action space, which in this case is to go left.\n",
    "This hurts exploration, although it's not such a large effect because we also use epsilon-greedy exploration.\n",
    "\n",
    "Play with the hyperparameters for this algorithm if you wish.\n",
    "Can you find better hyperparameters?\n",
    "Change the random seed as well, to see if your hyperparameters are still good for different random conditions.\n",
    "\n",
    "What happens if the Q function is initialized randomly instead of uniformly to $0$?\n",
    "Why does training become much slower?\n",
    "\n",
    "Answer: Learning becomes slower because the algorithm first has to \"unlearn\" any bad actions that were randomly initialized with a high value.\n",
    "However, this depends on the type and scale of random initialization.\n",
    "If the scale is reduced from $0.5$ to e.g. $0.1$, learning can be even faster than zero initialization, because it counteracts the argmax/exploration issue mentioned above.\n",
    "Keep in mind that this is only the case in tabular RL!\n",
    "If a neural net is initialized with all zeroes, the gradients are uniform and it cannot learn.\n",
    "In deep reinforcement learning, initialization is always random.\n",
    "\n",
    "Be sure to set to the values back to their original values before generating the figures for submission!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2 (3 Points)\n",
    "\n",
    "For the previous task, we used the 4x4 version of the FrozenLake environment because Q-Learning requires many samples and converges quite slowly.\n",
    "Run the code cell below to try your Q-Learning implementation on the full 8x8 version of the environment.\n",
    "\n",
    "The algorithm should reach a success rate around $60%$, but sometimes the success rate drops sharply during training.\n",
    "In my experimentation, the algorithm doesn't learn with `SEED = 6` (leave all other hyperparameters at their defaults).\n",
    "In addition, there are regions of the environment (e.g. the bottom left) where the Q function still differs from the optimal Q function computed in questions 1 and 2, with no sign that they are improving.\n",
    "Given this, it is reasonable to wonder if this algorithm will ever converge to the optimal policy.\n",
    "\n",
    "\\## SOLUTION ##\n",
    "\n",
    "Under what conditions (if any) is Q-Learning guaranteed to converge to the optimal policy?\n",
    "Are these conditions fulfilled in our implementation (assuming `nsteps` is set to a very large number)?\n",
    "Answer these questions below in 2-3 sentences (3 points).\n",
    "\n",
    "Yes, Q-Learning is guaranteed to converge to the optimal policy under some minimal conditions.\n",
    "These conditions are 1) that all state-action pairs are visited infinitely often, and 2) the learning rate must eventually become very small (but not too quickly).\n",
    "We could increase `nsteps` and `t_decay` to very large numbers to guarantee exploration, and the learning rate decreases linearly to $0$, so our implementation fulfills all these conditions.\n",
    "\n",
    "\\## END ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run QLearning on the 8x8 environment\n",
    "SEED = 1\n",
    "_ = q_learning(env, gamma=0.99, alpha_init=0.2, n_steps=400000, t_decay=200000, random_init=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
