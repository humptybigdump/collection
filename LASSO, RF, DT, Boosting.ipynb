{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b50700",
   "metadata": {},
   "source": [
    "## LASSO (least absolute shrinkage and selection operator):\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: center;\">\n",
    "<span style=\"font-family: 'Helvetica'; font-size: 14pt;\">\n",
    "    <b>In statistics and machine learning</b>, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a <b>regression analysis method</b> that performs both <b>variable selection</b> and <b>regularization</b> in order to enhance the <b>prediction accuracy</b> and <b>interpretability</b> of the resulting statistical model.\n",
    "</span>\n",
    "</div>\n",
    "\n",
    "### Statistics and Machine Learning: \n",
    "    These are two fields where Lasso is commonly used. Statistics is the science of making decisions using data, and machine learning is a field of artificial intelligence where systems can learn from data.\n",
    "\n",
    "### Lasso (Least absolute shrinkage and selection operator): \n",
    "    Lasso is a method used for prediction models, particularly in linear regression.\n",
    "\n",
    "### Regression analysis method: \n",
    "    Regression analysis is a statistical method used to understand the relationship between dependent and independent variables. In simple terms, you're trying to find a function that best fits your data.\n",
    "\n",
    "### Variable selection: \n",
    "    This is the process of selecting which variables to include in the model. This is important because not all variables contribute significantly to the prediction. Some variables may even make the model perform worse.\n",
    "\n",
    "### Regularization: \n",
    "    Regularization is a technique used to prevent overfitting, which is when a model learns the training data too well and performs poorly on unseen data. Regularization addresses overfitting by adding a penalty term to the loss function. This penalty discourages learning a more complex model and encourages learning a simpler model, thus reducing the risk of overfitting.\n",
    "    \n",
    "    There are several types of regularization, two of the most common ones being L1 and L2 regularization:\n",
    "\n",
    "    L1 Regularization (Lasso): Adds a penalty equivalent to the absolute value of the magnitude of the coefficients.\n",
    "\n",
    "    L2 Regularization (Ridge): Adds a penalty equivalent to the square of the magnitude of the coefficients.\n",
    "\n",
    "    In the context of linear regression, the loss function is typically the sum of squared residuals. Regularization adds a term to this function that penalizes large values of the coefficients. \n",
    "    \n",
    "    Lasso Regularization Formula\n",
    "\n",
    "    The objective of linear regression is to minimize the residual sum of squares (RSS), given by:\n",
    "\n",
    "    RSS = Σ (y_i - (α + Σ β_j * x_ij))^2\n",
    "\n",
    "    where:\n",
    "\n",
    "        y_i is the observed output,\n",
    "        α is the intercept,\n",
    "        β_j is the coefficient for variable j,\n",
    "        x_ij is the i-th observation on the j-th variable.\n",
    "        \n",
    "    The Lasso regression then introduces a penalty term to this objective, resulting in the following minimization objective:\n",
    "\n",
    "    Lasso = Σ (y_i - (α + Σ β_j * x_ij))^2 + λ Σ |β_j|\n",
    "\n",
    "    where:\n",
    "\n",
    "    λ is a tuning parameter that decides how much we want to penalize the flexibility of our model. The greater the value of λ, the greater is the penalization and simpler is the resulting model. This is the parameter we adjust to control overfitting.\n",
    "\n",
    "### Prediction accuracy: \n",
    "    This refers to how close the model's predictions are to the actual values. A model with high prediction accuracy is good because it means it makes predictions that are close to the actual values.\n",
    "\n",
    "### Interpretability: \n",
    "    This is about how well we can understand the model. An interpretable model is one where we can easily understand the relationship between the inputs and the output.\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5440afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b56ce9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset consists of 10 physiological variables (age, sex, weight, blood pressure) \n",
    "# measured on 442 patients, and an indication of disease progression after one year.\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "\n",
    "y = diabetes.target\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16c5d1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  \n",
       "0 -0.002592  0.019907 -0.017646  \n",
       "1 -0.039493 -0.068332 -0.092204  \n",
       "2 -0.002592  0.002861 -0.025930  \n",
       "3  0.034309  0.022688 -0.009362  \n",
       "4 -0.002592 -0.031988 -0.046641  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9789059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Coefficients:\n",
      "age     29.254013\n",
      "sex   -261.706469\n",
      "bmi    546.299723\n",
      "bp     388.398341\n",
      "s1    -901.959668\n",
      "s2     506.763241\n",
      "s3     121.154351\n",
      "s4     288.035267\n",
      "s5     659.268951\n",
      "s6      41.376701\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Linear Regression Coefficients:\")\n",
    "print(pd.Series(lr.coef_, index=X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d47f0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso Coefficients:\n",
      "age      0.000000\n",
      "sex   -102.887375\n",
      "bmi    554.780358\n",
      "bp     300.339972\n",
      "s1      -0.000000\n",
      "s2      -0.000000\n",
      "s3    -238.030096\n",
      "s4       0.000000\n",
      "s5     331.459416\n",
      "s6       0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# lasso regression\n",
    "lasso = Lasso(alpha=0.2)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nLasso Coefficients:\")\n",
    "print(pd.Series(lasso.coef_, index=X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9242179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Squared Error (Linear Regression):  2821.7509810013107\n",
      "Mean Squared Error (Lasso):  2806.108707743924\n"
     ]
    }
   ],
   "source": [
    "# compute mean squared error on test set for linear regression\n",
    "lr_pred = lr.predict(X_test)\n",
    "lr_mse = mean_squared_error(y_test, lr_pred)\n",
    "\n",
    "# compute mean squared error on test set for lasso\n",
    "lasso_pred = lasso.predict(X_test)\n",
    "lasso_mse = mean_squared_error(y_test, lasso_pred)\n",
    "\n",
    "print(\"\\nMean Squared Error (Linear Regression): \", lr_mse)\n",
    "print(\"Mean Squared Error (Lasso): \", lasso_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e82f4",
   "metadata": {},
   "source": [
    "## Ridge regression:\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: center;\">\n",
    "<span style=\"font-family: 'Helvetica'; font-size: 14pt;\">\n",
    "    Similar to LASSO regression, Ridge regression uses regularization to prevent overfitting. They both work by adding a penalty to the loss function that the model minimizes. The key differences between them lie in the form of this penalty.\n",
    "</span>\n",
    "</div>\n",
    "\n",
    "### Regularization: \n",
    "    Ridge Regression, on the other hand, uses L2 regularization. It adds a penalty equivalent to the square of the magnitude of coefficients. It minimizes the sum of the squared coefficients (||β||2^2).\n",
    "    \n",
    "### Feature Selection:\n",
    "    Lasso has the ability to perform feature selection. It can shrink some of the less important features' coefficients to zero, thus eliminating them from the model altogether. Ridge, however, doesn't have this feature selection property. It will shrink the coefficients towards zero, but it will not set any of them exactly to zero (unless the regularization parameter α is set to an extremely high value).\n",
    "    \n",
    "### Multicollinearity:\n",
    "    When faced with highly correlated (multicollinear) features, Ridge regression tends to distribute the coefficient load among them, while Lasso is likely to pick one and disregard the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bedaedb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Coefficients:\n",
      "age     29.254013\n",
      "sex   -261.706469\n",
      "bmi    546.299723\n",
      "bp     388.398341\n",
      "s1    -901.959668\n",
      "s2     506.763241\n",
      "s3     121.154351\n",
      "s4     288.035267\n",
      "s5     659.268951\n",
      "s6      41.376701\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Linear Regression Coefficients:\")\n",
    "print(pd.Series(lr.coef_, index=X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a000bbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge Coefficients:\n",
      "age     39.663529\n",
      "sex   -213.846880\n",
      "bmi    505.914292\n",
      "bp     341.714474\n",
      "s1    -108.806301\n",
      "s2     -70.575810\n",
      "s3    -211.906580\n",
      "s4     160.193540\n",
      "s5     332.773542\n",
      "s6      77.680452\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ridge regression\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nRidge Coefficients:\")\n",
    "print(pd.Series(ridge.coef_, index=X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38455cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Squared Error (Linear Regression):  2821.7509810013107\n",
      "Mean Squared Error (Ridge):  2805.401298319341\n"
     ]
    }
   ],
   "source": [
    "# compute mean squared error on test set for linear regression\n",
    "lr_pred = lr.predict(X_test)\n",
    "lr_mse = mean_squared_error(y_test, lr_pred)\n",
    "\n",
    "# compute mean squared error on test set for ridge\n",
    "ridge_pred = ridge.predict(X_test)\n",
    "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
    "\n",
    "print(\"\\nMean Squared Error (Linear Regression): \", lr_mse)\n",
    "print(\"Mean Squared Error (Ridge): \", ridge_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bfa244",
   "metadata": {},
   "source": [
    "## Decision Tree:\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: center;\">\n",
    "<span style=\"font-family: 'Helvetica'; font-size: 14pt;\">\n",
    "    Decision Trees are a type of supervised learning algorithm that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In the decision tree, we split the population or sample into two or more homogeneous sets (or sub-populations) based on the most significant splitter/differentiator in input variables.\n",
    "</span>\n",
    "</div>\n",
    "\n",
    "### Selection of the Best Attribute:: \n",
    "    The first step is to select the best attribute for the root node. This is done using different attribute selection measures like Information Gain, Gini Index, Chi-Square, etc.\n",
    "    \n",
    "### Tree Creation:\n",
    "    After finding the root node, the data is divided into subsets. The subsets should be created in such a way that each subset contains data with the same value for an attribute. This process is repeated for each branch (recursive partitioning).\n",
    "    \n",
    "### Stop Condition:\n",
    "    This process of recursive partitioning might result in a tree with too many branches and depth, leading to overfitting. So we must set a stopping criteria to stop the tree growth. This could be a certain depth is reached, minimum samples at a node, etc.\n",
    "    \n",
    "### DecisionTreeRegressor:\n",
    "    A Machine Learning model that uses a decision tree to predict a continuous outcome variable (regression). It belongs to the family of supervised learning algorithms.\n",
    "    \n",
    "### How it works:\n",
    "    The DecisionTreeRegressor, like all decision trees, works by splitting the dataset into distinct subsets based on certain conditions. The goal is to create subsets that are as pure as possible, meaning that the instances in each subset should have similar target values. The decision tree algorithm accomplishes this by selecting the conditions that produce the most significant improvement in purity for each split. This process is repeated recursively until a stopping condition is met.\n",
    "    \n",
    "    For regression, the purity or quality of a split is usually measured by the reduction in variance, which is the squared standard deviation of the target value. The variance is calculated as follows:\n",
    "    \n",
    "    1) For each feature, the dataset is sorted by that feature's values.\n",
    "    2) Then for each unique value, the dataset is split into two: one subset with instances whose feature value is below or equal to the threshold, and one with instances above that threshold.\n",
    "    3) The variance of the target values in each subset is calculated and combined in a weighted sum, where the weights are the proportions of instances in each subset.\n",
    "    4) The best split is the one that has the smallest weighted variance.\n",
    "    5) The algorithm keeps track of the feature and threshold that produces the best split.\n",
    "    6) The process is repeated recursively for each subset until a stopping condition is met, such as reaching a maximum depth, or if the improvement in variance reduction falls below a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5383bb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Squared Error (Linear Regression):  2821.7509810013107\n",
      "Mean Squared Error (Decision Tree Regressor):  3616.769894653006\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor(random_state=42, max_depth=3)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "\n",
    "\n",
    "print(\"\\nMean Squared Error (Linear Regression): \", mse_lr)\n",
    "print(\"Mean Squared Error (Decision Tree Regressor): \", mse_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194778df",
   "metadata": {},
   "source": [
    "## Random Forest Regression:\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: center;\">\n",
    "<span style=\"font-family: 'Helvetica'; font-size: 14pt;\">\n",
    "    Random Forest Regression is a machine learning algorithm that employs an ensemble of decision trees to perform regression tasks. \"Ensemble\" here means that it uses multiple learning models internally, and the final prediction is made by averaging the predictions of each individual model.\n",
    "</span>\n",
    "</div>\n",
    "\n",
    "### Bootstrapping: \n",
    "    Random Forest starts by picking 'n' random subsets from the original dataset, with replacement (meaning the same instance can be selected multiple times). This technique is called bootstrapping. Each subset is then used to train a separate decision tree.\n",
    "    \n",
    "### Random Feature Selection:\n",
    "    At each node in each decision tree, Random Forest selects a random subset of features rather than using all features to determine the best split. This randomness ensures that the trees are de-correlated and makes the model more robust against overfitting.\n",
    "    \n",
    "### Training and Prediction:\n",
    "    Each tree is grown to the maximum depth and makes its prediction independently. The final prediction of the Random Forest is the average of the predictions of all trees for regression tasks.\n",
    "    \n",
    "###    Differences to DecisionTreeRegressor:\n",
    "\n",
    "    1) Ensemble vs Single Model: The fundamental difference between a RandomForestRegressor and a DecisionTreeRegressor is that the former is an ensemble of decision trees, whereas the latter is a single decision tree. This means a RandomForestRegressor can capture more complexity than a single DecisionTreeRegressor.\n",
    "\n",
    "    2) Overfitting: Decision trees are notorious for overfitting to the training data, especially if the trees are allowed to grow too deep or complex. Random forests, however, are less prone to overfitting. This is because each tree in the ensemble is trained on a different subset of the data and uses a random subset of features to make splits. The randomness ensures that each tree is somewhat different, and the final prediction, which is an average of all the trees, is less likely to be influenced by the noise or fluctuations in the data.\n",
    "\n",
    "    3) Variance and Bias: RandomForestRegressor generally has lower variance and higher bias than a single DecisionTreeRegressor. This is because averaging the predictions of many trees can reduce the variance (at the cost of a slight increase in bias), leading to a better overall model.\n",
    "\n",
    "    4)Interpretability: Decision trees are relatively simple to interpret because you can visualize the entire tree structure and follow the decision path. Random forests, on the other hand, are more challenging to interpret due to the large number of trees. However, they offer feature importance metrics, which can provide insights about which features have contributed the most to the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27855f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Linear Regression): 2821.7509810013107\n",
      "Mean Squared Error (Random Forest Regressor): 2737.34564593494\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, max_depth = 3)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Compare Mean Squared Errors\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"Mean Squared Error (Linear Regression): {mse_lr}\")\n",
    "print(f\"Mean Squared Error (Random Forest Regressor): {mse_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c4cf09",
   "metadata": {},
   "source": [
    "## Boosting:\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<div style=\"max-width: 800px; margin: 0 auto; text-align: center;\">\n",
    "<span style=\"font-family: 'Helvetica'; font-size: 14pt;\">\n",
    "    Boosting is a type of ensemble learning technique that aims to convert a set of weak learners into a single strong learner. A weak learner is a model that performs relatively poorly: its accuracy is above chance, but just barely. Boosting algorithms iteratively train a sequence of weak learners, with each iteration aiming to correct the mistakes made by the previous learners. The idea is that by combining many weak models, we can produce a powerful, robust model that has lower bias and variance. Here are a few popular boosting methods:\n",
    "</span>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### AdaBoost (Adaptive Boosting): \n",
    "    This is one of the first and most simple boosting algorithms. Each new weak learner is trained with a weighted version of the data, where the weights depend on the errors of the previous weak learner: instances that were misclassified by the previous learner are given more weight, while instances that were correctly classified are given less weight. This encourages the new weak learner to focus more on the harder cases.\n",
    "\n",
    "### Gradient Boosting: \n",
    "    This approach is more general than AdaBoost. Instead of tweaking instance weights, Gradient Boosting fits the new weak learner to the residual errors made by the previous learner. Then it adds this new learner into the ensemble, effectively taking a step in the direction that minimizes the overall training loss (hence the \"gradient\" in \"gradient boosting\"). This can be used with any differentiable loss function, not just the one used by AdaBoost.\n",
    "\n",
    "### XGBoost (Extreme Gradient Boosting): \n",
    "    XGBoost is a more efficient and flexible version of gradient boosting. It uses a more regularized model formalization to control overfitting, which gives it better performance. It also includes several useful features for handling missing values and making the algorithm computationally efficient.\n",
    "\n",
    "### LightGBM: \n",
    "    This is a gradient boosting framework that uses tree-based learning algorithms and follows the leaf-wise approach. It differs from other tree-based algorithms by choosing to split the tree leaf-wise while other algorithms do it level-wise. It can handle the large size of data and takes lower memory to run.\n",
    "\n",
    "### CatBoost: \n",
    "    CatBoost can automatically deal with categorical variables without showing the type conversion error, which helps you to focus on tuning your model better rather than sorting out trivial errors. It yields state-of-the-art results without extensive data training typically required by other machine learning methods, and it provides powerful out-of-the-box support for the more descriptive data formats that accompany many business problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e19c0d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Linear Regression): 2821.7509810013107\n",
      "Mean Squared Error (XGBoost Regressor): 3582.7540439353597\n"
     ]
    }
   ],
   "source": [
    "xgbr = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators = 1000, seed = 42)\n",
    "xgbr.fit(X_train, y_train)\n",
    "y_pred_xgb = xgbr.predict(X_test)\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"Mean Squared Error (Linear Regression): {mse_lr}\")\n",
    "print(f\"Mean Squared Error (XGBoost Regressor): {mse_xgb}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
