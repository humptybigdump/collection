{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial Data Analysis I \n",
    "\n",
    "## Inferential and bivariate statistics - Exercise\n",
    "\n",
    "###  Exercise 1: Inferential statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will take a closer look at the groundwater data from the last lesson, and infer information about the entire population in the area of the Hardtwald forest based on the measured samples. \n",
    "\n",
    "First, read in the dataset 'Data_GW_KA.csv' (or the corresponding excel file). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last exercise we  already characterised the parameter groundwater temperature, which roughly had the shape of a normal distribution. Now, let's assume you need 50 representative temperature values for some modelling exercise (but you only have 39 measurements). \n",
    "\n",
    "In order to generate these 50 values, you can fit a normal distribution to the measured values, and then use Python to create the desired number of random (but representative) values, which should have the same statistical characteristics as the measured data. To do so, you first need to find out which type of normal distribution (mean, std) fits best to the measured data. \n",
    "\n",
    "- Use the function `scipy.stats.norm.fit()` to define two outputs ($\\mu$, $\\sigma$) based on the groundwater tempertaure data as input. \n",
    "\n",
    "- Print the mean and standard deviation to inspect them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function `scipy.stats.norm.rvs()` you can now generate a specific number of normally distributed values. As inputs for the function you need to specific the mean, the standard deviation, and the number of values (e.g. 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, have a look at the generated values, especially at their mean value and standard deviation. Are they identical to the empirical values from the measured samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most likely the values for mean and standard deviation differ slightly to the empirical ones. This might be due to the small number (n=50) of generated values. \n",
    "\n",
    "- Generate another set of random values, now with n = 500,000 (German principle: \"viel hilft viel!\")\n",
    "\n",
    "- Then compare the new mean and standard deviation again to the empirical measures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean value and standard deviation should now be close to the empirically derived values. However, the large number of generated values leads to a different issue. \n",
    "\n",
    "- Calculate the minimum and maximum value of the generated 500,000 values. Are they physically reasonable, given the range of the measured data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the measured data these values seem too small and too high, respectively. The reason is that with this large number of randomly generated values,  some values at the extremely unlikely tails of the distributions get picked as well. \n",
    "\n",
    "You can avoid this by working with truncated distributions. The function `scipy.stats.truncnorm.rvs()` can generate such distributions using two scaling parameters (*a*, *b*) to cut off extreme values: \n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?a&space;=&space;(minimum&space;-&space;mean)/&space;std\" title=\"a = (minimum - mean)/ std\" />\n",
    "\n",
    "und \n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?b&space;=&space;(maximum&space;-&space;mean)/&space;std\" title=\"b = (maximum - mean)/ std\" />\n",
    "\n",
    "- Think of a resonable minimum and maximum value for your theoretical distribution of groundwater temperatures, and calculate *a* and *b* accordingly.\n",
    "\n",
    "- Use them in `scipy.stats.truncnorm.rvs()`to generate 500,000 random values, and check the statistical characteristic of those values (e.g. using `scipy.stats.describe()`). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [7]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new values are much more reasonable to represent groundwater temperatures than the un-truncated ones, and could now be used for further modelling / analyses. \n",
    "\n",
    "There are also functions to fit other theoretical distributions beside a normal distribution to exisiting data. You can find an overview on available distributions in `scipy` here: https://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "\n",
    "To visusalise the measured data and the fitted distribution, we can now plot them using `matplotlib`. \n",
    "\n",
    "- First, generate a sequence x-values to plot the probability density function on using `numpy.linspace()`\n",
    "\n",
    "- Then, calculate the corresponding y-values using `scipy.stats.norm.pdf(x, mean, variance)` with the estimated mean and variance from above. \n",
    "\n",
    "- Finally, plot the probability density function as a line (`plt.plot(x, y)`), and the measured data as a histogram (`pyplot.hist(data, density =True)`) into the same plot. The argument `density = True` ensures that the measured values of your histogram are normalized to the probability (instead of the frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [8]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Bivariate statistics\n",
    "\n",
    "After looking at one individual parameter above, we now analyse the relationships between two measured variables. \n",
    "\n",
    "First, read in the shortened data set on groundwater in Karlsruhe from Koch et al. (2020) ('Data_GW_KA_short.csv') as a dataframe using pandas. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explorative Data Analysis: Histograms and scatterplots\n",
    "\n",
    "If you are confronted with a new data set, the first step is to get an overview on the measured variables, their values and some basic characteristics. Also, it is recommended to always visualize the data, e.g. with histograms for each parameter, as this is the easiest way to identify outliers, patterns in the data, etc. If interested in the relationship between two variables, a scatterplot is good option. \n",
    "\n",
    "The Python package `seaborn` has a very useful function (`seaborn.jointplot()`) to plot the scatterplot of two variables, and the histograms of the marginal distribution in one go. `seaborn` is a Python package similar to `matplotlib`, which is also contained in Anaconda and offers speficic functionalities for visualising data. \n",
    "\n",
    "- Import `seaborn` with the abbreviation 'sns', and use `seaborn.jointplot(data=dataframe, x=variable1, y=variable2)` to visualise two measured variables (free choice) from the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an quick overview on all bivariate relations in the entire data set, there is the function `seaborn.pairplot()` that combines histograms and scatterplots for all variables. \n",
    "\n",
    "- Apply the function `seaborn.pairplot()` to the groundwater data set. Depending on the computational capacity of your laptop this might take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [11]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation coefficients\n",
    "\n",
    "Now we would like to quantify the relationship between individual variables. The basic measure for this is the covariance: \n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?cov_{xy}&space;=&space;\\frac{1}{1-n}\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})\" title=\"cov_{xy} = \\frac{1}{1-n}\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})\" />\n",
    "\n",
    "To calculate the covariance for all variable pairs in a Pandas DataFrame, you can use the function `pandas.DataFrame.cov()`. \n",
    "\n",
    "- First, remove the columns with the well names from the data set to avoid any errors resulting from the string data type `DataFrame.drop(column = 'Name')`. \n",
    "\n",
    "- Then calculate the covariances using `DataFrame.cov()`, and print the resulting covariance matrix. \n",
    "\n",
    "- Interpret the shown values with respect to the strength of the bivariate relationship, and compare the covariances with the visual impressression form the figure above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [12]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strength of the individual relations is not easy to interpret because the values of the variables and thus their (co)variances vary quite a lot. Dividing the covariance by the product of the individual standard deviations solves this problem, and returns Pearson's correlation coefficient. \n",
    "\n",
    "- Use the Pandas function `DataFrame.corr()` to calculate the correlation coefficient matrix for your data set, and print it. \n",
    "\n",
    "- Compare the results to the covariance matrix and the pairplot above. Which is the variable pair with the strongest linear relationship?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [13]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the correlation coefficients in this dataset are quite small. One reason might be that the relations are not strictly linear, but more complex (see pairplot above). \n",
    "\n",
    "- Use `scipy.stats.spearmanr(x, y)` to calculate Spearman's correlation coefficient for a chosen variable pair. This function outputs the correlation coefficient (statistic) and the p-value (measure for the statistical significance, more on that later...)\n",
    "\n",
    "- Compare the Spearman coefficient to the one from Pearson above. Do the values differ, and what does this mean for the kind of bivariate relationship?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [14]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned on the slides, there is a third commonly-used correlation coefficient, which can also be used for discrete and ordinal data. \n",
    "\n",
    "- Pick a suitable variable pair from the dataset, and use `scipy.stats.kendalltau()` to calculate Kendall's correlation coefficient. \n",
    "\n",
    "- Check for differences between Pearson's, Spearman's and Kendall's correlation coefficient and interpret them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice way to visualise matrix values (e.g. from a correlation matrix) is a heatmap plot. \n",
    "\n",
    "- Use `seaborn.heatmap(data)` to visualise the correlation matrix from Pandas above. By adding the argument `annot=True` you can plot the numbers of the coefficients as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [16]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is time left you can visit the seaborn webpage (https://seaborn.pydata.org/examples/index.html) to see more examples of data visualisation. \n",
    "\n",
    "### END\n",
    "\n",
    "#### References: \n",
    "\n",
    "Koch et al. (2020), Groundwater fauna in an urban area: natural or affected? https://hess.copernicus.org/preprints/hess-2020-151/hess-2020-151.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDA1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
