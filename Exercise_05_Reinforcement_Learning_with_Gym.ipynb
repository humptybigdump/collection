{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxgvN1YlSnxK"
   },
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/300px-Reinforcement_learning_diagram.svg.png).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Reinforcement Learning is a special form of machine learning, where an agent interacts with an environment, conducts observations on the effects of actions and collects rewards.\n",
    "\n",
    "The goal of reinforcement learning is to learn an optimal policy, so that given a state an agent is able to decide what it should do next.\n",
    "\n",
    "In this exercise we will look into one of  fundamental algorithms that are capable of solving MDPs, namely [Policy Iteration](https://en.wikipedia.org/wiki/Markov_decision_process#Policy_iteration).\n",
    "\n",
    "By the time you complete this lab, you should know:\n",
    "\n",
    "- The relevant pieces for a reinforcement learning system\n",
    "- The basics of *[gym](https://gym.openai.com/envs/#classic_control)* to conduct your own RL experiments\n",
    "- Why Policy Iteration can be slower than Value Iteration\n",
    "- The differences of value and policy iteration compared with Q-Learning\n",
    "- How Q-Learning converges towards a stable policy\n",
    "    - Some optional extensions to Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP\n",
    "\n",
    "A Markov decision process is a 4-tuple $(S,A,P_{a},R_{a})$\n",
    "\n",
    "![MPD](mdp.png \"MDP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvdmBl8GajjF"
   },
   "source": [
    "## Problem\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. (However, the ice is slippery, so you won't always move in the direction you intend.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wYUHIokU_EI"
   },
   "source": [
    "## Setup\n",
    "\n",
    "To begin we'll need to install all the required python package dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjQ08kksR2c2"
   },
   "outputs": [],
   "source": [
    "#!pip install --quiet gym stabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8MH3Ij6rAL_z"
   },
   "source": [
    "### Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWdytOiH-LFr"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZgQh5-QCBeDI"
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import random\n",
    "import heapq\n",
    "import collections\n",
    "\n",
    "# Reinforcement Learning environments\n",
    "import gym\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GzgwlDeZhfxU"
   },
   "source": [
    "\n",
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mi8myW9Wheef"
   },
   "outputs": [],
   "source": [
    "# Define the default figure size\n",
    "plt.rcParams['figure.figsize'] = [16, 4]\n",
    "\n",
    "def create_numerical_map(env):\n",
    "    \"\"\"Convert the string map of the environment to a numerical version\"\"\"\n",
    "    numerical_map = np.zeros(env.env.desc.shape)\n",
    "    i = 0\n",
    "    for row in env.env.desc:\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            if col.decode('UTF-8') == 'S':\n",
    "                numerical_map[i, j] = 0\n",
    "            elif col.decode('UTF-8') == 'G':\n",
    "                numerical_map[i, j] = 1\n",
    "            elif col.decode('UTF-8') == 'F':\n",
    "                numerical_map[i, j] = 2\n",
    "            elif col.decode('UTF-8') == 'H':\n",
    "                numerical_map[i, j] = 3\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return numerical_map\n",
    "\n",
    "\n",
    "def visualize_env(env):\n",
    "    \"\"\"Plot the environment\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    # Hide grid lines\n",
    "    ax.grid(False)\n",
    "    # Hide axes ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('The frozen Lake')\n",
    "    i = ax.imshow(create_numerical_map(env), cmap=cm.jet)\n",
    "    plt.show()\n",
    "    print('the start is blue, holes are red, ice is yellow and the goal is teal')\n",
    "\n",
    "def visualize_policy(env, policy, ax=None, title=None):\n",
    "    \"\"\"Plot the policy in the environment\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    font_size = 10 if env.observation_space.n > 16 else 20\n",
    "    i = 0\n",
    "    for row in env.env.desc:\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            s = i * env.env.desc.shape[0]+j\n",
    "            if policy[s] == 0:\n",
    "                ax.annotate(\"L\", xy=(j, i), xytext=(j, i), ha=\"center\",\n",
    "                            va=\"center\", size=font_size, color=\"white\")\n",
    "            elif policy[s] == 1:\n",
    "                ax.annotate(\"D\", xy=(j, i), xytext=(j, i), ha=\"center\",\n",
    "                            va=\"center\", size=font_size, color=\"white\")\n",
    "            elif policy[s] == 2:\n",
    "                ax.annotate(\"R\", xy=(j, i), xytext=(j, i), ha=\"center\",\n",
    "                            va=\"center\", size=font_size, color=\"white\")\n",
    "            elif policy[s] == 3:\n",
    "                ax.annotate(\"U\", xy=(j, i), xytext=(j, i), ha=\"center\",\n",
    "                            va=\"center\", size=font_size, color=\"white\")\n",
    "            j += 1\n",
    "        i += 1\n",
    "\n",
    "    # Hide grid lines\n",
    "    ax.grid(False)\n",
    "    # Hide axes ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    if title is None:\n",
    "        ax.set_title('Policy for the Frozen Lake')\n",
    "    else:\n",
    "        ax.set_title(title)\n",
    "    ax.imshow(create_numerical_map(env), cmap=cm.jet)\n",
    "    return\n",
    "\n",
    "\n",
    "def visualize_v(env, v, ax=None, title=None):\n",
    "    \"\"\"Plot value function values in the environment\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    font_size = 10 if env.observation_space.n > 16 else 20\n",
    "    i = 0\n",
    "    for row in env.env.desc:\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            s = i * env.env.desc.shape[0]+j\n",
    "            ax.annotate(\"{:.2f}\".format(v[s]), xy=(j, i), xytext=(j, i), ha=\"center\",\n",
    "                        va=\"center\", size=font_size, color=\"white\")\n",
    "            j += 1\n",
    "        i += 1\n",
    "\n",
    "    # Hide grid lines\n",
    "    ax.grid(False)\n",
    "    # Hide axes ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    if title is None:\n",
    "        ax.set_title('State Value Function for the Frozen Lake')\n",
    "    else:\n",
    "        ax.set_title(title)\n",
    "    ax.imshow(create_numerical_map(env), cmap=cm.jet)\n",
    "    return\n",
    "\n",
    "\n",
    "def compute_v_from_q(env, q):\n",
    "    \"\"\"Compute the v function given the q function, maximizing over the actions of a given state.\"\"\"\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    i = 0\n",
    "    for row in env.env.desc:\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            s = i * env.env.desc.shape[0]+j\n",
    "            v[s] = np.max(q[s, :])\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return v\n",
    "\n",
    "def compute_policy_from_q(env, q):\n",
    "    \"\"\"Compute the policy function given the q function, finding the action that yields the maximum of a given state.\"\"\"\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    i = 0\n",
    "    for row in env.env.desc:\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            s = i * env.env.desc.shape[0]+j\n",
    "            policy[s] = np.argmax(q[s, :])\n",
    "            j += 1\n",
    "        i += 1![MPD](mdp.png \"MDP\")\n",
    "    return policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rTC-P1vd-5-y"
   },
   "source": [
    "#### Frozen Lake Environments\n",
    "\n",
    "![Frozen Lake](frozen_lake.gif \"Frozen Lake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sknx1oOiaL7J"
   },
   "outputs": [],
   "source": [
    "# register variants of the frozen lake without execution uncertainty i.e. deterministic environments\n",
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.78,  # optimum = .8196\n",
    ")\n",
    "\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery8x8-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '8x8', 'is_slippery': False},\n",
    "    max_episode_steps=200,\n",
    "    reward_threshold=0.99,  # optimum = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FH2aMLY_jrjQ"
   },
   "outputs": [],
   "source": [
    "def evaluate_episode(env, policy, discount_factor):\n",
    "    \"\"\"Evaluates a policy by running it until termination and collect its reward\"\"\"\n",
    "    state = env.reset()\n",
    "    total_return = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        state, reward, done, _ = env.step(int(policy[state]))\n",
    "        # Calculate the total\n",
    "        total_return += (discount_factor ** step * reward)\n",
    "        step += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_return\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, discount_factor=0.95, number_episodes=1000):\n",
    "    \"\"\" Evaluates a policy by running it n times\"\"\"\n",
    "    return np.mean([evaluate_episode(env, policy, discount_factor) for _ in range(number_episodes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy and Value Iteraton Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxwwTshweK8i"
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "max_iterations = 1000\n",
    "discount_factor = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BzfpVLxA-T4W"
   },
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gsl3GswnX1I6"
   },
   "outputs": [],
   "source": [
    "# Deterministic environments\n",
    "env_name = 'FrozenLakeNotSlippery-v0'\n",
    "#env_name = 'FrozenLakeNotSlippery8x8-v0'\n",
    "\n",
    "# Stochastic environments\n",
    "#env_name = 'FrozenLake-v0'\n",
    "#env_name = 'FrozenLake8x8-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8WwK53WADNp"
   },
   "source": [
    "Create the environment with the previously selected name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "ARVwYFcHAB78",
    "outputId": "2f4c131f-cca2-4d82-ebc4-12cfa87f5890",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "env.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S : starting point, safe  \n",
    "F : frozen surface, safe  \n",
    "H : hole, fall to your doom  \n",
    "G : goal, where the frisbee is located  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generated the frozen lake with config: ' + env_name)\n",
    "visualize_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Environment (Object)\n",
    "\n",
    "**TASK :**\n",
    "Analyze the environment object and figure out its *observation-* and *actionspace* as well as its *reward range*.\n",
    "\n",
    "What is the size of the observation space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size of the action space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the range of rewards?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward_range"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The episode ends when you reach the goal or fall in a hole.  \n",
    "You receive a reward of 1 if you reach the goal, and zero otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-pzYcAtuiHJ9"
   },
   "source": [
    "### Uncertainty in Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "F5OmhQ8sVLHK",
    "outputId": "167f15d0-f60c-43af-ebee-14e265d552bd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "print(\"the initial state is: {}\".format(s))\n",
    "env.render()\n",
    "\n",
    "# The agent should go down\n",
    "print(\"executing action 1, should go down\")\n",
    "s1, r, d, _ = env.step(1)\n",
    "print(\"new state is: {} done: {}\".format(s1, d))\n",
    "env.render()\n",
    "\n",
    "# The agent should go up\n",
    "print(\"executing action 3, should go up\")\n",
    "s1, r, d, _ = env.step(3)\n",
    "print(\"new state is: {} done: {}\".format(s1, d))\n",
    "env.render()\n",
    "\n",
    "# The agent should go right\n",
    "print(\"executing action 2, should go right\")\n",
    "s1, r, d, _ = env.step(2)\n",
    "print(\"new state is: {} done: {}\".format(s1, d))\n",
    "env.render()\n",
    "\n",
    "# The agent should go left\n",
    "print(\"executing action 0, should go left\")\n",
    "s1, r, d, _ = env.step(0)\n",
    "print(\"new state is: {} done: {}\".format(s1, d))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, env, discount_factor, mode):\n",
    "    \"\"\" Iteratively evaluate the value function under the given policy\"\"\"\n",
    "    # Initialize the state value function\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.env.nS):\n",
    "            if mode == \"policy\":\n",
    "                v[s] = evaluate_action(s, v, prev_v, policy, env, discount_factor)\n",
    "            elif mode == \"optimal_policy\":\n",
    "                v[s] = evaluate_max_action(s, v, prev_v, env, discount_factor)\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= 1e-4):\n",
    "            break\n",
    "    return v, iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_action(s, v, prev_v, policy, env, discount_factor):\n",
    "    # Retrieve the action under the current policy\n",
    "    a = policy[s]\n",
    "    expected_reward = 0\n",
    "    expected_discounted_return = 0\n",
    "    # Calculate the expected reward and the expected discounted return | p = probability\n",
    "    for p, s1, r, _ in env.env.P[s][a]:\n",
    "        ### TASK: define the expected_reward and the expected_discounted_return\n",
    "        expected_reward += \n",
    "        expected_discounted_return += \n",
    "    # Calculate the V-Value\n",
    "    return expected_reward + expected_discounted_return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: \n",
    "\n",
    "```python:\n",
    "expected_reward += p*r\n",
    "expected_discounted_return += discount_factor*p*prev_v[s1]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policy $\\pi^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_max_action(s, v, prev_v, env, discount_factor):\n",
    "    # Initialize the action value function\n",
    "    q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    # Iterate over each action\n",
    "    for a in range(env.action_space.n):\n",
    "        expected_reward = 0\n",
    "        expected_discounted_return = 0\n",
    "        # Calculate the expected reward and the expected discounted return | p = probability\n",
    "        for p, s1, r, _ in env.env.P[s][a]:\n",
    "            ### TASK: define the expected_reward and the expected_discounted_return\n",
    "            expected_reward += \n",
    "            expected_discounted_return += \n",
    "        # Calculate the Q-Value\n",
    "        q[s, a] = expected_reward + expected_discounted_return\n",
    "    ### TASK: define the value function with respect to q\n",
    "    # Choose the max Q-Value over all actions\n",
    "    return np.max(q[s, :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: \n",
    "\n",
    "```python:\n",
    "expected_reward += p*r\n",
    "expected_discounted_return += discount_factor*p*prev_v[s1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(v, policy, env, discount_factor):\n",
    "    \"\"\" Improve the policy given a value-function \"\"\"\n",
    "    # Initialize the policy\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    # Initialize the action value function\n",
    "    q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    for s in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            q[s,a] = np.sum([p * (r + discount_factor * v[s1]) for p, s1, r, _ in  env.env.P[s][a]])\n",
    "        policy[s] = np.argmax(q[s,:])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CASyoXI9jAZW"
   },
   "source": [
    "## Policy Iteration\n",
    "![Policy Iteration](policy_iteration.png \"Policy Iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "**TASK :**\n",
    "Add the missing steps for the policy iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "B1PWKWKVjQbI",
    "outputId": "5f500ce3-d8d6-49d1-ce4c-aa77b0024a80",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def policy_iteration(env, discount_factor, max_iterations):\n",
    "    \"\"\" Policy-Iteration algorithm \"\"\"\n",
    "    # Initialize the policy\n",
    "    policy = np.zeros(env.observation_space.n)*2\n",
    "    for i in range(max_iterations):\n",
    "        # TASK: evaluate the current policy\n",
    "        v, iteration = \n",
    "        # TASK: define the new policy\n",
    "        new_policy = \n",
    "        if (np.all(policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at iteration #{:d}'.format((i)))\n",
    "            break\n",
    "        # Plot the current policy\n",
    "        title_p = 'Policy Improvement #{:d}'.format((i+1))\n",
    "        title_v = '#Policy Evaluations {:d}'.format(iteration)\n",
    "        fig, ax = plt.subplots(1,2)\n",
    "        visualize_v(env, v, ax[0], title_v)\n",
    "        visualize_policy(env, new_policy, ax[1], title_p)\n",
    "        policy = new_policy\n",
    "    return policy, v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: \n",
    "\n",
    "```python:\n",
    "# TASK: evaluate the current policy\n",
    "v, iteration = policy_evaluation(policy, env, discount_factor, \"policy\")\n",
    "# TASK: define the new policy\n",
    "new_policy = policy_improvement(v, policy, env, discount_factor)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm and evaluate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Determine the optimal value function and policy given the model of the environment\n",
    "policy_opt, v_opt = policy_iteration(env, discount_factor, 1000)\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "# Evalutate the found value function and policy given the model of the environment\n",
    "policy_return = evaluate_policy(env, policy_opt, discount_factor, num_episodes)\n",
    "print('Average return of the policy: {:.2f}'.format(policy_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6wYUHIokU_EI",
    "8MH3Ij6rAL_z",
    "JWdytOiH-LFr",
    "GzgwlDeZhfxU",
    "rTC-P1vd-5-y",
    "-pzYcAtuiHJ9",
    "zhrrLKXk0ElG",
    "CASyoXI9jAZW",
    "lU4gmOQcAjR_",
    "4CdfVP4DilJf",
    "wK6bzLs_iqeG",
    "5KUNPRHdAstO",
    "tny1fTdaIkR6"
   ],
   "name": "Exercise 04 - Reinforcement Learning with Gym and Pytorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "fanka_panda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a4f8acac8e3d997faba56f7fe806aa29f4cf031bc3e767e58d3bbbcc655e173b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
