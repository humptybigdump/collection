{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cover_e3.png\" style=\"width:800px;height:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we will see:\n",
    "* A good Initialization will help you to train your network.\n",
    "* Regularization will help you reduce overfitting.\n",
    "* Regularization will drive your weights to lower values.\n",
    "* L2 regularization and Dropout are two very effective regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Regularization is a technique that helps machine learning models generalize better by making modifications in the learning algorithm. This helps prevent overfitting and allows our model to work better on data that it hasn't seen during training. In this Notebook, we will learn about the different regularize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries:\n",
    "first we will import all the packages that are required for this exercise. \n",
    "- [numpy](www.numpy.org) is the main package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs in Python.\n",
    "- np.random.seed(1) is used to keep all the random function calls consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, run the following cell to load the packages and the planar dataset you will try to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_2D_dataset\n",
    "train_X, train_Y, test_X, test_Y = load_2D_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation \n",
    "In this notebook, we will train a Neural Network with two hidden layers and ReLU as an activation function.\n",
    "\n",
    "**Here is our neural network**:\n",
    "<img src=\"NN_e3.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "Because we implemented a lot of the required functions to train a neural network and many activation functions and them derivatives in the second exercise, we will not implement them again in this exercise. Still, we will import them from `utils.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import backward_propagation  # an Implementation for the backpropagation for the network in this exercise\n",
    "from utils import update_parameters     # Optimize the parameters of the network using the gradient\n",
    "from utils import predict_dec           # make an inference\n",
    "from utils import sigmoid, relu         # activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    z1 = np.dot(W1, X) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = relu(z2)\n",
    "    z3 = np.dot(W3, a2) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    # Test if the shape correct is\n",
    "    assert(a3.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n",
    "    \n",
    "    return a3, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model's parameters:\n",
    "We will begin to use random weights that we will optimize using backward propagation. The number of parameters we have to initialize is \n",
    "\n",
    "**Question:**  why we divide the initial weights by $n^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims, init_method =\"xavier\"):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "    if init_method == \"xavier\":\n",
    "        for l in range(1, L):\n",
    "            parameters['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) / np.sqrt(layer_dims[l - 1])\n",
    "            parameters['b'+str(l)] = np.zeros((layer_dims[l], 1))\n",
    "            \n",
    "    elif init_method == \"zeros\":\n",
    "        for l in range(1, L):\n",
    "            parameters['W' + str(l)] = np.zeros((layer_dims[l], layer_dims[l-1]))\n",
    "            parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  L2 Regularization\n",
    "The standard way to avoid overfitting is called L2 regularization. It consists of appropriately modifying your cost function, from: $$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}$$ To: $$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n",
    "\n",
    "Here, λ is the regularization parameter.\n",
    "\n",
    "<img src=\"rg.png\" style=\"width:450px;height:250px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Loss function: \n",
    "we will implement the loss function with equation 1, this function takes the output of forward_propagation and the true label as input and returns the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(a3, Y):\n",
    "    m = Y.shape[1]\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n",
    "    loss = 1./m * np.nansum(logprobs)\n",
    "    ### END CODE HERE ###\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Implement the loss function with equation 2, using `compute_loss`. This function takes the output of forward_propagation and the true label as input and returns the loss.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(a3, Y, parameters, lambd):\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    \n",
    "    cross_entropy_cost = compute_cost(a3, Y) # This gives you the cross-entropy part of the cost\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 1 line)\n",
    "    L2_regularization_cost = lambd * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) / (2 * m)\n",
    "    ### END CODER HERE ###\n",
    "    \n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "we want to train our model. We will combine all functions from the forward propagation calculation , then estimate the loss function `compute_cost_with_regularization` and the gradient `backward_propagation_with_regularization`,  update the parameters `update_parameters` to reduce the loss and lastley estimate the accuracy of the model using `predict` function.\n",
    "\n",
    "**Exercise:** Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import backward_propagation_with_regularization  # an Implementation for the backpropagation for the network with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "def optimize(X, Y, X_test, Y_test, learning_rate = 0.01, num_iterations = 15000, print_cost = True,  lambd = 0, init_method =\"xavier\"):\n",
    "    \n",
    "    grads      = {}\n",
    "    costs      = {'train': [], 'test':[]} # to keep track of the loss\n",
    "    accuracies = {'train': [], 'test':[]} # to keep track of the accuracy\n",
    "    # to keep track of the accuracy\n",
    "    m = X.shape[1] # number of examples\n",
    "    layers_dims = [X.shape[0], 60,35, 1]\n",
    "    \n",
    "\n",
    "    parameters = initialize_parameters(layers_dims,init_method)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        a3, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function\n",
    "        if lambd == 0:\n",
    "            cost = compute_cost(a3, Y)\n",
    "        else:\n",
    "            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n",
    "        # Back Propagation\n",
    "        if lambd == 0:\n",
    "            grads = backward_propagation(X, Y, cache)\n",
    "        elif lambd != 0:\n",
    "            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the loss every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "            # Compute the predictions \n",
    "            predictions_train = predict_dec(X,parameters)\n",
    "            predictions_test  = predict_dec(X_test,parameters)\n",
    "            # Compute the accuracy \n",
    "            accuracy_train = np.mean(predictions_train[0,:] == Y[0,:])\n",
    "            accuracy_test  = np.mean(predictions_test[0,:]  == Y_test[0,:])\n",
    "            print(\"The train accuracy  after iteration {}: {}\".format(i, accuracy_train))\n",
    "            print(\"The test accuracy after iteration {}: {}\".format(i, accuracy_test))\n",
    "            # Cost function\n",
    "            a3_a, _ = forward_propagation(X_test, parameters)\n",
    "            if lambd == 0:\n",
    "                cost_test = compute_cost(a3_a, Y_test)\n",
    "            else:\n",
    "                cost_test = compute_cost_with_regularization(a3_a, Y_test, parameters, lambd)\n",
    "            costs['train'].append(cost)\n",
    "            costs['test'].append(cost_test)\n",
    "            accuracies['train'].append(accuracy_train)            \n",
    "            accuracies['test'].append(accuracy_test)\n",
    "\n",
    "            \n",
    "    # plot the loss, accuracy\n",
    "    fig1, (ax1, ax2) = plt.subplots(figsize=(10,12), nrows=2, ncols=1)\n",
    "    ax1.plot(costs['train'],label='train_loss')\n",
    "    ax1.plot(costs['test'] ,label='test_loss')\n",
    "    ax1.set_ylabel('cost')\n",
    "    ax1.set_xlabel('iterations (per hundreds)')\n",
    "    ax1.set_title(\"Learning rate =\" + str(learning_rate))\n",
    "    ax1.legend(loc='best')\n",
    "    ax2.plot(accuracies['train'],label='train_acc')\n",
    "    ax2.plot(accuracies['test'] ,label='test_acc')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.set_xlabel('iterations (per hundreds)')\n",
    "    ax2.set_title(\"Learning rate =\" + str(learning_rate))\n",
    "    ax2.legend(loc='best')\n",
    "    plt.show()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will train the network without regularization and with zero initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = optimize(train_X, train_Y, test_X, test_Y , \n",
    "                      learning_rate = 0.3, \n",
    "                      num_iterations = 3000, \n",
    "                      lambd = 0, \n",
    "                      init_method= \"zeros\",\n",
    "                      print_cost = True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, there is no improvement in accuracy or reduction in loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = optimize(train_X, train_Y, test_X, test_Y , \n",
    "                      learning_rate = 0.35, \n",
    "                      num_iterations = 7000, \n",
    "                      lambd = 0, \n",
    "                      init_method= \"xavier\",\n",
    "                      print_cost = True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train accuracy is 94.8%, while the test accuracy is 92.4%. This is the baseline model. We will observe the effects of regularization on this model. Let's record the results.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_decision_boundary\n",
    "plt.title(\"Model without regularization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75, 0.40])\n",
    "axes.set_ylim([-0.75, 0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(x.T,parameters), train_X, train_Y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = optimize(train_X, train_Y, test_X, test_Y , \n",
    "                      learning_rate = 0.35, \n",
    "                      num_iterations = 7000, \n",
    "                      lambd = 0.5, \n",
    "                      init_method= \"xavier\",\n",
    "                      print_cost = True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model with regularization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75, 0.40])\n",
    "axes.set_ylim([-0.75, 0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(x.T,parameters), train_X, train_Y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "Dropout is a regularization technique that we use to prevent overfitting in our neural network models. We ignore randomly selected neurons from the network while training. This prevents the activations of those neurons continuing down the line, and the weight updates are not applied to them during back propagation. The weights of neurons are tuned to identify specific features; neurons that neighbor them become dependent on this, which can lead to overfitting because these neurons can get specialized to the training data. When neurons are randomly dropped, the neighboring neurons step in and learn the representation, leading to multiple different representations being learned by the network. This make the network generalize better and prevents the model from overfitting. One import thing to keep in mind is that dropout layers should not be used when you are performing predictions or testing your model. This would make the model lose valuable information and would lead to a loss in performance.\n",
    "\n",
    " We generally use a dropout probability between 0.2 and 0.5. This probability refers to the probability by which a neuron will be dropped from training. \n",
    " \n",
    " <img src=\"do.png\" style=\"width:600px;height:400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import backward_propagation_with_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    z1 = np.dot(W1, X) + b1\n",
    "    a1 = relu(z1)\n",
    "\n",
    "    # Steps 1-4 below correspond to the Steps 1-4 described above. \n",
    "    D1 = np.random.rand(a1.shape[0], a1.shape[1])     # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n",
    "    D1 = (D1 < keep_prob)                             # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n",
    "    a1 = a1 * D1                                      # Step 3: shut down some neurons of A1\n",
    "    a1 = a1/keep_prob                                 # Step 4: scale the value of neurons that haven't been shut down\n",
    "\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = relu(z2)\n",
    "\n",
    "    D2 = np.random.rand(a2.shape[0], a2.shape[1])     # Step 1: initialize matrix D2 = np.random.rand(..., ...)\n",
    "    D2 = (D2 < keep_prob)                             # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)\n",
    "    a2 = a2 * D2                                      # Step 3: shut down some neurons of A2\n",
    "    a2 = a2/keep_prob                                 # Step 4: scale the value of neurons that haven't been shut down\n",
    "\n",
    "    z3 = np.dot(W3, a2) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    cache = (z1, D1, a1, W1, b1, z2, D2, a2, W2, b2, z3, a3, W3, b3)\n",
    "    \n",
    "    return a3, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part, we used the regularization and added a new confirmation to the cost function. As a result, the gradient of the cost function to the weights changed. In Dropout, we change the feedforward network by switching off many units, and therefore the gradient of the cost function to the weights will change because the number of weights we want to update in each iteration will be smaller.\n",
    "\n",
    "**Note:** When we evaluate the network, we don't use dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "def optimize(X, Y, X_test, Y_test, learning_rate = 0.01, num_iterations = 15000, print_cost = True,  keep_prob = 1, init_method =\"xavier\"):\n",
    "    \n",
    "    grads      = {}\n",
    "    costs      = {'train': [], 'test':[]} # to keep track of the loss\n",
    "    accuracies = {'train': [], 'test':[]} # to keep track of the accuracy\n",
    "    # to keep track of the accuracy\n",
    "    m = X.shape[1] # number of examples\n",
    "    layers_dims = [X.shape[0], 60,35, 1]    \n",
    "\n",
    "    parameters = initialize_parameters(layers_dims,init_method)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.        \n",
    "        if keep_prob == 1:\n",
    "            a3, cache = forward_propagation(X, parameters)\n",
    "            \n",
    "        elif keep_prob < 1:    \n",
    "            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n",
    "        \n",
    "        # Compute Cost\n",
    "        cost = compute_cost(a3, Y)\n",
    "        \n",
    "        # Back Propagation\n",
    "        if keep_prob == 1:\n",
    "            grads = backward_propagation(X, Y, cache)\n",
    "        elif keep_prob < 1:\n",
    "            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n",
    "\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the loss every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "            # Compute the predictions \n",
    "            predictions_train = predict_dec(X,parameters)\n",
    "            predictions_test  = predict_dec(X_test,parameters)\n",
    "            # Compute the accuracy \n",
    "            accuracy_train = np.mean(predictions_train[0,:] == Y[0,:])\n",
    "            accuracy_test  = np.mean(predictions_test[0,:]  == Y_test[0,:])\n",
    "            print(\"The train accuracy  after iteration {}: {}\".format(i, accuracy_train))\n",
    "            print(\"The test accuracy after iteration {}: {}\".format(i, accuracy_test))\n",
    "            # Cost function\n",
    "            a3_a, _ = forward_propagation(X_test, parameters)\n",
    "            cost_test = compute_cost(a3_a, Y_test)\n",
    "            costs['train'].append(cost)\n",
    "            costs['test'].append(cost_test)\n",
    "            accuracies['train'].append(accuracy_train)            \n",
    "            accuracies['test'].append(accuracy_test)\n",
    "\n",
    "            \n",
    "    # plot the loss, accuracy\n",
    "    fig1, (ax1, ax2) = plt.subplots(figsize=(10,12), nrows=2, ncols=1)\n",
    "    ax1.plot(costs['train'],label='train_loss')\n",
    "    ax1.plot(costs['test'] ,label='test_loss')\n",
    "    ax1.set_ylabel('cost')\n",
    "    ax1.set_xlabel('iterations (per hundreds)')\n",
    "    ax1.set_title(\"Learning rate =\" + str(learning_rate))\n",
    "    ax1.legend(loc='best')\n",
    "    ax2.plot(accuracies['train'],label='train_acc')\n",
    "    ax2.plot(accuracies['test'] ,label='test_acc')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.set_xlabel('iterations (per hundreds)')\n",
    "    ax2.set_title(\"Learning rate =\" + str(learning_rate))\n",
    "    ax2.legend(loc='best')\n",
    "    plt.show()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = optimize(train_X, train_Y, test_X, test_Y , \n",
    "                      learning_rate = 0.35, \n",
    "                      num_iterations = 7000, \n",
    "                      keep_prob = 1., \n",
    "                      init_method= \"xavier\",\n",
    "                      print_cost = True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = optimize(train_X, train_Y, test_X, test_Y , \n",
    "                      learning_rate = 0.35, \n",
    "                      num_iterations = 7000, \n",
    "                      keep_prob = 0.3, \n",
    "                      init_method= \"xavier\",\n",
    "                      print_cost = True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model with dropout\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75, 0.40])\n",
    "axes.set_ylim([-0.75, 0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(x.T,parameters), train_X, train_Y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "During training, we know that our neural networks have a tendency to overfit to the training data over many iterations, and then they are unable to generalize what they have learned to perform well on the test set. One way of overcoming this problem is to stop the training earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "def optimize(X, Y, X_test, Y_test, learning_rate = 0.01, num_iterations = 15000, print_cost = True, init_method =\"xavier\"):\n",
    "    \n",
    "    grads      = {}\n",
    "    costs      = {'train': [], 'test':[]} # to keep track of the loss\n",
    "    accuracies = {'train': [], 'test':[]} # to keep track of the accuracy\n",
    "    # to keep track of the accuracy\n",
    "    m = X.shape[1] # number of examples\n",
    "    layers_dims = [X.shape[0], 60,35, 1]    \n",
    "\n",
    "    parameters = initialize_parameters(layers_dims,init_method)\n",
    "    \n",
    "    # parameters for early stopping\n",
    "    old_cost   = 0.\n",
    "    counter    = 0 \n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.        \n",
    "        a3, cache = forward_propagation(X, parameters)\n",
    "            \n",
    "        # Compute Cost\n",
    "        cost = compute_cost(a3, Y)\n",
    "        \n",
    "        # Early stopping\n",
    "        if np.abs(cost - old_cost) <= 0.00009:\n",
    "            print( np.abs(cost - old_cost))\n",
    "            if counter == 5:\n",
    "                print(\"Stop the training, there is no reduction in the loss function\")\n",
    "                break\n",
    "            counter += 1\n",
    "        else:\n",
    "            # Reset the counter \n",
    "            counter = 0\n",
    "        old_cost = cost\n",
    "        \n",
    "        # Back Propagation\n",
    "        grads = backward_propagation(X, Y, cache)\n",
    "\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the loss every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "            # Compute the predictions \n",
    "            predictions_train = predict_dec(X,parameters)\n",
    "            predictions_test  = predict_dec(X_test,parameters)\n",
    "            # Compute the accuracy \n",
    "            accuracy_train = np.mean(predictions_train[0,:] == Y[0,:])\n",
    "            accuracy_test  = np.mean(predictions_test[0,:]  == Y_test[0,:])\n",
    "            print(\"The train accuracy  after iteration {}: {}\".format(i, accuracy_train))\n",
    "            print(\"The test accuracy after iteration {}: {}\".format(i, accuracy_test))\n",
    "            # Cost function\n",
    "            a3_a, _ = forward_propagation(X_test, parameters)\n",
    "            cost_test = compute_cost(a3_a, Y_test)\n",
    "            costs['train'].append(cost)\n",
    "            costs['test'].append(cost_test)\n",
    "            accuracies['train'].append(accuracy_train)            \n",
    "            accuracies['test'].append(accuracy_test)\n",
    "\n",
    "            \n",
    "    # plot the loss, accuracy\n",
    "    fig1, (ax1, ax2) = plt.subplots(figsize=(10,12), nrows=2, ncols=1)\n",
    "    ax1.plot(costs['train'],label='train_loss')\n",
    "    ax1.plot(costs['test'] ,label='test_loss')\n",
    "    ax1.set_ylabel('cost')\n",
    "    ax1.set_xlabel('iterations (per hundreds)')\n",
    "    ax1.set_title(\"Learning rate =\" + str(learning_rate))\n",
    "    ax1.legend(loc='best')\n",
    "    ax2.plot(accuracies['train'],label='train_acc')\n",
    "    ax2.plot(accuracies['test'] ,label='test_acc')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.set_xlabel('iterations (per hundreds)')\n",
    "    ax2.set_title(\"Learning rate =\" + str(learning_rate))\n",
    "    ax2.legend(loc='best')\n",
    "    plt.show()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = optimize(train_X, train_Y, test_X, test_Y , \n",
    "                      learning_rate = 0.35, \n",
    "                      num_iterations = 7000, \n",
    "                      init_method= \"xavier\",\n",
    "                      print_cost = True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.6804623413945013 - 0.7129636167732121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "During training, we know that our neural networks have a tendency to overfit to the training data over many iterations, and then they are unable to generalize what they have learned to perform well on the test set. One way of overcoming this problem is to stop the training earlier.\n",
    "\n",
    "**Homework**: Homework: implement the early stop algorithm to stop the training of the network if no reduction of the loss function occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Data Science with Python By Rohan Chopra, Aaron England, Mohamed Noordeen Alaudeen (packtpub)\n",
    "4. Deep Learning Specialization on Coursera\n",
    "5. https://github.com/Kulbear/deep-learning-coursera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
