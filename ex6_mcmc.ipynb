{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC for Inference of a Parameter Function\n",
    "\n",
    "In this notebook, we develop a Markov Chain Monte Carlo Algorithm for the inference of a parameter\n",
    "function. We consider a problem similar to the ODE example on exercise sheet two. However, we do not prescribe\n",
    "a fixed parameter coefficient $u$, but rather extend the problem to a parameter function $u(t)$.\n",
    "This makes the inverse problem formally infinite-dimensional.\n",
    "The forward model is characterized by the ODE\n",
    "$$\n",
    "    \\frac{dx}{dt} = u(t)x(t),\\quad t\\in [0, 1],\\quad x(0) = 1.\n",
    "$$\n",
    "As the true parameter function, we choose $u_{true}(t) = 5 \\sin(4\\pi t)$. We further assume that data is\n",
    "given as the solution of the forward problem at a number of times $\\{t_i\\}_{i=1}^d$, perturbed by\n",
    "zero-centered Gaussian noise,\n",
    "$$\n",
    "    y_i = x(t_i) + \\eta,\\quad \\eta \\sim \\mathcal{N}(0,\\sigma_{noise}^2),\\quad i=1,2,\\ldots,d.\n",
    "$$\n",
    "Consequently, we can define the likelihood for observing the data given a parameter function $u$ as\n",
    "$$\n",
    "    l(y|u) \\propto \\exp \\Bigl( -\\frac{1}{2\\sigma_{noise}^2} || \\mathcal{B}x(u)-y ||^2 \\Bigr),\n",
    "$$\n",
    "where $x(u)$ is the implicitly defined solution of the ODE and $B$ denotes a projection operator onto\n",
    "the observation locations.\n",
    "\n",
    "Moving on, we prescribe an infinite-dimensional prior in the form of a Gaussian random field.\n",
    "In particular, we choose a zero mean, $\\bar{u}_{prior} = 0\\ \\forall t$ and a square exponential or\n",
    "radial basis covariance function,\n",
    "$$\n",
    "    \\mathrm{cov}(t_i, t_j) = \\sigma_{prior}^2 \\exp\\Bigl( -\\frac{1}{2l^2}(t_i-t_j)^2 \\Bigr).\n",
    "$$\n",
    "\n",
    "Here, $\\sigma_{prior}^2$ denotes the field variance and $l$ the characteristic length of correlation\n",
    "between different time points.\n",
    "\n",
    "To solve this problem numerically, we discretize the parameter function and ODE solution on a uniform\n",
    "grid,\n",
    "$$\n",
    "    \\mathcal{G}_t = \\{ t_n = n\\Delta t;\\ n=0,1,\\ldots,N;\\ \\Delta t = \\mathrm{const}>0 \\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `ODEInverseProblem` in the file `ode_inverse_problem` implements the functionalities for\n",
    "the solution of the discretized inverse problem. For a given parameter function, it provides methods to compute the\n",
    "numerical ODE solution and evaluate the log prior, likelihood and posterior. It further has \n",
    "multiple attributes to call. We will introduce the most important here, but you can also find them\n",
    "under the `@property` decorators at the bottom of the `ode_inverse_problem` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ode_inverse_problem as oip\n",
    "import mcmc_sampler as mcmc\n",
    "\n",
    "%matplotlib widget\n",
    "plt.close('all')\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ODEInverseProblem` class requires two settings dictionaries in its constructor. The\n",
    "`settings_ode` dict specifies the parameters for the discretization and numerical solution of the\n",
    "ODE problem. `settings_inverse` contains the parameters that define the inverse problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_ode = {\n",
    "    'initial-condition':        1,\n",
    "    'start-time':               0,\n",
    "    'end-time':                 1,\n",
    "    'time-step-size':           1e-2,\n",
    "    'solver':                   'explicit-euler'\n",
    "}\n",
    "\n",
    "settings_inverse = {\n",
    "    'exact-solution':           lambda t: 5 * np.sin(4 * np.pi * t),\n",
    "    'prior-mean':               lambda t: np.zeros(t.size),\n",
    "    'prior-variance':           10,\n",
    "    'prior-correlation-length': 0.1,\n",
    "    'num-data-points':          50,\n",
    "    'data-noise-variance':      1e-2\n",
    "}\n",
    "\n",
    "BIProblem = oip.ODEInverseProblem(settings_ode, settings_inverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BIProblem` object has attributes that yield the forward solution of the ODE problem for the\n",
    "exact parameter function and the prescribed discretization. We can further contrast it with the\n",
    "\"exact solution\" (evaluated on a very fine grid). Lastly, `BIProblem` initializes data by perturbing\n",
    "the exact solution on `num-data-points` locations of the fine time grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.set_title('Numerical ODE solution')\n",
    "ax.plot(BIProblem.grid, BIProblem.forward_exact_solution, label='Approximation')\n",
    "ax.plot(BIProblem.grid_fine,  BIProblem.forward_exact_solution_fine, label='exact')\n",
    "ax.scatter(*BIProblem.data, color='grey', label='data')\n",
    "ax.set_xlim((BIProblem.grid[0], BIProblem.grid[-1]))\n",
    "ax.set_xlabel(r'$t$')\n",
    "ax.set_ylabel(r'$x(t)$')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the goal of this exercise to develop an MCMC procedure for the described inverse problem.\n",
    "The class `MCMCSampler` in the file `mcmc_sampler` implements such an MCMC framework. For its\n",
    "initialization, it requires  a proposal and a kernel function. The proposal function is used to \n",
    "create new samples from a given state. The kernel function accepts or rejects a step, given the \n",
    "current state of a chain and a proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Implement the preconditioned Crank-Nicolson proposal and the corresponding\n",
    "              form of the Metropolis-Hastings kernel.\n",
    "              \n",
    "*The proposal function takes the following input*:\n",
    "  1. A `numpy` random number generator\n",
    "  2. A sampling prefactor that corresponds to the cholesky factor of the prior covariance\n",
    "     matrix\n",
    "  3. The current sample\n",
    "  4. The step size $\\beta$\n",
    "   \n",
    "*The kernel function takes as input*:\n",
    "  1. A `numpy` random number generator\n",
    "  2. The current state and the proposed sample\n",
    "  3. The \"log probabilities\" of the current and proposed samples. For the PCN proposal, this is\n",
    "     the log likelihood (not the negative log-likelihood!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proposal(rng, sampling_prefactor, current_sample, step_size):\n",
    "\n",
    "    return proposal\n",
    "\n",
    "def kernel(rng, current_sample, current_log_prob, proposed_sample, proposal_log_prob):\n",
    "\n",
    "    # Return next sample and corresponding log probability (current sample or proposal)\n",
    "    # Boolean flag is_accepted indicates if sample has been accepted\n",
    "    return next_sample, next_log_prob, is_accepted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the sampler with the `likelihood_only=True` flag. This means that only the\n",
    "log-likelihood is passed to the kernel function. We can then call the `sample` method with\n",
    "canonical MCMC settings. The `num-statistics-batches` setting allows for the subdivision of the\n",
    "iterations after the burn-in into batches. This can be useful to examine convergence.\n",
    "The `sample` function returns the four variables `sample_norm`, `accept_ratio`, `mean` and `variance`.\n",
    "`sample_norm` holds the norm of the MCMC sample at each iteration. It defines a particular\n",
    "*quantity of interest* (QOI), which we can analyze and visualize instead of the sample itself.\n",
    "`accept_ratio`, `mean` and `variance` comprise the acceptance ratio, the sample mean and the sample\n",
    "variance for each batch (all iterations after burn-in if the number of batches is chosen to be 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_mcmc = {\n",
    "    'num-samples':              2e4,\n",
    "    'num-burnin':               1e4,\n",
    "    'step-size':                0.01,\n",
    "    'num-statistics-batches':   1,\n",
    "    'initial-sample':           BIProblem.prior_mean\n",
    "}\n",
    "\n",
    "Sampler = mcmc.MCMCSampler(BIProblem, proposal, kernel, likelihood_only=True)\n",
    "sample_norm, accept_ratio, mean, variance = Sampler.sample(settings_mcmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Run the sampler with the given settings. Print out the acceptance ratio, plot the\n",
    "sample norm, and the posterior mean and 95% confidence interval (assume that the posterior is\n",
    "approximately Gaussian) and compare to the exact solution.\n",
    "\n",
    "Now vary the prior parameters `prior-variance` and `prior-correlation-length`. What do you observe and why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian_inference_ss23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
