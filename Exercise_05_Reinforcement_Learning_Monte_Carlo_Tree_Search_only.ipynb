{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxgvN1YlSnxK"
   },
   "source": [
    "# Reinforcement Learning &ndash; Monte Carlo Tree Search\n",
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/300px-Reinforcement_learning_diagram.svg.png).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Reinforcement Learning is a special form of machine learning, where an agent interacts with an environment, conducts observations on the effects of actions and collects rewards.\n",
    "\n",
    "The goal of reinforcement learning is to learn an optimal policy, so that given a state an agent is able to decide what it should do next.\n",
    "\n",
    "In this exercise we will look into tow fundamental algorithms that are capable of solving MDPs, namely Monte Carlo Tree Search [Monte Carlo Tree Search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search) and [Q-Learning](https://en.wikipedia.org/wiki/Q-learning) (optional).\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the time you complete this lab, you should know:\n",
    "\n",
    "- The relevant pieces for a reinforcement learning system\n",
    "- The basics of *[gym](https://gym.openai.com/envs/#classic_control)* to conduct your own RL experiments\n",
    "- How Monte Carlo evaluations works\n",
    "- How Monte Carlo Tree Search works\n",
    "- The Advantages of MCTS vs. MC evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP\n",
    "\n",
    "A Markov decision process is a 4-tuple $(S,A,P_{a},R_{a})$\n",
    "\n",
    "![MDP](mdp.png \"MDP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvdmBl8GajjF"
   },
   "source": [
    "## Problem\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. (However, the ice is slippery, so you won't always move in the direction you intend.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wYUHIokU_EI"
   },
   "source": [
    "## Setup\n",
    "\n",
    "To begin we'll need to install all the required python package dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjQ08kksR2c2"
   },
   "outputs": [],
   "source": [
    "#!pip install --quiet gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8MH3Ij6rAL_z"
   },
   "source": [
    "### Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWdytOiH-LFr"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZgQh5-QCBeDI"
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import random\n",
    "import heapq\n",
    "import collections\n",
    "import math\n",
    "\n",
    "# Reinforcement Learning environments\n",
    "import gym\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GzgwlDeZhfxU"
   },
   "source": [
    "\n",
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mi8myW9Wheef"
   },
   "outputs": [],
   "source": [
    "# Define the default figure size\n",
    "plt.rcParams['figure.figsize'] = [16, 4]\n",
    "\n",
    "def create_numerical_map(env):\n",
    "    \"\"\"Convert the string map of the environment to a numerical version\"\"\"\n",
    "    numerical_map = np.zeros(env.env.desc.shape)\n",
    "    i = 0\n",
    "    for row in env.env.desc:\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            if col.decode('UTF-8') == 'S':\n",
    "                numerical_map[i, j] = 2\n",
    "            elif col.decode('UTF-8') == 'G':\n",
    "                numerical_map[i, j] = 1\n",
    "            elif col.decode('UTF-8') == 'F':\n",
    "                numerical_map[i, j] = 2\n",
    "            elif col.decode('UTF-8') == 'H':\n",
    "                numerical_map[i, j] = 3\n",
    "            j += 1\n",
    "        i += 1\n",
    "    numerical_map[env.unwrapped.s//i, env.unwrapped.s%i] = 0\n",
    "    return numerical_map\n",
    "\n",
    "\n",
    "def visualize_env(env):\n",
    "    \"\"\"Plot the environment\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    # Hide grid lines\n",
    "    ax.grid(False)\n",
    "    # Hide axes ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('The frozen Lake')\n",
    "    i = ax.imshow(create_numerical_map(env), cmap=cm.jet)\n",
    "    plt.show()\n",
    "    print('the position is blue, holes are red, ice is yellow and the goal is teal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rTC-P1vd-5-y"
   },
   "source": [
    "#### Deterministic Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sknx1oOiaL7J"
   },
   "outputs": [],
   "source": [
    "# register variants of the frozen lake without execution uncertainty i.e. deterministic environments\n",
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.78,  # optimum = .8196\n",
    ")\n",
    "\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery8x8-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '8x8', 'is_slippery': False},\n",
    "    max_episode_steps=200,\n",
    "    reward_threshold=0.99,  # optimum = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BzfpVLxA-T4W"
   },
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gsl3GswnX1I6"
   },
   "outputs": [],
   "source": [
    "# Deterministic environments\n",
    "env_name = 'FrozenLakeNotSlippery-v0'\n",
    "#env_name = 'FrozenLakeNotSlippery8x8-v0'\n",
    "\n",
    "# Stochastic environments\n",
    "#env_name = 'FrozenLake-v0'\n",
    "#env_name = 'FrozenLake8x8-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8WwK53WADNp"
   },
   "source": [
    "Create the environment with the previously selected name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "ARVwYFcHAB78",
    "outputId": "2f4c131f-cca2-4d82-ebc4-12cfa87f5890"
   },
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "print('Generated the frozen lake with config: ' + env_name)\n",
    "env.reset()\n",
    "visualize_env(env)\n",
    "env.unwrapped.s = 4\n",
    "visualize_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Environment (Object)\n",
    "\n",
    "**TASK :**\n",
    "Analyze the environment object and figure out its *observation-* and *actionspace* as well as its *reward range*.\n",
    "\n",
    "What is the size of the observation space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size of the action space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the range of rewards?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-pzYcAtuiHJ9"
   },
   "source": [
    "### Uncertainty in Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "F5OmhQ8sVLHK",
    "outputId": "167f15d0-f60c-43af-ebee-14e265d552bd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "actions = {0:\"left \",\n",
    "           1:\"down \",\n",
    "           2:\"right\",\n",
    "           3:\"up   \"}\n",
    "\n",
    "s = env.reset()\n",
    "print(\"the initial state is: {}\".format(s))\n",
    "visualize_env(env)\n",
    "\n",
    "# The agent should go right\n",
    "print(\"executing action 2, should go right\")\n",
    "s1, r, d, _ = env.step(2)\n",
    "print(\"new state is: {} done: {}\".format(s1, d))\n",
    "visualize_env(env)\n",
    "\n",
    "# The agent should go left\n",
    "print(\"executing action 0, should go left\")\n",
    "s1, r, d, _ = env.step(0)\n",
    "print(\"new state is: {} done: {}\".format(s1, d))\n",
    "visualize_env(env)\n",
    "\n",
    "# The agent should go down\n",
    "print(\"executing action 1, should go down\")\n",
    "s1, r, d, _ = env.step(1)\n",
    "print(\"new state is: {} done: {}\".format(s1, d))\n",
    "visualize_env(env)\n",
    "\n",
    "# The agent should go up\n",
    "print(\"executing action 3, should go up\")\n",
    "s1, r, d, _ = env.step(3)\n",
    "print(\"new state is: {} done: {}\".format(s1, d))\n",
    "visualize_env(env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Evaluator/Search\n",
    "* Simulate trajectories through the MDP from the current state $s_t$\n",
    "* Apply model-free RL to simulated episodes\n",
    "\n",
    "![Monte Carlo Evaluator/Search](./img/monte_carlo_search.png)\n",
    "\n",
    "### Monte Carlo Estimate\n",
    "###  $\\hat{V}(s)=\\frac{1}{K}\\sum_{k=1}^{K}{G_t}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCS:\n",
    "    def __init__(self, env, state = 0, iterations = 1000, discount_factor = 0.8):\n",
    "        # maximum length of evaluation\n",
    "        self.number_iterations = iterations\n",
    "        # discount factor for future rewards\n",
    "        self.discount_factor = discount_factor\n",
    "        # environment\n",
    "        self.env = env\n",
    "        # initial state\n",
    "        self.state = state\n",
    "        self.env.unwrapped.s = self.state\n",
    "        self.actions_q = np.zeros(self.env.action_space.n)\n",
    "        self.actions_q_max = np.zeros(self.env.action_space.n)\n",
    "        self.actions_visits = np.zeros(self.env.action_space.n)\n",
    "        #visualize_env(self.env)\n",
    "    \n",
    "    def best_action(self):\n",
    "        for i in range(self.number_iterations):\n",
    "            action = self.random_action()\n",
    "            g = self.simulate(action)\n",
    "            self.actions_q[action] += g\n",
    "            if g > self.actions_q_max[action]:\n",
    "                self.actions_q_max[action] = g\n",
    "            self.actions_visits[action] += 1\n",
    "        self.actions_q = np.divide(self.actions_q, self.actions_visits, out=np.zeros_like(self.actions_q), where=self.actions_visits!=0)\n",
    "        return np.argmax(self.actions_q)\n",
    "    \n",
    "    def random_action(self):\n",
    "        return random.randint(0,self.env.action_space.n-1)\n",
    "    \n",
    "    def simulate(self, action):\n",
    "        self.env.reset()\n",
    "        self.env.unwrapped.s = self.state\n",
    "        done = False\n",
    "        depth = 0\n",
    "        g = 0\n",
    "        state, r, done, _ = self.env.step(action)\n",
    "        g += r*self.discount_factor**depth\n",
    "        depth +=1\n",
    "        while not done:\n",
    "            action = self.random_action()\n",
    "            state, r, done, _ = self.env.step(action)\n",
    "            g += r*self.discount_factor**depth\n",
    "            depth +=1\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "sim = gym.make(env_name)\n",
    "env.reset()\n",
    "sim.reset()\n",
    "# set initial state\n",
    "state = 0\n",
    "env.unwrapped.s = state\n",
    "mcs = MCS(sim, state, iterations=10000)\n",
    "visualize_env(env)\n",
    "action = mcs.best_action()\n",
    "print(\"the best action is action {}, {}\".format(action, actions[action]))\n",
    "print(env.step(action))\n",
    "visualize_env(env)\n",
    "\n",
    "print(\"avg V(s):\\t {0:.3f}, max V(s):\\t {1:.3f}\".format(np.mean(mcs.actions_q), np.max(mcs.actions_q_max)))\n",
    "\n",
    "for key, val in actions.items():\n",
    "    print(\"avg Q(s,{2}):\\t {0:.3f}, max Q(s,{2}):\\t {1:.3f}\".format(mcs.actions_q[key], mcs.actions_q_max[key], val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search\n",
    "* Simulate trajectories through the MDP from the current state $s_t$ building a tree\n",
    "* Apply model-free RL to simulated episodes\n",
    "\n",
    "### In-Tree and Out-of-Tree\n",
    "* Selection Policy (improves): select actions maximizing action values\n",
    "* Simulation Policy (fixed): selection actions randomly\n",
    "\n",
    "### Balance Exploration and Exploitation\n",
    "\n",
    "### $UCT(s,a) = \\hat{Q}(s,a)+c\\sqrt{\\frac{\\ln{N(s)}}{N(s,a)}}$\n",
    "\n",
    "### Phases\n",
    "* Selection\n",
    "* Expansion\n",
    "* Simulation\n",
    "* Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state=0, action=-1, done=False, parent={}):\n",
    "        # current state of the environment\n",
    "        self.state = state\n",
    "        # number of trajectories that passed through this node\n",
    "        self.visits = 0\n",
    "        # average v value that results from starting in this node\n",
    "        self.v_value = 0\n",
    "        # action that led to this node\n",
    "        self.action = action\n",
    "        # untried actions (i.e. the actions that have not been explored)\n",
    "        self.untried_actions = [0, 1, 2, 3]\n",
    "        # parent node pointer\n",
    "        self.parent = parent\n",
    "        # children node pointers\n",
    "        self.children = []\n",
    "        # flag that indicates that the node is terminal (e.g. the environment is in a terminal state)\n",
    "        self.done = done\n",
    "        \n",
    "    def uct(self, c = 0.7):\n",
    "        \"\"\"Calculate the UCT value for a given child node (i.e. the value from executing a in s)\"\"\"\n",
    "        # if the node has not been visited return a high UCT score, forcing expansion\n",
    "        if self.visits == 0:\n",
    "            return 100\n",
    "        # if the node has been visited calculate it using the UCB formula\n",
    "        return self.v_value + c* math.sqrt(math.log(self.parent.visits)/self.visits)\n",
    "    \n",
    "    def best_child(self):\n",
    "        \"\"\"Return the best child based on the maximum UCT value.\"\"\"\n",
    "        uct_values = []\n",
    "        for child in self.children:\n",
    "            uct_values.append(child.uct())\n",
    "        uct_index = np.argmax(uct_values)\n",
    "        return self.children[uct_index]\n",
    "    \n",
    "    def max_action_value(self):\n",
    "        \"\"\"Return the child with the highest action value.\"\"\"\n",
    "        v_values = []\n",
    "        for child in self.children:\n",
    "            v_values.append(child.v_value)\n",
    "        v_values_index = np.argmax(v_values)\n",
    "        return self.children[v_values_index]\n",
    "    \n",
    "    def max_visits(self):\n",
    "        \"\"\"Return the child with the highest visit count.\"\"\"\n",
    "        visits = []\n",
    "        for child in self.children:\n",
    "            visits.append(child.visits)\n",
    "        visits_index = np.argmax(visits)\n",
    "        return self.children[visits_index]\n",
    "    \n",
    "    def str(self):\n",
    "        if not self.parent:\n",
    "            return \"s:{}, N(s,{}):{}, \\tV(s):{:.3f}, parent:{}\".format(self.state, \"none \", self.visits, self.v_value, self.parent)\n",
    "        else:\n",
    "            return \"s:{}, N(s,{}):{}, \\tQ(s,a):{:.3f}, parent:{}\".format(self.state, actions[self.action], self.visits, self.v_value,  self.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, env, state = 0, iterations = 1000, discount_factor = 0.8):\n",
    "        # maximum number of simulations\n",
    "        self.number_iterations = iterations\n",
    "        # discount factor for future rewards\n",
    "        self.discount_factor = discount_factor\n",
    "        # environment\n",
    "        self.env = env\n",
    "        # initial state\n",
    "        self.state = state\n",
    "        self.env.unwrapped.s = self.state\n",
    "        #visualize_env(self.env)\n",
    "    \n",
    "    def select(self, node):\n",
    "        # if the node has no untried actions left, choose the best child using UCB1\n",
    "        while len(node.untried_actions) == 0:\n",
    "            node = node.best_child()\n",
    "        return node\n",
    "    \n",
    "    def expand(self, node):\n",
    "        # expand the node with a random action\n",
    "        if not node.done:\n",
    "            action = np.random.choice(node.untried_actions)\n",
    "            node.untried_actions.remove(action)\n",
    "\n",
    "            self.env.reset()\n",
    "            self.env.unwrapped.s = node.state\n",
    "            state, r, done, _ = self.env.step(action)\n",
    "            child = Node(state, action, done, node)\n",
    "            node.children.append(child)\n",
    "            return child, r\n",
    "        else:\n",
    "            self.env.reset()\n",
    "            self.env.unwrapped.s = node.parent.state\n",
    "            state, r, done, _ = self.env.step(node.action)\n",
    "            return node, r\n",
    "    \n",
    "    def simulate(self, node):\n",
    "        \"\"\"Monte Carlo Evaluator\"\"\"\n",
    "        self.env.reset()\n",
    "        self.env.unwrapped.s = node.state\n",
    "        done = False\n",
    "        depth = 0\n",
    "        g = 0\n",
    "        action = self.random_action()\n",
    "        state, r, done, _ = self.env.step(self.random_action())\n",
    "        g += r*self.discount_factor**depth\n",
    "        depth +=1\n",
    "        while not done:\n",
    "            action = self.random_action()\n",
    "            state, r, done, _ = self.env.step(self.random_action())\n",
    "            g += r*self.discount_factor**depth\n",
    "            depth +=1\n",
    "        return g\n",
    "       \n",
    "    def update(self,node,g):\n",
    "        depth = 0\n",
    "        while node.parent:\n",
    "            node.visits += 1\n",
    "            node.v_value = (node.v_value*(node.visits-1)+g*self.discount_factor**depth)/node.visits\n",
    "            node = node.parent\n",
    "            depth += 1\n",
    "        node.visits += 1\n",
    "        node.v_value = (node.v_value*(node.visits-1)+g*self.discount_factor**depth)/node.visits\n",
    "            \n",
    "    def best_action(self, root):\n",
    "        for i in range(self.number_iterations):\n",
    "            self.env.reset()\n",
    "            self.env.unwrapped.s = root.state\n",
    "            node = self.select(root)\n",
    "            child, r = self.expand(node)\n",
    "            if not child.done:\n",
    "                g = self.simulate(child)\n",
    "            else:\n",
    "                g = r\n",
    "            self.update(child, g)\n",
    "        return root.max_action_value().action\n",
    "    \n",
    "    def random_action(self):\n",
    "        return random.randint(0,env.action_space.n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "sim = gym.make(env_name)\n",
    "env.reset()\n",
    "sim.reset()\n",
    "# set initial state\n",
    "state = 0\n",
    "env.unwrapped.s = state\n",
    "mcts = MCTS(sim, state, iterations = 10000)\n",
    "visualize_env(env)\n",
    "root_node = Node(state)\n",
    "action = mcts.best_action(root_node)\n",
    "print(root_node.str())\n",
    "\n",
    "print(root_node.children[0].str())\n",
    "print(root_node.children[1].str())\n",
    "print(root_node.children[2].str())\n",
    "print(root_node.children[3].str())\n",
    "print(\"the best action is action {}, {}\".format(action, actions[action]))\n",
    "print(env.step(action))\n",
    "visualize_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plan_mcs(iterations, state = 0, output = False):\n",
    "    env = gym.make(env_name)\n",
    "    sim = gym.make(env_name)\n",
    "    env.reset()\n",
    "    sim.reset()\n",
    "    # initialize the Monte Carlo Evaluator\n",
    "    mcs = MCS(sim, state, iterations = iterations)\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        mcs.state = state\n",
    "        action = mcs.best_action()\n",
    "        steps += 1\n",
    "        # take one step in the environment\n",
    "        state, r, done, _ = env.step(action)\n",
    "    if output:\n",
    "        visualize_env(env)\n",
    "        print(\"reached state: {}, after {}\".format(state, steps))\n",
    "    \n",
    "    if done and r != 1:\n",
    "        r = -1\n",
    "    return steps, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plan_mcts(iterations, state = 0, output = False):\n",
    "    env = gym.make(env_name)\n",
    "    sim = gym.make(env_name)\n",
    "    env.reset()\n",
    "    sim.reset()\n",
    "    # initialize the Monte Carlo Tree Search\n",
    "    mcts = MCTS(sim, state, iterations = iterations)\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        root_node = Node(state)\n",
    "        action = mcts.best_action(root_node)\n",
    "        steps += 1\n",
    "        # take one step in the environment\n",
    "        state, r, done, _ = env.step(action)\n",
    "    if output:\n",
    "        visualize_env(env)\n",
    "        print(\"reached state: {}, after {}\".format(state, steps))\n",
    "    if done and r != 1:\n",
    "        r = -1\n",
    "    return steps, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of MCS and MCTS\n",
    "* MCTS requires less iterations to reach the goal state\n",
    "* Due to the uniform action exploration in the plan_mcs function the variance estimates for all actions are less skewed as they are for MCTS, thus reaching the goal more frequently (but slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify evaluation points\n",
    "iterations = [25, 50, 100, 200, 400]\n",
    "#iterations = [200, 400, 800, 1600]\n",
    "# specify number of runs for each evaluation point\n",
    "runs = 10\n",
    "\n",
    "# initialize empty metrics\n",
    "avg_mcs_steps = []\n",
    "std_mcs_steps = []\n",
    "avg_mcts_steps = []\n",
    "std_mcts_steps = []\n",
    "avg_mcs_rs = []\n",
    "std_mcs_rs = []\n",
    "avg_mcts_rs = []\n",
    "std_mcts_rs = []\n",
    "\n",
    "mcs_steps = []\n",
    "mcts_steps = []\n",
    "mcs_rs = []\n",
    "mcts_rs = []\n",
    "\n",
    "start_state = 1\n",
    "\n",
    "for it in iterations:\n",
    "    # reset counters\n",
    "    mcs_steps = []\n",
    "    mcts_steps = []\n",
    "    mcs_rs = []\n",
    "    mcts_rs = []\n",
    "    for i in range(runs):\n",
    "        # solve MDP with MCS\n",
    "        mcs_step, mcs_r = plan_mcs(it, start_state, False)\n",
    "        # solve MDP with MCTS\n",
    "        mcts_step, mcts_r = plan_mcts(it, start_state, False)\n",
    "        # track metrics counters\n",
    "        mcs_steps = np.append(mcs_steps,mcs_step)\n",
    "        mcts_steps = np.append(mcts_steps,mcts_step)\n",
    "        mcs_rs = np.append(mcs_rs,mcs_r/mcs_steps)\n",
    "        mcts_rs = np.append(mcts_rs,mcts_r/mcts_steps)\n",
    "        \n",
    "    # aggregate values\n",
    "    avg_mcs_steps = np.append(avg_mcs_steps,np.mean(mcs_steps))\n",
    "    std_mcs_steps = np.append(std_mcs_steps, np.std(mcs_steps))\n",
    "    avg_mcts_steps = np.append(avg_mcts_steps,np.mean(mcts_steps))\n",
    "    std_mcts_steps = np.append(std_mcts_steps,np.std(mcts_steps))\n",
    "    avg_mcs_rs = np.append(avg_mcs_rs,np.mean(mcs_rs))\n",
    "    std_mcs_rs = np.append(std_mcs_rs,np.std(mcs_rs))\n",
    "    avg_mcts_rs = np.append(avg_mcts_rs,np.mean(mcts_rs))\n",
    "    std_mcts_rs = np.append(std_mcts_rs,np.std(mcts_rs))\n",
    "    \n",
    "fig, ax = plt.subplots(1, 2)\n",
    "# Plot the average episode length\n",
    "ax[0].errorbar(iterations, avg_mcs_steps, yerr=std_mcs_steps, color=\"red\", label='MCS')\n",
    "ax[0].errorbar(iterations, avg_mcts_steps, yerr=std_mcts_steps, color=\"blue\", label='MCTS')\n",
    "ax[0].set(xlabel='#Simulations', ylabel='Steps', title='Average Episode Length')\n",
    "ax[0].grid()\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot the average episode reward\n",
    "ax[1].errorbar(iterations, avg_mcs_rs, yerr=std_mcs_rs, color=\"red\", label='MCS')\n",
    "ax[1].errorbar(iterations, avg_mcts_rs, yerr=std_mcts_rs, color=\"blue\", label='MCTS')\n",
    "ax[1].set(xlabel='#Simulations', ylabel='Reward', title='Average Step Reward')\n",
    "ax[1].grid()\n",
    "ax[1].legend();"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6wYUHIokU_EI",
    "8MH3Ij6rAL_z",
    "JWdytOiH-LFr",
    "GzgwlDeZhfxU",
    "rTC-P1vd-5-y",
    "-pzYcAtuiHJ9",
    "zhrrLKXk0ElG",
    "CASyoXI9jAZW",
    "lU4gmOQcAjR_",
    "4CdfVP4DilJf",
    "wK6bzLs_iqeG",
    "5KUNPRHdAstO",
    "tny1fTdaIkR6"
   ],
   "name": "Exercise 04 - Reinforcement Learning with Gym and Pytorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
