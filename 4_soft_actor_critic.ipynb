{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Homework 4: Soft Actor Critic (10 Pts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All homeworks are self-contained. They can be completed in their respective notebooks.\n",
    "To edit and re-run code, you can therefore simply edit and restart the code cells below.\n",
    "There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window).\n",
    "This file should automatically be synced with your Google Drive. We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout.\n",
    " However, you will need to re-mount your Google Drive and re-install packages with every new instance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your work will be stored in a folder called `drl_ws22` by default to prevent Colab\n",
    "# instance timeouts from deleting your edits.\n",
    "# We do this by mounting your google drive on the virtual machine created in this colab\n",
    "# session. For this, you will likely need to sign in to your Google account and copy a\n",
    "# passcode into a field below\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create paths in your google drive\n",
    "DRIVE_PATH = '/content/gdrive/My\\ Drive/drl_ws22'\n",
    "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
    "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
    "    ! mkdir $DRIVE_PATH\n",
    "\n",
    "# the space in `My Drive` causes some issues,\n",
    "# make a symlink to avoid this\n",
    "SYM_PATH = '/content/drl_ws22'\n",
    "if not os.path.exists(SYM_PATH):\n",
    "    !ln -s $DRIVE_PATH $SYM_PATH\n",
    "! cd $SYM_PATH"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install **python** and **system** packages\n",
    "\n",
    "# install required system dependencies\n",
    "!apt-get install -y xvfb x11-utils\n",
    "\n",
    "# install required python dependencies\n",
    "!pip install matplotlib numpy tqdm torch stable_baselines3 gym==0.21.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by importing all the necessary python modules and defining some helper\n",
    "functions which you do not need to change. Still, make sure you are aware of\n",
    "what they do."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Imports and utility\n",
    "# Progress bar\n",
    "\n",
    "import os\n",
    "import gym\n",
    "import copy\n",
    "import tqdm\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import collections, random\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, List\n",
    "from torch.distributions import Normal\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(0)\n",
    "\n",
    "class ProgressBar:\n",
    "    def __init__(self, num_iterations: int, verbose: bool = True):\n",
    "        if verbose:  # create a nice little progress bar\n",
    "            self.scalar_tracker = tqdm.tqdm(total=num_iterations, desc=\"Scalars\", bar_format=\"{desc}\",\n",
    "                                            position=0, leave=True)\n",
    "            progress_bar_format = '{desc} {n_fmt:' + str(\n",
    "                len(str(num_iterations))) + '}/{total_fmt}|{bar}|{elapsed}<{remaining}'\n",
    "            self.progress_bar = tqdm.tqdm(total=num_iterations, desc='Iteration', bar_format=progress_bar_format,\n",
    "                                          position=1, leave=True)\n",
    "        else:\n",
    "            self.scalar_tracker = None\n",
    "            self.progress_bar = None\n",
    "\n",
    "    def __call__(self, _steps: int = 1, **kwargs):\n",
    "        if self.progress_bar is not None:\n",
    "            formatted_scalars = {key: \"{:.3e}\".format(value[-1] if isinstance(value, list) else value)\n",
    "                                 for key, value in kwargs.items()}\n",
    "            description = (\"Scalars: \" + \"\".join([str(key) + \"=\" + value + \", \"\n",
    "                                                  for key, value in formatted_scalars.items()]))[:-2]\n",
    "            self.scalar_tracker.set_description(description)\n",
    "            self.progress_bar.update(_steps)\n",
    "\n",
    "# specify the path to save the recordings of this run to.\n",
    "data_path = '/content/drl_ws22/exercise_4'\n",
    "data_path = os.path.join(data_path, time.strftime(\"%d-%m-%Y_%H-%M\"))\n",
    "if not (os.path.exists(data_path)):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "# disable the actual display to prevent errors with colab\n",
    "from pyvirtualdisplay import Display\n",
    "_display = Display(visible=False, size=(1400, 900))\n",
    "_ = _display.start()\n",
    "\n",
    "# this function will automatically save your figure into your google drive folder (if correctly mounted!)\n",
    "def save_figure(save_name: str) -> None:\n",
    "    assert save_name is not None, \"Need to provide a filename to save to\"\n",
    "    plt.savefig(os.path.join(data_path, save_name + \".png\"))\n",
    "\n",
    "\n",
    "def evaluate_rollout(evaluation_environment: gym.Env, soft_actor_critic) -> float:\n",
    "    \"\"\"\n",
    "    Performs a full rollout using the mean of the current policy.\n",
    "    :param evaluation_environment: The environment used for evaluation. In our case, a Pendulum environment\n",
    "    :param soft_actor_critic: An instance of the SAC class defined above\n",
    "    :return: The total score for the rollout\n",
    "    \"\"\"\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = evaluation_environment.reset()\n",
    "    while not done:  # alternate between collecting one step of data and updating SAC with one mini-batch\n",
    "        action, log_probabilities = soft_actor_critic.policy(torch.from_numpy(np.array(state)).float(), deterministic=True)\n",
    "        scaled_action = evaluation_environment.action_space.high[0] * action.item()\n",
    "        next_state, reward, done, info = evaluation_environment.step([[scaled_action]])\n",
    "        # need to wrap action in a list because of the video recording\n",
    "        state = next_state  # go to the next environment step\n",
    "        score += reward  # keep track of cumulative reward for recording\n",
    "    return score\n",
    "\n",
    "\n",
    "def v_function_visualization(evaluation_environment: gym.Env,\n",
    "                             soft_actor_critic,\n",
    "                             current_step: int = 0,\n",
    "                             resolution: int = 100):\n",
    "    \"\"\"\n",
    "    Visualizes a numerical approximation of the value function by evaluating the Q-Function for a wide range\n",
    "    :param evaluation_environment:\n",
    "    :param soft_actor_critic:\n",
    "    :param current_step:\n",
    "    :param resolution:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.clf()\n",
    "\n",
    "    max_speed = 8\n",
    "    x = np.linspace(-np.pi, np.pi, num=resolution)\n",
    "    y = np.linspace(-max_speed, max_speed, num=resolution)\n",
    "    state_evaluation_grid = np.transpose([np.tile(x, len(y)), np.repeat(y, len(x))])\n",
    "    input_observations = torch.Tensor(np.array([np.cos(state_evaluation_grid[:, 0]),\n",
    "                                                np.sin(state_evaluation_grid[:, 0]),\n",
    "                                                state_evaluation_grid[:, 1]])).T\n",
    "\n",
    "    evaluations = []\n",
    "\n",
    "    for position, action in enumerate(np.linspace(evaluation_environment.action_space.low,\n",
    "                                                  evaluation_environment.action_space.high,\n",
    "                                                  50)):\n",
    "        action_tensor = torch.Tensor(np.full((len(state_evaluation_grid), 1), fill_value=action))\n",
    "\n",
    "        q1_val = soft_actor_critic.q_net_1(input_observations, action_tensor)\n",
    "        q2_val = soft_actor_critic.q_net_2(input_observations, action_tensor)\n",
    "        q1_q2 = torch.cat([q1_val, q2_val], dim=1)\n",
    "        reward_evaluation_grid = torch.min(q1_q2, 1, keepdim=True)[0]\n",
    "        reward_evaluation_grid = reward_evaluation_grid.reshape((resolution, resolution))\n",
    "\n",
    "        evaluations.append(reward_evaluation_grid.detach().numpy())\n",
    "\n",
    "    plt.title(f\"Numerically integrated V-function at step {current_step}\")\n",
    "    plt.xlabel(r\"$\\theta$\")\n",
    "    plt.ylabel(r\"$\\dot{\\theta}$\")\n",
    "    heatmap = plt.contourf(x, y, np.array(evaluations).max(axis=0), levels=100,\n",
    "                           cmap=plt.get_cmap(\"jet\"), zorder=0)\n",
    "    plt.colorbar(heatmap)\n",
    "    save_figure(save_name=f\"numerical_v_function_{current_step:04d}\")\n",
    "\n",
    "\n",
    "def plot_metrics(metrics: Dict[str, List[float]]):\n",
    "    \"\"\"\n",
    "    Plots various metrics recorded during training\n",
    "    :param metrics:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if len(metrics) > 0:\n",
    "        plt.clf()\n",
    "        plt.figure(figsize=(16, 9))\n",
    "        for position, (key, value) in enumerate(metrics.items()):\n",
    "            plt.subplot(len(metrics), 1, position + 1)\n",
    "            plt.plot(range(len(value)), np.array(value))\n",
    "            plt.ylabel(key.title())\n",
    "        plt.xlabel(\"Recorded Steps\")\n",
    "        plt.tight_layout()\n",
    "        save_figure(f\"training_metrics\")\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def evaluate(evaluation_environment: gym.Env, soft_actor_critic,\n",
    "             num_evaluation_rollouts: int = 10):\n",
    "    \"\"\"\n",
    "    Perform num_evaluation_rollouts rollouts on the evaluation environment using the current policy and average over\n",
    "    the achieved scores. Also plot a visualization of the first of these rollouts and a numerical integration\n",
    "    of the value function\n",
    "    :param evaluation_environment: The environment to evaluate. Will perform num_evaluation_rollouts full rollouts on\n",
    "      this environment\n",
    "    :param soft_actor_critic: Instance of SAC used to determine the actions\n",
    "    :param num_evaluation_rollouts: Number of rollouts to evaluate for\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for rollout_idx in range(num_evaluation_rollouts):\n",
    "        rollout_score = evaluate_rollout(evaluation_environment=evaluation_environment,\n",
    "                                         soft_actor_critic=soft_actor_critic\n",
    "                                         )\n",
    "        scores.append(rollout_score)\n",
    "    mean_score = np.mean(scores)\n",
    "    return {\"score\": mean_score}\n",
    "\n",
    "\n",
    "def visualize_rollout(soft_actor_critic, step: int):\n",
    "    # evaluation_environment = DummyVecEnv([lambda: gym.make('Pendulum-v1')])\n",
    "    # keep a second environment for evaluation purposes\n",
    "    from stable_baselines3.common.env_util import make_vec_env\n",
    "    evaluation_environment = make_vec_env('Pendulum-v1')\n",
    "    visualization_environment = VecVideoRecorder(evaluation_environment,\n",
    "                                                 video_folder=data_path,\n",
    "                                                 record_video_trigger=lambda x: x == 0,\n",
    "                                                 video_length=200,  # 200 steps per rollout\n",
    "                                                 name_prefix=f\"Pendulum_{step:05d}\")\n",
    "    evaluate_rollout(evaluation_environment=visualization_environment, soft_actor_critic=soft_actor_critic)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Soft Actor Critic**\n",
    "\n",
    "In this exercise, we will re-implement the Soft Actor Critic (SAC) algorithm. SAC is an off-policy actor-critic method\n",
    "that is widely used in the community due to its sample efficiency and (relative) stability. It acts under a maximum\n",
    "entropy principle to ensure sufficient exploration during training, and also employs tricks like the reparameterization\n",
    "trick, polyak-updates of target Q networks and Twin-Delayed Q-Functions.\n",
    "\n",
    "\n",
    "## Pendulum\n",
    "We will showcase the SAC algorithm on the very classic [Pendulum](https://www.gymlibrary.dev/environments/classic_control/pendulum/) control environment.\n",
    "The goal in this environment is to actuate a pendulum such that it stays upward without too much movement.\n",
    "It has a one-dimensional action space that represents the torque acting on the pendulum,\n",
    "and a 2d internal *state* that is the angle $\\theta$ and the angular velocity $\\dot{\\theta}$ of the pendulum at the current time step.\n",
    "The *external* state or observation is a 3-tuple ($\\cos(\\theta)$, $\\sin(\\theta)$, $\\dot{\\theta}$).\n",
    "\n",
    "\n",
    "## Replay Buffer\n",
    "We start by defining our replay buffer which is used to store samples seen during the rollouts that can then\n",
    "be used for training later on. You do *not* need to implement anything here."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Functional code\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_limit: int, batch_size: int):\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)  # use a dequeue as a buffer\n",
    "\n",
    "    def put(self, transition: tuple) -> None:\n",
    "        \"\"\"\n",
    "        Adds a transition to the buffer.\n",
    "        :param transition: (s, a, r, s', done) pair sampled by having the policy act on the environment\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self) -> tuple:\n",
    "        mini_batch = random.sample(self.buffer, self.batch_size)  # get self.batch_size random samples from the buffer\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []  # initialize list of (s, a, r, s', done) tuples\n",
    "\n",
    "        for transition in mini_batch:  # parse all transitions into their lists.\n",
    "            state, action, reward, next_state, done = transition\n",
    "            states.append(state)\n",
    "            actions.append([action])\n",
    "            rewards.append([reward])\n",
    "            next_states.append(next_state)\n",
    "            dones.append([float(done)])\n",
    "\n",
    "        return torch.tensor(states, dtype=torch.float), \\\n",
    "               torch.tensor(actions, dtype=torch.float), \\\n",
    "               torch.tensor(rewards, dtype=torch.float), \\\n",
    "               torch.tensor(next_states, dtype=torch.float), \\\n",
    "               torch.tensor(dones, dtype=torch.float)\n",
    "\n",
    "    def size(self) -> int:\n",
    "        return len(self.buffer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **TASK 1: Actor/Policy network** (2+2=4 Points)\n",
    "\n",
    "Next, we will set up the actor/policy.\n",
    "\n",
    "### Task 1.1: Tanh Squashing (2 Points)\n",
    "The original SAC implementation squashes its actions into the $[-1, 1]$ range using a *tanh* activation.\n",
    "To ensure proper probabilities for these actions, it also squashes the log probabilities of each action accordingly.\n",
    "You can show using the change of variables theorem that\n",
    "\n",
    "\\begin{align}\n",
    "    \\log \\pi(\\boldsymbol{a}|\\boldsymbol{s}) = \\log \\mu(\\boldsymbol{u}|\\boldsymbol{s})-\\sum_{i=1}^D \\log\\left(1-\\tanh^2(u_i)\\right)\n",
    "\\end{align}\n",
    "\n",
    "for (squashed) actions $\\boldsymbol{a}$, states $\\boldsymbol{s}$, proposed (unsquashed) actions $\\boldsymbol{u}$ and a policy\n",
    "distribution $\\mu$ (which you shouldn't confuse with the mean, which is sometimes also called $\\mu$...)\n",
    "In our case, the dimensionality $D=1$.\n",
    "You will need to squash the action itself, as well as its log probability.\n",
    "\n",
    "### Task 1.2: Training from 2 Q Networks (2 Points)\n",
    "As SAC is using Twin-Delayed Q-Networks to prevent the overestimatio bias in the Q-Values,\n",
    "the actor/policy needs to choose the minimum of both available Q-Networks for its loss function.\n",
    "For this, you will need to evaluate the action using both Q-Networks, and then simply choose their minimum."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, learning_rate: float, entropy_alpha: float):\n",
    "        super(PolicyNet, self).__init__()  # make sure that the policy network is registered as a pytorch module\n",
    "\n",
    "        # specify neurons per layer. \"fc\" is short for \"fully_connected layer\".\n",
    "        self.common_mlp = nn.Linear(3, 128)\n",
    "        self.mean_mlp = nn.Linear(128, 1)\n",
    "        self.std_mlp = nn.Linear(128, 1)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)  # use adam optimizer\n",
    "\n",
    "        self.entropy_alpha = entropy_alpha  # weight of the entropy term\n",
    "\n",
    "    def forward(self, state: torch.Tensor, deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Choose an action and calcualte its probability for the current state.\n",
    "        :param state: The state to choose the action for\n",
    "        :param deterministic: Whether to draw an action from\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        state = F.relu(self.common_mlp(state))\n",
    "        mean = self.mean_mlp(state)\n",
    "        std = F.softplus(self.std_mlp(state))  # we need the standard deviation to be >0\n",
    "        normal_distribution = Normal(mean, std)\n",
    "\n",
    "        if deterministic:\n",
    "            action = mean\n",
    "        else:\n",
    "            action = normal_distribution.rsample()\n",
    "        log_probabilities = normal_distribution.log_prob(action)\n",
    "\n",
    "        # the original SAC implementation also squishes the action into [-1, 1] using a tanh activation.\n",
    "        # to keep the probabilities correct, they account for this using the update below.\n",
    "\n",
    "        ### TODO ###\n",
    "        ### Your code starts here ###\n",
    "\n",
    "        ### Your code ends here ###\n",
    "        return real_action, real_log_probabilities\n",
    "\n",
    "    def train_step(self, q_net_1, q_net_2, mini_batch: tuple) -> Dict[str, float]:\n",
    "        states, _, _, _, _ = mini_batch\n",
    "        actions, log_probabilities = self.forward(states)\n",
    "        entropy = -self.entropy_alpha * log_probabilities\n",
    "\n",
    "        # evaluate both q-networks for the current state-action pair\n",
    "        # and use their minimum (see Twin-Delayed Q functions)\n",
    "\n",
    "        ### TODO ###\n",
    "        ### Your code starts here ###\n",
    "\n",
    "        ### Your code ends here ###\n",
    "\n",
    "        loss = -(min_q + entropy).mean()  # \"-\" for gradient ascent\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return {\"entropy\": entropy.mean().item(),\n",
    "                \"policy_loss\": loss.item()}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we implement our Q-Network. This will be used to evaluate (\"criticize\") the actions that the actor proposes.\n",
    "Note that this is only *one* Q-Network, and that the SAC class below will use multiple of those for the Twin-Delayed\n",
    "Q-Functions.\n",
    "\n",
    "## Task 2: Polyak Updates ( 2 Points)\n",
    "For increasing stability in the update of the Q Networks, SAC uses polyak updates of each Q Network.\n",
    "The update is given as\n",
    "\\begin{align}\n",
    "    \\beta'_i = (1-\\tau)\\beta'_i+(\\tau)\\beta_i\n",
    "\\end{align}\n",
    "\n",
    "for Q-Network parameters $\\beta_1$, $\\beta_2$ and an update rate $\\tau$. Note that the slides use a reverse\n",
    "order of $(1-\\tau)$ and $\\tau$, which corresponds to values of $\\tau$ close to $1$ rather than close to $0$\n",
    "as done in the code.\n",
    "\n",
    "Hint: Use the `parameters().data` attribute of torch.Tensor to access the parameters. You can copy them\n",
    "using `parameters.data.copy_`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, learning_rate: float, q_net_update_rate: float):\n",
    "        super(QNet, self).__init__()  # make sure that the Q Network is registered as a pytorch module\n",
    "        # specify network parameters. \"fc\" is short for \"fully_connected layer\".\n",
    "        self.state_layer = nn.Linear(3, 64)\n",
    "        self.action_layer = nn.Linear(1, 64)\n",
    "        self.common_layer = nn.Linear(128, 32)\n",
    "        self.fc_out = nn.Linear(32, 1)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)  # use adam optimizer\n",
    "        self.q_net_update_rate = q_net_update_rate\n",
    "\n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        h1 = F.relu(self.state_layer(state))\n",
    "        h2 = F.relu(self.action_layer(action))\n",
    "        cat = torch.cat([h1, h2], dim=1)\n",
    "        q_evaluation = F.relu(self.common_layer(cat))\n",
    "        q_evaluation = self.fc_out(q_evaluation)\n",
    "        return q_evaluation\n",
    "\n",
    "    def train_step(self, target_values: torch.Tensor, mini_batch: tuple) -> float:\n",
    "        \"\"\"\n",
    "        Train the network for a single mini-batch update\n",
    "        :param target_values: The target values to regress to\n",
    "        :param mini_batch: A tuple (state, action, reward, next_state, done). For this update,\n",
    "          only the action and state are needed\n",
    "        :return: The mean loss for this update step\n",
    "        \"\"\"\n",
    "        states, actions, _, _, _ = mini_batch  # get action and state from current mini_batch\n",
    "        evaluation = self.forward(states, actions)\n",
    "\n",
    "        # calculate the loss and its gradients; update the network based on them\n",
    "        loss = F.smooth_l1_loss(evaluation, target_values).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def polyak_update(self, target_network):\n",
    "        \"\"\"\n",
    "        Soft update the target network with the parameters of this network\n",
    "        :param target_network:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ### TODO ###\n",
    "        ### Your code starts here ###\n",
    "\n",
    "        ### Your code ends here ###"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we combine the Q-Network(s) and the Actor to build our SoftActorCritic class.\n",
    "This class (roughly) implements the Pseudo-code shown in Slide Set 7, Slide 40.\n",
    "\n",
    "## Task 3: Q-Network Targets (4 Points)\n",
    "For SAC, you will need to implement the targets of the Q-Networks. These are calculated using the rule on Slide Set 7, Slide 35, i.e.,\n",
    "\\begin{align}\n",
    "y_t = r_t +\\gamma\\left(Q_{\\beta'}(\\boldsymbol{s}'_t, \\boldsymbol{a}') - \\alpha \\log \\pi(\\boldsymbol{a}'|\\boldsymbol{s}'_t)\\right), \\qquad \\text{where ~} \\boldsymbol{a}'\\approx \\pi(\\boldsymbol{a}|\\boldsymbol{s}_t')\n",
    "\\end{align}\n",
    "\n",
    "Hints:\n",
    "* Remember that $\\beta'$ are the parameters of the **target** Q-Network\n",
    "* In our case, $Q_{\\beta'}$ needs to be evaluated as the **minimum** of **both** target Q-Networks\n",
    "* You can only do a next step in your next step prediction if there is one, i.e., if the environment is **not** done."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SoftActorCritic:\n",
    "    def __init__(self, args):\n",
    "        learning_rate = args[\"learning_rate\"]\n",
    "        q_net_update_rate = args[\"q_net_update_rate\"]\n",
    "        batch_size = args[\"batch_size\"]\n",
    "        self.discount_factor = args[\"discount_factor\"]\n",
    "\n",
    "        # initialize two Q-networks and their respective target networks\n",
    "        self.q_net_1 = QNet(learning_rate, q_net_update_rate)\n",
    "        self.q_net_2 = QNet(learning_rate, q_net_update_rate)\n",
    "\n",
    "        self.q_net_1_target = copy.deepcopy(self.q_net_1)\n",
    "        self.q_net_2_target = copy.deepcopy(self.q_net_2)\n",
    "\n",
    "        # get a replay buffer and a policy network\n",
    "        self.memory = ReplayBuffer(buffer_limit=args[\"buffer_limit\"], batch_size=batch_size)\n",
    "\n",
    "        self.policy = PolicyNet(learning_rate=learning_rate,\n",
    "                                entropy_alpha=args[\"entropy_alpha\"])\n",
    "\n",
    "    def train_step(self) -> Dict[str, float]:\n",
    "        mini_batch = self.memory.sample()\n",
    "        q_targets = self.calculate_q_targets(mini_batch)\n",
    "\n",
    "        # update both q networks\n",
    "        q_net_1_loss = self.q_net_1.train_step(q_targets, mini_batch)\n",
    "        q_net_2_loss = self.q_net_2.train_step(q_targets, mini_batch)\n",
    "\n",
    "        # update the policy\n",
    "        policy_metrics = self.policy.train_step(self.q_net_1, self.q_net_2, mini_batch)\n",
    "\n",
    "        # polyak updates for the target q networks\n",
    "        self.q_net_1.polyak_update(self.q_net_1_target)\n",
    "        self.q_net_2.polyak_update(self.q_net_2_target)\n",
    "\n",
    "        return {\"q_net_1_loss\": q_net_1_loss,\n",
    "                \"q_net_2_loss\": q_net_2_loss,\n",
    "                **policy_metrics}\n",
    "\n",
    "    def calculate_q_targets(self, mini_batch: tuple) -> torch.Tensor:\n",
    "        _, _, rewards, next_states, dones = mini_batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ### TODO ###\n",
    "            ### Your code starts here ###\n",
    "\n",
    "            ### Your code ends here ###\n",
    "        return target"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Running the algorithm\n",
    "\n",
    "That's it for SAC. The code below defines arguments/hyperparameters, as well as a general training loop. You do\n",
    "*not* need to change any code here (unless you want to fiddle with the parameters). If everything is implemented\n",
    "correctly, you should see training improvements after a couple thousand steps, and have a converged solution after\n",
    "30000-40000 steps.\n",
    "The code will save\n",
    "* a couple of training metrics,\n",
    "* a contour plot of a numerical integration of the value function,\n",
    "i.e., the maximum of a number of Q-function evaluations on a grid of the state-space\n",
    "* a .mp4 video of the pendulum swinging.\n",
    "\n",
    "For the homework, you only need to send in the last of each, i.e., the one at iteration 50000, but you can also\n",
    "turn in all of the plots as usual."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def __setitem__(self, key, val):\n",
    "        setattr(self, key, val)\n",
    "\n",
    "    num_training_steps = 51000  # @param {type: \"integer\"}\n",
    "    learning_rate = 3.0e-4  # @param {type: \"number\"}\n",
    "    entropy_alpha = 0.02  # @param {type: \"number\"}\n",
    "    discount_factor = 0.98  # @param {type: \"number\"}\n",
    "    batch_size = 64  # @param {type: \"integer\"}\n",
    "    buffer_limit = 100000  # @param {type: \"integer\"}\n",
    "    reward_scale = 0.1  # @param {type: \"number\"}\n",
    "    q_net_update_rate = 0.002  # @param {type: \"number\"}\n",
    "\n",
    "def main(args: Args):\n",
    "    environment = gym.make('Pendulum-v1')\n",
    "    evaluation_environment = DummyVecEnv([lambda: gym.make('Pendulum-v1')])\n",
    "    # keep a second environment for evaluation purposes.\n",
    "    # We wrap it in a Dummy Vector Environment for compatibility with\n",
    "    # the visualization utility\n",
    "\n",
    "    soft_actor_critic = SoftActorCritic(args=args)\n",
    "\n",
    "    reward_scale = args[\"reward_scale\"]\n",
    "    num_training_steps = args[\"num_training_steps\"]\n",
    "\n",
    "    # logging utility\n",
    "    logging_frequency = 100  # log progress every 100 steps\n",
    "    plot_frequency = 5000\n",
    "    progress_bar = ProgressBar(num_iterations=num_training_steps)\n",
    "\n",
    "    state = environment.reset()  # restart the environment, i.e., go back to some initial state\n",
    "    full_metrics = {\"score\": []}\n",
    "    train_step_metrics = {}\n",
    "    for current_step in range(num_training_steps):\n",
    "        if current_step % logging_frequency == 0:  # log every logging_frequency steps\n",
    "            for key, value in train_step_metrics.items():\n",
    "                if key not in full_metrics:\n",
    "                    full_metrics[key] = []\n",
    "                full_metrics[key].append(value)\n",
    "\n",
    "            evaluation_recordings = evaluate(evaluation_environment=evaluation_environment,\n",
    "                                             soft_actor_critic=soft_actor_critic)\n",
    "\n",
    "            progress_bar(_steps=logging_frequency,\n",
    "                         score=evaluation_recordings.get(\"score\"),\n",
    "                         **train_step_metrics)\n",
    "\n",
    "            full_metrics[\"score\"].append(evaluation_recordings.get(\"score\"))\n",
    "\n",
    "        if current_step % plot_frequency == 0:  # plot visualizations\n",
    "            v_function_visualization(evaluation_environment=evaluation_environment,\n",
    "                                     soft_actor_critic=soft_actor_critic,\n",
    "                                     current_step=current_step)\n",
    "            visualize_rollout(soft_actor_critic=soft_actor_critic,\n",
    "                              step=current_step)\n",
    "            plot_metrics(full_metrics)\n",
    "\n",
    "        # alternate between collecting one step of data and updating SAC with one mini-batch\n",
    "        action, log_probabilities = soft_actor_critic.policy(torch.from_numpy(np.array(state)).float())\n",
    "        next_state, reward, done, info = environment.step([environment.action_space.high[0] * action.item()])\n",
    "\n",
    "        # safe (s, a, r, s', done) tuple in memory buffer\n",
    "        soft_actor_critic.memory.put((state, action.item(), reward * reward_scale, next_state, done))\n",
    "\n",
    "        if done:\n",
    "            state = environment.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "        # wait until there are enough rollouts in the memory buffer before starting the training\n",
    "        if soft_actor_critic.memory.size() > 1000:\n",
    "            train_step_metrics = soft_actor_critic.train_step()\n",
    "            train_step_metrics[\"buffer_size\"] = soft_actor_critic.memory.size()\n",
    "    environment.close()\n",
    "\n",
    "\n",
    "args = Args()\n",
    "main(args=args)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eQx7oDGeeKWj"
   ],
   "name": "2_dqn_atari.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
