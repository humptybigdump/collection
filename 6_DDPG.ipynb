{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc2fcf66-a275-479e-8d6e-20baac59d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from collections import deque\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31283171-0274-4a4e-9a04-527cc6079400",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient\n",
    "\n",
    "DDPG is an Actor-Critic RL algorithm. It has a policy approximation model and a value approximation model. Its policy network produces continuous actions, and combines them with a DQN-like q-value estimator.\n",
    "\n",
    "DDPG uses 2 main models. The Actor network learns the policy and the Critic network learns the q-value. The Actor network receives the state as input and outputs an action vector, corresponding to the continuous action space (e.g. joint velocities), thus it produces a deterministic policy. The Critic receives the output of the Actor, combines it with the state, and approximates a q-value. \n",
    "\n",
    "Since the learned the policy is deterministic, we need to implement some kind of exploration during the training process. To make DDPG policies explore better, a noise to their actions. Originally an Ornstein-Uhlenbeck (https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.) noise was proposed, however it is often used with normal-distributed noise or parameter noise, directly applied to the network parameters.\n",
    "\n",
    "## Agent\n",
    "\n",
    "Similarly to the PolicyGradient agent, the DDPG agent also has \"frozen\" models. Both the Actor and the Critic network have their target Actor and target Critic counterparts.\n",
    "\n",
    "In case of the Critic, the learning step is almost identical to the learning step of the DQN. The only difference is, that here, the actions are directly fed into the network, as input, and the output is the corresponding q-value, instead of having multiple q-values for each possible action.\n",
    "\n",
    "The interesting part is the training of the Actor. Since the goal is to maximize the q-value with the policy, and we already have a differentiable q-value approximator, the solution is sort of obvious. We just feed the output of the Actor $\\pi_\\phi$ into our Critic $c_\\theta$, along with the state, negate the result and use it as loss.\n",
    "\n",
    "$$\n",
    "loss_{actor} = -c_\\Theta (s_t, \\pi_\\Phi (s_t))\n",
    "$$\n",
    "\n",
    "If the Critic is doing its job well, and the Actor minimizes the negated output of the Critic, then it should maximize a well approximated q-value, thus also approaching a well working policy.\n",
    "\n",
    "## Replay buffer\n",
    "\n",
    "Similar to the case of the DQN, a replay buffer is used to store gathered experience, and later sample from it during the training phase. In this case however, we adjust the samplint method to faver resent experiences over older experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cda979ca-9d0f-495f-8870-29cee1b60464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=(400, 300), n_actions=2, **kwargs):\n",
    "        super(Actor, self).__init__(**kwargs)\n",
    "        self.layers = []\n",
    "        for i, u in enumerate(units):\n",
    "            self.layers.append(tf.keras.layers.Dense(u, activation=tf.nn.leaky_relu,\n",
    "                                                     kernel_initializer=tf.keras.initializers.glorot_normal()))\n",
    "        last_init = tf.random_normal_initializer(stddev=0.0005)\n",
    "        self.layers.append(tf.keras.layers.Dense(n_actions, activation='tanh', kernel_initializer=last_init))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = inputs\n",
    "        for l in self.layers:\n",
    "            outputs = l(outputs)\n",
    "        return outputs\n",
    "    \n",
    "class Critic(tf.keras.layers.Layer):\n",
    "    def __init__(self, state_units=(400, 300), action_units=(300,), units=(150,), **kwargs):\n",
    "        super(Critic, self).__init__(**kwargs)\n",
    "        self.layers_state = []\n",
    "        for u in state_units:\n",
    "            self.layers_state.append(tf.keras.layers.Dense(u, activation=tf.nn.leaky_relu,\n",
    "                                                           kernel_initializer=tf.keras.initializers.glorot_normal()))\n",
    "\n",
    "        self.layers_action = []\n",
    "        for u in action_units:\n",
    "            self.layers_action.append(tf.keras.layers.Dense(u, activation=tf.nn.leaky_relu,\n",
    "                                                            kernel_initializer=tf.keras.initializers.glorot_normal()))\n",
    "\n",
    "        self.layers = []\n",
    "        for u in units:\n",
    "            self.layers.append(tf.keras.layers.Dense(u, activation=tf.nn.leaky_relu,\n",
    "                                                     kernel_initializer=tf.keras.initializers.glorot_normal()))\n",
    "        last_init = tf.random_normal_initializer(stddev=0.00005)\n",
    "        self.layers.append(tf.keras.layers.Dense(1, kernel_initializer=last_init))\n",
    "\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        p_action = inputs['action']\n",
    "        p_state = inputs['state']\n",
    "\n",
    "        for l in self.layers_action:\n",
    "            p_action = l(p_action)\n",
    "\n",
    "        for l in self.layers_state:\n",
    "            p_state = l(p_state)\n",
    "\n",
    "        outputs = self.add([p_state, p_action])\n",
    "        for l in self.layers:\n",
    "            outputs = l(outputs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "                self.x_prev\n",
    "                + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "                + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "            \n",
    "class DDPGAgent:\n",
    "    def __init__(self, action_space, observation_shape, gamma=0.99, tau=0.001, epsilon=0.05):\n",
    "        self.action_space = action_space\n",
    "        self.tau = tau  # target network weight adaptation\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.actor = Actor(n_actions=action_space.shape[0])\n",
    "        self.critic = Critic()\n",
    "\n",
    "        self.target_actor = Actor(n_actions=action_space.shape[0])\n",
    "        self.target_critic = Critic()\n",
    "\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "        self.noise = OUActionNoise(mean=np.zeros(np.array(self.action_space.sample()).shape),\n",
    "                                   std_deviation=float(0.2) * np.ones(1))\n",
    "\n",
    "        self._init_networks(observation_shape)\n",
    "\n",
    "    def _init_networks(self, observation_shape):\n",
    "        initial_state = np.zeros([1, observation_shape])\n",
    "\n",
    "        initial_action = self.actor(initial_state)\n",
    "        self.target_actor(initial_state)\n",
    "\n",
    "        critic_input = {'action': initial_action, 'state': initial_state}\n",
    "        self.critic(critic_input)\n",
    "        self.target_critic(critic_input)\n",
    "\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "\n",
    "    @staticmethod\n",
    "    def update_target(model_target, model_ref, tau=0.0):\n",
    "        new_weights = [tau * ref_weight + (1 - tau) * target_weight for (target_weight, ref_weight) in\n",
    "                       list(zip(model_target.get_weights(), model_ref.get_weights()))]\n",
    "        model_target.set_weights(new_weights)\n",
    "\n",
    "    def act(self, observation, explore=True, random_action=False):\n",
    "        if random_action or np.random.uniform(0, 1) < self.epsilon:\n",
    "            a = self.action_space.sample()\n",
    "        else:\n",
    "            a = self.actor(observation).numpy()[:, 0]\n",
    "            if explore:\n",
    "                a += self.noise()\n",
    "        a = np.clip(a, self.action_space.low, self.action_space.high)\n",
    "        return a\n",
    "\n",
    "    def compute_target_q(self, rewards, next_states, dones):\n",
    "        actions = self.target_actor(next_states)\n",
    "        critic_input = {'action': actions, 'state': next_states}\n",
    "        next_q = self.target_critic(critic_input)\n",
    "        target_q = rewards + (1 - dones) * next_q * self.gamma\n",
    "        return target_q\n",
    "\n",
    "    def get_actor_grads(self, states):\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor(states)\n",
    "            critic_input = {'action': actions, 'state': states}\n",
    "            qs = self.critic(critic_input)\n",
    "            loss = -tf.math.reduce_mean(qs)\n",
    "        gradients = tape.gradient(loss, self.actor.trainable_variables)\n",
    "        gradients = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gradients]\n",
    "        return gradients, loss\n",
    "\n",
    "    def get_critic_grads(self, states, actions, target_qs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            critic_input = {'action': actions, 'state': states}\n",
    "            qs = self.critic(critic_input)\n",
    "            loss = tf.reduce_mean(tf.abs(target_qs - qs))\n",
    "        gradients = tape.gradient(loss, self.critic.trainable_variables)\n",
    "        gradients = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gradients]\n",
    "        return gradients, loss\n",
    "\n",
    "    def learn(self, states, actions, rewards, next_states, dones):\n",
    "        target_qs = self.compute_target_q(rewards, next_states, dones)\n",
    "\n",
    "        actor_grads, actor_loss = self.get_actor_grads(states)\n",
    "        critic_grads, critic_loss = self.get_critic_grads(states, actions, target_qs)\n",
    "\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "        self.target_update()\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def target_update(self):\n",
    "        DDPGAgent.update_target(self.target_critic, self.critic, self.tau)\n",
    "        DDPGAgent.update_target(self.target_actor, self.actor, self.tau)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.p_indices = [0.5 / 2]\n",
    "\n",
    "    def put(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append([state, action, np.expand_dims(reward, -1), next_state, np.expand_dims(done, -1)])\n",
    "\n",
    "    def sample(self, batch_size=1, unbalance=0.8):\n",
    "        p_indices = None\n",
    "        if random.random() < unbalance:\n",
    "            self.p_indices.extend((np.arange(len(self.buffer) - len(self.p_indices)) + 1)\n",
    "                                  * 0.5 + self.p_indices[-1])\n",
    "            p_indices = self.p_indices / np.sum(self.p_indices)\n",
    "        sample_idx = np.random.choice(len(self.buffer),\n",
    "                                      size=min(batch_size, len(self.buffer)),\n",
    "                                      replace=False,\n",
    "                                      p=p_indices)\n",
    "        sample = [self.buffer[s_i] for s_i in sample_idx]\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*sample))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2060c7-ad34-49be-b975-98bebb7cc60b",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training loop is similar to the other training loops we saw until now. Except, for periodically updating and adapting the parameter noise, and we let the agent collect some initial experience in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8b78f6a-5345-4ab1-aaac-8b2cf363a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(env, agent, num_episodes=1, max_steps=200, render=False):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_return = 0.0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not (done or steps > max_steps):\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            action = agent.act(np.array([obs]))\n",
    "            obs, r, done, _, _ = env.step(action)\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "        total_return += episode_return\n",
    "    return total_return / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0952a592-1234-4e11-aba7-6c3545190bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, actor loss 7.015113830566406, critic loss 0.44769567251205444 , avg return -1115.7896571918664\n",
      "epoch 25, actor loss 21.003501892089844, critic loss 0.1957494020462036 , avg return -1170.2190757357748\n",
      "epoch 50, actor loss 44.12150192260742, critic loss 0.310014933347702 , avg return -1237.6062782935878\n",
      "epoch 75, actor loss 53.26805877685547, critic loss 0.46954214572906494 , avg return -1312.3806947671394\n",
      "epoch 100, actor loss 52.87529754638672, critic loss 0.46217361092567444 , avg return -1145.0513225605082\n",
      "epoch 125, actor loss 43.268592834472656, critic loss 0.41431134939193726 , avg return -443.8112816932462\n",
      "epoch 150, actor loss 42.63667297363281, critic loss 0.4188898503780365 , avg return -615.337327283951\n",
      "epoch 175, actor loss 26.33926773071289, critic loss 0.43491464853286743 , avg return -243.64586758290628\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m128\u001b[39m):\n\u001b[0;32m     26\u001b[0m     s_states, s_actions, s_rewards, s_next_states, s_dones \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m     actor_l, critic_l \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_rewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_next_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_dones\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     ep_actor_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m actor_l\n\u001b[0;32m     29\u001b[0m     ep_critic_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m critic_l\n",
      "Cell \u001b[1;32mIn[23], line 165\u001b[0m, in \u001b[0;36mDDPGAgent.learn\u001b[1;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[0;32m    162\u001b[0m critic_grads, critic_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_critic_grads(states, actions, target_qs)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(actor_grads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcritic_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_update()\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m actor_loss, critic_loss\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:1140\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_gradients_aggregation \u001b[38;5;129;01mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m   1139\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[1;32m-> 1140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:634\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[0;32m    633\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[1;32m--> 634\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:1166\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_internal_apply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[1;32m-> 1166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_apply_gradients_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\tensorflow\\python\\distribute\\merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[1;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[1;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(strategy, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribution_strategy_context\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[0;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:1216\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[1;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[0;32m   1213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[1;32m-> 1216\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[0;32m   1221\u001b[0m     _, var_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2637\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   2634\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[0;32m   2635\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2636\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m-> 2637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2639\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[0;32m   2640\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3710\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   3707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[0;32m   3708\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[0;32m   3709\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[1;32m-> 3710\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3716\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   3712\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[0;32m   3713\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[0;32m   3714\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[0;32m   3715\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[1;32m-> 3716\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[0;32m   3718\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:595\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    594\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[1;32m--> 595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:1213\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step_xla(grad, var, \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(var)))\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:224\u001b[0m, in \u001b[0;36m_BaseOptimizer._update_step\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(variable) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_dict:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe optimizer cannot recognize variable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis usually means you are trying to call the optimizer to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.keras.optimizers.legacy.\u001b[39m\u001b[38;5;132;01m{self.__class__.__name__}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    223\u001b[0m     )\n\u001b[1;32m--> 224\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\adam.py:200\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    198\u001b[0m     v_hat\u001b[38;5;241m.\u001b[39massign(tf\u001b[38;5;241m.\u001b[39mmaximum(v_hat, v))\n\u001b[0;32m    199\u001b[0m     v \u001b[38;5;241m=\u001b[39m v_hat\n\u001b[1;32m--> 200\u001b[0m variable\u001b[38;5;241m.\u001b[39massign_sub((\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m) \u001b[38;5;241m/\u001b[39m (tf\u001b[38;5;241m.\u001b[39msqrt(v) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon))\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:1090\u001b[0m, in \u001b[0;36mVariable._OverloadOperator.<locals>._run_op\u001b[1;34m(a, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_op\u001b[39m(a, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1089\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 1090\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tensor_oper(a\u001b[38;5;241m.\u001b[39mvalue(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1407\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.binary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1403\u001b[0m   \u001b[38;5;66;03m# force_same_dtype=False to preserve existing TF behavior\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m   \u001b[38;5;66;03m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[0;32m   1405\u001b[0m   \u001b[38;5;66;03m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[0;32m   1406\u001b[0m   x, y \u001b[38;5;241m=\u001b[39m maybe_promote_tensors(x, y)\n\u001b[1;32m-> 1407\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1409\u001b[0m   \u001b[38;5;66;03m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;66;03m# object that can implement the operator with knowledge of itself\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1413\u001b[0m   \u001b[38;5;66;03m# original error from the LHS, because it may be more\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m   \u001b[38;5;66;03m# informative.\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(y), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__r\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m op_name):\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1767\u001b[0m, in \u001b[0;36m_mul_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1765\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m sparse_tensor\u001b[38;5;241m.\u001b[39mSparseTensor(y\u001b[38;5;241m.\u001b[39mindices, new_vals, y\u001b[38;5;241m.\u001b[39mdense_shape)\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1767\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:529\u001b[0m, in \u001b[0;36mmultiply\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmath.multiply\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    481\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39mregister_binary_elementwise_api\n\u001b[0;32m    482\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmultiply\u001b[39m(x, y, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    484\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns an element-wise x * y.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03m  For example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;124;03m   * InvalidArgumentError: When `x` and `y` have incompatible shapes or types.\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 529\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_math_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:7324\u001b[0m, in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   7322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   7323\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 7324\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   7325\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMul\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   7326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   7327\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "env = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "\n",
    "agent = DDPGAgent(env.action_space, env.observation_space.shape[0])\n",
    "for i in range(1001):\n",
    "    obs, _ = env.reset()\n",
    "    # gather experience\n",
    "    agent.noise.reset()\n",
    "    ep_actor_loss = 0\n",
    "    ep_critic_loss = 0\n",
    "    steps = 0\n",
    "    for j in range(200):\n",
    "        steps += 1\n",
    "        env.render()\n",
    "        action = agent.act(np.array([obs]), random_action=(i < 1))\n",
    "        # execute action\n",
    "        new_obs, r, done, _, _ = env.step(action)\n",
    "        replay_buffer.put(obs, action, r, new_obs, done)\n",
    "        obs = new_obs\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Learn from the experiences in the replay buffer.\n",
    "    for _ in range(128):\n",
    "        s_states, s_actions, s_rewards, s_next_states, s_dones = replay_buffer.sample(64)\n",
    "        actor_l, critic_l = agent.learn(s_states, s_actions, s_rewards, s_next_states, s_dones)\n",
    "        ep_actor_loss += actor_l\n",
    "        ep_critic_loss += critic_l\n",
    "        \n",
    "    if i % 25 == 0:\n",
    "        avg_return = compute_avg_return(env, agent, num_episodes=2, render=False)\n",
    "        print(\n",
    "            f'epoch {i}, actor loss {ep_actor_loss / steps}, critic loss {ep_critic_loss / steps} , avg return {avg_return}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387af69-45de-4ff8-a7db-43b01eda44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_avg_return(env, agent, num_episodes=10, render=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0916dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
