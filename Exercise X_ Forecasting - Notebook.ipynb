{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Science and AI for Energy Systems** \n",
    "\n",
    "Karlsruhe Institute of Technology\n",
    "\n",
    "Institute of Automation and Applied Informatics\n",
    "\n",
    "Summer Term 2024\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise X: Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem X.2 (programming) - Load Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. First let us quickly validate our results from exercise X.1 using the LSTM cell implementation in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As all our biases are zero we can set bias=False\n",
    "lstm = nn.LSTMCell(1,1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The wights of the LSTMCell are stored in lstm.wight_ih and lstm.weight_hh\n",
    "print(lstm.weight_ih)\n",
    "print(lstm.weight_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to initialize all weights to 1 to match the LSTM from exercise X.1\n",
    "# Therefore use nn.init.ones_() on all weights of the LSTMCell object (lstm.weight_ih, lstm.weight_hh)\n",
    "nn.init.ones_(lstm.weight_ih)\n",
    "nn.init.ones_(lstm.weight_hh)\n",
    "\n",
    "print(lstm.weight_ih)\n",
    "print(lstm.weight_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now initialize h, C and x as torch.tensor with the values from the exercise\n",
    "h = torch.tensor([[0.0]])\n",
    "C = torch.tensor([[-1.0]])\n",
    "x = torch.tensor([[1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now compute the forward pass of the LSTMCell by calling your initialized LSTMCell object with the input x and the hidden states h and C\n",
    "lstm(x, (h, C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Now we forecast electrical load using multiple deep learning (DL) forecasting algorithms by Neuralforecast utilizing this [dataset](https://data.open-power-system-data.org/time_series/2020-10-06/time_series_60min_singleindex.csv). The documentation for the dataset can be found [here](https://github.com/Open-Power-System-Data/datapackage_timeseries/blob/2020-10-06/main.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) We first load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documentation: https://github.com/Open-Power-System-Data/datapackage_timeseries/blob/2020-10-06/main.ipynb\n",
    "#loading data:\n",
    "\n",
    "# URL of the CSV file\n",
    "url = \"https://data.open-power-system-data.org/time_series/2020-10-06/time_series_60min_singleindex.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Display the column names and data types\n",
    "print(\"\\nColumn names and data types:\")\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Then we format the data so that it follows the format required by the DL library we are using: Neural Forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns we need from the large dataset\n",
    "target_var = 'DE_transnetbw_load_actual_entsoe_transparency'\n",
    "timestamp = 'utc_timestamp'\n",
    "\n",
    "#assigning a unique id for the time series\n",
    "data['unique_id'] = 1\n",
    "#changing format to datetime\n",
    "data[timestamp] = pd.to_datetime(data[timestamp])\n",
    "#setting date as index to facilitate selecting sets\n",
    "data['ds'] = data[timestamp]\n",
    "data.set_index(timestamp, inplace=True)\n",
    "#localizing index for compatability:\n",
    "data.index = data.index.tz_localize(None)\n",
    "\n",
    "#selecting only the needed columns and ignoring first value as it doesn't have a value\n",
    "data = data.loc['2015-01-01':, ['unique_id', 'ds', target_var]]\n",
    "\n",
    "#changing column names to suit neuralforecast\n",
    "data.rename(columns={target_var:'y'}, inplace=True)\n",
    "\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) We split the data into trainset and testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:'2019']\n",
    "test = data['2020':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) We import the algorithms we are interested in, initialize the models with hyperparameters and then pass this to the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast import NeuralForecast as NF\n",
    "from neuralforecast.core import NeuralForecast as TNF\n",
    "from neuralforecast.models import LSTM, NHITS, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 24\n",
    "historical = 24 * 7\n",
    "lstm_model = LSTM(h=horizon,                 # Forecast horizon\n",
    "                  input_size=historical,        # length of previous series to use for forecast\n",
    "                  max_steps=1000,               # Number of epochs to train\n",
    "                  scaler_type='standard',       # Type of scaler to normalize data\n",
    "                  encoder_n_layers=2,\n",
    "                  encoder_hidden_size=100,      # Defines the size of the hidden state of the LSTM\n",
    "                  decoder_hidden_size=512,\n",
    "                  learning_rate=0.0003,\n",
    "                  batch_size=32,\n",
    "                 )\n",
    "\n",
    "mlp_model = MLP(h=horizon,                 # Forecast horizon\n",
    "                  input_size=historical,        # length of previous series to use for forecast\n",
    "                  max_steps=100,               # Number of epochs to train\n",
    "                  scaler_type='standard',       # Type of scaler to normalize data\n",
    "                 )\n",
    "\n",
    "\n",
    "#creating the model and specifying that we have hourly time series\n",
    "nf = NF(models=[lstm_model, mlp_model], freq='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) We pass the training set to the model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) We then use the model for predicting the next 24 hours given the previous week on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daterange = pd.date_range('2020-01-01 00:00', '2020-09-23 00:00', freq='24h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = [nf.predict(test.loc[d:d + pd.Timedelta(hours=7*24), :]) for d in daterange]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_df = pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.index = predictions_df.ds\n",
    "predictions_df.index = predictions_df.index.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = predictions_df.drop(['ds'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([test.y, predictions_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predictions for the complete test set\n",
    "results.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predictions for one month (e.g. July)\n",
    "results['2020-07':'2020-07'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predictions for one month (e.g. July)\n",
    "results['2020-03':'2020-03'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.losses.numpy import mae\n",
    "mae_lstm = mae(results['y'], results['LSTM'])\n",
    "print(\"MAE of LSTM is: \", mae_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_lstm = mae(results['y'], results['MLP'])\n",
    "print(\"MAE of MLP is: \", mae_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the losses for each weekday\n",
    "wd_dict = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "\n",
    "for i in range(7):\n",
    "    wd_results = results[results.index.weekday == i]\n",
    "    wd_mae = mae(wd_results['y'], wd_results['LSTM'])\n",
    "    print(\"MAE of LSTM for \", wd_dict[i], \" is: \", wd_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) Now play a little with the hyperparameters for LSTM. Leave the forecasting horizon and input size as they are.\n",
    "*optional:* You can play with AutoLSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.auto import AutoLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
