{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sUc1_-1Ue8C"
   },
   "source": [
    "## ***Uncertainty Quantification***\n",
    "\n",
    "Exercise 02, \n",
    "May 06,2021\n",
    "\n",
    "\n",
    "Prof. Dr. M.Frank\n",
    "\n",
    "\n",
    "Dr. Jonas Kusch\n",
    "\n",
    "\n",
    "Pia Stammer\n",
    "\n",
    "\n",
    "Maqsood Rajput\n",
    "\n",
    "-----------------------------------------------------------------------------\n",
    "\n",
    "Note: To render Jupyter Notebook and to run code on Bw-cloud instance, please sign up at https://e5188803.ka.bw-cloud-instance.org/ \n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "#### **EXERCISE SHEET 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hCtg8t6OUfe6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sobol_seq\n",
    "from pyDOE import *\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fr50oLoSUt2H"
   },
   "source": [
    "&nbsp;\n",
    "\n",
    "**EXERCISE 1)**\n",
    "\n",
    "Execute the following four sections:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f0icRVYzU5Q6"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   a) In this section we compare different random numbers to approximate the value of  π . What does the code do exactly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZuBV-Jm4VBMK"
   },
   "outputs": [],
   "source": [
    "def generateSamples (N):\n",
    "  samples=np.empty((N,2,5))\n",
    "  labels=list()\n",
    "  # MC\n",
    "  samples[:,:,0]=np.random.uniform(0,1,(N,2))\n",
    "  labels.append('Monte Carlo')\n",
    "\n",
    "  # Latin hypercube\n",
    "  samples[:,:,1]=lhs(2,samples=N,criterion=\"c\")\n",
    "  labels.append('Latin Hyp')\n",
    "\n",
    "  # Halton Set\n",
    "  halton_sample = tfp.mcmc.sample_halton_sequence(2, N,randomized=True, seed=127)\n",
    "  samples[:,:,2] = halton_sample.numpy()\n",
    "  labels.append('Halton')\n",
    "\n",
    "  # Sobol Set\n",
    "  samples[:,:,3]=sobol_seq.i4_sobol_generate(2, N)\n",
    "  labels.append('Sobol')\n",
    "\n",
    "  # Additive Recurrence\n",
    "  alphax=np.random.uniform(0,1,(1,1))\n",
    "  alphay=np.random.uniform(0,1,(1,1))\n",
    "  x=np.cumsum(np.ones((N,1))*alphax)%1\n",
    "  y=np.cumsum(np.ones((N,1))*alphay)%1\n",
    "  samples[:,0,4]=x\n",
    "  samples[:,1,4]=y\n",
    "  labels.append('Additive rec')\n",
    "\n",
    "  return [samples,labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1PjCNE5VGR7"
   },
   "outputs": [],
   "source": [
    "#Numbers of samples \n",
    "N=10000\n",
    "\n",
    "#Generate samples\n",
    "[samples,labels]=generateSamples(N)\n",
    "\n",
    "K= np.size(samples,2)\n",
    "\n",
    "#Check whether samples fall into unit circle\n",
    "inside=np.empty((N,K),dtype=bool)\n",
    "\n",
    "for k in range(K):\n",
    "  inside[:,k]=(samples[:,0,k]**2+samples[:,1,k]**2 <1)\n",
    "\n",
    "#Plot results\n",
    "fig, ax = plt.subplots(3,K,figsize=(4*K, 12))\n",
    "\n",
    "for k in range(K):\n",
    "\n",
    "  #label axes\n",
    "  ax[0,k].set_title(labels[k])\n",
    "  if k==0:\n",
    "    ax[0,k].set_ylabel('All points', rotation=90, size='large')\n",
    "    ax[1,k].set_ylabel('All points zoom', rotation=90, size='large')\n",
    "    ax[2,k].set_ylabel('Error estimating $\\pi$', rotation=90, size='large')\n",
    "    \n",
    "  #Scatter samples and color according to position (in/outside circle)\n",
    "  ax[0,k].scatter(samples[inside[:,k],0,k], samples[inside[:,k],1,k], c='green',s=3,marker='^')\n",
    "  ax[0,k].scatter(samples[~inside[:,k],0,k], samples[~inside[:,k],1,k], s=3,c='red')\n",
    "\n",
    "  #Show zoomed region (point structure)\n",
    "  ax[1,k].scatter(samples[inside[:,k],0,k], samples[inside[:,k],1,k], c='green',s=3,marker='^')\n",
    "  ax[1,k].scatter(samples[~inside[:,k],0,k], samples[~inside[:,k],1,k], s=3,c='red')\n",
    "  ax[1,k].set_xlim([0.6, 0.8])\n",
    "  ax[1,k].set_ylim([0.6, 0.8])\n",
    "\n",
    "  #Show error convergence\n",
    "  xmean=np.divide(np.cumsum(inside[:,k]),range(1,N+1))\n",
    "  ax[2,k].loglog(abs(math.pi-4*xmean))\n",
    "  ax[2,k].loglog(range(1,N+1),np.divide(1,np.sqrt(range(1,N+1))))\n",
    "  ax[2,k].set_xlim([10**0, N])\n",
    "  ax[2,k].set_ylim([1/N, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G37ChkJzVTor"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   b) What are we computing here? What happens if the number of batches tends towards infinity. How can you interpret the observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HCOQgSvkVgUj"
   },
   "outputs": [],
   "source": [
    "def giveError(N):\n",
    "\n",
    "  #Generate samples\n",
    "  [samples,labels]=generateSamples(N)\n",
    "  K= np.size(samples,2)\n",
    "\n",
    "  #Check whether samples fall into unit circle\n",
    "  inside=np.empty((N,K),dtype=bool)\n",
    "  error=np.empty((N,5))\n",
    "\n",
    "  for k in range(K):\n",
    "    inside[:,k]=(samples[:,0,k]**2+samples[:,1,k]**2 <1)\n",
    "    \n",
    "    #Compute error\n",
    "    xmean=np.divide(np.cumsum(inside[:,k]),range(1,N+1))\n",
    "    error[:,k]= abs(math.pi-4*xmean)\n",
    "\n",
    "  return error, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSgx8U9pVkMv"
   },
   "outputs": [],
   "source": [
    "#Number of samples per batch\n",
    "nSamples=10000\n",
    "\n",
    "#Number of batches\n",
    "nBatches=20\n",
    "\n",
    "for i in range(nBatches):\n",
    "  #print(\"Batch {0} of {1}\".format(i+1,nBatches))\n",
    "  if i==0:\n",
    "    [err , labels] = giveError(nSamples)\n",
    "  else:\n",
    "    err=err + giveError(nSamples)[0]\n",
    "\n",
    "err = np.divide(err, nBatches)\n",
    "\n",
    "#Plot results\n",
    "fig, ax = plt.subplots(1,K,figsize=(4*K, 4))\n",
    "for k in range(K):\n",
    "\n",
    "  #label axes \n",
    "  ax[k].set_title(\"{0}, {1} runs\".format(labels[k], nBatches))\n",
    "\n",
    "  if k==0:\n",
    "    ax[k].set_ylabel('Error estimating $\\pi$', rotation=90, size='large')\n",
    "\n",
    "  ax[k].loglog(err[:,k],label=labels[k])\n",
    "  ax[k].loglog(range(1,nSamples+1),np.divide(1,np.sqrt(range(1,nSamples+1))),label='order 1/2')\n",
    "  ax[k].loglog(range(1,nSamples+1),np.divide(1,np.arange(1,nSamples+1)**(2/3)),label='order 2/3')\n",
    "  ax[k].loglog(range(1,nSamples+1),np.divide(1,range(1,nSamples+1)),label='order 1')\n",
    "\n",
    "  #label curves\n",
    "  ax[k].legend(shadow=True, fancybox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SkobqF0JWY65"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   c) Interpret the graph created by c) and comment on random numbers vs. quasi random numbers (or Monte-Carlo vs. Quasi-Monte-Carlo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ghDVJZE3Whni"
   },
   "outputs": [],
   "source": [
    "#Compute and plot log(N)^d/N for different values of N and d\n",
    "toNormalize=True\n",
    "\n",
    "N=10**np.arange(1,19)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "for d in range(1,11):\n",
    "  err=np.divide(np.log(N)**d,N)\n",
    "  if toNormalize:\n",
    "    err = np.divide(err,err[0])\n",
    "  plt.loglog(N, err,label=\"dim = {0}\".format(d))\n",
    "\n",
    "err = np.divide(1,np.sqrt(N))\n",
    "\n",
    "if toNormalize:\n",
    "  err = np.divide(err, err[0])\n",
    "\n",
    "plt.loglog(N,err,label=\"1/sqrt(N)\", c=\"black\",linewidth=4)\n",
    "plt.title(\"Order of convergence for log(N)^d/N\")\n",
    "plt.xlabel(\"N\")\n",
    "_ = plt.legend(shadow=True, fancybox=True,loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GWl_zg59WlIk"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   d) It is often said, that Monte-Carlo does not suffer from the Curse of dimensionality.          \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   How does this statement and the observations from d) fit together?\n",
    "\n",
    "&nbsp; \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   *Note: Don't worry if the graph doesn't appear immediately, computation can take a minute*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KEIHt_3EXBOr"
   },
   "outputs": [],
   "source": [
    "def volb1(dim):\n",
    "  return math.pi**(dim/2)/math.gamma(dim/2 + 1)\n",
    "\n",
    "volb= np.vectorize(volb1)\n",
    "\n",
    "def volc(dim):\n",
    "  return 2**dim\n",
    "  \n",
    "#Halton set\n",
    "\n",
    "dim = 10\n",
    "nSamples = 10**np.arange(1,7)\n",
    "nBatches = 10\n",
    "error = np.empty((np.size(nSamples),dim))\n",
    "\n",
    "sample = tfp.mcmc.sample_halton_sequence(dim, num_results=nSamples[np.size(nSamples)-1],randomized=True, seed=127)\n",
    "sample= sample.numpy()\n",
    "\n",
    "for batch in range(1,nBatches+1):\n",
    "  #print(\"Batch {0} of {1}\".format(i+1,nBatches))\n",
    "\n",
    "  for i in range(np.size(nSamples)):\n",
    "    n=nSamples[i]\n",
    "    inside = np.empty((n,dim),dtype=bool)\n",
    "    r=np.random.randint(0,nSamples[np.size(nSamples)-1]-n+1)\n",
    "    p=sample[r:r+n-1,:]\n",
    "    # sequencer=ghalton.Halton(dim)\n",
    "    # p= sequencer.get(10)\n",
    "    # p=np.random.shuffle(p)\n",
    "    s= 2*np.subtract(p,0.5)\n",
    "    s=np.cumsum(s**2,1)\n",
    "    inside = sum(s <= 1)\n",
    "    volbmc = np.multiply(np.divide(inside,n),volc(np.arange(1,dim+1)))\n",
    "    error [i,:]= np.divide(np.absolute(volbmc - volb(np.arange(1,dim+1))),volb(np.arange(1,dim+1)))\n",
    "    \n",
    "\n",
    "  if batch==1:\n",
    "    Error=error\n",
    "  else:\n",
    "    Error= Error + error\n",
    "\n",
    "Error = np.divide(Error,nBatches)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "for d in range(1,dim):\n",
    "  plt.loglog(nSamples, Error[:,d], label=\"Halton {0}\".format(d+1),linestyle='dashed')\n",
    "\n",
    "\n",
    " #Uniform random numbers\n",
    "\n",
    "for batch in range(1,nBatches+1):\n",
    "  #print(\"Batch {0} of {1}\".format(i+1,nBatches))\n",
    "  i=0\n",
    "  for n in nSamples:\n",
    "    inside = np.empty((n,dim),dtype=bool)\n",
    "    p=np.random.uniform(0,1,(n,dim))\n",
    "    s= 2*np.subtract(p,0.5)\n",
    "    s=np.cumsum(s**2,1)\n",
    "    inside = sum(s <= 1)\n",
    "    volbmc = np.multiply(np.divide(inside,n),volc(np.arange(1,dim+1)))\n",
    "    error [i,:]= np.divide(np.absolute(volbmc - volb(np.arange(1,dim+1))),volb(np.arange(1,dim+1)))\n",
    "    i = i+1\n",
    "\n",
    "  if batch==1:\n",
    "    Error=error\n",
    "  else:\n",
    "    Error= Error + error\n",
    "\n",
    "Error = np.divide(Error,nBatches)\n",
    "\n",
    "\n",
    "for d in range(1,dim):\n",
    "  plt.loglog(nSamples, Error[:,d], label=\"MC {0}\".format(d+1))\n",
    "\n",
    "_ = plt.legend(shadow=True, fancybox=True,loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f-71AfqOXPCD"
   },
   "source": [
    "&nbsp;\n",
    "\n",
    "**EXERCISE 2)**\n",
    "\n",
    "Consider the SIR-Model ([1]-[4]) for the spread of infectious diseases like the Corona virus.\n",
    "\n",
    "The total population N is divided into the groups of **susceptible**, **infected** and **removed** (i.e. dead or recovered) individuals: \n",
    "**N = S + I + R**\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "![alt text](https://drive.google.com/uc?export=view&id=1Cv8I_JHky6dLBO9JTobvYkKAaL9Qy0_X)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "The model assumes that\n",
    "- every individual can only be infected once, then they are either immune or dead\n",
    "- the categories dead and immune are summarized in the compartment \"Removed\"\n",
    "- there is no incubation time\n",
    "- the respective rates are constant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j6BGLVAZd3Ol"
   },
   "source": [
    "The following system of equations describes the flow of population between the three compartments\n",
    "\n",
    "&nbsp;\n",
    "$$ \\frac{dS(t)}{dt} = - \\beta S(t) I(t)$$\n",
    "\n",
    "&nbsp;\n",
    "$$ \\frac{dI(t)}{dt} = \\beta S(t) I(t) - \\gamma I(t)$$\n",
    "\n",
    "&nbsp;\n",
    "$$ \\frac{dR(t)}{dt} = \\gamma I(t)$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "$N(t)=N,\\;\\; S(t)\\geq0,\\;\\; I(t)\\geq0, \\;\\;R(t)\\geq0$,\n",
    "\n",
    "where the parameter $\\beta$ represents the infection rate (number of infections per infected person per time unit (day)) and $\\gamma$ represents the combined death and recovery rate (number of dead or recovered individuals per infectious person per day). \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  (a) Derive a formula for the maximum number of infectious individuals depending on $\\beta$ and $\\gamma$.           \n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;   How do $S(t), I(t) $and $R(t)$ behave for $t \\rightarrow \\infty$ ?\n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  What can you say about the growth rate of infections?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bE6nHrL6HSHS"
   },
   "source": [
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  (b) Run the code below for different choices of $\\beta$ and $\\gamma$, how does the number of infections react?            \n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  Can you explain the development of the different curves for specific parameter choices? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M64Vkb1nHFD0"
   },
   "outputs": [],
   "source": [
    "### (b) ###\n",
    "\n",
    "#Set parameters\n",
    "N = 80000000  #Population size\n",
    "S_0 = N       #Number susceptible at time 0\n",
    "I_0 = 1       #Number infected at time 0\n",
    "R_0 = 0       #Number removed (recovered & immune or dead) at time 0\n",
    "\n",
    "beta = 0.9/9/N          # number infections caused by one infected person per day\n",
    "                        #(Currently in Germany, one person infects 0.9 other people while infectious (reproduction number) and is infectious for 8-10 days)\n",
    "gamma = 0.0353 + 0.025  # death + recovery rate, number dead or recovered per day\n",
    "\n",
    "t_0 = 0\n",
    "t_end = 200            # timesteps\n",
    "h=5                    # time step = h days\n",
    "\n",
    "def f_S(I_t,S_t,beta):\n",
    "  return -beta*I_t*S_t\n",
    "\n",
    "def f_I(I_t,S_t,beta,gamma):\n",
    "  return beta*S_t*I_t-gamma*I_t\n",
    "\n",
    "def f_R(I_t,gamma):\n",
    "  return gamma*I_t\n",
    "\n",
    "def Euler (f_I,f_S,f_R,t_0,I_0,S_0,R_0,h,T,beta,gamma):\n",
    "  times = t_0 + np.arange(T+1)*h\n",
    "  I=np.zeros(T+1,)\n",
    "  I[0]=I_0\n",
    "\n",
    "  S=np.zeros(T+1,)\n",
    "  S[0]=S_0\n",
    "\n",
    "  R=np.zeros(T+1,)\n",
    "  R[0]=R_0\n",
    "  \n",
    "  for t in range(T):\n",
    "    S[t+1] = min(max(S[t] + h*f_S(I[t],S[t],beta),0),N)\n",
    "    I[t+1] = min(max(I[t] + h*f_I(I[t],S[t],beta,gamma),0),N)\n",
    "    R[t+1] = min(max(R[t] + h*f_R(I[t],gamma),0),N)\n",
    "  return S, I, R\n",
    "\n",
    "S, I, R = Euler(f_I,f_S,f_R,t_0,I_0,S_0,R_0,h,t_end,beta,gamma)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(h*np.arange(t_end+1),S,label=\"Number Susceptible\")\n",
    "plt.plot(h*np.arange(t_end+1),I,label=\"Number Infected\")\n",
    "plt.plot(h*np.arange(t_end+1),R,label=\"Number Recovered or Dead\")\n",
    "_=plt.legend(shadow=True, fancybox=True)\n",
    "print(\"Reproduction Number = {0}\".format(beta*9*N))\n",
    "print(\"Max. number of infections = {0}\".format(gamma/beta*math.log(gamma/beta)-gamma/beta-gamma/beta*math.log(S_0)+N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DbVOfcKeHGVq"
   },
   "source": [
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  (c) The last weeks have shown that there is a lot of uncertainty in Corona virus predictions, since the actual parameter values depend on uncertain/unknown factors \n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  such as the effect of restrictive measures, discipline of the population or quality and availability of intensive care. \n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  Therefore we now assume the parameters $Q=(\\beta,\\gamma)^T$ are normally distributed with\n",
    "\n",
    "$$\\text{ mean } \\bar q =\\begin{pmatrix}  0.1333/N \\\\ 0.1 \\end{pmatrix}$$\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n",
    "$$\\text{ covariance matrix } \\sigma_q^2 = \\begin{pmatrix} (0.05/N)^2 & 0\\\\ 0 & 0.02^2\\end{pmatrix}.$$\n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  We are interested in the mean and the variance of $S, I $ and $R$ over a time span of 1000 days. \n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  To approximate these, use Monte-Carlo and\n",
    "quasi-Monte-Carlo sampling and fill in the indicated blanks in the second code block.           \n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  Run the code for an increasing number of samples, what do you observe? \n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  Take a look at the expected values for $S, I, R$ $\\pm$ the respective standard deviations. \n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  In which sense are the values unrealistic? Could the model be extended to be more realistic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jTKAPwDOnW5X"
   },
   "outputs": [],
   "source": [
    "### (c) ###\n",
    "\n",
    "#Numbers of samples \n",
    "M=10000\n",
    "\n",
    "#Generate samples for MC and quasi-MC\n",
    "q_bar = np.zeros(2,)\n",
    "q_bar[0] = 1.2/9/N\n",
    "q_bar[1] = 0.08\n",
    "sigma_q = np.zeros((2,2))\n",
    "sigma_q[0][0] = (0.04/N)**2\n",
    "sigma_q[1][1] = 0.02**2\n",
    "\n",
    "## Fill in the blanks below ###\n",
    "\n",
    "## Generate random samples of beta and gamma for MC and qMC (according to distribution given in exercise above)\n",
    "samples_MC=  #random sample\n",
    "\n",
    "samples_qMC= #// \n",
    "             # You might need several lines here, use quasi-random sequence of your choice from Exercise 1\n",
    "             #//\n",
    "\n",
    "#Initialise \n",
    "mu_qMC = np.zeros((3,t_end+1))\n",
    "mu_MC = np.zeros((3,t_end+1))\n",
    "Sn_qMC = np.zeros((3,t_end+1))\n",
    "Sn_MC = np.zeros((3,t_end+1))\n",
    "\n",
    "result_i_qMC = np.zeros((3,t_end+1))\n",
    "result_i_MC = np.zeros((3,t_end+1))\n",
    "\n",
    "## This loop incrementally computes mean and variances for S, I and R over random samples for MC and qMC\n",
    "for i in range(M):\n",
    "    \n",
    "  ## Look up incremental mean and variance computation and fill in blanks\n",
    "\n",
    "                #// Fill in Monte Carlo mean/variance computation here...\n",
    " result_i_MC =  #   :\n",
    "                #   :\n",
    "  mu_qMC =      #   :\n",
    "  Sn_qMC =      #//\n",
    "    \n",
    "\n",
    "                  #// Fill in quasi-Monte Carlo mean/variance computation here...\n",
    "  result_i_qMC =  #   :\n",
    "                  #   :\n",
    "  mu_MC =         #   :\n",
    "  Sn_MC =         #//\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(h*np.arange(t_end+1),mu_qMC[1,:],linestyle='dotted',color='blue', label = \"Expected number of infections (quasi MC)\")\n",
    "plt.plot(h*np.arange(t_end+1),mu_MC[1,:],color='blue', label = \"Expected number of infections (MC)\")\n",
    "plt.plot(h*np.arange(t_end+1),Sn_qMC[1,:],linestyle='dotted',color='red', label = \"Std. Dev of number of infections (quasi MC)\")\n",
    "plt.plot(h*np.arange(t_end+1),Sn_MC[1,:],color='red', label = \"Std. Dev number of infections (MC)\")\n",
    "plt.title(\"qMC vs. MC for number of infections\")\n",
    "_=plt.legend(shadow=True, fancybox=True)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(27,11))\n",
    "\n",
    "ax[0].plot(h*np.arange(t_end+1),mu_MC[0,:],label=\"Expected number Susceptible\",color='C0')\n",
    "ax[0].plot(h*np.arange(t_end+1),mu_MC[0,:]+Sn_MC[0,:],linestyle='dotted',color='C0', label=r'$E[S] \\pm \\sigma_S$')\n",
    "ax[0].plot(h*np.arange(t_end+1),mu_MC[0,:]-Sn_MC[0,:],linestyle='dotted',color='C0')\n",
    "ax[0].plot(h*np.arange(t_end+1),mu_MC[1,:],label=\"Expected number Infected\",color='C1')\n",
    "ax[0].plot(h*np.arange(t_end+1),mu_MC[1,:]+Sn_MC[1,:],linestyle='dotted',color='C1', label=r'$E[I] \\pm \\sigma_S$')\n",
    "ax[0].plot(h*np.arange(t_end+1),mu_MC[1,:]-Sn_MC[1,:],linestyle='dotted',color='C1')\n",
    "ax[0].plot(h*np.arange(t_end+1),mu_MC[2,:],label=\"Expected number Recovered or Dead\",color='C2')\n",
    "ax[0].plot(h*np.arange(t_end+1),mu_MC[2,:]+Sn_MC[2,:],linestyle='dotted',color='C2', label=r'$E[R] \\pm \\sigma_S$')\n",
    "ax[0].plot(h*np.arange(t_end+1),mu_MC[2,:]-Sn_MC[2,:],linestyle='dotted',color='C2')\n",
    "ax[0].set_title(\"S,I,R expected values & standard dev. Monte Carlo\")\n",
    "_=ax[0].legend(shadow=True, fancybox=True)\n",
    "\n",
    "\n",
    "ax[1].plot(h*np.arange(t_end+1),mu_qMC[0,:],label=\"Expected number Susceptible\",color='C0')\n",
    "ax[1].plot(h*np.arange(t_end+1),mu_qMC[0,:]+Sn_qMC[0,:],linestyle='dotted',color='C0', label=r'$E[S] \\pm \\sigma_S$')\n",
    "ax[1].plot(h*np.arange(t_end+1),mu_qMC[0,:]-Sn_qMC[0,:],linestyle='dotted',color='C0')\n",
    "ax[1].plot(h*np.arange(t_end+1),mu_qMC[1,:],label=\"Expected number Infected\",color='C1')\n",
    "ax[1].plot(h*np.arange(t_end+1),mu_qMC[1,:]+Sn_qMC[1,:],linestyle='dotted',color='C1', label=r'$E[I] \\pm \\sigma_S$')\n",
    "ax[1].plot(h*np.arange(t_end+1),mu_qMC[1,:]-Sn_qMC[1,:],linestyle='dotted',color='C1')\n",
    "ax[1].plot(h*np.arange(t_end+1),mu_qMC[2,:],label=\"Expected number Recovered or Dead\",color='C2')\n",
    "ax[1].plot(h*np.arange(t_end+1),mu_qMC[2,:]+Sn_qMC[2,:],linestyle='dotted',color='C2', label=r'$E[R] \\pm \\sigma_S$')\n",
    "ax[1].plot(h*np.arange(t_end+1),mu_qMC[2,:]-Sn_qMC[2,:],linestyle='dotted',color='C2')\n",
    "ax[1].set_title(\"S,I,R expected values & standard dev. quasi-Monte Carlo\")\n",
    "_=ax[1].legend(shadow=True, fancybox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "##### Some literature on the SIR-Model:\n",
    "\n",
    "[1] *Kermack, W. O., & McKendrick, A. G.* (1927). A contribution to the mathematical theory of epidemics. Proceedings of the royal society of london. Series A, Containing papers of a mathematical and physical character, 115(772), 700-721.\n",
    "\n",
    "[2] *Brauer, F.* (2005). The Kermack–McKendrick epidemic model revisited. Mathematical biosciences, 198(2), 119-131.\n",
    "\n",
    "[3] *Satsuma, J., Willox, R., Ramani, A., Grammaticos, B., & Carstea, A. S.* (2004). Extending the SIR epidemic model. Physica A: Statistical Mechanics and its Applications, 336(3-4), 369-375.\n",
    "\n",
    "[4] *Nesteruk, I.* (2020). SIR-simulation of Corona pandemic dynamics in Europe. medRxiv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2U8OZQvndx5"
   },
   "source": [
    "###### &nbsp;\n",
    "\n",
    "**EXERCISE 3)**\n",
    "\n",
    "Assume that we have a sequence of random numbers $(Z_l)_{l\\in \\mathbb{N}}$ with $Z = \\lim_{l \\rightarrow \\infty} Z_l.^{[1]}$ We wish to approximate $E[Z]$ with Multi-Level Monte Carlo (MLMC) by sampling from finitely many $Z_l$ with $l = 0,...,L$. The MLMC estimator Y is given by\n",
    "\n",
    "$$ Y = \\sum_{l=0}^L Y_l, \\quad Y_l = N_l^{-1}\\sum_{n=1}^{N_l}(Z^{(l,n)}-Z^{(l-1,n)})$$\n",
    "\n",
    "with $Z_{-1} =0$.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  (a) Show that \n",
    "\n",
    "$$ E[Y] = E[Z_L], \\quad \\text{Var}[Y] = \\sum_{l=0}^L N_l^{-1}V_l, \\text{ with } V_l = \\text{Var}[Z_l-Z_{l-1}]. $$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  (b) Show that the mean square error (MSE) of the MLMC estimator $Y$ with maximum level $L$ is given by \n",
    "\n",
    "$$ MSE = E[(Y-E[Z])^2] = \\text{Var}[Y] + (E[Y]-E[Z])^2.$$\n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;     Interpret the two terms on the right hand side by using the results from part (a). What is the difference to the lecture?                         \n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  How does the choice of $L, N_0, ... , N_L$ affect each term?\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  (c) We now want to choose the maximum level $L$ as well as the number of samples for each level such that $MSE \\leq \\epsilon^2$                             \n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  while the costs are minimal. Assume that the given sequence fulfills\n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  (i) $\\vert Z_l-Z\\vert \\leq c_1 2^{-\\alpha l}$,\n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  (ii) $V_l \\leq c_2 2^{-\\beta l}$,\n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  (iii) $K_l \\leq c_3 2^{\\gamma l}$,\n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  with costs $K_l$. Show that the optimal parameters $L, N_0, ..., N_L$ in terms of $\\alpha, \\beta, \\gamma, c_{1,2,3} $ are\n",
    "\n",
    "$$L = \\left\\lceil{-\\frac{1}{\\alpha}\\log_2\\left(\\frac{\\varepsilon}{\\sqrt{2}c_1}\\right)}\\right\\rceil, \\quad N_l = \\left\\lceil{\\frac{2c_2}{\\varepsilon^2}2^{-(\\gamma+\\beta)l/2}\\sum_{k = 0}^{L} 2^{(\\gamma-\\beta)k/2}}\\right\\rceil.$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  **Hint:** Start with $\\text{Var}[Y]=\\varepsilon^2/2$ and $(E[Y]-E[Z])^2=\\varepsilon^2/2$ to ensure $MSE=\\varepsilon^2$.\n",
    "\n",
    "----\n",
    "$^{[1]}$ You can think of $Z_l$ as an approximation of an ODE solution resulting from a numerical discretization with refinement level $l$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GhWe-JUyypgu"
   },
   "source": [
    "&nbsp;\n",
    "\n",
    "**EXERCISE 4)**\n",
    "\n",
    "We now want to use the results from Exercise 3 in an application. For this, run the code below and consider questions (a)-(c).\n",
    "\n",
    "Let $Z=\\mathcal{u}(t_{end},X)$, where $X$ is uniformly distributed in [0,1] and $\\mathcal{u}$ is the solution of\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    " \\mathcal{u'} = -\\lambda \\mathcal{u}, \\;\\:\\: \\mathcal{u}(t=0,X) = X,\n",
    "$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "The sequence $Z_l$ is given by an Euler method\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "\\mathcal{u_l}(t_{n+1}) = \\mathcal{u_l}(t_{n}) - \\Delta t_c 2^{-l} \\lambda \\mathcal{u_l}(t_{n}), \\;\\:\\: \\mathcal{u_l}(0,X) = X.\n",
    "$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  (a) Determine $\\alpha, \\beta, \\gamma$ and estimate $c_1,c_2,c_3$ from Exercise 3 c).\n",
    "                             \n",
    " &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **Hint:** Use a Taylor expansion to estimate $c_1,c_2,c_3$.\n",
    "\n",
    " &nbsp;\n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  (b) Look at the code and add the parameters for this problem.\n",
    "\n",
    " &nbsp;\n",
    " \n",
    " &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  (c) Think of advantages and disadvantages of MLMC. Can we obtain the expected value with given accuracy if the problem parameters are not known?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIUrXPUB0UZB"
   },
   "outputs": [],
   "source": [
    "#Define required functions\n",
    "\n",
    "#Function to determine sequence Zl\n",
    "def ForwardEuler (l,tEnd,dtCoarse,X,lambda_c):\n",
    "  if l==-1:\n",
    "    Pl = 0\n",
    "    cost = 0\n",
    "    return Pl, cost\n",
    "  \n",
    "  dt = dtCoarse*2**(-l)\n",
    "  Nt = round(tEnd/dt)\n",
    "\n",
    "  u=X\n",
    "\n",
    "  for i in range(1,Nt+1):\n",
    "    uNew = u -dt*lambda_c*u\n",
    "    u=uNew\n",
    "\n",
    "  Pl = u\n",
    "  cost = Nt\n",
    "\n",
    "  return Pl, cost\n",
    "\n",
    "### Multi-Level Monte Carlo Estimator ###\n",
    "def MCLevelEstimator (l, Nl, tEnd, dtCoarse, lambda_c):\n",
    "  Yl=0\n",
    "  for i in range(1,Nl+1):\n",
    "    X=np.random.uniform(0,1,(1,1))\n",
    "    ZlM, costLM = ForwardEuler (l-1,tEnd,dtCoarse,X,lambda_c)\n",
    "    Zl, costL = ForwardEuler (l,tEnd,dtCoarse,X,lambda_c)\n",
    "    Yl = Yl + Zl -ZlM\n",
    "\n",
    "  Yl = (1/Nl)*Yl\n",
    "  cost= (costLM + costL)*Nl \n",
    "\n",
    "  return Yl, cost\n",
    "\n",
    "def MLMCEstimator (alpha, beta, gamma, c1, c2, c3, epsilon, tEnd, lambda_c):\n",
    "  dtCoarse=1/lambda_c\n",
    "\n",
    "  #Compute L according to Exercise 3c)\n",
    "  L = math.ceil (math.log2(epsilon/(np.sqrt(2)*c1))/(-alpha))\n",
    "  print(\"Max number of levels is {0}\".format(L))\n",
    "\n",
    "  #Compute number of MC samples for all levels according to Exercise 2c)\n",
    "  N= np.zeros((L+1,1))\n",
    "  sumL= sum(2**((gamma-beta)*np.arange(L+1)/2))\n",
    "\n",
    "  for l in range(L+1):\n",
    "    N[l]=math.ceil(2*c2/(epsilon**2)*2**(-0.5*(gamma+beta)*l)*sumL)\n",
    "  \n",
    "  Y=0\n",
    "  cost=0\n",
    "\n",
    "  for l in range(L+1):\n",
    "    print(\"Computing level {0}\".format(l))\n",
    "    print(\"Number MC samples is {0}\".format(N[l]))\n",
    "    Yl, costL = MCLevelEstimator(l,int(N[l]),tEnd,dtCoarse,lambda_c)\n",
    "    Y = Y + Yl\n",
    "    cost = cost + costL\n",
    "  \n",
    "  return Y, cost, L\n",
    "\n",
    "\n",
    "### Monte Carlo Estimator ###\n",
    "def MCEstimator (L, N, tEnd, dt, lambda_c):\n",
    "  Yl = 0\n",
    "  for i in range(1,N+1):\n",
    "    Z=np.random.uniform(0,1,(1,1))\n",
    "    Pl , costL = ForwardEuler (L,tEnd,dt,Z,lambda_c)\n",
    "    Yl = Yl + Pl\n",
    "  \n",
    "  Yl= (1/N)*Yl\n",
    "  cost = costL*N\n",
    "\n",
    "  return Yl, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hAVLi68A0ZhN"
   },
   "outputs": [],
   "source": [
    "#Set parameters and compare methods MLMC and MC\n",
    "\n",
    "#Problem settings\n",
    "tEnd = 2\n",
    "lambda_c = 0.5\n",
    "dtCoarse = 1/lambda_c\n",
    "\n",
    "#Desired level of accuracy\n",
    "epsilon = 0.001\n",
    "\n",
    "#-----------MLMC------------#\n",
    "#Properties of numerical discretization\n",
    "#Convergence/growth parameters\n",
    "alpha = \n",
    "beta =                                    #\n",
    "gamma =                                   #  \\\n",
    "#Constants                                   fill in values for parameters and constants\n",
    "c1 =                                      #  /\n",
    "c2 =                                      #\n",
    "c3 = \n",
    "\n",
    "#Approximate expectation value with MLMC & return cost and max. refinement level\n",
    "Y, cost, L = MLMCEstimator (alpha, beta, gamma, c1, c2, c3, epsilon, tEnd, lambda_c)\n",
    "\n",
    "#------------MC-------------#\n",
    "#Estimate max. samples to reach desired accuracy with MC\n",
    "V0 = c1**2\n",
    "nMC = math.ceil(V0/epsilon**2)\n",
    "\n",
    "print(\"Number of MC samples is {0}\".format(nMC))\n",
    "\n",
    "YMC, costMC = MCEstimator (L,nMC, tEnd, dtCoarse, lambda_c)\n",
    "\n",
    "#----------Results----------#\n",
    "print(\"MC error is {0}\".format(abs(0.5*math.exp(-lambda_c*tEnd)-YMC)))\n",
    "print(\"MLMC error is {0}\".format(abs(0.5*math.exp(-lambda_c*tEnd)-Y)))\n",
    "\n",
    "if costMC/cost > 1:\n",
    "  print(\"MC is {0} times more expensive than MLMC\".format(costMC/cost))\n",
    "else:\n",
    "  print(\"MLMC is {0} times more expensive than MC\".format(cost/costMC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CwUTVy0m3R-n"
   },
   "source": [
    "&nbsp;\n",
    "\n",
    "### **Additional Exercises**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4gUJg5X3lqJ"
   },
   "source": [
    "&nbsp;\n",
    "\n",
    "**EXERCISE 5)**\n",
    "\n",
    "Use the Chebyshev's Lemma to prove the Weak Law of Large Numbers.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "*Chebyshev's Lemma:* \n",
    "\n",
    "Let $X$ be a random variable with mean $\\mu$ and variance $\\sigma^2$. Then for any real number $r$ we have \n",
    "&nbsp;\n",
    "\n",
    "$$ P(| X - \\mu | \\geq r\\sigma) \\leq \\frac{1}{r^2}.$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "*The Weak Law of Large Numbers:*\n",
    "\n",
    "Let $X_1,X_2,...$ be an infinite sequence of iid random variables with finite mean $E(X_1) = E(X_2) = ... = \\mu$ and finite variance $Var(X_1) = Var(X_2) = ... = \\sigma^2$, and define the sequence of means \n",
    "&nbsp;\n",
    "\n",
    "$$ \\bar X_n = \\frac{1}{n}(X_1 + ... + X_n).$$\n",
    "&nbsp;\n",
    "\n",
    "Show that, as $n \\rightarrow \\infty$, $\\bar X_n$ converges to $\\mu$ in probability, i.e. for every $\\epsilon > 0$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "$$ \\lim_{n \\to \\infty} P(|\\bar X_n -\\mu | \\geq \\epsilon) = 0.$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**EXERCISE 6)**\n",
    "\n",
    "Consider the random ODE\n",
    "\n",
    "$$ \\frac{du}{dt}(t,Z) = \\alpha(Z) u(t,Z), \\;\\;\\; u(0,Z) = \\beta(Z).$$\n",
    "\n",
    "Assume that $\\alpha$ is uniformly distributed on the interval $[a,b]$, and that $\\beta$ is Gaussian normal distributed.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  (a) Compute the expected value and the variance of $u(t,Z)$ at a fixed time $t$.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  (b) Use Python to approximate the expectation value of $u(t,Z)$ by Monte-Carlo sampling.         \n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  How does the approximation behave with the sample size?\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  (c) Approximate also the standard deviation of $u(t,Z)$."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "f0icRVYzU5Q6",
    "G37ChkJzVTor",
    "SkobqF0JWY65",
    "GWl_zg59WlIk",
    "EgS6QUtnlrHX",
    "GhWe-JUyypgu"
   ],
   "name": "Exercise_2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
