{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geodatenanalyse 1\n",
    "\n",
    "## Übung 7: Schließende und bivariate Statistik\n",
    "\n",
    "###  1. Schließende Statistk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dieser Übung wollen wir uns die Grundwasserdaten aus der letzten Übung genauer anschauen, und von den gemessenenen Stichproben auf die Gesamtgrundheit der Werte im Hardtwald schließen. \n",
    "\n",
    "Lest dazu zunächst wieder den Datensatz (Data_GW_KA.csv) in Python ein, und unterteilt ihn in die Daten aus dem Bereich des Waldes und der Stadt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der letzten Übung habt Ihr herausgefunden, dass man die Verteilung der gemessenenen Grundwassertemperaturen in der Stadt als normalverteilt annehmen kann. Angenommen Ihr wollt anschließend eine Modellierung durchführen und benötigt 50 zufällig verteilte Werte, die die gleiche statistische Verteilung aufweisen wie die gemessenen Grundwassertemperaturen. \n",
    "\n",
    "Dazu müsst Ihr zuerst herausfinden, welche Normalverteilung am besten auf Eure Daten passt. Also welchen Mittelwert und welche Varianz diese theoretische Verteilung hat. Benutzt dafür die Funktion `scipy.stats.norm.fit()`, die zwei Outputs ($\\mu$, $\\sigma$<sup>2</sup>) hat und als Input den gewünschten Datensatz braucht. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der Funktion `scipy.stats.norm.rvs()` könnt Ihr nun beliebig viele Zufallswerte generieren. Als Inputs müsst Ihr dafür die zwei statistischen Momente der Normalverteilung angeben, sowie die gewünschte Zahl an Werten (50). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspiziert nun die erzeugten Werte. Lasst Euch zur Kontrolle auch Mittelwert und Varianz der erzeugten Werte anzeigen, und vergleicht diese mit den Werten der angepassten theoretischen Normalverteilung. Stimmen diese überein?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wahrscheinlich stimmen die Mittelwerte und Varianzen nicht genau überein. Das könnte an der recht geringen Anzahl an generierten Werten (50) liegen. Versucht es daher mal mit 500.000 Werten, und überprüft die statistischen Parameter erneut. Frei nach dem Prinzip \"Viel hilft viel\". \n",
    "\n",
    "Hinweis: mit n=500.000 kann das je nach Laptop etwas dauern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mittelwert und Varianz sollten nun näher an den angepassten Werten von oben liegen. Allerdings führt die große Anzahl an Werten (neben längeren Rechenzeiten) bei vielen Verteilungen zu einem anderen Problem. Bestimmt für den eben erzeugten Datensatz den Minimal- und Maximalwert. Was fällt dabei auf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basierend auf den gemessenen Daten erscheinen diese Werte viel zu niedrig, bzw. zu hoch. Das liegt daran, dass mit einer hohen Anzahl an Zufallswerten auch Werte in den Extrembereichen mit sehr geringen Wahrscheinlichkeiten generiert werden. \n",
    "\n",
    "Das lässt sich vermeiden, indem man mit gestutzten (engl. truncated) Verteilungen arbeitet. Die Funktion `scipy.stats.truncnorm.rvs()` generiert solche Verteilungen. Dafür müssen vor den statistischen Momenten zwei Skalierungsparameter (*a*, *b*) angegeben werden: \n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?a&space;=&space;(Minimalwert&space;-&space;mean)/&space;std\" title=\"a = (Minimalwert - mean)/ std\" />\n",
    "\n",
    "und \n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?b&space;=&space;(Maximalwert&space;-&space;mean)/&space;std\" title=\"b = (Maximalwert - mean)/ std\" />\n",
    "\n",
    "Überlegt Euch nun sinnvolle Minimal- und Maximalwerte für Eure theoretische Verteilung, berechnet *a* und *b*, und erzeugt damit eine gestutzte Verteilung für die Grundwassertemperaturen. Schaut euch dann die deskriptiven Merkmale der erzeugten Werte an.\n",
    "\n",
    "Hinweis: Als Funktionn für die Quadratwurzel könnt ihr `math.sqrt()` benutzen. Mit `scipy.stats.describe()` könnt Ihr Euch mit einem Befehl alle wichtigen statistitschen Parameter für eine Variable anzeigen lassen ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [7]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Werte erfüllen unsere Bedinungen nun deutlich besser, und wir könnten sie für weitere Berechnungen nutzen. \n",
    "\n",
    "Natürlich gibt es auch Funktionen zum Anpassen an andere theoretische Verteilungen neben der Normalverteilung. Eine Übersicht über die in `scipy` verfügbaren Verteilungen findet Ihr hier: https://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "\n",
    "Das Ganze wird nochmal anschaulicher, wenn man die beiden Verteilungen graphisch gegenüberstelle. Stellt dafür mit Hilfe von `matplotlib` in einem Plot die Daten als Histogram (`pyplot.hist(data, density =True)`), sowie die Wahrscheinlichkeitsdichtefunktion als Linienplot (`pyplot.plot()`) dar. \n",
    "\n",
    "Erzeugt für die Darstellung der Wahrscheinlichkeitsdichtefunktion mit Hilfe von `numpy.linspace()` die benötigten x-Werte, und berechnet die dazugehörigen y-Werte mit Hilfe der Funktion `scipy.stats.norm.pdf(x, mean, variance)`. \n",
    "\n",
    "Hinweise: Das Argument `density = True` normiert die y-Werte in Eurem Histogram sodass statt der Häufigkeit die Wahrscheinlichkeit angezeigt wird. Ansonsten hätten Histogramm und Linienplot hier sehr unterschiedliche Y-Achsenwerte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [8]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bivariate Statistik\n",
    "\n",
    "Nach der Betrachtung einzelner Parameter oben, geht es in dieser Übung um die Untersuchung der Beziehung zwischen zwei Parametern. \n",
    "\n",
    "Lest dazu zunächst einen gekürzten Datensatz mit den Grundwasserparametern aus Koch et al. (2020) (Data_GW_KA_short.csv) in Python ein. Am einfachsten geht das mit dem `pandas` Package (dazu mehr in den nächsten Wochen). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explorative Datenanalyse: Histogramme und Scatterplots\n",
    "\n",
    "Mit einem neuen Datensatz konfrontiert, ist es sinnvoll sich zuerst visuell einen Überblick über die vorliegenden Parameter, Werte und empirischen Verteilungen zu verschaffen. Für jeden Parameter einzeln geht dies gut über ein Histogramm (siehe oben). \n",
    "\n",
    "Um die Histogramme (d.h. die Randverteilungen) zweier Parameter zusammen mit ihrem gemeinsamen Scatterplot darzustellen, gibt es in dem Python-Package `seaborn` die praktische Funktion `seaborn.jointplot()`. `seaborn` ist wie `matplotlib` bereits in Anaconda enthalten, hat einen sehr ähnlichen Syntax und bietet viele spezielle Funktionen zur Visualisierung von statistischen Daten.\n",
    "\n",
    "Importiert `seaborn` in Euer Notebook und probiert `seaborn.jointplot(x, y)` mit einem beliebigen Parameterpaar `data['name1'], data['name2']` aus dem Datensatz aus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um einen Überblick über die bivariaten Beziehungen in dem gesamten Datensatz zu bekommen, eignet sich die Funktion `seaborn.pairplot()`, die Histogramme und Scatterplots für einen Datensatz kombiniert. Probiert diese Funktion mal für die Grundwasserdaten aus, und macht Euch ein erstes Bild der Beziehungen zwischen den Parameterpaaren. \n",
    "\n",
    "Achtung: Je nach Rechenleistung kann das Ausführen des Codes ein paar Sekunden dauern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [11]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Korrelationskoeffizienten\n",
    "\n",
    "Nun wollen wir die Beziehungen zwischen den einzelnen Parametern noch genauer qunatifizieren. Das grundlegende Maß dafür ist die Kovarianz. \n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?cov_{xy}&space;=&space;\\frac{1}{1-n}\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})\" title=\"cov_{xy} = \\frac{1}{1-n}\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})\" />\n",
    "\n",
    "Um die Kovarianzen für alle Parameterpaare in einem Datensatz zu berechnen, gibt es in `pandas` die Funktion `pandas.DataFrame.cov()`. Nutzt diese um Euch die Kovarianzen aller Parameterpaare der Grundwasserdaten, also die Kovarianzmatrix, anzeigen zu lassen (`print()`). Interpretiert den Wert der Kovarianz im Hinblick auf die Beziehung zwischen den beiden Parametern. Passt der Wert zu der Beobachtung in der Abbildung oben?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [13]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Stärke der Beziehungen zwischen den Parameterpaaren lässt sich aufgrund der sehr unterschiedlichen Varianzen (s. Diagonale der Kovarianzmatrix), allerdings nur schwer vergleichen. Berechnet nun für ein Parameterpaar, z.B. mit einer sehr großen Kovarianz, den Korrelationskoeffizienten nach Pearson, in dem Ihr die entsprechende Kovarianz durch das Produkt der Standardabweichungen teilt. \n",
    "\n",
    "Um die Korrelationsmatrix zu berechnen, gibt es in `pandas` den praktischen Befehl `pandas.DataFrame.corr()`. Per default wird dabei der Koeffizient nach Pearson berechnet. Was lässt sich also über die Korrelationen sagen? Vergleicht die Korrelationen mit den Kovarianzen von oben! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [14]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generell sind die Korrelationen in dem Datensatz sehr niedrig, was daran liegen könnte, dass es zwischen den Parametern keine lineare Beziehungen, sondern komplexere Assoziationen gibt. Berechnet nun den Korrelationskoeffizienten über die Funktion `scipy.stats.spearmanr()`. Wie verhalten sich die Werte im Vergleich zu oben?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie in der Vorlesung erwähnt gibt es noch einen dritten häufig verwendeten Korrelationskoeffizienten, der besonders für diskrete Daten geeignet ist. Wählt aus dem Datensatz einen diskreten Parameter aus, und bestimmt mit Hilfe von `scipy.stats.kendalltau()` die Korrelation zu einem weiteren beliebigen Parameter. Überprüft danach ob und wie sich die drei Korrelationskoeffizienten (`scipy.stats.pearsonr()`) für dieses Parameterpaar unterscheiden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [16]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Visualisierung von Werten in Matrizen bieten sich sog. heatmaps an. Ganz einfach geht dies mit der Funktion `seaborn.heatmap()`. Mit dem zusätzlichen Argument `annot=True` könnt Ihr Euch auch die Zahlenwerte in den Kästchen für eine der Matrizen oben anzeigen lassen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [17]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn Ihr noch Zeit habt, gibt es eine schöne Galerie mit Beispielen zu Datenvisualisierung mit seaborn (https://seaborn.pydata.org/examples/index.html), die es sich anzuschauen lohnt. \n",
    "\n",
    "\n",
    "### Ende\n",
    "\n",
    "#### Referenzen: \n",
    "\n",
    "Koch et al. (2020), Groundwater fauna in an urban area: natural or affected? https://hess.copernicus.org/preprints/hess-2020-151/hess-2020-151.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
