{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE has two parts: an encoder and a decoder. Instead of mapping each image to a single point in z-space, the encoder outputs the means and covariance matrices of a multivariate normal distribution where all of the dimensions are independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "print(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and Decoder\n",
    "For your encoder and decoder, you can use similar architectures that you've seen before, with some tweaks. For example, for the decoder, you can use the DCGAN generator architecture. For the encoder, you can use a classifier that you used before, and instead of having it produce 1 classification output of whether something is a cat or not, for example, you can have it produce 2 different outputs, one for mean and one for standard deviation. Each of those outputs will have dimensionality z to model the z dimensions in the multivariate normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, im_chan=1, output_chan=32, hidden_dim=16):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.z_dim = output_chan\n",
    "        self.disc = nn.Sequential(\n",
    "            self._block(im_chan, hidden_dim),\n",
    "            self._block(hidden_dim, hidden_dim * 2),\n",
    "            self._block(hidden_dim * 2, output_chan * 2, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def _block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "\n",
    "    def forward(self, images):\n",
    "        disc_pred = self.disc(images)\n",
    "        encoding = disc_pred.view(len(disc_pred), -1)\n",
    "        # The stddev output is treated as the log of the variance of the normal \n",
    "        # distribution by convention and for numerical stability\n",
    "        return encoding[:, :self.z_dim], encoding[:, self.z_dim:].exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (disc): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "im_chan = 1\n",
    "z_dim = 32\n",
    "\n",
    "# Create the Discriminator\n",
    "encode = Encoder(im_chan, z_dim).to(device)\n",
    "\n",
    "# Print the model\n",
    "print(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim=32, im_chan=1, hidden_dim=64):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.gen = nn.Sequential(\n",
    "            self._block(z_dim, hidden_dim * 4),\n",
    "            self._block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n",
    "            self._block(hidden_dim * 2, hidden_dim),\n",
    "            self._block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def _block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        x = noise.view(len(noise), self.z_dim, 1, 1)\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (gen): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose2d(32, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2))\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(z_dim, im_chan)\n",
    "\n",
    "# Print the model\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reparameterization Trick and VAE\n",
    "\n",
    "You can't backpropagate through a random sample. In `PyTorch`, you can do this by sampling using the `rsample` method, such as in `Normal(mean, stddev).rsample()`. This is equivalent to `torch.randn(z_dim) * stddev + mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim=32, im_chan=1, hidden_dim=64):\n",
    "        super(VAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.encode = Encoder(im_chan, z_dim)\n",
    "        self.decode = Decoder(z_dim, im_chan)\n",
    "\n",
    "    def forward(self, images):\n",
    "        q_mean, q_stddev = self.encode(images)\n",
    "        q_dist = Normal(q_mean, q_stddev)\n",
    "        z_sample = q_dist.rsample() # Sample once from each distribution, using the `rsample` notation\n",
    "        decoding = self.decode(z_sample)\n",
    "        return decoding, q_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encode): Encoder(\n",
      "    (disc): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2))\n",
      "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decode): Decoder(\n",
      "    (gen): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): ConvTranspose2d(32, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2))\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vae = VAE(z_dim=32, im_chan=1).to(device)\n",
    "\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchsummary\n",
    "\n",
    "from torchsummary import summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 13, 13]             272\n",
      "       BatchNorm2d-2           [-1, 16, 13, 13]              32\n",
      "         LeakyReLU-3           [-1, 16, 13, 13]               0\n",
      "            Conv2d-4             [-1, 32, 5, 5]           8,224\n",
      "       BatchNorm2d-5             [-1, 32, 5, 5]              64\n",
      "         LeakyReLU-6             [-1, 32, 5, 5]               0\n",
      "            Conv2d-7             [-1, 64, 1, 1]          32,832\n",
      "           Encoder-8       [[-1, 32], [-1, 32]]               0\n",
      "   ConvTranspose2d-9            [-1, 256, 3, 3]          73,984\n",
      "      BatchNorm2d-10            [-1, 256, 3, 3]             512\n",
      "             ReLU-11            [-1, 256, 3, 3]               0\n",
      "  ConvTranspose2d-12            [-1, 128, 6, 6]         524,416\n",
      "      BatchNorm2d-13            [-1, 128, 6, 6]             256\n",
      "             ReLU-14            [-1, 128, 6, 6]               0\n",
      "  ConvTranspose2d-15           [-1, 64, 13, 13]          73,792\n",
      "      BatchNorm2d-16           [-1, 64, 13, 13]             128\n",
      "             ReLU-17           [-1, 64, 13, 13]               0\n",
      "  ConvTranspose2d-18            [-1, 1, 28, 28]           1,025\n",
      "          Sigmoid-19            [-1, 1, 28, 28]               0\n",
      "          Decoder-20            [-1, 1, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 715,537\n",
      "Trainable params: 715,537\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.50\n",
      "Params size (MB): 2.73\n",
      "Estimated Total Size (MB): 3.23\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(vae,(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction Loss\n",
    "Reconstruction loss refers to the distance between the real input image (that you put into the encoder) and the generated image (that comes out of the decoder). Explicitly, the reconstruction loss term is E(logp(x|z)), the log probability of the true image given the latent value.\n",
    "\n",
    "For MNIST, you can treat each grayscale prediction as a binary random variable (also known as a Bernoulli distribution) with the value between 0 and 1 of a pixel corresponding to the output brightness, so you can use the binary cross entropy loss between the real input image and the generated image in order to represent the reconstruction loss term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss = nn.BCELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Divergence\n",
    "\n",
    "KL divergence, mentioned in a video (on Inception Score) last week, allows you to evaluate how different one probability distribution is from another. You care about two distributions and finding how different they are: (1) the learned latent space $q(z|x)$ that your encoder is trying to model and (2) your prior on the latent space $p(z)$, which you want your learned latent space to be as close as possible to. If both of your distributions are normal distributions, you can calculate the KL divergence, or $DKL(q(z|x)∥p(z))$, based on a simple formula. \n",
    "\n",
    "The latent prior $p(z)$ is a simple distribution for the latent space with a mean of zero and a standard deviation of one in each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.kl import kl_divergence\n",
    "\n",
    "def kl_divergence_loss(q_dist):\n",
    "    return kl_divergence(\n",
    "        q_dist, Normal(torch.zeros_like(q_dist.mean), torch.ones_like(q_dist.stddev))\n",
    "    ).sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.002\n",
    "# Setup Adam optimizers for vae:\n",
    "vae_opt  = optim.Adam(vae.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  1024\n",
    "im_chan    = 1 \n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [  \n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST\n",
    "dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transform,\n",
    "                       download=True)\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weight Initialization\n",
    "weights_init(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encode): Encoder(\n",
       "    (disc): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2))\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decode): Decoder(\n",
       "    (gen): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): ConvTranspose2d(32, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer_real = SummaryWriter(f\"logs_vae/real_images\")\n",
    "writer_fake = SummaryWriter(f\"logs_vae/recon_images\")\n",
    "step = 0\n",
    "\n",
    "vae.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 32, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "fixed_latent = torch.randn(batch_size, z_dim, 1, 1).to(device)\n",
    "print(fixed_latent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "Epoch [0/10] Batch 0/59                   Loss VAE: 792486.0000\n",
      "Epoch [1/10] Batch 0/59                   Loss VAE: 161875.0469\n",
      "Epoch [2/10] Batch 0/59                   Loss VAE: 128755.1953\n",
      "Epoch [3/10] Batch 0/59                   Loss VAE: 118238.9766\n",
      "Epoch [4/10] Batch 0/59                   Loss VAE: 113607.9297\n",
      "Epoch [5/10] Batch 0/59                   Loss VAE: 110856.3203\n",
      "Epoch [6/10] Batch 0/59                   Loss VAE: 109380.6875\n",
      "Epoch [7/10] Batch 0/59                   Loss VAE: 108001.7188\n",
      "Epoch [8/10] Batch 0/59                   Loss VAE: 108881.4453\n",
      "Epoch [9/10] Batch 0/59                   Loss VAE: 107004.8438\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Target labels not needed! <3 unsupervised\n",
    "    for batch_idx, (real_images, _) in enumerate(dataloader):\n",
    "        real_images = real_images.to(device)\n",
    "        recon_images, encoding = vae(real_images)\n",
    "\n",
    "        ############################\n",
    "        #  Update VAE: minimize reconstruction_loss + kl_divergence_loss\n",
    "        ###########################\n",
    "        loss = reconstruction_loss(recon_images, real_images) + kl_divergence_loss(encoding).sum()\n",
    "        vae_opt.zero_grad() # Clear out the gradients\n",
    "        loss.backward()\n",
    "        vae_opt.step()\n",
    "\n",
    "        ############################\n",
    "        \n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss VAE: {loss:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                recon_images = vae.decode(fixed_latent)\n",
    "                # take out (up to) 32 examples\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real_images[:32], normalize=True\n",
    "                )\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    recon_images[:32], normalize=True\n",
    "                )\n",
    "\n",
    "                writer_real.add_image(\"real_images\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"recon_images\", img_grid_fake, global_step=step)\n",
    "            \n",
    "            writer_real.add_scalar(\"Loss_VAE\",loss.item(), global_step=step)\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "1. https://deepgenerativemodels.github.io/notes/vae/\n",
    "2. http://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/slides/lec17.pdf \n",
    "3. Build Better Generative Adversarial Networks Cousera.org\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
