{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e42763e",
   "metadata": {},
   "source": [
    "# Stochastic Simulation\n",
    "\n",
    "*Winter Semester 2024/25*\n",
    "\n",
    "14.02.2025\n",
    "\n",
    "Prof. Sebastian Krumscheid<br>\n",
    "Assistants: Stjepan Salatovic, Louise Kluge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5187ec",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">\n",
    "Exercise sheet 13\n",
    "</h3>\n",
    "\n",
    "---\n",
    "\n",
    "<h1 align=\"center\">\n",
    "Markov Chain Monte Carlo\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f636d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal\n",
    "from typing import Literal\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82b0a91-6d0f-436d-bc1e-bfc75cc60fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=14)          # Controls default text sizes\n",
    "plt.rc('axes', titlesize=16)     # Fontsize of the axes title\n",
    "plt.rc('axes', labelsize=14)     # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=12)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=12)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=14)    # legend fontsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a22d93e-085e-4028-80ae-ed19ec47c31a",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Consider the Random Walk Metropolis-Hastings (RWMH) algorithm with proposal density $q(x,y) = g_\\sigma(y-x)$ and target density\n",
    "$f\\colon\\mathbb{R}\\to \\mathbb{R}^+$.\n",
    "Let $g_\\sigma$ denote the density of the $\\mathcal{N}(0,\\sigma^2)$ distribution and suppose that\n",
    "\\begin{equation*}\n",
    "f(y) =\\frac{1}{Z} \\exp{\\biggl[-\\biggl(\\frac{1}{4}y^4-\\frac{1}{2}y^2+\\frac{1}{4}\\biggr)\\biggr]}\\;,\n",
    "\\end{equation*}\n",
    "where $Z$ is such that $f$ is a PDF on $\\mathcal{X}=\\mathbb{R}$. \n",
    "Suppose we wish to estimate $\\mu = \\mathbb{E}_f(\\phi)$ for a suitable function $\\phi\\colon\\mathbb{R}\\to\\mathbb{R}$. Let $\\hat\\mu_n^{\\text{MH}}$ be the estimator for $\\mu$ based on the Markov chain of length $n$ generated by the RWMH algorithm. \n",
    "Derive asymptotic confidence intervals for $\\mu$ at probability level $\\alpha$ using the CLT for Metropolis-Hastings Markov Chains. Within your simulations, estimate this confidence interval and stop the Markov chain once the half-length of the interval is smaller than a given tolerance $\\tau>0$. The estimation has to be carried out on-the-fly, that is while the Markov chain evolves. Implement  the following heuristics to estimate the required _time-average variance constant_ (TAVC) and compare their performance for different functions $\\phi$; namely $\\phi(x) = x^p, p \\in \\mathbb{N}$.\n",
    "The TAVC is the asymptotic variance introduced in Lemma 8.16 of the lecture notes.\n",
    "\n",
    "1. **Initial positive sequence estimator:** \\begin{equation*}\n",
    "        \\tilde\\sigma^2 \\approx \\hat\\sigma_{\\text{pos},n}^2 :=\n",
    "        \\hat{c}_n(0) + 2\\sum_{k=1}^K\\bigl(\\hat{c}_n(2k) + \\hat{c}_n(2k+1) \\bigr),\n",
    "      \\end{equation*}\n",
    "      where $K$ is the largest integer such that\n",
    "      $\\hat{c}_n(2k) + \\hat{c}_n(2k+1)>0$ for all $k=1,\\dots,\n",
    "      K$. Here,\n",
    "      \\begin{equation*}\n",
    "        \\hat{c}_n(k) := \\frac{1}{n - k - 1}\\sum_{i=1}^{n-k}\\Bigl(\\phi(X_i) - \\hat\\mu_n^{\\text{MH}}\\Bigr)\\Bigl(\\phi(X_{i+k}) - \\hat\\mu_n^{\\text{MH}}\\Bigr)\n",
    "      \\end{equation*}\n",
    "      is an appropriate covariance estimator in this context.\n",
    "\n",
    "2. **Initial monotone sequence estimator:** \\begin{equation*}\n",
    "        \\tilde\\sigma^2 \\approx \\hat\\sigma_{\\text{mon},n}^2 :=\n",
    "        \\hat{c}_n(0) + 2 \\sum_{k=1}^K\\min_{1\\le j\\le k}\\bigl\\{\\hat{c}_n(2j) + \\hat{c}_n(2j+1)\\bigr\\},\n",
    "      \\end{equation*}\n",
    "      where $K$ and $\\hat{c}_n$ are as for the initial\n",
    "      positive sequence estimator above.\n",
    "\n",
    "3. **Batch means estimator:** Suppose the Markov chain is\n",
    "      $X_1,\\dots, X_n$ at iteration $n$. Divide these $n$ values into\n",
    "      $N_b\\in\\mathbb{N}$ batches, each of length $N_\\ell = n/N_b$. A\n",
    "      typical decomposition is $N_\\ell = n^{1-a}$ and $N_b = n^a$ for\n",
    "      $a\\in[0,1]$, for example $a=0.5$, modulo integer rounding. Let\n",
    "      \\begin{equation*}\n",
    "        \\hat\\mu_i =  \\frac{1}{N_\\ell} \\sum_{j= (i-1)N_\\ell + 1}^{iN_\\ell}\\phi(X_j)\\;,\\quad i=1,\\dots, N_b\\;,\n",
    "      \\end{equation*}\n",
    "      be the sample mean of the $i$-th batch. For $N_\\ell$\n",
    "      sufficiently large, one can consider the batch means\n",
    "      $\\hat\\mu_1, \\hat\\mu_2,\\dots,\\hat\\mu_{N_b}$ to be approximately\n",
    "      mutually independent. Consequently, one can estimate the time\n",
    "      average variance constant $\\sigma^2$ by the sample variance\n",
    "      estimator for independent realizations:\n",
    "      \\begin{equation*}\n",
    "        \\tilde\\sigma^2 \\approx \\hat\\sigma_{\\text{BM},n}^2 := \\frac{N_l}{N_b-1}\\sum_{i=1}^{N_b}{\\bigl(\\hat\\mu_i-\\hat\\mu_n^{\\text{MH}}\\bigr)}^2\\;.\n",
    "      \\end{equation*}\n",
    "\n",
    "In addition, instead of using all of the points in the Markov chain, one can as well discard a burn-in time $B$ and replace $\\sum_{k=1}$ with $\\sum_{k=B}$ in the above estimators. \n",
    "Experiment with the effects of burn-in time on the above asymptotic variance estimators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78362f23-55bd-48bf-b262-7e09aed0ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(y: float) -> float:\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2731a0f-eaf0-417c-8be2-edc29ef70dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_covars(X: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Computes the autocovariance of a one-dimensional array.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): A one-dimensional numpy array of numerical data.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: A one-dimensional numpy array containing the autocovariance values.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bcf0d8f-43ef-4874-918d-4963d71fe096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_K(cov: np.array) -> int:\n",
    "    \"\"\"\n",
    "    Determines the first index where the sum of adjacent autocovariance values becomes non-positive.\n",
    "\n",
    "    Args:\n",
    "        cov (np.array): A one-dimensional numpy array of autocovariance values.\n",
    "    \n",
    "    Returns:\n",
    "        int: The index of the first non-positive sum of adjacent autocovariance values.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdc1fc92-5ccd-4002-bce3-a4e15998707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_ipse(X: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Initial positive sequence estimator (IPSE).\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): A one-dimensional numpy array representing the Markov chain.\n",
    "    \n",
    "    Returns:\n",
    "        float: The IPSE estimate of the variance.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e11491f-3fd6-4ccd-99af-fbc94451f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_imse(X: np.array):\n",
    "    \"\"\"\n",
    "    Initial monotone sequence estimator (IPSE).\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): A one-dimensional numpy array representing the Markov chain.\n",
    "    \n",
    "    Returns:\n",
    "        float: The IMSE estimate of the variance.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfad4b57-14f4-42ee-bef2-23b22adf2214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_bm(X: np.array, a: float=0.5) -> float:\n",
    "    \"\"\"\n",
    "    Batch means estimator (BM).\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): A one-dimensional numpy array representing the Markov chain.\n",
    "        a (float, optional): The exponent used to determine the number of batches. Defaults to 0.5.\n",
    "    \n",
    "    Returns:\n",
    "        float: The BM estimate of the variance.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1d3199d-949f-4cd3-8001-0b0bc780189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RWMH(\n",
    "    sigma: float,\n",
    "    tavc_est: Literal[\"IPS\", \"IMS\", \"BM\"],\n",
    "    alpha: float,\n",
    "    tol: float,\n",
    "    min_iter: int = 50,\n",
    "    max_iter: int = 1000,\n",
    "    burn_in: int = 0\n",
    ")-> np.array:\n",
    "    \"\"\"\n",
    "    Performs the Random Walk Metropolis-Hastings (RWMH) algorithm with adaptive stopping based on confidence interval width.\n",
    "\n",
    "    Args:\n",
    "        sigma (float): The standard deviation of the proposal distribution.\n",
    "        tavc_est (Literal[\"IPS\", \"IMS\", \"BM\"]): The method used for variance estimation. Must be one of \"IPS\", \"IMS\", or \"BM\".\n",
    "        alpha (float): The significance level used in the confidence interval calculation.\n",
    "        tol (float): The tolerance level for the width of the confidence interval. Sampling stops when the confidence interval width is below this threshold.\n",
    "        min_iter (int, optional): The minimum number of iterations to perform before checking the stopping criterion. Defaults to 50.\n",
    "        max_iter (int, optional): The maximum number of iterations to perform. Defaults to 1000.\n",
    "        burn_in (int, optional): The number of initial samples to discard. Defaults to 0.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: An array of samples generated by the RWMH algorithm.\n",
    "        list: A list of confidence interval widths for each iteration after the minimum number of iterations.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d52ade-5638-4c42-87cb-4d5d7992f83d",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Consider the following probability density function in $\\mathbb{R}^2$, $\\mathbf{x}=(x_1,x_2)^T\\in\\mathbb{R}^2$,\n",
    "$$\n",
    "f(\\mathbf{x}) = \\frac{1}{Z} \\left[\\exp\\left\\{-(1-x_1)^2 - (x_2-x_1^2)^2\\right\\}+\\exp\\left\\{-(x_1+1)^2 - (x_2+3+x_1^2)^2 \\right\\}\\right], \n",
    "$$\n",
    "where $Z$ is the (unknown) normalization constant. To generate samples\n",
    "from $f$, we consider Metropolis-Hastings (MH) type MCMC\n",
    "algorithms. Let us denote with\n",
    "$$\n",
    "p(\\mathbf{x};\\mathbf{\\mu},\\Sigma)=\\frac{1}{\\sqrt{\\operatorname{det}(2\\pi\\Sigma)}}e^{-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu})^T\\Sigma^{-1}(\\mathbf{x}-\\mathbf{\\mu})}\n",
    "$$\n",
    "the pdf of a $\\mathcal{N}(\\mathbf{\\mu},\\Sigma)$ multivariate Gaussian random variable and by\n",
    "\n",
    "- $K_1$, the Markov kernel associated to an _Independent\n",
    "  Sampler_ MH algorithm with proposal density $q_1(\\mathbf{x},\\mathbf{y}) =\n",
    "  \\frac{1}{2}p(\\mathbf{y};(1,1)^T,\n",
    "  0.5\\,I_{2\\times2})+\\frac{1}{2}p(\\mathbf{y};(-1,-3)^T, 0.5\\,I_{2\\times2})$\n",
    "- $K_2$, the Markov kernel associated to a _Random Walk_ MH\n",
    "  algorithm with proposal density\n",
    "  $q_2(\\mathbf{x},\\mathbf{y})=p(\\mathbf{y};\\mathbf{x},\\sigma^2\\,I_{2\\times2})$ , where $\\sigma=0.15.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dcd055-09e9-4633-8f9a-cabe9952a337",
   "metadata": {},
   "source": [
    "1. Write the explicit expression of the densities corresponding to the Markov kernels $K_1$ and $K_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f105f-3bd4-4903-a43e-598a9de713aa",
   "metadata": {},
   "source": [
    "2. Consider now the kernel $K(\\omega) = \\omega K_1+(1-\\omega)K_2$, with $\\omega\\in [0,1]$. Show that $K(\\omega)$ is a reversible Markov kernel that has $f$ as invariant distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327079c0-9d52-414a-ba0b-8369410dae21",
   "metadata": {},
   "source": [
    "3. Implement a MCMC algorithm that uses the Markov kernel $K(\\omega)$ to estimate $\\mathbb{E}_f[\\|\\mathbf{X}\\|^2]$, with $\\mathbf{X}=(X_1,X_2)^T\\sim f$. \n",
    "Include the usual MCMC diagnostic plots in your experiment and describe how you would choose the sample size (length of the chain) to guarantee an error on the computation of $\\mathbb{E}_f[\\|\\mathbf{X}\\|^2]$ smaller than $\\varepsilon=1$ with confidence at least $0.95$. \n",
    "Estimate also the expected acceptance rate $\\chi(\\omega)$ of the implemented algorithm. \n",
    "Try at least two different values for $\\omega$.\n",
    "\n",
    "    **Hint:** [`scipy.stats.multivariate_normal`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3298dc-4812-4f8a-bf61-aeed316b7cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    Target distribution function.\n",
    "\n",
    "    Args:\n",
    "        x (float): The x-coordinate.\n",
    "        y (float): The y-coordinate.\n",
    "\n",
    "    Returns:\n",
    "        float: The value of the function at (x, y).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80164276-bad3-4727-b173-e123339bae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def K1(x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    First component of the mixture kernel.\n",
    "\n",
    "    Args:\n",
    "        x (float): The x-coordinate.\n",
    "        y (float): The y-coordinate.\n",
    "\n",
    "    Returns:\n",
    "        float: The value of the first kernel component at (x, y).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4982e8b2-3093-4d29-9961-7d4ccf7aa292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def K2(x: float, y: float, xm: float, ym: float, sigma: float) -> float:\n",
    "    \"\"\"\n",
    "    Second component of the mixture kernel.\n",
    "\n",
    "    Args:\n",
    "        x (float): The x-coordinate.\n",
    "        y (float): The y-coordinate.\n",
    "        xm (float): The mean x-coordinate of the kernel.\n",
    "        ym (float): The mean y-coordinate of the kernel.\n",
    "        sigma (float): The standard deviation of the kernel.\n",
    "\n",
    "    Returns:\n",
    "        float: The value of the second kernel component at (x, y).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d8560fe-1cda-425c-a1f4-c7ac060a3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def K(x: float, y: float, xm: float, ym: float, w: float, sigma: float) -> float:\n",
    "    \"\"\"\n",
    "    Proposal distribution function.\n",
    "\n",
    "    Args:\n",
    "        x (float): The x-coordinate.\n",
    "        y (float): The y-coordinate.\n",
    "        xm (float): The mean x-coordinate of the proposal distribution.\n",
    "        ym (float): The mean y-coordinate of the proposal distribution.\n",
    "        w (float): The weight parameter.\n",
    "        sigma (float): The standard deviation of the proposal distribution.\n",
    "\n",
    "    Returns:\n",
    "        float: The value of the proposal distribution at (x, y).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92842ae5-9698-400a-a997-9eb756525952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCMC(w: float, sigma: float, N: int = 1000) -> np.array:\n",
    "    \"\"\"\n",
    "    Implements the Metropolis-Hastings MCMC algorithm.\n",
    "\n",
    "    Args:\n",
    "        w (float): The weight parameter for the proposal distribution.\n",
    "        sigma (float): The standard deviation of the proposal distribution.\n",
    "        N (int, optional): The number of samples to generate. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        np.array: An array of samples generated by the MCMC algorithm.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07971a94-2882-4cb0-8a2e-bfb74ce1da5e",
   "metadata": {},
   "source": [
    "4. Write a closed-form formula for the expected acceptance rate\n",
    "  $\\chi(\\omega)$ from the expression of the kernel $K(\\omega)$. How\n",
    "  could one use the results in the previous point to find the value of\n",
    "  $\\omega$ that leads to a target acceptance rate, say\n",
    "  $\\chi(\\omega)=0.4$?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
