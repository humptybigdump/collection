{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxgvN1YlSnxK"
   },
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/300px-Reinforcement_learning_diagram.svg.png).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Reinforcement Learning is a special form of machine learning, where an agent interacts with an environment, conducts observations on the effects of actions and collects rewards.\n",
    "\n",
    "The goal of reinforcement learning is to learn an optimal policy, so that given a state an agent is able to decide what it should do next.\n",
    "\n",
    "In this exercise we will look into tow fundamental algorithms that are capable of solving MDPs, namely Monte Carlo Tree Search [Monte Carlo Tree Search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search) and [Q-Learning](https://en.wikipedia.org/wiki/Q-learning) (optional).\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the time you complete this lab, you should know:\n",
    "\n",
    "- The relevant pieces for a reinforcement learning system\n",
    "- The basics of *[gym](https://gym.openai.com/envs/#classic_control)* to conduct your own RL experiments\n",
    "- How Monte Carlo evaluations works\n",
    "- How Monte Carlo Tree Search works\n",
    "- The Advantages of MCTS vs. MC evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP\n",
    "\n",
    "A Markov decision process is a 4-tuple $(S,A,P_{a},R_{a})$\n",
    "\n",
    "![MDP](mdp.png \"MDP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvdmBl8GajjF"
   },
   "source": [
    "## Problem\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. (However, the ice is slippery, so you won't always move in the direction you intend.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wYUHIokU_EI"
   },
   "source": [
    "## Setup\n",
    "\n",
    "To begin we'll need to install all the required python package dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjQ08kksR2c2"
   },
   "outputs": [],
   "source": [
    "#!pip install --quiet gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8MH3Ij6rAL_z"
   },
   "source": [
    "### Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWdytOiH-LFr"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZgQh5-QCBeDI"
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import random\n",
    "import heapq\n",
    "import collections\n",
    "import math\n",
    "\n",
    "# Reinforcement Learning environments\n",
    "import gym\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GzgwlDeZhfxU"
   },
   "source": [
    "\n",
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mi8myW9Wheef"
   },
   "outputs": [],
   "source": [
    "# Define the default figure size\n",
    "plt.rcParams['figure.figsize'] = [16, 4]\n",
    "\n",
    "def create_numerical_map(env):\n",
    "    \"\"\"Convert the string map of the environment to a numerical version\"\"\"\n",
    "    numerical_map = np.zeros(env.env.desc.shape)\n",
    "    i = 0\n",
    "    for row in env.env.desc:\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            if col.decode('UTF-8') == 'S':\n",
    "                numerical_map[i, j] = 2\n",
    "            elif col.decode('UTF-8') == 'G':\n",
    "                numerical_map[i, j] = 1\n",
    "            elif col.decode('UTF-8') == 'F':\n",
    "                numerical_map[i, j] = 2\n",
    "            elif col.decode('UTF-8') == 'H':\n",
    "                numerical_map[i, j] = 3\n",
    "            j += 1\n",
    "        i += 1\n",
    "    numerical_map[env.unwrapped.s//i, env.unwrapped.s%i] = 0\n",
    "    return numerical_map\n",
    "\n",
    "\n",
    "def visualize_env(env):\n",
    "    \"\"\"Plot the environment\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    # Hide grid lines\n",
    "    ax.grid(False)\n",
    "    # Hide axes ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('The frozen Lake')\n",
    "    i = ax.imshow(create_numerical_map(env), cmap=cm.jet)\n",
    "    plt.show()\n",
    "    print('the position is blue, holes are red, ice is yellow and the goal is teal')\n",
    "\n",
    "def visualize_policy(env, policy, ax=None, title=None):\n",
    "    \"\"\"Plot the policy in the environment\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    font_size = 10 if env.observation_space.n > 16 else 20\n",
    "    i = 0\n",
    "    for row in env.env.desc:\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            s = i * env.env.desc.shape[0]+j\n",
    "            if policy[s] == 0:\n",
    "                ax.annotate(\"L\", xy=(j, i), xytext=(j, i), ha=\"center\",\n",
    "                            va=\"center\", size=font_size, color=\"white\")\n",
    "            elif policy[s] == 1:\n",
    "                ax.annotate(\"D\", xy=(j, i), xytext=(j, i), ha=\"center\",\n",
    "                            va=\"center\", size=font_size, color=\"white\")\n",
    "            elif policy[s] == 2:\n",
    "                ax.annotate(\"R\", xy=(j, i), xytext=(j, i), ha=\"center\",\n",
    "                            va=\"center\", size=font_size, color=\"white\")\n",
    "            elif policy[s] == 3:\n",
    "                ax.annotate(\"U\", xy=(j, i), xytext=(j, i), ha=\"center\",\n",
    "                            va=\"center\", size=font_size, color=\"white\")\n",
    "            j += 1\n",
    "        i += 1\n",
    "\n",
    "    # Hide grid lines\n",
    "    ax.grid(False)\n",
    "    # Hide axes ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    if title is None:\n",
    "        ax.set_title('Policy for the Frozen Lake')\n",
    "    else:\n",
    "        ax.set_title(title)\n",
    "    ax.imshow(create_numerical_map(env), cmap=cm.jet)\n",
    "    return\n",
    "\n",
    "\n",
    "def visualize_v(env, v, ax=None, title=None):\n",
    "    \"\"\"Plot value function values in the environment\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    font_size = 10 if env.observation_space.n > 16 else 20\n",
    "    i = 0\n",
    "    for row in env.env.desc:\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            s = i * env.env.desc.shape[0]+j\n",
    "            ax.annotate(\"{:.2f}\".format(v[s]), xy=(j, i), xytext=(j, i), ha=\"center\",\n",
    "                        va=\"center\", size=font_size, color=\"white\")\n",
    "            j += 1\n",
    "        i += 1\n",
    "\n",
    "    # Hide grid lines\n",
    "    ax.grid(False)\n",
    "    # Hide axes ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    if title is None:\n",
    "        ax.set_title('State Value Function for the Frozen Lake')\n",
    "    else:\n",
    "        ax.set_title(title)\n",
    "    ax.imshow(create_numerical_map(env), cmap=cm.jet)\n",
    "    return\n",
    "\n",
    "\n",
    "def compute_v_from_q(env, q):\n",
    "    \"\"\"Compute the v function given the q function, maximizing over the actions of a given state.\"\"\"\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    i = 0\n",
    "    for row in env.env.desc:\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            s = i * env.env.desc.shape[0]+j\n",
    "            v[s] = np.max(q[s, :])\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return v\n",
    "\n",
    "def compute_policy_from_q(env, q):\n",
    "    \"\"\"Compute the policy function given the q function, finding the action that yields the maximum of a given state.\"\"\"\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    i = 0\n",
    "    for row in env.env.desc:\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            s = i * env.env.desc.shape[0]+j\n",
    "            policy[s] = np.argmax(q[s, :])\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rTC-P1vd-5-y"
   },
   "source": [
    "#### Deterministic Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sknx1oOiaL7J"
   },
   "outputs": [],
   "source": [
    "# register variants of the frozen lake without execution uncertainty i.e. deterministic environments\n",
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.78,  # optimum = .8196\n",
    ")\n",
    "\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery8x8-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '8x8', 'is_slippery': False},\n",
    "    max_episode_steps=200,\n",
    "    reward_threshold=0.99,  # optimum = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FH2aMLY_jrjQ"
   },
   "outputs": [],
   "source": [
    "def evaluate_episode(env, policy, discount_factor):\n",
    "    \"\"\"Evaluates a policy by running it until termination and collect its reward\"\"\"\n",
    "    state = env.reset()\n",
    "    total_return = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        state, reward, done, _ = env.step(int(policy[state]))\n",
    "        # Calculate the total\n",
    "        total_return += (discount_factor ** step * reward)\n",
    "        step += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_return\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, discount_factor=0.95, number_episodes=1000):\n",
    "    \"\"\" Evaluates a policy by running it n times\"\"\"\n",
    "    return np.mean([evaluate_episode(env, policy, discount_factor) for _ in range(number_episodes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy and Value Iteraton Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxwwTshweK8i"
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "max_iterations = 1000\n",
    "num_episodes = 100\n",
    "discount_factor = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BzfpVLxA-T4W"
   },
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gsl3GswnX1I6"
   },
   "outputs": [],
   "source": [
    "# Deterministic environments\n",
    "env_name = 'FrozenLakeNotSlippery-v0'\n",
    "#env_name = 'FrozenLakeNotSlippery8x8-v0'\n",
    "\n",
    "# Stochastic environments\n",
    "#env_name = 'FrozenLake-v0'\n",
    "#env_name = 'FrozenLake8x8-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8WwK53WADNp"
   },
   "source": [
    "Create the environment with the previously selected name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "ARVwYFcHAB78",
    "outputId": "2f4c131f-cca2-4d82-ebc4-12cfa87f5890"
   },
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "print('Generated the frozen lake with config: ' + env_name)\n",
    "env.reset()\n",
    "visualize_env(env)\n",
    "env.unwrapped.s = 4\n",
    "visualize_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Environment (Object)\n",
    "\n",
    "**TASK :**\n",
    "Analyze the environment object and figure out its *observation-* and *actionspace* as well as its *reward range*.\n",
    "\n",
    "What is the size of the observation space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size of the action space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the range of rewards?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-pzYcAtuiHJ9"
   },
   "source": [
    "### Uncertainty in Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "F5OmhQ8sVLHK",
    "outputId": "167f15d0-f60c-43af-ebee-14e265d552bd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "actions = {0:\"left\",\n",
    "           1:\"down\",\n",
    "           2:\"right\",\n",
    "           3:\"up\"}\n",
    "\n",
    "s = env.reset()\n",
    "print(\"the initial state is: {}\".format(s))\n",
    "visualize_env(env)\n",
    "\n",
    "# The agent should go right\n",
    "print(\"executing action 2, should go right\")\n",
    "s1, r, d, _ = env.step(2)\n",
    "print(\"new state is: {} done: {}\".format(s1, d))\n",
    "visualize_env(env)\n",
    "\n",
    "# The agent should go left\n",
    "print(\"executing action 0, should go left\")\n",
    "s1, r, d, _ = env.step(0)\n",
    "print(\"new state is: {} done: {}\".format(s1, d))\n",
    "visualize_env(env)\n",
    "\n",
    "# The agent should go down\n",
    "print(\"executing action 1, should go down\")\n",
    "s1, r, d, _ = env.step(1)\n",
    "print(\"new state is: {} done: {}\".format(s1, d))\n",
    "visualize_env(env)\n",
    "\n",
    "# The agent should go up\n",
    "print(\"executing action 3, should go up\")\n",
    "s1, r, d, _ = env.step(3)\n",
    "print(\"new state is: {} done: {}\".format(s1, d))\n",
    "visualize_env(env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Evaluator/Search\n",
    "* Simulate trajectories through the MDP from the current state $s_t$\n",
    "* Apply model-free RL to simulated episodes\n",
    "\n",
    "![Monte Carlo Evaluator/Search](./img/monte_carlo_search.png)\n",
    "\n",
    "### Monte Carlo Estimate\n",
    "###  $\\hat{V}(s)=\\frac{1}{K}\\sum_{k=1}^{K}{G_t}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCE:\n",
    "    def __init__(self, env, state = 0, iterations = 1000, discount_factor = 0.95):\n",
    "        # maximum length of evaluation\n",
    "        self.number_iterations = iterations\n",
    "        # discount factor for future rewards\n",
    "        self.discount_factor = 0.95\n",
    "        # environment\n",
    "        self.env = env\n",
    "        # initial state\n",
    "        self.state = state\n",
    "        self.env.unwrapped.s = self.state\n",
    "        #visualize_env(self.env)\n",
    "    \n",
    "    def evaluate_v(self):\n",
    "        # determine v\n",
    "        v_avg = 0.0\n",
    "        v_max = 0.0\n",
    "        for i in range(self.number_iterations):\n",
    "            v = self.simulate(random.randint(0,self.env.action_space.n-1))\n",
    "            if v > v_max:\n",
    "                v_max = v\n",
    "            v_avg += v\n",
    "        v_avg /= self.number_iterations\n",
    "        return v_avg, v_max\n",
    "    \n",
    "\n",
    "    def evaluate_q(self, action):\n",
    "        # determine v\n",
    "        q_avg = 0.0\n",
    "        q_max = 0.0\n",
    "        for i in range(self.number_iterations):\n",
    "            q = self.simulate(action)\n",
    "            if q > q_max:\n",
    "                q_max = q\n",
    "            q_avg += q\n",
    "        q_avg /= self.number_iterations\n",
    "        return q_avg, q_max\n",
    "    \n",
    "    def best_action(self):\n",
    "        actions_q = np.zeros(self.env.action_space.n)\n",
    "        actions_visits = np.zeros(self.env.action_space.n)\n",
    "        for i in range(self.number_iterations):\n",
    "            action = random.randint(0,self.env.action_space.n-1)\n",
    "            actions_q[action] += self.simulate(action)\n",
    "            actions_visits[action] += 1\n",
    "        actions_q = np.divide(actions_q, actions_visits, out=np.zeros_like(actions_q), where=actions_visits!=0)\n",
    "        return np.argmax(actions_q)\n",
    "    \n",
    "    def simulate(self, action):\n",
    "        self.env.reset()\n",
    "        self.env.unwrapped.s = self.state\n",
    "        done = False\n",
    "        depth = 0\n",
    "        g = 0\n",
    "        state, r, done, _ = self.env.step(action)\n",
    "        g += r*self.discount_factor**depth\n",
    "        depth +=1\n",
    "        while not done:\n",
    "            action = random.randint(0,self.env.action_space.n-1)\n",
    "            state, r, done, _ = self.env.step(action)\n",
    "            g += r*self.discount_factor**depth\n",
    "            depth +=1\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_evaluation = MCE(env,0,discount_factor=0.95, iterations=1000)\n",
    "print(\"avg V(s):\\t {0:.3f}, max V(s):\\t {1:.3f}\".format(*mc_evaluation.evaluate_v()))\n",
    "\n",
    "for key, val in actions.items():\n",
    "    print(\"avg Q(s,{2}):\\t {0:.3f}, max Q(s,{2}):\\t {1:.3f}\".format(*mc_evaluation.evaluate_q(key), val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search\n",
    "* Simulate trajectories through the MDP from the current state $s_t$ building a tree\n",
    "* Apply model-free RL to simulated episodes\n",
    "\n",
    "### In-Tree and Out-of-Tree\n",
    "* Selection Policy (improves): select actions maximizing action values\n",
    "* Simulation Policy (fixed): selection actions randomly\n",
    "\n",
    "### Balance Exploration and Exploitation\n",
    "\n",
    "### $UCT(s,a) = \\hat{Q}(s,a)+c\\sqrt{\\frac{\\ln{N(s)}}{N(s,a)}}$\n",
    "\n",
    "### Phases\n",
    "* Selection\n",
    "* Expansion\n",
    "* Simulation\n",
    "* Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state=0, action=-1, done=False, parent={}):\n",
    "        # current state of the environment\n",
    "        self.state = state\n",
    "        # number of trajectories that passed through this node\n",
    "        self.visits = 0\n",
    "        # average v value that results from starting in this node\n",
    "        self.v_value = 0\n",
    "        # action that led to this node\n",
    "        self.action = action\n",
    "        # untried actions (i.e. the actions that have not been explored)\n",
    "        self.untried_actions = [0, 1, 2, 3]\n",
    "        # parent node pointer\n",
    "        self.parent = parent\n",
    "        # children node pointers\n",
    "        self.children = []\n",
    "        # flag that indicates that the node is terminal (e.g. the environment is in a terminal state)\n",
    "        self.done = done\n",
    "        \n",
    "    def uct(self, c = 5):\n",
    "        \"\"\"Calculate the UCT value for a given child node (i.e. the value from executing a in s)\"\"\"\n",
    "        # if the node has not been visited return a high UCT score, forcing expansion\n",
    "        if self.visits == 0:\n",
    "            return 1000\n",
    "        # if the node has been visited calculate it using the UCB formula\n",
    "        return self.v_value + c* math.sqrt(math.log(self.parent.visits)/self.visits)\n",
    "    \n",
    "    def best_child(self):\n",
    "        \"\"\"Return the best child based on the maximum UCT value.\"\"\"\n",
    "        uct_values = []\n",
    "        for child in self.children:\n",
    "            uct_values.append(child.uct())\n",
    "        uct_index = np.argmax(uct_values)\n",
    "        return self.children[uct_index]\n",
    "    \n",
    "    def max_action_value(self):\n",
    "        \"\"\"Return the child with the highest action value.\"\"\"\n",
    "        v_values = []\n",
    "        for child in self.children:\n",
    "            v_values.append(child.v_value)\n",
    "        v_values_index = np.argmax(v_values)\n",
    "        return self.children[v_values_index]\n",
    "    \n",
    "    def max_visits(self):\n",
    "        \"\"\"Return the child with the highest visit count.\"\"\"\n",
    "        visits = []\n",
    "        for child in self.children:\n",
    "            visits.append(child.visits)\n",
    "        visits_index = np.argmax(visits)\n",
    "        return self.children[visits_index]\n",
    "    \n",
    "    def str(self):\n",
    "        if not self.parent:\n",
    "            return \"s:{}, N(s):{}, \\ta: {}, \\tQ(s, a):{:.3f}, parent:{}\".format(self.state, self.visits, \"none\", self.v_value, self.parent)\n",
    "        else:\n",
    "            return \"s:{}, N(s):{}, \\ta: {}, \\tQ(s, a):{:.3f}, parent:{}\".format(self.state, self.visits, actions[self.action], self.v_value, self.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, env, state = 0, iterations = 1000, discount_factor = 0.95):\n",
    "        # maximum number of simulations\n",
    "        self.number_iterations = iterations\n",
    "        # discount factor for future rewards\n",
    "        self.discount_factor = discount_factor\n",
    "        # environment\n",
    "        self.env = env\n",
    "        # initial state\n",
    "        self.state = state\n",
    "        self.env.unwrapped.s = self.state\n",
    "        #visualize_env(self.env)\n",
    "    \n",
    "    def select(self, node):\n",
    "        # if the node has no untried actions left, choose the best child using UCB1\n",
    "        while len(node.untried_actions) == 0:\n",
    "            node = node.best_child()\n",
    "        return node\n",
    "    \n",
    "    def expand(self, node):\n",
    "        # expand the node with a random action\n",
    "        if not node.done:\n",
    "            action = np.random.choice(node.untried_actions)\n",
    "            node.untried_actions.remove(action)\n",
    "\n",
    "            self.env.reset()\n",
    "            self.env.unwrapped.s = node.state\n",
    "            state, r, done, _ = self.env.step(action)\n",
    "            child = Node(state, action, done, node)\n",
    "            node.children.append(child)\n",
    "            return child, r\n",
    "        else:\n",
    "            self.env.reset()\n",
    "            self.env.unwrapped.s = node.parent.state\n",
    "            state, r, done, _ = self.env.step(node.action)\n",
    "            return node, r\n",
    "    \n",
    "    def simulate(self, node):\n",
    "        \"\"\"Monte Carlo Evaluator\"\"\"\n",
    "        self.env.reset()\n",
    "        self.env.unwrapped.s = node.state\n",
    "        done = False\n",
    "        depth = 0\n",
    "        g = 0\n",
    "        action = random.randint(0,env.action_space.n-1)\n",
    "        state, r, done, _ = self.env.step(action)\n",
    "        g += r*self.discount_factor**depth\n",
    "        depth +=1\n",
    "        while not done:\n",
    "            action = random.randint(0,env.action_space.n-1)\n",
    "            state, r, done, _ = self.env.step(action)\n",
    "            g += r*self.discount_factor**depth\n",
    "            depth +=1\n",
    "        return g\n",
    "       \n",
    "    def update(self,node,g):\n",
    "        depth = 0\n",
    "        while node.parent:\n",
    "            node.visits += 1\n",
    "            node.v_value = (node.v_value*(node.visits-1)+g*self.discount_factor**depth)/node.visits\n",
    "            node = node.parent\n",
    "            depth += 1\n",
    "        node.visits += 1\n",
    "        node.v_value = (node.v_value*(node.visits-1)+g*self.discount_factor**depth)/node.visits\n",
    "            \n",
    "    def best_action(self, root):\n",
    "        for i in range(self.number_iterations):\n",
    "            self.env.reset()\n",
    "            self.env.unwrapped.s = root.state\n",
    "            node = self.select(root)\n",
    "            child, r = self.expand(node)\n",
    "            if not child.done:\n",
    "                g = self.simulate(child)\n",
    "            else:\n",
    "                g = r\n",
    "            self.update(child, g)\n",
    "        return root.max_action_value().action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "sim = gym.make(env_name)\n",
    "env.reset()\n",
    "sim.reset()\n",
    "# set initial state\n",
    "state = 0\n",
    "env.unwrapped.s = state\n",
    "mcts = MCTS(sim, state, iterations = 1000)\n",
    "\n",
    "root_node = Node(state)\n",
    "action = mcts.best_action(root_node)\n",
    "print(root_node.str())\n",
    "\n",
    "print(root_node.children[0].str())\n",
    "print(root_node.children[1].str())\n",
    "print(root_node.children[2].str())\n",
    "print(root_node.children[3].str())\n",
    "print(\"the best action is action {}, {}\".format(action, actions[action]))\n",
    "print(env.step(action))\n",
    "visualize_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plan_mce(iterations, output = False):\n",
    "    env = gym.make(env_name)\n",
    "    sim = gym.make(env_name)\n",
    "    env.reset()\n",
    "    sim.reset()\n",
    "    # set initial state\n",
    "    state = 0\n",
    "    # initialize the Monte Carlo Evaluator\n",
    "    mce = MCE(sim, state, iterations = iterations)\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        mce.state = state\n",
    "        action = mce.best_action()\n",
    "        steps += 1\n",
    "        # take one step in the environment\n",
    "        state, r, done, _ = env.step(action)\n",
    "    if output:\n",
    "        visualize_env(env)\n",
    "        print(\"reached state: {}, after {}\".format(state, steps))\n",
    "        \n",
    "    return steps, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plan_mcts(iterations, output = False):\n",
    "    env = gym.make(env_name)\n",
    "    sim = gym.make(env_name)\n",
    "    env.reset()\n",
    "    sim.reset()\n",
    "    # set initial state\n",
    "    state = 0\n",
    "    # initialize the Monte Carlo Tree Search\n",
    "    mcts = MCTS(sim, state, iterations = iterations)\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        root_node = Node(state)\n",
    "        action = mcts.best_action(root_node)\n",
    "        steps += 1\n",
    "        # take one step in the environment\n",
    "        state, r, done, _ = env.step(action)\n",
    "    if output:\n",
    "        visualize_env(env)\n",
    "        print(\"reached state: {}, after {}\".format(state, steps))\n",
    "        \n",
    "    return steps, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of MCE and MCTS\n",
    "* MCTS requires less iterations to reach the goal state\n",
    "* Due to the uniform action exploration in the plan_mce function the variance estimates for all actions are less skewed as they are for MCTS, thus reaching the goal more frequently (but slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# specify evaluation points\n",
    "iterations = [20 , 50, 100, 200, 500]\n",
    "# specify number of runs for each evaluation point\n",
    "runs = 50\n",
    "\n",
    "# initialize empty metrics\n",
    "avg_mce_steps = []\n",
    "avg_mcts_steps = []\n",
    "avg_mce_r = []\n",
    "avg_mcts_r = []\n",
    "\n",
    "for it in iterations:\n",
    "    # reset counters\n",
    "    mce_steps = 0\n",
    "    mcts_steps = 0\n",
    "    mce_rs = 0\n",
    "    mcts_rs = 0\n",
    "    for i in range(runs):\n",
    "        # solve MDP with MCE\n",
    "        mce_step, mce_r = plan_mce(it, False)\n",
    "        # solve MDP with MCTS\n",
    "        mcts_step, mcts_r = plan_mcts(it, False)\n",
    "        # increment counters\n",
    "        mce_steps += mce_step\n",
    "        mcts_steps += mcts_step\n",
    "        mce_rs += mce_r\n",
    "        mcts_rs += mcts_r\n",
    "\n",
    "    # aggregate values\n",
    "    avg_mce_steps.append(mce_steps/runs)\n",
    "    avg_mcts_steps.append(mcts_steps/runs)\n",
    "    avg_mce_r.append(mce_rs/runs)\n",
    "    avg_mcts_r.append(mcts_rs/runs)\n",
    "    \n",
    "fig, ax = plt.subplots(1, 2)\n",
    "# Plot the average episode length\n",
    "ax[0].plot(iterations, avg_mce_steps, color=\"red\", label='MCE')\n",
    "ax[0].plot(iterations, avg_mcts_steps, color=\"blue\", label='MCTS')\n",
    "ax[0].set(xlabel='#Simulations', ylabel='Steps', title='Average Episode Length')\n",
    "ax[0].grid()\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot the average episode reward\n",
    "ax[1].plot(iterations, avg_mce_r, color=\"red\", label='MCE')\n",
    "ax[1].plot(iterations, avg_mcts_r, color=\"blue\", label='MCTS')\n",
    "ax[1].set(xlabel='#Simulations', ylabel='Reward', title='Average Episode Reward')\n",
    "ax[1].grid()\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lU4gmOQcAjR_"
   },
   "source": [
    "## Q-Learning\n",
    "\n",
    "![Q-Learning](q_learning.png \"Q-Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Difference Error\n",
    "### $\\delta_t = \\underbrace{r_{t}}_{\\text{reward}} + \\underbrace{\\gamma}_{\\text{discount factor}} \\cdot \\underbrace{\\max_{a}Q(s_{t+1}, a)}_{\\text{estimate of optimal future value}} - \\underbrace{Q(s_{t}, a_{t})}_{\\text{estimate of optimal current value}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Difference Update\n",
    "### $Q^{new}(s_{t},a_{t}) \\leftarrow \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} + \\underbrace{\\alpha}_{\\text{learning rate}} \\cdot \\underbrace{\\delta_t}_\\text{temporal difference error}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transiton Tuple\n",
    "For ease of use we define a transition tuple that allows us to combine all the relevant information from one state to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = priority (only needed for (prioritized) experience replay)\n",
    "# s = state\n",
    "# a = action\n",
    "# s1 = successor state\n",
    "# r = reward\n",
    "# td_e = temporal difference error\n",
    "Transition = collections.namedtuple('Transition', ('p', 's', 'a', 's1', 'r', 'td_e'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory and Prioritized Experience Replay (optional)\n",
    "\n",
    "Experience Replay and prioritization of specific experiences are common techniques to make the training more data efficient.\n",
    "\n",
    "* [Paper - Experience Replay, 1992](https://link.springer.com/content/pdf/10.1007%2FBF00992699.pdf)\n",
    "* [Paper - Prioritized Experience Replay, 2015](https://arxiv.org/abs/1511.05952)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, config):\n",
    "        # transitions memory\n",
    "        self.transitions = []\n",
    "        # size of the memory\n",
    "        self.memory_size = config.memory_size\n",
    "        # size of the batches\n",
    "        self.batch_size = config.batch_size\n",
    "        # flag for prioritized experience replay\n",
    "        self.prioritized = config.prioritized\n",
    "        \n",
    "    def push(self, transition):\n",
    "        # if the memory is not yet full add the new transition\n",
    "        if len(self.transitions) < self.memory_size:\n",
    "            heapq.heappush(self.transitions, transition)\n",
    "        # if the memory is full remove the smallest transition and add the new transition \n",
    "        else:\n",
    "            del self.transitions[-1]\n",
    "            heapq.heappush(self.transitions, transition)\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        if self.prioritized:\n",
    "            return heapq.nsmallest(self.batch_size,self.transitions)\n",
    "        else: \n",
    "            return random.sample(sorted(self.transitions),self.batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.transitions)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Q Agent\n",
    "So far we have only defined simple function calls with\n",
    "```python\n",
    "def function_name(arg1, arg2):\n",
    "    # compute something with arg1 and arg2 and return something\n",
    "    if arg2 > 0:\n",
    "        something = other_function(arg1) - arg2\n",
    "    else:\n",
    "        something = arg1\n",
    "    return something\n",
    "```\n",
    "However for more complex tasks it is advisable to write object oriented code using classes. Classes provide a means of bundling data and functionality together. Creating a new class creates a new type of object, allowing new instances of that type to be made. Each class instance can have attributes attached to it for maintaining its state. Class instances can also have methods (defined by its class) for modifying its state.\n",
    "\n",
    "\n",
    "\n",
    "Hence we create a class **QAgent** that incorporates all the methods needed for Q-Learning.\n",
    "\n",
    "```python\n",
    "class QAgent:\n",
    "    \n",
    "    def __init__(self): # constructor method that gets called when the object is being created\n",
    "        \n",
    "    def td_error(self): # Temporal Difference Error\n",
    "        \n",
    "    def td_update(self): # Temporal Difference Update\n",
    "    \n",
    "    def train(self, env): # Train the agent     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK :**\n",
    "Add the missing formulars for the TD-error and the TD-update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, config):\n",
    "        # Maximum length of training\n",
    "        self.training_length = config.training_length\n",
    "        # Maximum length of an episode \n",
    "        self.episode_length = config.episode_length\n",
    "        # TD error update step size\n",
    "        self.learning_rate = config.learning_rate\n",
    "        # TD error update step size\n",
    "        self.discount_factor = config.discount_factor\n",
    "        # Enabling experience replay\n",
    "        self.replay_memory_enabled = True if config.config_replay_memory else False\n",
    "        # Initialize the replay memory of the agent\n",
    "        if self.replay_memory_enabled:\n",
    "            self.replay_memory = ReplayMemory(config.config_replay_memory)\n",
    "        \n",
    "    def td_error(self, q, s, a, s1, r):\n",
    "        # TASK: return the TD-Error\n",
    "        # Calculates the temporal difference error given the current model and transition\n",
    "        td_e = r + self.discount_factor*np.max(q[s1, :]) - q[s, a]\n",
    "        return td_e\n",
    "    \n",
    "    def td_update(self, q, t):\n",
    "        # TASK: return the update for the q value\n",
    "        # Calculates the adjusted action value (q) given the td error from a single transition\n",
    "        q = q + self.learning_rate * t.td_e\n",
    "        return q\n",
    "\n",
    "    def td_replay(self, q, q_target):\n",
    "        # Use the replay memory to run additional updates\n",
    "        if len(self.replay_memory) >= self.replay_memory.batch_size:\n",
    "            for t in self.replay_memory.replay(self.replay_memory.batch_size):\n",
    "                # Recalculate the temporal difference error for this transition\n",
    "                td_e = self.td_error(q_target, t.s, t.a, t.s1, t.r)\n",
    "                # Create an updated transition tuple\n",
    "                updated_t = Transition(-td_e, t.s, t.a, t.s1, t.r, td_e)\n",
    "                # Save the transition in replay memory\n",
    "                self.replay_memory.push(updated_t)\n",
    "                # Update model / q table\n",
    "                q[t.s, t.a] = self.td_update(q_target[t.s, t.a], updated_t)\n",
    "        return q\n",
    "    \n",
    "    def epsilon_greedy_noise(self, episode):\n",
    "        epsilon = np.random.randn(1, env.action_space.n)*(1./(episode+1))\n",
    "        a = np.argmax(self.q_target[s, :] + epsilon)\n",
    "        return a, epsilon\n",
    "    \n",
    "    def epsilon_greedy_linear(self, env, episode):\n",
    "        epsilon = (1-(episode+1)/self.training_length)\n",
    "        if epsilon > np.random.rand():\n",
    "            a = np.random.randint(env.action_space.n)\n",
    "        else:\n",
    "            a = np.argmax(self.q_target[s, :])\n",
    "        return a, epsilon\n",
    "    \n",
    "    def train(self, env):\n",
    "        # Initialize the model / q table with zeros/random\n",
    "        self.q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "        # Create a target model / q table\n",
    "        self.q_target = self.q\n",
    "        \n",
    "        ### METRICS\n",
    "        # create lists to contain various metrics that should be tracked during the training process\n",
    "        self.metrics = {\n",
    "            'return': np.zeros(self.training_length),\n",
    "            'q_avg': np.zeros(self.training_length),\n",
    "            'epsilon': np.zeros(self.training_length),\n",
    "            'td_error': np.zeros(self.training_length)\n",
    "        }\n",
    "        \n",
    "        for episode in range(self.training_length):\n",
    "            # Reset the environment and retrieve the initial state\n",
    "            s = env.reset()\n",
    "            # Set the 'done' flag to false\n",
    "            d = False\n",
    "            # Set the step of the episode to 0\n",
    "            step = 0\n",
    "            # Start the Q-Learning algorithm\n",
    "            while step < self.episode_length:\n",
    "                # Derive action from current policy (epsilon_greedy noise)\n",
    "                epsilon = np.random.randn(1, env.action_space.n)*(1./(episode+1))\n",
    "                a = np.argmax(self.q_target[s, :] + epsilon)\n",
    "\n",
    "                # Execute the action and generate a succesor state as well as receive an immediate reward\n",
    "                s1, r, d, _ = env.step(a)\n",
    "\n",
    "                # Calculate the temporal difference error\n",
    "                td_e = self.td_error(self.q_target, s, a, s1, r)\n",
    "\n",
    "                # Create a transition tuple\n",
    "                transition = Transition(-(td_e+0.001), s, a, s1, r, td_e)       \n",
    "\n",
    "                # Save the transition in replay memory\n",
    "                if self.replay_memory_enabled:\n",
    "                    self.replay_memory.push(transition)\n",
    "\n",
    "                # Update model / q table\n",
    "                self.q[s, a] = self.td_update(self.q_target[s, a], transition)\n",
    "\n",
    "                # Assign the current state the value of the successor state\n",
    "                s = s1\n",
    "\n",
    "                # Increment the step\n",
    "                step += 1\n",
    "\n",
    "                ### METRICS\n",
    "                # Accumulate the episode return\n",
    "                self.metrics['return'][episode] += self.discount_factor**step*r\n",
    "                # Track the temporal difference error\n",
    "                self.metrics['td_error'][episode] += td_e\n",
    "                # Track the max epsilon values\n",
    "                self.metrics['epsilon'][episode] += np.max(epsilon)\n",
    "                # Track the average q values\n",
    "                self.metrics['q_avg'][episode] = np.average(self.q)\n",
    "\n",
    "\n",
    "                # If we reached a terminal state abort the while loop reset the environment and start over\n",
    "                if d == True or step == 100:\n",
    "\n",
    "                    # At the end of the episode update the target model with the current model\n",
    "\n",
    "                    # If experience replay is enabled replay the experience collected so far\n",
    "                    if self.replay_memory_enabled:\n",
    "                        self.q_target = self.td_replay(self.q, self.q_target)\n",
    "                    else:\n",
    "                        self.q_target = self.q\n",
    "\n",
    "                    ### METRICS\n",
    "                    self.metrics['epsilon'][episode] /= step\n",
    "                    self.metrics['q_avg'][episode] /= step\n",
    "                    self.metrics['td_error'][episode] /= step\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Tuple\n",
    "For ease of use we define a configuration tuple that allows us to combine all the relevant configuration from into one object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfigQAgent = collections.namedtuple('ConfigQAgent', ('learning_rate',\n",
    "                                                       'training_length',\n",
    "                                                       'episode_length',                                                       'discount_factor',\n",
    "                                                       'config_replay_memory')\n",
    "                                     )\n",
    "\n",
    "ConfigReplayMemory = collections.namedtuple('ConfigReplayMemory', ('memory_size', \n",
    "                                                                   'batch_size', \n",
    "                                                                   'prioritized')\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and Train the Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Agent 1\n",
    "config_replay_memory = ConfigReplayMemory(500, 50, False)\n",
    "config_q_agent = ConfigQAgent(0.1, 400, 50, discount_factor, None)\n",
    "\n",
    "q_agent = QAgent(config_q_agent)\n",
    "q_agent.train(env)\n",
    "policy = compute_policy_from_q(env, q_agent.q_target)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average return: {:.2f}'.format(evaluate_policy(env, policy, q_agent.discount_factor, 1000)))\n",
    "print(\"Score over time: \" + str(sum(q_agent.metrics['return'])/q_agent.training_length))\n",
    "\n",
    "fig, axi = plt.subplots(1, 2)\n",
    "visualize_policy(env, policy, axi[0])\n",
    "visualize_v(env, compute_v_from_q(env, q_agent.q_target),axi[1])\n",
    "\n",
    "fig, ax = plt.subplots(1, 4)\n",
    "# Plot the return over time\n",
    "ax[0].plot(range(q_agent.training_length), q_agent.metrics['return'], \".\")\n",
    "ax[0].set(xlabel='episode', ylabel='reward', title='Return')\n",
    "ax[0].grid()\n",
    "\n",
    "# Plot the Q value over time\n",
    "ax[1].plot(range(q_agent.training_length), q_agent.metrics['q_avg'], \".\")\n",
    "ax[1].set(xlabel='episode', ylabel='Q Value', title='Average Q Value')\n",
    "ax[1].grid()\n",
    "\n",
    "# Plot the epsilon over time\n",
    "ax[2].plot(range(q_agent.training_length), q_agent.metrics['epsilon'], \".\")\n",
    "ax[2].set(xlabel='episode', ylabel='epsilon', title='Epsilon')\n",
    "ax[2].grid()\n",
    "\n",
    "# Plot the td error over time\n",
    "ax[3].plot(range(q_agent.training_length), q_agent.metrics['td_error'], \".\")\n",
    "ax[3].set(xlabel='episode', ylabel='TD Error', title='TD Error')\n",
    "ax[3].grid()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate different Hyperparameters (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_training_episodes(number_evaluation_points, number_evaluations):\n",
    "    ### Evaluate different episode lengths\n",
    "    training_lengths = np.linspace(1, 501, number_evaluation_points, dtype = int)\n",
    "    returns = np.zeros(number_evaluation_points)\n",
    "    for i in range(number_evaluation_points):\n",
    "        config_q_agent = ConfigQAgent(0.1, training_lengths[i], 100, discount_factor, None)\n",
    "        for j in range(number_evaluations):\n",
    "            q_agent = QAgent(config_q_agent)\n",
    "            q_agent.train(env)\n",
    "            returns[i] += np.max(q_agent.metrics['return'])\n",
    "        returns[i] /= number_evaluations\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set(xlabel='#Episodes', ylabel='Max. Return', title='#Episodes vs. Return')\n",
    "    ax.plot(training_lengths, returns, '-o');\n",
    "    \n",
    "evaluate_training_episodes(10, 10)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6wYUHIokU_EI",
    "8MH3Ij6rAL_z",
    "JWdytOiH-LFr",
    "GzgwlDeZhfxU",
    "rTC-P1vd-5-y",
    "-pzYcAtuiHJ9",
    "zhrrLKXk0ElG",
    "CASyoXI9jAZW",
    "lU4gmOQcAjR_",
    "4CdfVP4DilJf",
    "wK6bzLs_iqeG",
    "5KUNPRHdAstO",
    "tny1fTdaIkR6"
   ],
   "name": "Exercise 04 - Reinforcement Learning with Gym and Pytorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
