{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Setup\n",
    "\n",
    "If you prefer to work locally, see the following instructions for setting up Python in a virtual environment.\n",
    "You can then ignore the instructions in \"Colab Setup\".\n",
    "\n",
    "If you haven't yet, create a [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) environment using:\n",
    "```\n",
    "conda create --name rl_exercises\n",
    "conda activate rl_exercises\n",
    "```\n",
    "Torch recommends installation using conda rather than pip, so run:\n",
    "```\n",
    "conda install pytorch cpuonly -c pytorch\n",
    "```\n",
    "If you have a CUDA-enabled GPU and would like to use it, visit [the installation page](https://pytorch.org/get-started/locally/) to see the options available for different CUDA versions.\n",
    "The remaining dependencies can be installed with pip:\n",
    "```\n",
    "pip install matplotlib numpy \"gymnasium[classic-control, other]\"\n",
    "```\n",
    "\n",
    "Even if you are running the Jupyter notebook locally, please run the code cells in **Colab Setup**, because they define some global variables required later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup\n",
    "\n",
    "Google Colab provides you with a temporary environment for python programming.\n",
    "While this conveniently works on any platform and internally handles dependency issues and such, it also requires you to set up the environment from scratch every time.\n",
    "The \"Colab Setup\" section below will be part of **every** exercise and contains utility that is needed before getting started.\n",
    "\n",
    "There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window).\n",
    "Any changes you make to the Jupyter notebook itself should be saved to your Google Drive.\n",
    "We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout.\n",
    "However, you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Your work will be stored in a folder called `rl_ws24` by default to prevent Colab \n",
    "instance timeouts from deleting your edits.\n",
    "We do this by mounting your google drive on the virtual machine created in this colab \n",
    "session. For this, you will likely need to sign in to your Google account and allow\n",
    "access to your Google Drive files.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    COLAB = True\n",
    "except ImportError:\n",
    "    COLAB = False\n",
    "\n",
    "# Create paths in your google drive\n",
    "if COLAB:\n",
    "    DATA_ROOT = Path(\"/content/gdrive/My Drive/rl_ws24\")\n",
    "    DATA_ROOT.mkdir(exist_ok=True)\n",
    "else:\n",
    "    DATA_ROOT = Path.cwd() / \"rl_ws24\"\n",
    "\n",
    "# Install **python** packages\n",
    "if COLAB:\n",
    "    %pip install matplotlib numpy \"gymnasium[classic-control, other]\" torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Policy Gradient Methods\n",
    "\n",
    "In this homework we will be mainly working on Policy gradients (Lecture 5) \n",
    "and Natural Policy Gradients (Lecture 6). We are going to implement the \n",
    "REINFORCE, the Policy Gradient Theorem and the Natural Gradient algorithms. \n",
    "\n",
    "All homeworks are self-contained.\n",
    "They can be completed in their respective notebooks.\n",
    "Please fill in any missing code or answer any questions that are marked with `## TODO ##` statements.\n",
    "Questions not marked with `## TODO ##` are self-test questions and do **not** need to be answered for points.\n",
    "To edit and re-run code, you can simply edit and restart the code cells below.\n",
    "When you are finished, you will need to submit the notebook as well as all saved figures (see exercises) as a zip file via Ilias.\n",
    "\n",
    "We start by importing all the necessary python modules and defining some helper functions which you do not need to change.\n",
    "Still, make sure you are aware of what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Sequence\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "SEED = 2\n",
    "\n",
    "# specify the path to save the recordings of this run to.\n",
    "DATA_PATH = DATA_ROOT / \"exercise_2\" / time.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "\n",
    "def save_figure(fig: plt.Figure, save_name: str) -> None:\n",
    "    \"\"\"Saves a figure into your google drive folder or local directory\"\"\"\n",
    "    DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    path = DATA_PATH / (save_name + \".png\")\n",
    "    fig.savefig(str(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear System and Quadratic Reward Function\n",
    "In this exercise we will consider a relatively simple type of environment, a linear dynamical system with a quadratic reward function.\n",
    "Note that this environment is deterministic.\n",
    "The learning agent does not have any information about the system dynamics and the reward function. \n",
    "\n",
    "The linear dynamics are described as follows:\n",
    "\n",
    "\\begin{align}\n",
    "      \\boldsymbol{s'} = \n",
    "       \\boldsymbol{As} + \\boldsymbol{Ba},\n",
    "\\end{align}\n",
    "where $\\boldsymbol{s'}$ denotes the state in the next time step and $\\boldsymbol{a}$ is the action input to the system. The identites of the system are given as \n",
    "\\begin{align}\n",
    "    \\boldsymbol{A} = \\begin{bmatrix}\n",
    "                        1 & 0.1\\\\\n",
    "                        0 & 0.99\n",
    "                      \\end{bmatrix}, ~~~~~\n",
    "     \\boldsymbol{B} = \\begin{bmatrix}\n",
    "                        0 \\\\\n",
    "                        0.1 \n",
    "                       \\end{bmatrix}.\n",
    "\\end{align}\n",
    "Thus, we have a two dimensional state-space and a one dimensional action space.\n",
    "\n",
    "The immediate reward function, is given as\n",
    "\\begin{align}\n",
    "    r(\\boldsymbol{s_t}, a_t) = -\\boldsymbol{s_t}^T\\boldsymbol{M}\\boldsymbol{s_t} - a_t^2Q,\n",
    "\\end{align}\n",
    "resulting in an episode reward\n",
    "\\begin{align}\n",
    "    R(\\tau) = \\sum_t r(\\boldsymbol{s_t}, a_t)\n",
    "\\end{align}\n",
    "\n",
    "The code block which describes the linear system and its corresponding quadratic reward function \n",
    "is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_DIM = 2\n",
    "A_DIM = 1\n",
    "HORIZON = 50\n",
    "\n",
    "\n",
    "class LinEnv:\n",
    "    _A = np.array([[1, 0.1], [0, 0.99]])\n",
    "    _B = np.array([0, 0.1])\n",
    "    _M = np.array([[10, 0], [0, 1]])\n",
    "    _Q = np.array([1])\n",
    "    _s_init = np.array([2, 1])\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.t = 0\n",
    "        self._s = np.array([2, 1])\n",
    "        self.s_max = np.ones(2) * 12\n",
    "        self.s_min = -np.ones(2) * 12\n",
    "        self.a_max = 8\n",
    "        self.a_min = -8\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self._s = self._s_init\n",
    "        self.t = 0\n",
    "        return self._s.copy()\n",
    "\n",
    "    def step(self, action: np.ndarray) -> tuple[np.ndarray, float]:\n",
    "        reward = -self._s.T @ self._M @ self._s - action.T * self._Q * action\n",
    "        clipped_action = np.clip(action, self.a_min, self.a_max)\n",
    "        self._s = self._A @ self._s + self._B * clipped_action\n",
    "        self._s = np.clip(self._s, self.s_min, self.s_max)\n",
    "        self.t += 1\n",
    "        return self._s.copy(), reward.item()\n",
    "\n",
    "\n",
    "linear_env = LinEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Controller\n",
    "We consider a linear, stochastic controller (the policy) of the form\n",
    "\\begin{align}\n",
    "    \\pi(a|\\boldsymbol{s}) = \\mathcal{N}(a|\\boldsymbol{Ks}, \\sigma^2) =\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{1}{2}\\frac{(a-\\boldsymbol{Ks})^2}{\\sigma^2}},\n",
    "\\end{align}\n",
    "where $\\boldsymbol{K} = [k_1, k_2]$ and $\\sigma$ are the learnable parameters.\n",
    "Our learning algorithms will optimize these parameters.\n",
    "Note that since the environment is deterministic, the policy is the only source of stochasticity in our problem.\n",
    "\n",
    "The following code defines the linear controller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinPolicy:\n",
    "    _params: np.ndarray\n",
    "    _K: np.ndarray\n",
    "    _std: np.ndarray  # standard deviation, i.e. sigma\n",
    "\n",
    "    def __init__(self):\n",
    "        self._K = np.zeros(S_DIM)\n",
    "        self._std = np.ones(A_DIM)\n",
    "        self.update_params(np.array([-10, -10, 1]))\n",
    "\n",
    "    @property\n",
    "    def params(self) -> np.ndarray:\n",
    "        # return a copy so that params cannot be modified externally\n",
    "        return self._params.copy()\n",
    "\n",
    "    @property\n",
    "    def n_params(self) -> int:\n",
    "        return self._params.shape[0]\n",
    "\n",
    "    def update_params(self, params: np.ndarray) -> None:\n",
    "        assert len(params) == S_DIM + A_DIM\n",
    "        self._K[:] = params[:S_DIM]\n",
    "        self._std[:] = params[S_DIM:]\n",
    "        self._params = params\n",
    "\n",
    "    def get_mean(self, state: np.ndarray) -> np.ndarray:\n",
    "        # expand dims so that dimension matches that of action\n",
    "        # dot product removes one dimension\n",
    "        return np.expand_dims(state @ self._K, axis=-1)\n",
    "\n",
    "    def sample(self, state: np.ndarray, rng: np.random.Generator) -> np.ndarray:\n",
    "        mean = self.get_mean(state)\n",
    "        action = rng.normal(mean, np.abs(self._std))\n",
    "        return action\n",
    "\n",
    "    def grad_log_pi(self, states: np.ndarray, actions: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Get the gradient of the log probability of the given action(s) in the given state(s).\n",
    "        :param states: np.ndarray [..., S_DIM], where S_DIM is the state dimension\n",
    "        :param actions: np.ndarray [..., A_DIM], where A_DIM is the action dimension\n",
    "        \"\"\"\n",
    "        std_inv = 1 / (self._std + 1e-20)\n",
    "        z = actions - self.get_mean(states)\n",
    "        grad_K = z * states * (std_inv**2)\n",
    "        grad_sigma = -std_inv + (z**2) * (std_inv**3)\n",
    "        return np.concatenate((grad_K, grad_sigma), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our policy only contains 2 optimizable parameters, we can easily visualize the loss landscape.\n",
    "This is usually not possible when working with neural networks with thousands or millions of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy: LinPolicy, env: LinEnv) -> float:\n",
    "    \"\"\"Evaluate policy and return mean sum of rewards over the horizon. The policy is evaluated without variance, making the result deterministic.\"\"\"\n",
    "    ep_rewards = 0.0\n",
    "    s = env.reset()\n",
    "    for _ in range(HORIZON):\n",
    "        a = policy.get_mean(s)\n",
    "        s, r = env.step(a)\n",
    "        ep_rewards += r\n",
    "    return ep_rewards\n",
    "\n",
    "\n",
    "N_POINTS = 50\n",
    "\n",
    "\n",
    "def get_return_contours() -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute reward for policies with all combinations of parameters between b_min and b_max\n",
    "    in steps of n_points. The policy is tested with the mean predicted action and zero variance.\n",
    "    Since this is result is deterministic, each policy is only tested once.\n",
    "    :return:\n",
    "        k1s: np.ndarray [n_points]: values of k1 tested\n",
    "        k2s: np.ndarray [n_points]: values of k2 tested\n",
    "        rewards: np.ndarray [n_points, n_points]: reward for each policy\n",
    "    \"\"\"\n",
    "    test_env = LinEnv()\n",
    "    test_policy = LinPolicy()\n",
    "    k1s = np.linspace(start=-10, stop=0, num=N_POINTS)\n",
    "    k2s = np.linspace(start=-15, stop=0, num=N_POINTS)\n",
    "    rewards = np.zeros((N_POINTS, N_POINTS))\n",
    "    for i, k1 in enumerate(k1s):\n",
    "        for j, k2 in enumerate(k2s):\n",
    "            c_params = np.array([k1, k2, 1])\n",
    "            test_policy.update_params(c_params)\n",
    "            rewards[j, i] = evaluate_policy(\n",
    "                policy=test_policy,\n",
    "                env=test_env,\n",
    "            )\n",
    "    rewards = np.clip(rewards, -1500, 1500)\n",
    "    return k1s, k2s, rewards\n",
    "\n",
    "\n",
    "K1S, K2S, REWARDS = get_return_contours()\n",
    "\n",
    "\n",
    "def plot_return_contours(ax):\n",
    "    contour_set = ax.contour(*np.meshgrid(K1S, K2S), REWARDS)\n",
    "    ax.clabel(contour_set, inline=True, fontsize=10)\n",
    "    ax.set(title=\"Return Landscape\", xlabel=\"Controller K1\", ylabel=\"Controller K2\")\n",
    "\n",
    "\n",
    "# show contour plot of returns over the two optimizable parameters of controller\n",
    "fig, ax = plt.subplots()\n",
    "plot_return_contours(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Algorithms\n",
    "\n",
    "Next, we will create a base class for policy gradient algorithms, which all have the same structure.\n",
    "This structure is generally:\n",
    "\n",
    "---\n",
    "\n",
    "- **Repeat**  For $k=1, 2, \\dots$\n",
    "    - `collect_policy_samples()`: Run policy to sample trajectories \\{$\\tau_i$\\}, ${i=1,...,N}$ from $\\pi_{\\boldsymbol{\\theta}}(a|\\boldsymbol{s})$\n",
    "    - `estimate_grad()`: Estimate the gradient $\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$\n",
    "    - `grad_ascent_step()`: Update the parameters according to the estimated $\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$\n",
    "\n",
    "- **Until convergence**\n",
    "\n",
    "---\n",
    "\n",
    "Based on this general structure, in the following we define a base class `PolicyGradientAlgo`, which contains shared attributes and functions.\n",
    "The function `train` implements the main algorithm loop.\n",
    "In this exercise, we will use various learning rates (`lr`) for various algorithms, with and without baselines.\n",
    "The parameters `n_iters` and `batch_size` stay the same for the algorithms.\n",
    "Also note that for this continuous control problem, the discount factor $\\gamma$ is omitted since we set it to $1.0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingResults:\n",
    "    test_rewards: np.ndarray\n",
    "    grad_estimates: np.ndarray\n",
    "    parameters: np.ndarray\n",
    "\n",
    "\n",
    "class PolicyGradientAlgo:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: LinEnv,\n",
    "        policy: LinPolicy,\n",
    "        n_iters: int,\n",
    "        batch_size: int,\n",
    "        lr: float,\n",
    "        baseline: bool = False,\n",
    "    ) -> None:\n",
    "        self.env = env  # current environment\n",
    "        self.policy = policy  # current policy object\n",
    "        self.n_iters = n_iters  # number of total iterations\n",
    "        self.batch_size = batch_size  # number of traj. samples per iteration\n",
    "        self.lr = lr  # learning rate\n",
    "        self.baseline = baseline  # whether to use baseline or not\n",
    "\n",
    "    def train(\n",
    "        self, fig, line_rewards, line_grads, line_params, points_params, final_params,\n",
    "    ) -> TrainingResults:\n",
    "        \"\"\"\n",
    "        This function will perform the main loop of the policy gradient algorithms.\n",
    "        For plotting and saving purposes, it will return the estimated gradients,\n",
    "        the test rewards and the parameters of each iteration.\n",
    "        :return:\n",
    "            TrainingResults:\n",
    "                estimated_grads: np.ndarray [n_iters x n_params]\n",
    "                test_rewards: np.ndarray [n_iters]\n",
    "                parameters: np.ndarray[n_iters+1 x n_params]\n",
    "        \"\"\"\n",
    "        # initialize random state\n",
    "        rng = np.random.default_rng(SEED)\n",
    "\n",
    "        # initialize variables to store training results\n",
    "        iters = []\n",
    "        test_rewards = []\n",
    "        grads = []\n",
    "        parameters = [self.policy.params]\n",
    "\n",
    "        for k in range(self.n_iters):\n",
    "            iters.append(k + 1)\n",
    "            states, actions, rewards = self.collect_policy_samples(rng)\n",
    "            grads.append(self.estimate_grad(states, actions, rewards))\n",
    "            parameters.append(self.grad_ascent_step(grads[-1], states, actions))\n",
    "            self.policy.update_params(parameters[-1])\n",
    "            test_rewards.append(evaluate_policy(policy=self.policy, env=self.env))\n",
    "\n",
    "            if k % 10 == 9 or k == (self.n_iters - 1):\n",
    "                # update plots\n",
    "                display.clear_output(wait=True)\n",
    "                line_rewards.set_data(iters, test_rewards)\n",
    "                # average gradient over params\n",
    "                mean_abs_grads = np.abs(grads).mean(axis=-1)\n",
    "                line_grads.set_data(iters, mean_abs_grads)\n",
    "                parameters_np = np.stack(parameters).T[:2]\n",
    "                line_params.set_data(parameters_np)\n",
    "                points_params.set_data(parameters_np)\n",
    "                display.display(fig)\n",
    "\n",
    "        # update plots\n",
    "        final_params.set_data(parameters_np[:, -1:])\n",
    "\n",
    "        # clear duplicate plot\n",
    "        display.clear_output()\n",
    "\n",
    "        return TrainingResults(\n",
    "            np.stack(test_rewards), np.stack(grads), np.stack(parameters)\n",
    "        )\n",
    "\n",
    "    def collect_policy_samples(self, rng: np.random.Generator):\n",
    "        states = np.zeros((self.batch_size, HORIZON, S_DIM))\n",
    "        actions = np.zeros((self.batch_size, HORIZON, A_DIM))\n",
    "        rewards = np.zeros((self.batch_size, HORIZON))\n",
    "\n",
    "        for j in range(self.batch_size):\n",
    "            state = self.env.reset()\n",
    "            for t in range(HORIZON):\n",
    "                states[j, t] = state\n",
    "                actions[j, t] = self.policy.sample(state, rng)\n",
    "                state, rewards[j, t] = self.env.step(actions[j, t])\n",
    "\n",
    "        return states, actions, rewards\n",
    "\n",
    "    def estimate_grad(\n",
    "        self,\n",
    "        states: np.ndarray,\n",
    "        actions: np.ndarray,\n",
    "        rewards: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"This function returns the gradient estimate of the return.\n",
    "        :param rewards: all training rewards of the last iteration:\n",
    "            np.ndarray [batch_size x HORIZON]\n",
    "        :param states: all states of the last iteration:\n",
    "            np.ndarray [batch_size x HORIZON x S_DIM], where S_DIM is the state dimension\n",
    "        :param actions: all taken actions of the last iteration:\n",
    "            np.ndarray [batch_size x HORIZON x A_DIM], where A_DIM is the action dimension\n",
    "        :return: grad_estimate: the estimated gradient: np.ndarray [n_params]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def grad_ascent_step(\n",
    "        self,\n",
    "        grad_estimates: np.ndarray,\n",
    "        states: np.ndarray | None = None,\n",
    "        actions: np.ndarray | None = None,\n",
    "    ):\n",
    "        \"\"\"This function performs the gradient ascent step.\n",
    "        :param grad_estimates: estimates of the gradients for all parameters:\n",
    "            np.ndarray [n_params]\n",
    "        :param states all states of the last iteration:\n",
    "            np.ndarray [batch_size x HORIZON x S_DIM], where S_DIM is the state dimension\n",
    "        :param actions all taken actions of the last iteration:\n",
    "            np.ndarray [batch_size x HORIZON x A_DIM], where A_DIM is the action dimension\n",
    "        :return: updated policy parameters: np.ndarray [n_params]\n",
    "        \"\"\"\n",
    "        return self.policy.params + self.lr * grad_estimates\n",
    "\n",
    "\n",
    "def create_figure(n_iters: int):\n",
    "    fig, (ax_returns, ax_grads, ax_contour) = plt.subplots(1, 3, figsize=(12, 5))\n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "    ax_returns.set(title=\"Returns\", xlabel=\"Iterations\", ylabel=\"Mean Rewards\")\n",
    "    ax_returns.set_xlim(0, n_iters)\n",
    "    ax_returns.set_ylim(-2500, -500)\n",
    "    ax_grads.set(\n",
    "        title=\"Mean Abs Gradient\",\n",
    "        xlabel=\"Iterations\",\n",
    "        ylabel=\"Mean of Parameter Gradients\",\n",
    "    )\n",
    "    ax_grads.set_xlim(0, n_iters)\n",
    "    ax_grads.set_yscale(\"log\")\n",
    "    ax_grads.set_ylim(1, 5000)\n",
    "    plot_return_contours(ax_contour)\n",
    "    return fig, (ax_returns, ax_grads, ax_contour)\n",
    "\n",
    "\n",
    "def add_method(axs, label: str, color: str):\n",
    "    ax_returns, ax_grads, ax_contour = axs\n",
    "    line_rewards, *_ = ax_returns.plot([], [], label=label, color=color)\n",
    "    line_grads, *_ = ax_grads.plot([], [], label=label, color=color)\n",
    "    line_params, *_ = ax_contour.plot([], [], label=label, color=color, alpha=0.5)\n",
    "    points_params, *_ = ax_contour.plot([], [], \"x\", color=color, alpha=0.5)\n",
    "    final_params, *_ = ax_contour.plot([], [], \"x\", color=\"red\")\n",
    "    ax_returns.legend()\n",
    "    return line_rewards, line_grads, line_params, points_params, final_params\n",
    "\n",
    "\n",
    "def plot_all_results(\n",
    "    axs,\n",
    "    results: Sequence[TrainingResults],\n",
    "    labels: Sequence[str],\n",
    "    colors: Sequence[str],\n",
    "):\n",
    "    ax_returns, ax_grads, ax_contour = axs\n",
    "    for result, label, color in zip(results, labels, colors):\n",
    "        iters = np.arange(len(result.test_rewards)) + 1\n",
    "        ax_returns.plot(iters, result.test_rewards, label=label, color=color)\n",
    "        mean_abs_grads = np.abs(result.grad_estimates).mean(axis=-1)\n",
    "        ax_grads.plot(iters, mean_abs_grads, color=color)\n",
    "        ax_contour.plot(\n",
    "            result.parameters[:, 0],\n",
    "            result.parameters[:, 1],\n",
    "            color=color,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        ax_contour.plot(\n",
    "            result.parameters[:, 0],\n",
    "            result.parameters[:, 1],\n",
    "            \"x\",\n",
    "            color=color,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        ax_contour.plot(\n",
    "            result.parameters[-1, 0],\n",
    "            result.parameters[-1, 1],\n",
    "            \"x\",\n",
    "            color=\"r\",\n",
    "        )\n",
    "\n",
    "    for ax in axs:\n",
    "        # Shrink all axes' height by 10% on the bottom\n",
    "        box = ax.get_position()\n",
    "        ax.set_position([box.x0, box.y0 + box.height * 0.15,\n",
    "                        box.width, box.height * 0.85])\n",
    "\n",
    "    ax_returns.legend(\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0.2, -0.15),\n",
    "        fancybox=True,\n",
    "        shadow=True,\n",
    "        ncol=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: REINFORCE (3 Points)\n",
    "We start with the most basic algorithm, **REINFORCE**. The **pseudocode** is given as\n",
    "\n",
    "---\n",
    "\n",
    "- **Repeat**  For $k=1, 2, \\dots$\n",
    "    - Run policy to sample trajectories \\{$\\tau_i$\\}, ${i=1,...,N}$ from $\\pi_{\\boldsymbol{\\theta}}(a|\\boldsymbol{s})$\n",
    "    - Estimate the gradient:\n",
    "$\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}) \\approx \\frac{1}{N}\\sum_i^N \\left(\\sum_t^H \\nabla_{\\boldsymbol{\\theta}} \\log \\pi_{\\boldsymbol{\\theta}}(a_{i,t}|\\boldsymbol{s}_{i,t})\\right)\\left(\\sum_t^H r(\\boldsymbol{s}_{i,t}, a_{i,t})\\right)$\n",
    "    - Update the parameters:\n",
    "$\\boldsymbol{\\theta}\\leftarrow \\boldsymbol{\\theta} + \\alpha \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$\n",
    "\n",
    "- **Until convergence**\n",
    "\n",
    "---\n",
    "\n",
    "The following class inherits from the `PolicyGradientAlgo` class and only needs to override the function `estimate_grad` according to the pseudo code given above.\n",
    "This function estimates the gradient of the return with respect to the parameters of the policy. \n",
    "\n",
    "Your task is to implement the `estimate_grad` function according to the pseudocode shown above (**Don't implement `grad_ascent_step()`!**).\n",
    "Through the object attribute `self.baseline`, we decide if we would like to estimate the gradient with or without a baseline. \n",
    "Make sure that both options are working in your code.\n",
    "\n",
    "Use the following baseline\n",
    "\\begin{align}\n",
    "    b = \\frac{1}{N} \\sum_i^N \\sum_t^H r(\\boldsymbol{s}_{i,t}, a_{i,t})\n",
    "\\end{align}\n",
    "\n",
    "*Hint: To get the gradient of the log policy for current state $\\boldsymbol{s}_{i,t}$ and current action $a_{i,t}$ from the rollout i at time step t, you will need to call `self.policy.grad_log_pi(current_state, current_action)`, where current_state is $\\boldsymbol{s}_{i,t}$ and current action is $a_{i,t}$.*\n",
    "\n",
    "After you completed the implementation, run the cell.\n",
    "During training, it plots the reward curve, the magnitude of the gradients, and the trajectory of the parameters through the parameter space.\n",
    "\n",
    "*Hint: If your algorithm learns with a baseline and doesn't learn without a baseline, don't worry, it probably won't.*\n",
    "*Continue on to the self-test questions, which should help you verify if your implementation is correct even if it doesn't learn.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE(PolicyGradientAlgo):\n",
    "    def estimate_grad(\n",
    "        self,\n",
    "        states: np.ndarray,\n",
    "        actions: np.ndarray,\n",
    "        rewards: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"This function returns the gradient estimate of the return.\n",
    "        :param rewards: all training rewards of the last iteration:\n",
    "            np.ndarray [batch_size x HORIZON]\n",
    "        :param states: all states of the last iteration:\n",
    "            np.ndarray [batch_size x HORIZON x S_DIM], where S_DIM is the state dimension\n",
    "        :param actions: all taken actions of the last iteration:\n",
    "            np.ndarray [batch_size x HORIZON x A_DIM], where A_DIM is the action dimension\n",
    "        :return: grad_estimate: the estimated gradient: np.ndarray [n_params]\n",
    "        \"\"\"\n",
    "        ## SOLUTION ##\n",
    "        # compute return as sum of rewards over time\n",
    "        returns = np.sum(rewards, axis=1)\n",
    "\n",
    "        # if using baseline, subtract mean of returns\n",
    "        if self.baseline:\n",
    "            returns = returns - np.mean(returns)\n",
    "\n",
    "        # get grad_log_pi for every time step in every trajectory\n",
    "        grad_log_pis = self.policy.grad_log_pi(states, actions)\n",
    "        # sum over time dimension\n",
    "        sum_grad_log_pis = grad_log_pis.sum(axis=1)\n",
    "        # scale grad_log_pi for each trajectory by returns of that trajectory\n",
    "        grad_estimates = sum_grad_log_pis * np.expand_dims(returns, axis=-1)\n",
    "        # take the mean over the batch\n",
    "        grad_estimate = grad_estimates.mean(axis=0)\n",
    "        ## END ##\n",
    "\n",
    "        assert grad_estimate.shape == (3,)\n",
    "        return grad_estimate\n",
    "\n",
    "\n",
    "N_ITERS = 300\n",
    "N_ITERS_W_BASELINE = 150\n",
    "BATCH_SIZE = 25\n",
    "\n",
    "fig, axs = create_figure(n_iters=N_ITERS)\n",
    "lines = add_method(axs, label=\"REINFORCE\", color=\"cornflowerblue\")\n",
    "\n",
    "reinforce = REINFORCE(\n",
    "    env=linear_env,\n",
    "    policy=LinPolicy(),\n",
    "    n_iters=N_ITERS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=1e-4,\n",
    "    baseline=False,\n",
    ")\n",
    "reinforce_results = reinforce.train(fig, *lines)\n",
    "\n",
    "lines = add_method(axs, label=\"REINFORCE with baseline\", color=\"blue\")\n",
    "\n",
    "reinforce_with_baseline = REINFORCE(\n",
    "    env=linear_env,\n",
    "    policy=LinPolicy(),\n",
    "    n_iters=N_ITERS_W_BASELINE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=1e-3,\n",
    "    baseline=True,\n",
    ")\n",
    "reinforce_with_baseline_results = reinforce_with_baseline.train(fig, *lines)\n",
    "\n",
    "save_figure(fig, \"reinforce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Test Questions (optional)\n",
    "\n",
    "Compare the performance of the algorithm with and without a baseline.\n",
    "Notice that with a baseline, we use a learning rate of $10^{-3}$, whereas without a baseline, we use a learning rate of $10^{-4}$.\n",
    "Why is this necessary?\n",
    "\n",
    "Obviously REINFORCE without a baseline is not a very reliable algorithm.\n",
    "What happens if you decrease or increase the learning rate?\n",
    "Is it possible to find a learning rate that converges to a better policy without making learning unstable?\n",
    "Don't forget to try different random seeds (e.g. `SEED = 5`).\n",
    "\n",
    "What happens if you increase the batch size?\n",
    "Why does this tend to stabilize training?\n",
    "How much do you need to increase the batch size so that the policy never makes a \"jump\" across parameter space?\n",
    "\n",
    "What is the role of the stochastic policy here?\n",
    "If the environment is deterministic, why should we complicate things by adding randomness to the policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Policy Gradient Theorem (3 Points)\n",
    "\n",
    "**REINFORCE** suffers from high variance in the gradient estimates.\n",
    "One way to reduce this high variance is to exploit the temporal structure of the trajectory when estimating returns.\n",
    "This means using the Monte-Carlo estimate of the return to go from the current state, instead of the total return for the whole trajectory.\n",
    "The pseudo code is given as\n",
    "\n",
    "---\n",
    "\n",
    "- **Repeat**  For $k=1, 2, \\dots$\n",
    "    - Run policy to sample trajectories \\{$\\tau_i$\\}, ${i=1,...,N}$ from $\\pi_{\\boldsymbol{\\theta}}(a|\\boldsymbol{s})$\n",
    "    - Estimate the gradient:\n",
    "$\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}) \\approx \\frac{1}{N} \\sum_i^N \\sum_t^H \\nabla_{\\boldsymbol{\\theta}}\\log \\pi_{\\boldsymbol{\\theta}}(a_{i,t}|\\boldsymbol{s}_{i,t})\\left(\\sum_{k=t}^H r(\\boldsymbol{s}_{i,k}, a_{i,k})\\right)$\n",
    "    - Update the parameters:\n",
    "$\\boldsymbol{\\theta}\\leftarrow \\boldsymbol{\\theta} + \\alpha \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$\n",
    "\n",
    "- **Until convergence**\n",
    "\n",
    "---\n",
    "\n",
    "The following class inherits from the `PolicyGradientAlgo` class and only needs to override the function `estimate_grad` according to the pseudo code given above.\n",
    "This function estimates the gradient of the return with respect to the parameters of the policy. \n",
    "\n",
    "Your task is to implement the `estimate_grad` function according to the pseudocode shown above (**Don't implement `grad_ascent_step()`!**).\n",
    "Through the object attribute `self.baseline`, we decide if we would like to estimate the gradient with or without a baseline. \n",
    "Make sure that both options are working in your code.\n",
    "\n",
    "Use the following **time-dependent** baseline for the return starting from state $s_t$\n",
    "\\begin{align}\n",
    "    b_t = \\frac{1}{N} \\sum_i^N \\sum_{k=t}^H r(\\boldsymbol{s}_{i,k}, a_{i,k})\n",
    "\\end{align}\n",
    "\n",
    "*Hint: To get the gradient of the log policy for current state $\\boldsymbol{s}_{i,t}$ and current action $a_{i,t}$ from the rollout i at time step t, you will need to call `self.policy.grad_log_pi(current_state, current_action)`, where current_state is $\\boldsymbol{s}_{i,t}$ and current action is $a_{i,t}$.*\n",
    "\n",
    "After you completed the implementation, run the cell.\n",
    "During training, it plots the reward curve, the magnitude of the gradients, and the trajectory of the parameters through the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientTheorem(PolicyGradientAlgo):\n",
    "    def estimate_grad(\n",
    "        self,\n",
    "        states: np.ndarray,\n",
    "        actions: np.ndarray,\n",
    "        rewards: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        This function returns the gradient estimate of the return.\n",
    "        :param rewards: all training rewards of the last iteration: np.ndarray\n",
    "                     [batch_size x HORIZON]\n",
    "        :param states: all states of the last iteration: np.ndarray\n",
    "                     [batch_size x HORIZON x S_DIM], where S_DIM is the state dimension\n",
    "        :param actions: all taken actions of the last iteration: np.ndarray\n",
    "                     [batch_size x HORIZON x A_DIM], A_DIM is the action dimension\n",
    "        :return: grad_estimate: the estimated gradient: np.ndarray [n_params]\n",
    "        \"\"\"\n",
    "        ## SOLUTION ##\n",
    "        # reverse the rewards in the time dimension\n",
    "        rewards_time_reversed = rewards[:, ::-1]\n",
    "        # compute the cumulative sums in the time dimension\n",
    "        returns_to_go_time_reversed = np.cumsum(rewards_time_reversed, axis=1)\n",
    "        # reverse the cumulative sums to get the return to go\n",
    "        returns_to_go = returns_to_go_time_reversed[:, ::-1]\n",
    "\n",
    "        # if using baseline, subtract mean of returns to go\n",
    "        if self.baseline:\n",
    "            # take mean over batch, but leave time dimension\n",
    "            # baseline must depend on time\n",
    "            returns_to_go = returns_to_go - np.mean(returns_to_go, axis=0)\n",
    "\n",
    "        # get grad_log_pi for every time step in every trajectory\n",
    "        grad_log_pis = self.policy.grad_log_pi(states, actions)\n",
    "        # scale grad_log_pi for each trajectory by returns of that trajectory\n",
    "        grad_estimates = grad_log_pis * np.expand_dims(returns_to_go, axis=-1)\n",
    "        # take the sum over time and mean over the batch\n",
    "        grad_estimate = grad_estimates.sum(axis=1).mean(axis=0)\n",
    "        ## END ##\n",
    "\n",
    "        assert grad_estimate.shape == (3,)\n",
    "        return grad_estimate\n",
    "\n",
    "\n",
    "N_ITERS = 300\n",
    "N_ITERS_W_BASELINE = 150\n",
    "BATCH_SIZE = 25\n",
    "\n",
    "fig, axs = create_figure(n_iters=N_ITERS)\n",
    "lines = add_method(axs, label=\"Policy Gradient Theorem\", color=\"olive\")\n",
    "\n",
    "pgt = PolicyGradientTheorem(\n",
    "    env=linear_env,\n",
    "    policy=LinPolicy(),\n",
    "    n_iters=N_ITERS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=1e-4,\n",
    "    baseline=False,\n",
    ")\n",
    "pgt_results = pgt.train(fig, *lines)\n",
    "\n",
    "lines = add_method(axs, label=\"Policy Gradient Theorem with baseline\", color=\"green\")\n",
    "\n",
    "pgt_with_baseline = PolicyGradientTheorem(\n",
    "    env=linear_env,\n",
    "    policy=LinPolicy(),\n",
    "    n_iters=N_ITERS_W_BASELINE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=1e-3,\n",
    "    baseline=True,\n",
    ")\n",
    "pgt_with_baseline_results = pgt_with_baseline.train(fig, *lines)\n",
    "\n",
    "save_figure(fig, \"policy_gradient_theorem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Test Questions (optional)\n",
    "\n",
    "How do the performance and stability compare to the REINFORCE algorithm?\n",
    "\n",
    "What happens if you increase or decrease the learning rate for the case with and without a baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Natural Policy Gradient (3 Points)\n",
    "\n",
    "A common approach in policy search is to apply a trust region constraint, where the policy update is bounded.\n",
    "It has been shown that trust regions highly stabilize the learning process. \n",
    "\n",
    "A trust region approach which can be applied to continous control problems with parametric policy distributions is **Natural Policy Gradient**.\n",
    "Here, the KL-constraint is approximated with the second order Taylor approximation, which results in the **Fisher Information** matrix $F$.\n",
    "\n",
    "Concretely, we can calculate $F$ as \n",
    "\\begin{align}\n",
    "    \\boldsymbol{F} = \\frac{1}{NH} \\sum_i^N \\sum_t^H \\left[\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{i,t}|s_{i,t})\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{i,t}|s_{i,t})^T\\right].\n",
    "\\end{align}\n",
    "\n",
    "The policy's parameter update is then given as\n",
    "\\begin{align}\n",
    "    \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\alpha\\boldsymbol{F}^{-1}\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}),\n",
    "\\end{align}\n",
    "where for the gradient estimate $\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$ standard techniques like the policy gradient theorem are used.\n",
    "\n",
    "In Natural Policy Gradient, the learning rate parameter $\\alpha ~~(\\eta^{-1}\\text{ in the slides})$ can be solved in closed-form by finding the optimal solution of the dual function to the according Lagrangian (see Task 4). More specifically, $\\alpha$ is given as \n",
    "\\begin{align}\n",
    "    \\alpha = \\sqrt{\\frac{4\\epsilon}{\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})^{T}\\boldsymbol{F}^{-1}\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})}},\n",
    "\\end{align}\n",
    "where $\\epsilon$ is the hyperparameter, bounding the expected KL between the new and the old policy.\n",
    "\n",
    "Since we will use the gradient estimate from the **Policy Gradient Theorem**, the following class inherits from the **PolicyGradientTheorem** class, i.e. you need to have properly solved Task 2 in order to be able to solve this task.\n",
    "\n",
    "Implement the function `get_fisher_information()`, which will return the Fisher information matrix given state-action trajectories.\n",
    "Apply the equations mentioned above.\n",
    "Then implement the function `grad_ascent_step()`, which calculates and returns the new parameters of the policy according to the update rule mentioned above.\n",
    "When implementing the update, please also use the closed-form solution to $\\alpha$ to scale the update.\n",
    "\n",
    "*Note: You will need to have properly solved Task 2 in order to be able to solve this task.*\n",
    "\n",
    "*Hint: To get the gradient of the log policy for current state $\\boldsymbol{s}_{i,t}$ and current action $a_{i,t}$ from the rollout i at time step t, you will need to call `self.policy.grad_log_pi(current_state, current_action)`, where current_state is $\\boldsymbol{s}_{i,t}$ and current action is $a_{i,t}$.*\n",
    "\n",
    "After you completed the implementation, run the cell.\n",
    "During training, it plots the reward curve, the magnitude of the gradients, and the trajectory of the parameters through the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaturalPolicyGradient(PolicyGradientTheorem):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        eps: float,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        # we don't use lr in natural gradient, so set to zero\n",
    "        super().__init__(*args, lr=0.0, **kwargs)\n",
    "        self.eps = eps\n",
    "\n",
    "    def get_fisher_information(\n",
    "        self,\n",
    "        states: np.ndarray,\n",
    "        actions: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        This function calculates the Fisher Information matrix.\n",
    "        :param states: all states of the last iteration: np.ndarray\n",
    "                     [batch_size x HORIZON x S_DIM], where S_DIM is the state dimension\n",
    "        :param actions: all taken actions of the last iteration: np.ndarray\n",
    "                     [batch_size x HORIZON x A_DIM], A_DIM is the action dimension\n",
    "        :return: F: returns the Fisher information matrix: np.ndarray [n_params x n_params]\n",
    "        \"\"\"\n",
    "        ## SOLUTION ##\n",
    "        # get grad_log_pi for every time step in every trajectory\n",
    "        grad_log_pis = self.policy.grad_log_pi(states, actions)\n",
    "        # compute outer product by inserting dimensions and using broadcasting\n",
    "        outer_product = grad_log_pis[..., np.newaxis, :] * grad_log_pis[..., np.newaxis]\n",
    "        F = np.mean(outer_product, axis=(0, 1))\n",
    "        ## END ##\n",
    "\n",
    "        assert F.shape == (3, 3)\n",
    "        return F\n",
    "\n",
    "    def grad_ascent_step(\n",
    "        self,\n",
    "        grad_estimates: np.ndarray,\n",
    "        states: np.ndarray,\n",
    "        actions: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Performs an updated on the parameters of the policy according to the Natural Policy Gradient Rule by using\n",
    "        the current gradient estimate (grad_estimate) of the Policy Gradient theorem with baseline, the\n",
    "        state trajectories and the action trajectories.\n",
    "        :param grad_estimates: estimates of the gradients for all parameters:\n",
    "            np.ndarray [n_params]\n",
    "        :param states all states of the last iteration:\n",
    "            np.ndarray [batch_size x HORIZON x S_DIM], where S_DIM is the state dimension\n",
    "        :param actions all taken actions of the last iteration:\n",
    "            np.ndarray [batch_size x HORIZON x A_DIM], where A_DIM is the action dimension\n",
    "        :return: updated policy parameters: np.ndarray [n_params]\n",
    "        \"\"\"\n",
    "        ## SOLUTION ##\n",
    "        F = self.get_fisher_information(states, actions)\n",
    "        F_inv = np.linalg.inv(F)\n",
    "        ng = F_inv @ grad_estimates\n",
    "        alpha = np.sqrt(4 * self.eps / (grad_estimates.T @ F_inv @ grad_estimates))\n",
    "        new_policy_params = self.policy.params + alpha * ng\n",
    "        ## END ##\n",
    "\n",
    "        assert new_policy_params.shape == (3,)\n",
    "        return new_policy_params\n",
    "\n",
    "\n",
    "N_ITERS_NG = 20\n",
    "BATCH_SIZE = 25\n",
    "\n",
    "fig, axs = create_figure(n_iters=N_ITERS_NG)\n",
    "lines = add_method(axs, label=\"Natural Policy Gradient\", color=\"orange\")\n",
    "\n",
    "npg = NaturalPolicyGradient(\n",
    "    env=linear_env,\n",
    "    policy=LinPolicy(),\n",
    "    n_iters=N_ITERS_NG,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    eps=0.5,\n",
    "    baseline=True,\n",
    ")\n",
    "npg_results = npg.train(fig, *lines)\n",
    "\n",
    "save_figure(fig, \"natural_policy_gradient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Test Questions (optional)\n",
    "\n",
    "How does this algorithm compare to the two first-order methods used earlier?\n",
    "\n",
    "What happens if you increase or decrease `eps` (comparable to the learning rate)?\n",
    "How does the algorithm respond to different batch sizes?\n",
    "\n",
    "The use of a baseline made a massive difference with the first-order methods used earlier.\n",
    "What happens if you remove the baseline here?\n",
    "Can you tune the hyperparameters such that it converges anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will plot the results from all algorithms into one plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all algorithms in one plot\n",
    "fig, axs = create_figure(n_iters=300)\n",
    "plot_all_results(\n",
    "    axs,\n",
    "    results=[\n",
    "        reinforce_results,\n",
    "        reinforce_with_baseline_results,\n",
    "        pgt_results,\n",
    "        pgt_with_baseline_results,\n",
    "        npg_results,\n",
    "    ],\n",
    "    labels=[\n",
    "        \"REINFORCE\",\n",
    "        \"REINFORCE with baseline\",\n",
    "        \"Policy Gradient Theorem\",\n",
    "        \"Policy Gradient Theorem with baseline\",\n",
    "        \"Natural Policy Gradient\",\n",
    "    ],\n",
    "    colors=[\"cornflowerblue\", \"blue\", \"olive\", \"green\", \"orange\"],\n",
    ")\n",
    "\n",
    "save_figure(fig, \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Natural Policy Gradient Step Size (4 Points)\n",
    "\n",
    "Recall that Natural Gradients use a Taylor approximation of the trust region problem, i.e., the objective is given as \n",
    "$$\\boldsymbol{g}^* = \\underset{\\boldsymbol{g}}{\\textrm{argmax}} ~~ \\boldsymbol{g}^T\\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J} ~~ \\textrm{s.t.} ~~ \\boldsymbol{g}^T \\boldsymbol{F} \\boldsymbol{g} \\leq \\epsilon.$$\n",
    "By introducing a Lagrangian multiplier $\\eta$ we can construct the corresponding Lagrangian\n",
    "$$ L(\\boldsymbol{g}, \\eta) = \\boldsymbol{g}^T \\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J} + \\eta \\left(\\epsilon - \\boldsymbol{g}^T\\boldsymbol{F}\\boldsymbol{g}\\right).$$\n",
    "\n",
    "\\## SOLUTION ##\n",
    "\n",
    "\\begin{align}\n",
    "    L(\\boldsymbol{g}, \\eta) &= \\boldsymbol{g}^T \\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J} + \\eta \\left(\\epsilon - \\boldsymbol{g}^T\\boldsymbol{F}\\boldsymbol{g}\\right) \\\\\n",
    "    \\nabla_{\\boldsymbol{g}} L &= \\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J} -2\\eta\\boldsymbol{Fg} \\\\\n",
    "    \\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J}  &= 2 \\eta\\boldsymbol{F}\\boldsymbol{g} \\\\\n",
    "    \\boldsymbol{g}   &= \\left(2\\eta\\boldsymbol{F}\\right)^{-1}\\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J} \n",
    "\\end{align}\n",
    "\n",
    "Substituting back into Lagrangian yields dual:\n",
    "\\begin{align}\n",
    "    g(\\boldsymbol{g}^*, \\eta) &= \\frac{1}{2}\\eta^{-1}\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{J}^T\\boldsymbol{F}^{-T}\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{J} + \\eta\\epsilon - \\eta\\frac{1}{2}\\eta^{-1}\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{J}^T\\boldsymbol{F}^{-T}\\boldsymbol{F}\\frac{1}{2}\\eta^{-1}\\boldsymbol{F}^{-1}\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{J}\\\\\n",
    "    &= \\frac{1}{2}\\eta^{-1}\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{J}^T\\boldsymbol{F}^{-T}\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{J} + \\eta\\epsilon - \\frac{1}{4}\\eta^{-1}\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{J}^T\\boldsymbol{F}^{-1}\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{J}\\\\\n",
    "    &= \\frac{1}{4}\\eta^{-1}\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{J}^T\\boldsymbol{F}^{-T}\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{J} + \\eta\\epsilon\n",
    "\\end{align}\n",
    "Derivative of dual w.r.t. $\\eta$:\n",
    "\\begin{align}\n",
    "    \\dfrac{g(\\boldsymbol{g}^*, \\eta)}{\\eta} &= - \\frac{1}{4}\\eta^{-2}\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{J}^T\\boldsymbol{F}^{-T}\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{J} + \\epsilon \\\\\n",
    "    \\rightarrow \\eta^{-1} &= \\sqrt{\\frac{4\\epsilon}{\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{J}^T\\boldsymbol{F}^{-T}\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{J}} }\n",
    "\\end{align}\n",
    "\n",
    "**Alternative Solution:**\n",
    "From the KKT Conditions we get the following equation system\n",
    "\n",
    "\\begin{align}\n",
    "    \\dfrac{\\partial L (\\boldsymbol{g}, \\eta)}{\\partial \\boldsymbol{g}} = 0  \\\\ \\dfrac{\\partial L (\\boldsymbol{g}, \\eta)}{\\partial \\eta} = 0 \n",
    "\\end{align}\n",
    "\n",
    "Solving (1) for $\\boldsymbol{g}$ \n",
    "\n",
    "\\begin{align*}\n",
    "&\\dfrac{\\partial L(\\boldsymbol{g}, \\eta)}{\\partial \\boldsymbol{g}} = 0\\\\\n",
    "&\\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J}  - 2\\eta\\boldsymbol{F}\\boldsymbol{g} = 0 \\\\\n",
    "&\\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J}  = 2 \\eta\\boldsymbol{F}\\boldsymbol{g} \\\\\n",
    "&\\boldsymbol{g}   = \\left(2\\eta\\boldsymbol{F}\\right)^{-1}\\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J} \n",
    "\\end{align*}\n",
    "Plugging into (2), solving for $\\eta$\n",
    "\\begin{align*}\n",
    "&\\dfrac{\\partial L(\\boldsymbol{g}, \\eta)}{\\partial \\eta} = \\epsilon - \\boldsymbol{g}^T \\boldsymbol{F}\\boldsymbol{g} = \\epsilon - \\left( \\left(2\\eta\\boldsymbol{F}\\right)^{-1}\\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J}\\right)^T \\boldsymbol{F} \\left(2\\eta\\boldsymbol{F}\\right)^{-1}\\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J} = \\epsilon - \\dfrac{1}{4\\eta^2} \\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J}^T \\boldsymbol{F}^{-1}\\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J} = 0\n",
    "\\\\\n",
    "& \\Leftrightarrow \\eta = \\dfrac{1}{2\\sqrt{\\epsilon}}\\sqrt{ \\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J}^T \\boldsymbol{F}^{-1}\\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J}}\n",
    "\\end{align*}\n",
    "Plug back into the solution of (1)\n",
    "$$g = \\dfrac{\\sqrt{\\epsilon}}{\\sqrt{ \\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J}^T \\boldsymbol{F}^{-1}\\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J}}} \\boldsymbol{F}^{-1}\\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J} = \\dfrac{2\\sqrt{\\epsilon}}{|| \\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J} ||_{\\boldsymbol{F}^{-1}}}  \\boldsymbol{F}^{-1}\\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J}$$\n",
    "\n",
    "\\## END ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Policy Gradients with Neural Network Policies\n",
    "\n",
    "The environment and policy we were using until now were relatively impractical for practical use.\n",
    "Instead of a linear stochastic controller, we usually use a neural network policy, making it tedious to compute the gradient explicitly.\n",
    "Also, inverting the Fisher Information matrix $F$ becomes too computationally intensive, since matrix inversion is an $O(n^3)$ algorithm, where $n$ is the number of parameters.\n",
    "\n",
    "Let's consider training neural network policies to solve the discrete pole balancing environment called 'CartPole'.\n",
    "We will use the ['CartPole-v1' environment from Gymnasium](https://gymnasium.farama.org/environments/classic_control/cart_pole/).\n",
    "\n",
    "![The CartPole Environment](https://gymnasium.farama.org/_images/cart_pole.gif)\n",
    "\n",
    "We will use the **REINFORCE** algorithm **with baselines** and **policy gradient theorem** as shown below. \n",
    "\n",
    "---\n",
    "\n",
    "- **Repeat**  For $k=1, 2, \\dots$\n",
    "    - Run policy to sample trajectories \\{$\\tau_i$\\}, ${i=1,...,N}$ from $\\pi_{\\boldsymbol{\\theta}}(a|\\boldsymbol{s})$\n",
    "    - Estimate the gradient:\n",
    "$\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) \\approx \\frac{1}{N} \\sum_i^N \\sum_t^H \\nabla_{\\boldsymbol{\\theta}} \\log \\pi_{\\boldsymbol{\\theta}}(a_{i,t}|\\boldsymbol{s}_{i,t}) \\left(Q_t - b_t \\right)$\n",
    "    - Update the parameters:\n",
    "$\\boldsymbol{\\theta}\\leftarrow \\boldsymbol{\\theta} + \\alpha \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$\n",
    "\n",
    "- **Until convergence**\n",
    "\n",
    "---\n",
    "This equation is quite similar to what we used before.\n",
    "The difference here is that our policy $\\pi_{\\boldsymbol{\\theta}}$ is no longer a linear controller but a non-linear neural network.\n",
    "Since we are using neural networks, we will no longer compute the gradient explicitly, but instead let autograd do it for us.\n",
    "Instead of computing $\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})$, we now need to compute a \"loss function\" $J(\\boldsymbol{\\theta})$, such that its gradient is the gradient estimate we want.\n",
    "Autograd will then compute $\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})$ for us implicitly.\n",
    "\n",
    "Also, we've generalized the return estimate to the form $(Q_t - b_t)$, where $Q_t$ is some estimate of the Q value at time $t$ (e.g. return to go), while $b_t$ is some baseline (but may be no baseline).\n",
    "\n",
    "We first import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.distributions as distributions\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train_once(\n",
    "    env: gym.Env,\n",
    "    policy: nn.Module,\n",
    "    discount: float,\n",
    "    baseline: bool,\n",
    "    pg_theorem: bool,\n",
    ") -> tuple[float, float]:\n",
    "    policy.train()\n",
    "\n",
    "    log_prob_actions: list[torch.Tensor] = []\n",
    "    rewards: list[float] = []\n",
    "\n",
    "    state, info = env.reset()\n",
    "    terminated = truncated = False\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        state = torch.from_numpy(state).to(DEVICE).unsqueeze(dim=0)\n",
    "\n",
    "        # get logits from neural network (unnormalized probabilities)\n",
    "        action_logits = policy(state)\n",
    "\n",
    "        # create a probability distribution using softmax on the logits\n",
    "        action_prob = F.softmax(action_logits, dim=-1)\n",
    "\n",
    "        # Sample discrete actions from a categorical distribution\n",
    "        # https://pytorch.org/docs/stable/distributions.html\n",
    "        dist = distributions.Categorical(action_prob)\n",
    "\n",
    "        # sample an action (without gradients)\n",
    "        action = dist.sample()\n",
    "\n",
    "        # Use distribution object to compute log probabilities for the sampled actions.\n",
    "        # These values have gradients and are used for optimization\n",
    "        log_prob_action = dist.log_prob(action)\n",
    "\n",
    "        state, reward, terminated, truncated, info = env.step(\n",
    "            action.squeeze(dim=0).cpu().numpy()\n",
    "        )\n",
    "\n",
    "        log_prob_actions.append(log_prob_action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    returns = calculate_returns(rewards, discount, baseline, pg_theorem)\n",
    "\n",
    "    loss = update_policy(\n",
    "        returns=torch.from_numpy(returns).to(DEVICE),\n",
    "        log_prob_actions=torch.cat(log_prob_actions),\n",
    "    )\n",
    "\n",
    "    return loss, returns[0].item()\n",
    "\n",
    "\n",
    "def evaluate_policy(envs: gym.Env, policy: nn.Module) -> float:\n",
    "    policy.eval()\n",
    "\n",
    "    return_ = 0\n",
    "\n",
    "    state, info = envs.reset()\n",
    "    terminated = truncated = False\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        state = torch.from_numpy(state).to(DEVICE).unsqueeze(dim=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # get logits from neural network (unnormalized probabilities)\n",
    "            action_logits = policy(state)\n",
    "\n",
    "        # create a probability distribution using softmax on the logits\n",
    "        action_prob = F.softmax(action_logits, dim=-1)\n",
    "\n",
    "        # select the action with the highest probability\n",
    "        action = torch.argmax(action_prob, dim=-1)\n",
    "\n",
    "        state, reward, terminated, truncated, info = envs.step(\n",
    "            action.squeeze(dim=0).cpu().numpy()\n",
    "        )\n",
    "\n",
    "        return_ += reward\n",
    "\n",
    "    return return_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1: Building a Neural Network in PyTorch (1 point)\n",
    "    \n",
    "This part of the code creates a feed forward neural network using PyTorch, which will form the policy  $\\pi_{\\boldsymbol{\\theta}}(a|\\boldsymbol{s})$.\n",
    "\n",
    "Implement the `__init__` method of the `MLP` class, which creates the neural network as a sequence of alternating `torch.nn.Linear` and `torch.nn.ReLU` layers.\n",
    "After the `__init__` method, `MLP` should have a `network` attribute which is a `torch.nn.Sequential` object.\n",
    "The neural network should have `input_size` input nodes, then hidden layers with sizes dictated by `hidden_sizes`, and finally `output_size` output nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        hidden_sizes: list[int],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        :param state_dim: dimension of the state space\n",
    "        :param state_dim: dimension of the actions\n",
    "        :param hidden_units: list of integers corresponding to hidden units\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        ## SOLUTION ##\n",
    "        ins = [input_size] + hidden_sizes\n",
    "        outs = hidden_sizes + [output_size]\n",
    "\n",
    "        layers = []\n",
    "        for size_in, size_out in zip(ins, outs):\n",
    "            layers.append(nn.Linear(size_in, size_out))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        layers.pop()  # remove final ReLU layer\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        ## END ##\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"forward pass of decoder\n",
    "        :param input:\n",
    "        :return: output mean\n",
    "        \"\"\"\n",
    "        return self.network(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Q-values\n",
    "The code block computes numpy arrays for Q-values which will be used to weight the $\\nabla_{\\boldsymbol{\\theta}} \\log \\pi_{\\boldsymbol{\\theta}}(a_{i,t}|\\boldsymbol{s}_{i,t})$ of each time step.\n",
    "\n",
    "Recall that the expression for the policy gradient \"loss\" is\n",
    "\n",
    "\\begin{align}\n",
    "    J = \\text{E}_{\\tau\\sim p(\\tau)}\\left[\\sum_{t=0}^H \\nabla_{\\boldsymbol{\\theta}} \\log \\pi_{\\boldsymbol{\\theta}}(a_{i,t}|\\boldsymbol{s}_{i,t}) (Q_t-b_t)\\right],\n",
    "\\end{align}\n",
    "where $ \\tau=(s_0, a_0, ...)$ is a trajectory, $Q_t$ is the Q-value at time $t$ and $b_t$ is a baseline.\n",
    "\n",
    "\n",
    "We can obtain four different cases, depending on whether we use returns to returns to go and whether we subtract a baseline or not:\n",
    "\n",
    "**Case 1: REINFORCE (pg_theorem = False)**\n",
    "\n",
    "Here we estimate $Q_t$ by the total discounted return for the entire trajectory, regardless of which time step the Q-value should be for. Therefore, the Q estimator is\n",
    "\n",
    "\\begin{align}\n",
    "    Q_t = \\text{R}(\\tau) = \\sum_{k=0}^H r_{k},\n",
    "\\end{align}\n",
    "\n",
    "**Case 2: Policy Gradient Theorem (pg_theorem = True)**\n",
    "\n",
    "Here, we subtract out any rewards from the past, since they cannot have resulted from the current action. Thus, the Q estimator is\n",
    "\n",
    "\\begin{align}\n",
    "    Q_t = \\text{R}(\\tau_{k \\gt t}) = \\sum_{k=t}^H r_{k}\n",
    "\\end{align}\n",
    "\n",
    "**Case 3: No Baseline (baseline = False)**\n",
    "\n",
    "Here we use the absolute $Q_t$ as is, with the baseline set to 0:\n",
    "\n",
    "\\begin{align}\n",
    "    b_t = 0\n",
    "\\end{align}\n",
    "\n",
    "**Case 4: With baseline (baseline = True)**\n",
    "\n",
    "We can subtract any baseline from the $Q_t$ as long as it doesn't depend on the trajectory. In this example, since we only have one environment, we take the mean of the $1000$ most recent $Q$ values.\n",
    "\n",
    "\\begin{align}\n",
    "    b_t = \\frac{1}{N}\\sum_i^{N} Q_t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(\n",
    "    rewards: list[float],\n",
    "    discount: float,\n",
    "    baseline: bool,\n",
    "    pg_theorem: bool,\n",
    ") -> np.ndarray:\n",
    "    returns = []\n",
    "    return_ = 0\n",
    "    for reward in reversed(rewards):\n",
    "        return_ = reward + return_ * discount\n",
    "        returns.append(return_)\n",
    "    returns.reverse()\n",
    "\n",
    "    if not pg_theorem:\n",
    "        returns = returns[:1] * len(returns)\n",
    "\n",
    "    returns = np.array(returns)\n",
    "\n",
    "    if baseline:\n",
    "        RECENT_RETURNS.extend(returns)\n",
    "        mean_return = np.mean(RECENT_RETURNS)\n",
    "        returns = returns - mean_return\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.2: Computing Loss (1 Points)\n",
    "\n",
    "Here we create a \"pseudoloss\" which is the weighted maximum likelihood, $\\sum_{t=0}^T \\log \\pi_{\\theta}(a_t|s_t)  (Q_t - b_t )$, using the stored `log_prob_actions` and `returns` values computed in the previous section.\n",
    "The gradient of this loss function with respect to the neural network parameters ($\\theta$) is the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(\n",
    "    returns: torch.Tensor,\n",
    "    log_prob_actions: torch.Tensor,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the loss and backpropagate the errors using pytorch optmizers.\n",
    "    :param returns: returns (maybe with a baseline) received from the previous trajectory\n",
    "        torch.Tensor [HORIZON]\n",
    "    :param log_prob_actions: log probability of each action from the previous trajectory\n",
    "        torch.Tensor [HORIZON]\n",
    "    :param optimizer: the torch optimizer object https://pytorch.org/docs/stable/optim.html\n",
    "    :return: magnitude of calculated loss (float)\n",
    "    \"\"\"\n",
    "\n",
    "    ## SOLUTION ##\n",
    "    loss = -(returns * log_prob_actions).sum()\n",
    "    ## END ##\n",
    "\n",
    "    ## Backpropagate using pytorch autograd. We will use the Adam Optimizer to do this, though any optimizer\n",
    "    ## would work in practice.\n",
    "    OPTIMIZER.zero_grad()\n",
    "    loss.backward()\n",
    "    OPTIMIZER.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters, run training, and plot reward curve\n",
    "\n",
    "We run the algorithm 5 times with different random seeds, and plot the mean reward curve with min/max bounds.\n",
    "\n",
    "You can play around with the hyperparameters and see how each of these affect the algorithms performance. However, please submit the saved figures with the default parameters given here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Hyperparameters\n",
    "# learning\n",
    "baseline = True\n",
    "pg_theorem = True\n",
    "learning_rate = 5e-3\n",
    "discount = 0.99\n",
    "\n",
    "# network architecture\n",
    "input_dim = rl_env.observation_space.shape[0]\n",
    "output_dim = rl_env.action_space.n\n",
    "hidden_sizes = [32, 16]\n",
    "\n",
    "# training length and accelerator\n",
    "N_RUNS = 5\n",
    "N_ITERATIONS = 200\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "all_train_returns = []\n",
    "all_test_returns = []\n",
    "\n",
    "# seeding\n",
    "rl_env.reset(seed=SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# setup plotting\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for run in range(N_RUNS):\n",
    "    policy = MLP(input_dim, output_dim, hidden_sizes).to(DEVICE)\n",
    "    OPTIMIZER = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "    RECENT_RETURNS = deque(maxlen=1000)\n",
    "\n",
    "    train_returns = np.zeros((N_ITERATIONS,))\n",
    "    test_returns = np.zeros((N_ITERATIONS,))\n",
    "    for episode in range(N_ITERATIONS):\n",
    "        loss, train_reward = train_once(rl_env, policy, discount, baseline, pg_theorem)\n",
    "\n",
    "        test_reward = evaluate_policy(rl_env, policy)\n",
    "\n",
    "        train_returns[episode] = train_reward\n",
    "        test_returns[episode] = test_reward\n",
    "\n",
    "    # update plots\n",
    "    display.clear_output(wait=True)\n",
    "    all_train_returns.append(train_returns)\n",
    "    all_test_returns.append(test_returns)\n",
    "    ax.clear()\n",
    "    ax.plot(np.mean(all_test_returns, axis=0))\n",
    "    ax.fill_between(\n",
    "        np.arange(N_ITERATIONS),\n",
    "        np.min(all_test_returns, axis=0),\n",
    "        np.max(all_test_returns, axis=0),\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    ax.set(\n",
    "        title=\"Mean, Min, and Max Eval Reward During Training\",\n",
    "        xlabel=\"Training Steps\",\n",
    "        ylabel=\"Mean Eval Reward (Min/Max)\",\n",
    "    )\n",
    "    display.display(fig)\n",
    "\n",
    "# clear duplicate plot\n",
    "display.clear_output()\n",
    "\n",
    "save_figure(fig, \"deep_policy_gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Test Questions (optional)\n",
    "\n",
    "Play with the hyperparameters for this algorithm if you wish.\n",
    "\n",
    "What happens if you make the neural network much larger, e.g. `hidden_sizes = [128, 128, 128]`?\n",
    "Why?\n",
    "What can we do to ensure that learning is stable despite bigger and deeper networks?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eQx7oDGeeKWj"
   ],
   "name": "2_dqn_atari.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
