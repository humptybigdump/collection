{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e42763e",
   "metadata": {},
   "source": [
    "# Stochastic Simulation\n",
    "\n",
    "*Winter Semester 2024/25*\n",
    "\n",
    "06.12.2024\n",
    "\n",
    "Prof. Sebastian Krumscheid<br>\n",
    "Assistants: Stjepan Salatovic, Louise Kluge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5187ec",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">\n",
    "Exercise sheet 05\n",
    "</h3>\n",
    "\n",
    "---\n",
    "\n",
    "<h1 align=\"center\">\n",
    "The Monte Carlo method\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f636d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interact\n",
    "from scipy.special import factorial\n",
    "from scipy.stats import uniform, norm, pareto, lognorm, rv_continuous\n",
    "from scipy.optimize import newton\n",
    "from typing import Callable, Tuple\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c5fd94c-4d77-4686-9c09-73cf01551096",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('axes', labelsize=14)     # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=12)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=12)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=14)    # legend fontsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6ec4b1",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Let $X = [X_1, X_2, ..., X_n] \\overset{\\text{i.i.d}}{\\sim} \\mathcal{U}([-1,1]^n)$ be a random vector uniformly distributed over the $n$-dimensional square $\\Gamma=[-1,1]^n$, and define the random variable $Z=\\mathbb{1}_{\\|X\\|_{l^2}<1}.$ Observe that\n",
    "\n",
    "\\begin{align*}\n",
    "I=\\mathbb{E}[Z]=\\int_\\Gamma \\mathbb{1}_{\\|x\\|_{l^2}<1}p(x)\\mathrm{d}x=\\frac{1}{|\\Gamma|}\\left|B(0,1)\\right|,\n",
    "\\end{align*}\n",
    "\n",
    "where $p(x)$ is the PDF of $\\mathcal{U}([-1,1]^n),$ and $\\left|B(0,1)\\right|$ is the volume of the $n$-dimensional sphere with center $0$ and radius $1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b3d0f-3d72-4ff1-80fc-cb59df756e82",
   "metadata": {},
   "source": [
    "1. Let $n=2$. Use Monte Carlo to approximate the value of $I$:\n",
    "$$\n",
    "\\overline{I}_N := \\frac{1}{N} \\sum_{k=1}^N Z_k\n",
    "$$\n",
    "For $N=10,100,1000,10000,$ compute $\\overline{I}_N$ as well as an approximate confidence interval and compare with the exact value $I$. In addition, plot the relative error $\\frac{|\\overline I_N -I|}{I}$ versus $N$ in logarithmic scale and verify the convergence rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b3dfe96-1d1a-4084-9948-d38e0eef42e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(N: int, n: int=2) -> np.array:\n",
    "    \"\"\"\n",
    "    Generates a uniform sample of size `N` in the hypercube `[-1, 1]^n`.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab1fee24-cae5-43f6-a4d5-6e87301298ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def I(N: int, n: int=2) -> float:\n",
    "    \"\"\"\n",
    "    Estimates `I` using `N` Monte Carlo samples in the hypercube `[-1, 1]^n`.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a300a8-2ecc-4443-9846-b74f85eb5788",
   "metadata": {},
   "source": [
    "2. **(On the choice of $N$):** By _a priori_ analysis (knowing that $Z\\sim \\text{Bernoulli}(p)$ with $p=\\pi/4$), determine three lower bounds for $N = N(\\alpha, \\epsilon)$ with $\\epsilon =10^{-2}$ and $\\alpha = 10^{-4}$ for ensuring that\n",
    "$$\n",
    "\\mathbb{P}\\left(\\left| \\overline{I}_N - \\pi/4 \\right| > \\epsilon \\right) < \\alpha\n",
    "$$\n",
    "using Chebycheff's inequality (rigorous), the Berry-Esseen Theorem\n",
    "(rigorous) and the leap of faith\n",
    "$$\n",
    "\\frac{ \\overline{I}_N - \\pi/4}{\\sqrt{\\mathrm{Var}(Z)/N}} \\sim N(0,1).\n",
    "$$\n",
    "\n",
    "    Discuss the advantages and disadvantages of using each bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf97234c-d973-42cc-b87e-f3efaf352521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_limit_bound(alpha: float, eps: float) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the minimum sample size for estimating `π / 4` based on the CLT.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "311e25fc-bb7a-42d0-a1cb-8c7a1ca06039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chebycheff_bound(alpha: float, eps: float) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the minimum sample size for estimating `π / 4` based on the Chebycheff bound.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2b3a8a6-27b9-421c-8954-c89962045166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def berry_essen_bound(alpha: float, eps: float) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the minimum sample size for estimating `π / 4` based on the Berry-Essen theorem.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41081780-6a7a-41e8-b1e2-37a8ee9a11f6",
   "metadata": {},
   "source": [
    "3. An important property of the MC method is that, under\n",
    "very weak regularity assumptions, an $O(N^{-1/2})$\n",
    "convergence rate holds independently of the dimensionality of the\n",
    "underlying problem.  To illustrate this, consider approximating $\\mathbb{E}[Z]$ as in the first point, for $n=6.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a83cb127-43c0-428e-bcbe-5f3d24447d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894026e6-225a-44ab-9990-822c5d51b002",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "A simulator would like to produce an unbiased estimate of\n",
    "$\\mathbb{E}(XY)$, where the two independent random variables $X$ and\n",
    "$Y$ have bounded first moments and can be generated by a stochastic\n",
    "simulation. To this end, she simulates $R\\in\\mathbb{N}$ replications\n",
    "$X_1,\\dots, X_R$ of $X$ and, independently of this, $R$ replications\n",
    "$Y_1,\\dots, Y_R$ of $Y$. She thus has the following two natural\n",
    "estimators for $\\mathbb{E}(XY)$ at her disposal:\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\text{Est}_1 := \\Biggl(\\frac{1}{R}\\sum_{r=1}^RX_r \\Biggr) \\Biggl(\\frac{1}{R}\\sum_{r=1}^RY_r \\Biggr)\\quad\\text{and}\\quad \\text{Est}_2 := \\frac{1}{R}\\sum_{r=1}^RX_rY_r\\;.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3e980-e7ef-4c4a-8878-b008fa6fe4e3",
   "metadata": {},
   "source": [
    "1. Verify that both estimators $\\text{Est}_1$ and $\\text{Est}_2$ are unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f2f07-7cd1-4fac-b472-0fb407261503",
   "metadata": {},
   "source": [
    "2. Show that $\\mathrm{Var}(\\text{Est}_1)<\\mathrm{Var}(\\text{Est}_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5931057e-0a4a-41e4-a82a-478de1535451",
   "metadata": {},
   "source": [
    "3. Use the delta method to show that $\\sqrt{R}(\\text{Est}_1-\\mu_x\\mu_y)\\overset{d}{\\rightarrow}N(0,\\tau^2)$. Find $\\tau^2$ explicitly and derive a $1-\\alpha$ asymptotic confidence interval. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f00533-dda2-49cc-bdd2-eaabe8e9a609",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Algorithm 1 proposes a sequential Monte Carlo method to compute the expectation $\\mathbb{E}[X]$ of a random variable $X$, where the sample size is doubled at each iteration until the estimated $1-\\alpha$ confidence interval based on a central limit theorem approximation is smaller than a prescribed tolerance $\\epsilon$. The algorithm then outputs the final sample size $N(\\epsilon,\\alpha)$, as well as the estimated value $\\bar X_N$.\n",
    "\n",
    "---\n",
    "**Algorithm 1** Sample Variance Based SMC\n",
    "\n",
    "- **Input:** $N_0$, distribution $\\lambda$, accuracy $\\epsilon>0$, confidence $1-\\alpha>0$.\n",
    "- **Output:** $\\overline X_{\\epsilon,\\alpha} $ (i.e., approximation of $\\mathbb E [ X]$ with $X\\sim\\lambda$), $N$.\n",
    "\n",
    "- Set $k=0$, generate $N_k$ i.i.d. replicas $\\{X_i\\}_{i=1}^{N_k}$ of $X\\sim\\lambda$ and \n",
    "\t\\begin{equation}\\tag{1}\n",
    "\t\\bar{X}_{N_k}=\\frac{1}{N_k}\\sum_{i=1}^{N_k}X_i,\n",
    "\t\\end{equation}\n",
    "\t\n",
    "    \\begin{equation}\\tag{2}\n",
    "    \\overline \\sigma^2_{N_k} := \\frac{1}{N_k-1} \\sum_{i=1}^{N_k} (X_i -\\overline X_{N_k})^2.\n",
    "    \\end{equation}\n",
    "\n",
    "- **while** $\\bar{\\sigma}_{N_k}C_{1-\\alpha/2}/\\sqrt{N_k} > \\epsilon$ **do**\n",
    "    - Set $k =k+1$ and $N_{k}=2N_{k-1}$.\n",
    "    - Generate a new batch of $N_k$ i.i.d. replicas $\\{X_i\\}_{i=1}^{N_k}$ of $X\\sim \\lambda$.\n",
    "    - Compute the sample variance $\\overline \\sigma^2_{N_k}$ by (2).\n",
    "- **end while**\n",
    "\n",
    "- Set $N =N_k$, generate i.i.d. samples $\\{X_i\\}_{i=1}^{N}$ of $\\lambda$ and compute the output sample mean $\\overline X_{\\epsilon,\\alpha}$.\n",
    "\n",
    "---\n",
    "\n",
    "Algorithm 1 can be particularly sensitive to the choice of initial sample size $N_0$, and as such, we would like to assess the robustness of such an algorithm in estimating $\\mathbb{E}[ X]$ for different distributions of $X$. For some values of $N_0$ ranging between $10$ and $50$,  consider $\\alpha = 10^{-1.5}$ and $\\epsilon =1/10$, and the following random variables:\n",
    "\n",
    "1. $X \\sim \\text{Pareto}(x_m=1,\\gamma=3.1)$ (i.e. with PDF $p(y) = \\mathbb{1}_{y>x_m} x_m^\\gamma \\gamma y^{-(\\gamma+1)}$), $\\,\\mathbb{E}[X]=\\frac{\\gamma x_m}{\\gamma-1}.$\n",
    "2. $X \\sim \\text{Lognormal}(\\mu=0,\\sigma=1)$,  $\\,\\mathbb{E}[X]=\\exp\\left(\\mu+\\frac{\\sigma^2}{2}\\right).$\n",
    "3. $X \\sim U([-1,1]),$ $\\,\\mathbb{E}[X]=0$.\n",
    "\n",
    "Repeat the simulation $K=20\\alpha^{-1}$ times and record the sample sizes $\\{N^{(i)}\\}_{i=1}^K$ as well as the computed sample means $\\{\\bar{X}_{\\epsilon,\\alpha}^{(i)}\\}_{i=1}^K$ returned by the algorithm for each run $i=1,...,K$. Estimate the probability of failure $\\overline p$ of the algorithm:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\overline p_K(N_0,\\epsilon,\\alpha)=\\frac{1}{K}\\sum_{i=1}^{K}\\mathbb{1}_{|\\bar{X}^{(i)}_{\\epsilon, \\alpha}-\\mathbb{E}[X]|>\\epsilon}.\n",
    "$$\n",
    "\n",
    "Then check whether  $\\overline p_K(N_0,\\epsilon,\\alpha) \\le \\alpha$ holds. Repeat your experiment for different values of $\\epsilon$ and $\\alpha$. Discuss your results. \n",
    "\n",
    "**Hint 1:** You may generate Pareto$(x_m,\\alpha)$ r.v. by inversion or have a look at [`scipy.stats.pareto`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pareto.html).\n",
    "\n",
    "**Hint 2:** The specified type hint [`rv_continuous`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.html) for the `dist` argument within the `algorithm_1` function down below indicates that you have the flexibility to use any of [SciPy's continous distributions](https://docs.scipy.org/doc/scipy/tutorial/stats/continuous.html). For instance, you can pass `dist=pareto(b=3.1)` or `dist=lognorm(s=1)`. Of course, while this is only one of many possible solutions, it allows a modular code implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cd28ebf-9bb9-433b-bc53-8e46c61887b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm_1(n: int, dist: rv_continuous, alpha: float, eps: float) -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    Estimates the mean of a random variable with distribution `dist` using the SMC Algorithm 1.\n",
    "    Returns the estimated mean and the final sample size.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b38143a-68ca-49ca-b7c1-0994ec3c988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_of_failure(algo: Callable, n: int, dist: rv_continuous, alpha: float, eps: float) -> float:\n",
    "    \"\"\"\n",
    "    Estimates the probability of failure for a given algorithm.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64d51d-e0ec-4c51-a02a-6358ae5e043c",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Compare Algorithm 1 with the sequential Monte Carlo method in Algorithm 2, where one realization is added at a time. \n",
    "\n",
    "---\n",
    "**Algorithm 2** One-at-a-time Sample Variance Based SMC\n",
    "\n",
    "- **Input:** $N_0$, distribution $\\lambda$, accuracy $\\epsilon>0$, confidence $1-\\alpha>0$.\n",
    "- **Output:** $\\overline X_{\\epsilon,\\alpha} $ (i.e, approximation of $\\mathbb E [ X]$ with $X\\sim\\lambda$), $N$.\n",
    "\n",
    "- Set $k=0$, generate $N_k$ i.i.d. samples $\\{X_i\\}_{i=1}^{N_k}$ of $\\lambda$ and compute the sample variance \n",
    "    $$\n",
    "    \\overline \\sigma^2_{N_k} := \\frac{1}{N_k-1} \\sum_{i=1}^{N_k} (X_i -\\overline{X}_{N_k})^2.\n",
    "    $$\n",
    "\n",
    "- **while** $\\bar{\\sigma}_{N_k}C_{1-\\alpha/2}/\\sqrt{N_k} > \\epsilon$ **do**\n",
    "    - Set $k =k+1$ and $N_{k}=N_{k-1} + 1$.\n",
    "    - Generate a new  i.i.d. sample $X^{(N_k+1)}$ of $\\lambda$.\n",
    "    - Compute\\begin{align}\n",
    "\\bar{\\mu}_{N_k+1}&=\\frac{N_k}{N_k+1}\\bar{\\mu} +\\frac{1}{N_k+1}X^{(N_k+1)}\\\\\n",
    "\\bar\\sigma^2_{N_k+1}&=\\frac{N_k-1}{N_k}\\sigma^2_{N_k}+\n",
    "\\frac{1}{N_k+1}(X^{(N_k+1)}-\\bar\\mu_{N_k})^2\n",
    "\t\t\\end{align}\n",
    "- **end while**\n",
    "\n",
    "- Set $N =N_k$, generate i.i.d. samples $\\{X_i\\}_{i=1}^{N}$ of $\\lambda$ and compute the output sample mean $\\overline X_{\\epsilon,\\alpha}$.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9b2a286-b213-4a62-afa7-b5524df99221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm_2(n: int, dist: rv_continuous, alpha: float, eps: float) -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    Estimate the mean of a random variable with distirbution `dist` using the SMC Algorithm 2.\n",
    "    Returns the estimated mean and the final sample size.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
