{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 4: POS Tagging und Language Modelling mit HMMs\n",
    "\n",
    "In dieser Aufgabe werden wir HMMs benutzen, um Wörter aus einem Text mit ihren *part of speech tags* (POS-tags) zu versehen. Auf Deutsch heißt part of speech Wortart. Wir weisen also jedem Wort seine Wortart (Substantiv, Adjektiv, Verb etc.) zu.\n",
    "\n",
    "Das erscheint erstmal einfach, aber gerade auf Englisch ist es das nicht: Das Wort \"house\" kann ein Gebäude sein, also ein Substantiv (POS-Tag \"NN\") oder \"Wohnraum breitstellen\", also ein Verb (POS-Tag \"VB\"). Es gibt also Ambiguitäten und wir hoffen, diese mit HMMs auflösen zu können.\n",
    "\n",
    "POS Tagging ist eine grundlegende Aufgabe in der Sprachverarbeitung und ist oft ein erster Schritt vor weiteren Verarbeitungsschritten.\n",
    "\n",
    "Wie immer laden wir zuerst Daten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21928 Sätze geladen\n"
     ]
    }
   ],
   "source": [
    "with open(\"data.txt\", encoding=\"utf-8\") as corpus_file:\n",
    "    raw_data = [line.lower().strip().split() for line in corpus_file]\n",
    "full_corpus = raw_data[::2]\n",
    "full_tagged_corpus = raw_data[1::2]\n",
    "del raw_data\n",
    "\n",
    "assert len(full_corpus) == len(full_tagged_corpus)\n",
    "\n",
    "print(f\"{len(full_corpus)} Sätze geladen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir trennen einen zufälligen Teil der Daten zum Testen ab. In einer echten Anwendung würden wir die Trennung und das Training N mal mit unterschiedlichen Seeds wiederholen, um auszuschließen, dass wir einen besonders einfachen Split bekommen haben. Wir nennen das [n-fold cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)). Hier verzichten wir darauf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1232)\n",
    "shuffled_indices = random.sample(range(len(full_corpus)), len(full_corpus))\n",
    "corpus = [full_corpus[i] for i in shuffled_indices]\n",
    "tagged_corpus = [full_tagged_corpus[i] for i in shuffled_indices]\n",
    "\n",
    "test_corpus, corpus = corpus[:2000], corpus[2000:]\n",
    "test_tagged_corpus, tagged_corpus = tagged_corpus[:2000], tagged_corpus[2000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier ein Beispiel aus den Daten. Es handelt sich um den Penn-Treebank Corpus, ein oft verwendetes Benchmark für POS Tagging. Er besteht aus Texten aus der Financial Times. Eine Liste der Tags und ihre Bedeutung finden Sie [hier](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) (bei uns gibt es noch ein paar mehr weil alle Satzzeichen ihre eigene Klasse haben)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['discount', 'rate', ':', '7', '%', '.', 'the', 'charge', 'on', 'loans', 'to', 'depository', 'institutions', 'by', 'the', 'new', 'york', 'federal', 'reserve', 'bank', '.']\n",
      "[('discount', 'nn'), ('rate', 'nn'), (':', ':'), ('7', 'cd'), ('%', 'nn'), ('.', '.'), ('the', 'dt'), ('charge', 'nn'), ('on', 'in'), ('loans', 'nns'), ('to', 'to'), ('depository', 'nn'), ('institutions', 'nns'), ('by', 'in'), ('the', 'dt'), ('new', 'nnp'), ('york', 'nnp'), ('federal', 'nnp'), ('reserve', 'nnp'), ('bank', 'nnp'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])\n",
    "print(list(zip(corpus[0], tagged_corpus[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir mit Textdaten arbeiten, ist es üblich, die Wörter in Indizes umzuwandeln, um nicht überall strings verwenden zu müssen. Dafür müssen wir erstmal feststellen, welche Wörter im Text vorkommen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im Text kommen 41464 unterschiedliche Wörter und 45 unterschiedliche tags vor.\n",
      "Die 10 häufigsten Wörter sind:\n",
      "[(',', 53567), ('the', 53148), ('.', 42888), ('of', 25225), ('to', 24567), ('a', 22034), ('in', 18669), ('and', 18116), (\"'s\", 10269), ('that', 9383)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for sentence in corpus:\n",
    "    vocabulary.update(sentence)\n",
    "\n",
    "tag_vocabulary = Counter()\n",
    "for sentence in tagged_corpus:\n",
    "    tag_vocabulary.update(sentence)\n",
    "\n",
    "print(f\"Im Text kommen {len(vocabulary)} unterschiedliche Wörter und {len(tag_vocabulary)} unterschiedliche tags vor.\")\n",
    "print(\"Die 10 häufigsten Wörter sind:\")\n",
    "print(vocabulary.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "counts = np.array([count for _, count in vocabulary.most_common()])\n",
    "\n",
    "plt.figure()\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für diese Aufgabe behandeln wir Satzzeichen auch als Wörter. Wir beobachten hier ein Problem, dass sehr typisch für die Sprachverarbeitung ist: Viele der Wörter sind sehr selten. Diese Wörter treiben unseren Speicherverbrauch in die Höhe und weil sie nur ein- oder zweimal vorkommen, können wir auch nichts sinnvolles über sie lernen. Wir verlieren also wenig, wenn wir diese Wörter alle zusammen unter \"unknown\" zusammenfassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18635 Wörter kommen nur einmal vor\n",
      "10808 unterschiedliche Wörter decken 95% des Corpus ab\n"
     ]
    }
   ],
   "source": [
    "cumulative = np.cumsum(counts)\n",
    "cumulative = cumulative / cumulative[-1]\n",
    "percentile_index = np.searchsorted(cumulative, 0.95)\n",
    "print(f\"{len(np.where(counts == 1)[0])} Wörter kommen nur einmal vor\")\n",
    "print(f\"{percentile_index+1} unterschiedliche Wörter decken 95% des Corpus ab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir fügen dem Vokabular noch zwei Sonderzeichen hinzu: Das schon erwähnte unknown word und ein Sonderzeichen für das Satzende."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [\"</s>\", \"<unk>\"] + [word for word, count in vocabulary.most_common(percentile_index+1)]\n",
    "tag_vocabulary = [\"EOS\"] + [tag for tag, count in tag_vocabulary.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_lookup = {word: i for i, word in enumerate(vocabulary)}\n",
    "tags_lookup = {tag: i for i, tag in enumerate(tag_vocabulary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt schätzen wir die Wahrscheinlichkeiten aus den Daten. Dazu zählen wir\n",
    "* `start_counts` Wie häufig eine Sequenz mit einem gegebenen Tag anfängt\n",
    "* `bigram_counts` Wie häufig ein Tag auf ein anderes Tag folgt\n",
    "* `label_counts` Wie häufig ein Wort ein bestimmtes Tag hat\n",
    "\n",
    "Für später sammeln wir außerdem\n",
    "* `word_start_counts` Wie häufig ein Satz mit einem gegeben Wort anfängt\n",
    "* `word_bigram_counts` Wie häufig ein Wort auf ein anderes folgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_start_counts = np.zeros((len(vocabulary),), dtype=int)\n",
    "word_bigram_counts = np.zeros((len(vocabulary), len(vocabulary)), dtype=int)\n",
    "tag_start_counts = np.zeros((len(tag_vocabulary),), dtype=int)\n",
    "tag_bigram_counts = np.zeros((len(tag_vocabulary), len(tag_vocabulary)), dtype=int)\n",
    "label_counts = np.zeros((len(vocabulary), len(tag_vocabulary)), dtype=int)\n",
    "\n",
    "for sentence, tagged in zip(corpus, tagged_corpus):\n",
    "    words = [vocabulary_lookup.get(word, 1) for word in sentence] + [0]\n",
    "    tags = [tags_lookup[tag] for tag in tagged] + [0]\n",
    "    \n",
    "    if len(words) == 0:\n",
    "        continue\n",
    "\n",
    "    word_start_counts[words[0]] += 1\n",
    "    tag_start_counts[tags[0]] += 1\n",
    "    \n",
    "    word_pairs = zip(words[:-1], words[1:])\n",
    "    for first, second in word_pairs:\n",
    "        word_bigram_counts[first, second] += 1\n",
    "\n",
    "    tag_pairs = zip(tags[:-1], tags[1:])\n",
    "    for first, second in tag_pairs:\n",
    "        tag_bigram_counts[first, second] += 1    \n",
    "    \n",
    "    for word, tag in zip(words, tags):\n",
    "        label_counts[word, tag] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt schätzen wir Wahrscheinlichkeiten aus den Zählungen. Dabei sehen wir ein zweites häufiges Problem der Sprachverarbeitung: Data sparsity. Obwohl manche Bigramme in der Realität vorkommen könnten, waren sie nicht in den Daten und ihre Wahrscheinlichkeit würde auf 0 geschätzt. Das wird unseren Modellen schaden, deshalb verwenden wir eine smoothing Technik: Wir addieren 1 zu jedem Zähler, als ob jedes Bigramm einmal im Text gesehen worden wäre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_start_counts[1:] += 1\n",
    "word_bigram_counts[1:] += 1\n",
    "tag_start_counts[1:] += 1\n",
    "tag_bigram_counts[1:] += 1\n",
    "label_counts[1:,1:] += 1\n",
    "\n",
    "word_unigram_counts = label_counts.sum(1)\n",
    "tag_unigram_counts = label_counts.sum(0)\n",
    "\n",
    "word_start_probs = word_start_counts / word_start_counts.sum()\n",
    "word_bigram_probs = word_bigram_counts / word_unigram_counts[:, np.newaxis]\n",
    "word_label_probs = label_counts / word_unigram_counts[:, np.newaxis]\n",
    "\n",
    "tag_start_probs = tag_start_counts / tag_start_counts.sum()\n",
    "tag_bigram_probs = tag_bigram_counts / tag_unigram_counts[:, np.newaxis]\n",
    "tag_label_probs = label_counts / tag_unigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTagger:\n",
    "    # Abstract base class\n",
    "    def classify(self, sentence):\n",
    "        # sentence: 1D np.array von Wortindizes\n",
    "        # return: Gleich langes 1D array mit Tagindizes\n",
    "        raise NotImplementedError\n",
    "        \n",
    "def evaluate(tagger):\n",
    "    total, correct = 0, 0\n",
    "\n",
    "    for sentence, tagged in zip(test_corpus, test_tagged_corpus):\n",
    "        words = np.array([vocabulary_lookup.get(word, 1) for word in sentence] + [0])\n",
    "        labels = np.array([tags_lookup[tag] for tag in tagged])\n",
    "\n",
    "        prediction = tagger.classify(words)\n",
    "\n",
    "        total += len(labels)\n",
    "        correct += np.sum(prediction[:-1] == labels)\n",
    "\n",
    "    print(f\"Accuracy: {correct / total:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wollen zuerst eine einfache Methode testen (wir nennen das eine Baseline): Wir weisen einfach jedem Wort das am häufigsten gesehene Tag zu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.6%\n"
     ]
    }
   ],
   "source": [
    "class SimpleTagger(POSTagger):\n",
    "    def __init__(self, assignment_probs):\n",
    "        self.assignments = assignment_probs.argmax(1)\n",
    "    \n",
    "    def classify(self, sentence):\n",
    "        sequence = self.assignments[sentence]\n",
    "        return sequence\n",
    "\n",
    "evaluate(SimpleTagger(word_label_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Baseline ist schon relativ stark. Hoffentlich kann das HMM noch etwas besser sein. Wir modellieren das Problem wie folgt:\n",
    "\n",
    "Die Wörter sind die Ausgaben des HMM, die wir beobachten können. Die POS tags sind die versteckten Zustände. Wir benutzen den *Viterbi-Algorithmus*, um die wahrscheinlichste Zustandsfolge (Tag-Folge) für eine Beobachtung (Wortfolge) zu bestimmen.\n",
    "\n",
    "Wir werden den EOS Zustand (Zustand 0) als den Start- und Endzustand benutzen. Dafür kopieren wir die Verteilung der Start-Tags als die Übergangswahrscheinlichkeiten dieses Startzustands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs = np.copy(tag_bigram_probs)\n",
    "transition_probs[0] = tag_start_probs\n",
    "\n",
    "emission_probs = tag_label_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den Algorithmus in der Praxis verwenden zu können, müssen wir aber eine Beobachtung machen: $v_t$ ist eine Multiplikation von Wahrschinlichkeiten und wird schon nach ca. 20 Schritten kleiner werden als die Präzision eines `double`. Deswegen benutzen wir *log-Wahrscheinlichkeiten*, wir berechnen also $\\log v_t$ statt $v_t$. Machen Sie sich klar, wie sich dabei die Formeln verändern.\n",
    "\n",
    "### Aufgabe a) (20 Punkte)\n",
    "Implementieren Sie den Viterbi Algorithmus. Wichtig: Verwenden Sie dabei die Log-Wahrschinlichkeiten!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM(POSTagger):\n",
    "    def __init__(self, transition, emission):\n",
    "        self.transition = transition\n",
    "        self.emission = emission\n",
    "        self.num_emissions, self.num_states = emission.shape\n",
    "        assert self.transition.shape == (self.num_states, self.num_states)\n",
    "        assert self.emission.shape == (self.num_emissions, self.num_states)\n",
    "        self.log_transition = np.log2(transition)\n",
    "        self.log_emission = np.log2(emission)\n",
    "\n",
    "    def classify(self, observation):\n",
    "        length = len(observation)\n",
    "        psi = np.zeros((length, self.num_states), dtype=np.int)\n",
    "        delta = np.full((self.num_states,), float(\"-inf\"))\n",
    "        delta[0] = 0\n",
    "        \n",
    "        for t in range(length):\n",
    "            inner = delta[:, np.newaxis] + self.log_transition\n",
    "            delta = np.max(inner, 0) + self.log_emission[observation[t]]\n",
    "            psi[t] = np.argmax(inner, 0)\n",
    "        \n",
    "        best_path = np.empty((length,), dtype=np.int)\n",
    "        best_path[length-1] = np.argmax(delta)\n",
    "        for t in reversed(range(1, length)):\n",
    "            best_path[t-1] = psi[t, best_path[t]]\n",
    "        return best_path\n",
    "    \n",
    "    def score(self, observation):\n",
    "        length = len(observation)\n",
    "        alpha = np.zeros((self.num_states,))\n",
    "        alpha[0] = 1\n",
    "        c = np.zeros((length,))\n",
    "        for t in range(length):\n",
    "            alpha = np.dot(alpha, self.transition) * self.emission[observation[t]]\n",
    "            c[t] = alpha.sum()\n",
    "            alpha /= c[t]\n",
    "        return np.log2(c).sum() + np.log2(alpha[0])\n",
    "    \n",
    "    def _score(self, observation):\n",
    "        length = len(observation)\n",
    "        alpha = np.zeros((self.num_states,))\n",
    "        alpha[0] = 1\n",
    "        for t in range(length):\n",
    "            alpha = np.dot(alpha, self.transition) * self.emission[observation[t]]\n",
    "        return np.log2(alpha.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir haben Nullwahrscheinlichkeiten, die beim Logarithmus Warnungen produzieren\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"divide by zero\") \n",
    "\n",
    "hmm = HMM(transition_probs,\n",
    "          emission_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können Ihre Implementierung an diesem Beispiel testen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.array([vocabulary_lookup.get(word, 1) for word in corpus[1000]] + [0])\n",
    "\n",
    "np.testing.assert_equal(hmm.classify(words),\n",
    "                           np.array([ 3,  3,  7,  9,  6,  5,  7, 11,  5,  1,  2,  4,  1,  2,  6, 13,  5,\n",
    "                                      6,  2,  9, 14,  9,  8, 17, 20, 12,  2,  4,  1,  8,  0]))\n",
    "# Log-Wahrscheinlichkeit des Viterbi-Pfads ist -246.277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.7%\n"
     ]
    }
   ],
   "source": [
    "evaluate(hmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn Ihre Implementierung stimmt, sollte das HMM eine höhere Accuracy als die Baseline erreicht haben.\n",
    "\n",
    "### Language Modeling\n",
    "\n",
    "Ein Sprachmodell bestimmt $P(w)$, also die Wahrscheinlichkeit, dass eine bestimmte Wortfolge auftritt. Wir können tatsächlich das gleiche HMM für die Sprachmodellierung einsetzen. Zuerst definieren wir wieder eine abstrakte Klasse und die Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def score(self, sentence):\n",
    "        # sentence: 1D np.array von Wortindizes\n",
    "        # return: Skalar. log-Wahrscheinlichkeit dieses Satzes\n",
    "        raise NotImplementedError\n",
    "\n",
    "def evaluate_lm(lm):\n",
    "    entropy, total = 0, 0\n",
    "    \n",
    "    for sentence in test_corpus:\n",
    "        words = np.array([vocabulary_lookup.get(word, 1) for word in sentence] + [0])\n",
    "\n",
    "        lprob = lm.score(words)\n",
    "        if lprob == float(\"-inf\"):\n",
    "            print(words)\n",
    "\n",
    "        total += len(words)\n",
    "        entropy -= lprob\n",
    "\n",
    "    print(f\"Perplexity: {2 ** (entropy / total):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wollen diesmal zwei Baselines vergleichen: Ein Unigram- und ein Bigramm-Sprachmodell. Das Unigramm-Modell hat weniger Parameter als das HMM (es hat genau $V$ Parameter, das HMM hat $T+T^2+V\\cdot T$) und das Bigramm-Modell hat mehr ($V^2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 731.2\n"
     ]
    }
   ],
   "source": [
    "class UnigramLanguageModel(LanguageModel):\n",
    "    def __init__(self, unigram_probs):\n",
    "        self.unigram_probs = np.log2(unigram_probs)\n",
    "    \n",
    "    def score(self, sentence):\n",
    "        return self.unigram_probs[sentence].sum()\n",
    "\n",
    "evaluate_lm(\n",
    "    UnigramLanguageModel(\n",
    "        word_unigram_counts / word_unigram_counts.sum())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 78.0\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(LanguageModel):\n",
    "    def __init__(self, start_probs, bigram_probs):\n",
    "        self.start_probs = np.log2(start_probs)\n",
    "        self.bigram_probs = np.log2(bigram_probs)\n",
    "    \n",
    "    def score(self, sentence):\n",
    "        lprob = self.start_probs[sentence[0]]\n",
    "        lprob += self.bigram_probs[sentence[:-1], sentence[1:]].sum()\n",
    "        return lprob\n",
    "\n",
    "evaluate_lm(\n",
    "    BigramLanguageModel(\n",
    "        word_start_probs,\n",
    "        word_bigram_probs)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe b1) Zusammen mit b2 20 Punkte\n",
    "\n",
    "Erweitern Sie Ihre HMM Implementierung so, dass Sie ein Sprachmodell implementiert. Verwenden Sie dafür den *Forward-Algorithmus*. Machen Sie sich klar, warum Sie hier keine log-Wahrscheinlichkeiten benutzen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 522.4\n"
     ]
    }
   ],
   "source": [
    "evaluate_lm(hmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wahrscheinlich hat die Evaluation eine Warnung und eine Perplexität von unendlich erzeugt. Das liegt an dem Problem, das wir oben beim Viterbi Algorithmus beschrieben hatten: Die $\\alpha_t$ werden zu klein, um von `double`s dargestellt zu werden.\n",
    "\n",
    "Da wir beim Forward-Algorithmus keine log-Wahrscheinlichkeiten verwenden können, müssen wir eine andere Technik einführen: Skalierung.\n",
    "\n",
    "Wir erweitern den Forward Algorithmus wie folgt:\n",
    "1. Für ein gegebenes $t$, berechne die $\\alpha_t$.\n",
    "2. Berechne $c_t = \\sum_j \\alpha_t(j)$.\n",
    "3. Normalisiere die $\\alpha_t$ zu $\\hat{\\alpha}_t$ mit $\\hat{\\alpha}_t(j) = \\frac{\\alpha_t(j)}{c_t}$\n",
    "4. Verwende für die Berechnung der $\\alpha_{t+1}$ die $\\hat{\\alpha}_t$ statt $\\alpha_t$.\n",
    "\n",
    "Um $P(O|\\lambda)$ zu bestimmen, können wir jetzt nicht mehr $\\hat{\\alpha}_T(0)$ nehmen, da es bereits skaliert ist. Genauso können wir nicht einfach ent-skalieren, denn den echten Wert können wir wieder numerisch nicht ausdrücken. Wir können aber durch Induktion zeigen, dass $\\hat{\\alpha}_t(j)=\\left(\\prod_{\\tau=1}^t \\frac{1}{c_\\tau}\\right)\\alpha_t(j)$. Daher:\n",
    "\n",
    "$$P(O|\\lambda) = \\alpha_T(0) = \\hat{\\alpha}_T(0) \\prod_{\\tau=1}^T c_\\tau$$\n",
    "\n",
    "Also\n",
    "\n",
    "$$\\log P(O|\\lambda) = \\log \\hat{\\alpha}_T(0) + \\sum_{\\tau=1}^T \\log c_\\tau$$\n",
    "\n",
    "Wir können also die Log-Wahrscheinlichkeit bestimmen, die für die Sprachmodellierung ausreicht.\n",
    "\n",
    "### Aufgabe b2)\n",
    "Implementieren Sie den Forward-Algorithmus mit Skalierung. Hinweis: Für die korrekte Berechnung der Perplexität, verwenden Sie den Logarithmus zur Basis 2 (`np.log2`). Wenn Ihre Implementierung korrekt ist, sollte die Perplexität des HMM-Sprachmodells zwischen der des Unigramm- und der des Bigramm-Sprachmodells liegen.\n",
    "\n",
    "Sie können Ihre Implementierung an diesem Beispiel testen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.array([vocabulary_lookup.get(word, 1) for word in corpus[1000]] + [0])\n",
    "\n",
    "score = hmm.score(words)\n",
    "assert abs(score + 243.095) < 1e-3, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
