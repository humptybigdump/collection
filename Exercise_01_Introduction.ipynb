{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running shell commands from inside the notebook\n",
    "To forward a command to the underlying shell of the computer prefix it with '!'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Windows, Mac OS and Linux, Google Colab\n",
    "! echo Hello world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case that you are missing some packages they can easily be installed in the virtual environment you created from inside the notebook.  \n",
    "You just need to follow these steps.  \n",
    "1. source/activate the environment\n",
    "    * Windows: `C:\\Users\\<USER>\\ml_env\\Scripts\\activate`\n",
    "    * Mac/Linux: `. /home/<USER>/workspaces/ml_env/bin/activate`\n",
    "2. run your desired command e.g. `python -m pip install <PACKAGE_NAME>`\n",
    "    \n",
    "Let's try to upgrade pip - the Python package manager. Uncomment the respective line depending on your host system and run the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "#! C:\\Users\\<USER>\\ml_env\\Scripts\\activate && python -m pip install --upgrade pip\n",
    "# Mac OS and Linux\n",
    "#! . /home/<USER>/workspaces/ml_env/bin/activate && python -m pip install --upgrade pip\n",
    "# Google Colab\n",
    "#! python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p3d7NFyNRgh0"
   },
   "source": [
    "# TensorFlow (TF 2.x) Tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F9YUsQtzRi8n"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, HTML, display\n",
    "\n",
    "Image(url= \"https://i.imgur.com/4nk5b4c.jpg\", width=700) # from Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TAYt54IhRzXV"
   },
   "source": [
    "Google's TensorFlow engine has a unique way of solving problems, allowing us to solve machine learning problems very efficiently. Nowadays, machine learning is used in almost all areas of life and work, with famous applications in computer vision, speech recognition, language translations, healthcare, and many more.\n",
    "\n",
    "Production-oriented and capable of handling different computational architectures (CPUs, GPUs, and now TPUs), TensorFlow is a framework for any kind of computation that requires high performance and easy distribution. It excels at deep learning, making it possible to create everything from shallow networks (neural networks made of a few layers) to complex deep networks for image recognition and natural language processing.\n",
    "\n",
    "In this Semseter we will use Tensorflow to implement different machine learning algorithms \n",
    "\n",
    "* **Feed Forward Neural Networks (FFNNs)** classification and regression based on features.\n",
    "* **Convolutional Neural Networks (CNNs)** image classification, object detection, video action recognition, etc.\n",
    "* **Generative Adversarial Networks (GANs)**  unsupervised generation of realistic images, etc.\n",
    "* **Deep Reinforcement Learning** game playing, robotics in simulation, self-play, neural architecture search, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "adoGygpkRzig"
   },
   "source": [
    "## Installing TensorFlow\n",
    "To install TensorFlow on your local machine you can use pip - the python package manager.\n",
    "```console\n",
    "!python3 -m pip install tensorflow\n",
    "```\n",
    "\n",
    "If you have Nvidia Gpu Support then you can install `tensorflow-gpu`\n",
    "```console\n",
    "!python3 -m pip install tensorflow-gpu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AezwWaY5SPoR"
   },
   "source": [
    "## Importing TensorFlow\n",
    "The first step is going to be to import TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wW7hW6vISI99"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "if(int(tf.__version__[0]) <= 1):\n",
    "    print('tensorflow {} detected; Please install tensorflow >= 2.0.0'.format(tf.__version__))\n",
    "else:\n",
    "    print('tensorflow {} detected'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G66ZqlImWKNE"
   },
   "source": [
    "The next step is to import all required packages for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pytest-shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U79YvVagWSi-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install some functions from tensorflow_docs - requires GIT to be installed\n",
    "# Windows\n",
    "#! C:\\Users\\<USER>\\ml_env\\Scripts\\activate && python -m pip install git+https://github.com/tensorflow/docs\n",
    "# Mac OS and Linux\n",
    "#! . /home/<USER>/workspaces/ml_env/bin/activate && python -m pip install git+https://github.com/tensorflow/docs\n",
    "# Google Colab\n",
    "#! python -m pip install git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2dpgLLZSrMp"
   },
   "source": [
    "\n",
    "## Tensors\n",
    "TensorFlow does have its own data structure for the purpose of performance and ease of use. Tensor is the data structure used in Tensorflow. You can think of a TensorFlow tensor as an n-dimensional array or list.\n",
    "\n",
    "**Example**: The shape of a tensor can be described with a vector $[ 10000, 256, 256, 3 ]$\n",
    "* 10,000 images\n",
    "* Each image has 256 rows\n",
    "* Each row has 256 pixels\n",
    "* Each pixel has 3 channels (RGB)\n",
    "\n",
    "Each tensor has a data type and a shape:  \n",
    "* **Data Types Include**: float32, int32, string and others.\n",
    "* **Shape**: Represents the dimension of data.\n",
    "\n",
    "Just like vectors and matrices tensors can have operations applied to them like addition, subtraction, dot product, cross product etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2nfYApwPSrmQ"
   },
   "outputs": [],
   "source": [
    "# Creating Tensors\n",
    "tensor1 = tf.Variable(300, tf.int16)\n",
    "tensor2 = tf.Variable(2.123, tf.float64)\n",
    "tensor3 = tf.Variable(\"Hello World\", tf.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6yw-fNeYUqYs"
   },
   "source": [
    "### Rank and Shape of Tensors\n",
    "Rank means the number of dimensions involved in the tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q9F9vJ1hUr4B"
   },
   "outputs": [],
   "source": [
    "rank_0_tensor = tf.constant(4)\n",
    "print(rank_0_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLdC3I-aUxxx"
   },
   "outputs": [],
   "source": [
    "rank_1_tensor = tf.constant([2.0, 3.0, 4.0])\n",
    "print(rank_1_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7vi_JDDxU0l6"
   },
   "outputs": [],
   "source": [
    "rank_2_tensor = tf.constant([[2, 2],\n",
    "                             [3, 3],\n",
    "                             [4, 4]], dtype=tf.float16)\n",
    "print(rank_2_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NrPntjIxVb-j"
   },
   "source": [
    "**To determine the rank** of a tensor we can call the following method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ixN48VUCVbh1"
   },
   "outputs": [],
   "source": [
    "tf.rank(rank_2_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xoxA30K8VwEk"
   },
   "source": [
    "The shape of a tensor is simply the number of elements that exist in each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1v1GRaxxVtYj"
   },
   "outputs": [],
   "source": [
    "rank_2_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NaFesQaLMVeU"
   },
   "source": [
    "## Using eager execution\n",
    "\n",
    "When developing deep and complex neural networks, you need to continuously experiment with architectures and data. This proved difficult in TensorFlow 1.0 because you always need to run your code from the beginning to end in order to check whether it worked. TensorFlow 2.x works in eager execution mode as default, which means that you develop and check your code step by step as you progress into your project.\n",
    "\n",
    "TensorFlow 1.x performed optimally because it executed its computations after compiling a static computational graph. All computations were distributed and connected into a graph as you compiled your network and that graph helped TensorFlow to execute computations, leveraging the available resources (multi-core CPUs of multiple GPUs) in the best way, and splitting operations between the resources in the most timely and efficient way. That also meant, in any case, that once you defined and compiled your graph, you could not change it at runtime but had to instantiate it from scratch, thereby incurring some extra work.\n",
    "\n",
    "In TensorFlow 2.x, you can still define your network, compile it, and run it optimally, but the team of TensorFlow developers has now favored, by default, a more experimental approach, allowing immediate evaluation of operations, thus making it easier to debug and to try network variations. This is called eager execution. Operations now return concrete values instead of pointers to parts of a computational graph to be built later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras\n",
    "Keras was popular because the API was clean and simple, allowing standard deep learning models to be defined, fit, and evaluated in just a few lines of code. In 2019, Google released a TensorFlow 2 that integrated the Keras API directly and promoted this interface as the default or standard interface for deep learning development on the platform.\n",
    "\n",
    "On hardware, Keras runs on a CPU, GPU, and Google's TPU. In this book, we'll test on a CPU and NVIDIA GPUs (specifically, the GTX 1060, GTX 1080Ti, RTX 2080Ti, V100, and Quadro RTX 8000 models):\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://static.packt-cdn.com/products/9781838821654/graphics/Images/B14853_01_01.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> Keras is a high-level library that sits on top of other deep learning frameworks. Keras is supported on CPU, GPU, and TPU. <br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oy_DwYWlbnAx"
   },
   "source": [
    "## Machine Lerarning Model with TensorFlow\n",
    "The basic structure of training a machine learning model is as follows.\n",
    "\n",
    "1. Import the dataset.\n",
    "2. Select the type of model.\n",
    "3. Train the model.\n",
    "4. Evaluate the model's effectiveness.\n",
    "5. Use the trained model to make predictions.\n",
    "\n",
    "A regression model can be used to predict the output of a continuous value, like a stock price or a time series. In contrast to a classification model, where the prediction is a discrete label, e.g. whether a picture contains a dog or a cat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jIUlWvsSyKT7"
   },
   "source": [
    "## Overfitting and underfitting\n",
    "Learning how to deal with overfitting is important. Although it is often possible to achieve high accuracy on the training set, what we really want is to develop models that generalize well to a testing set (or data they haven't seen before).\n",
    "\n",
    "The opposite of overfitting is underfitting. Underfitting occurs when there is still room for improvement on the test data. This can happen for a number of reasons: \n",
    "* If the model is not powerful enough,\n",
    "* is over-regularized, or \n",
    "* has simply not been trained long enough. \n",
    "\n",
    "This means the model has not learned the relevant patterns in the training data.\n",
    "\n",
    "If you train for too long though, the model will start to overfit and learn patterns from the training data that don't generalize to the test data. We need to strike a balance. Understanding how to train for an appropriate number of epochs as we'll explore below is a useful skill.\n",
    "\n",
    "To prevent overfitting, the best solution is to use more complete training data. The dataset should cover the full range of inputs that the model is expected to handle. Additional data may only be useful if it covers new and interesting cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0C7nPqjq04cj"
   },
   "source": [
    "## Dataset\n",
    "Data Set Information:\n",
    "\n",
    "The data has been produced using Monte Carlo simulations. The first 21\n",
    "features (columns 2-22) are kinematic properties measured by the particle\n",
    "detectors in the accelerator. The last seven features are functions of the\n",
    "first 21 features; these are high-level features derived by physicists to help\n",
    "discriminate between the two classes. There is an interest in using deep\n",
    "learning methods to obviate the need for physicists to manually develop such\n",
    "features. Benchmark results using Bayesian Decision Trees from a standard\n",
    "physics package and 5-layer neural networks are presented in the original\n",
    "paper. The last 500,000 examples are used as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applay to each row in the dataset this function is good part to \n",
    "# add some noise to the data or to make image processing\n",
    "def pack_row(*row):\n",
    "  label = row[0]\n",
    "  features = tf.stack(row[1:],1)\n",
    "  return features, label\n",
    "\n",
    "def load_train_validate_ds(N_TRAIN,N_VALIDATION,BATCH_SIZE):\n",
    "    # If you want to use the original data set then remove # from the next line\n",
    "    # gz = tf.keras.utils.get_file('HIGGS.csv.gz', 'http://mlphysics.ics.uci.edu/data/higgs/HIGGS.csv.gz')\n",
    "    # The Higgs dataset contains 11 000 000 examples, each with 28 features, and a binary class label\n",
    "    gz = tf.keras.utils.get_file('HIGGS.csv.gz', 'export_dataframe.csv.gz')\n",
    "    # The tf.data.experimental.CsvDataset class can be used to read csv records directly from a gzip \n",
    "    # file with no intermediate decompression step\n",
    "    FEATURES = 28\n",
    "    ds = tf.data.experimental.CsvDataset(gz,[float(),]*(FEATURES+1), compression_type=\"GZIP\")\n",
    "    # So instead of repacking each row individually make a new Dataset that takes batches \n",
    "    # of 10000-examples, applies the pack_row function to each batch, and then splits the\n",
    "    # batches back up into individual records:\n",
    "    packed_ds = ds.batch(10000).map(pack_row).unbatch()  \n",
    "    BUFFER_SIZE = int(1e4)\n",
    "    STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE\n",
    "    validate_ds = packed_ds.take(N_VALIDATION).cache()\n",
    "    train_ds = packed_ds.skip(N_VALIDATION).take(N_TRAIN).cache()\n",
    "    # These datasets return individual examples. Use the .batch method to create batches of\n",
    "    # an appropriate size for training. Before batching also remember to .shuffle and \n",
    "    #.repeat the training set.\n",
    "    validate_ds = validate_ds.batch(BATCH_SIZE)\n",
    "    train_ds = train_ds.shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE)\n",
    "    return train_ds, validate_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe0ndkJ1n5Z0"
   },
   "source": [
    "To see more information on this topic, dataset with TensorFlow you can see the following videos:\n",
    "* https://www.youtube.com/watch?v=oFFbKogYdfc\n",
    "* https://www.youtube.com/watch?v=TOP2aLxcuu8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# THIS DATASET IS TOO LARGE\n",
    "# To keep this tutorial relatively short use just the first 1000 samples for validation,\n",
    "# and the next 10 000 for training:\n",
    "n_train      = int(1e4) # The number of samples in training data\n",
    "n_validation = int(1e3)\n",
    "FEATURES = 28\n",
    "batch_size   = 500\n",
    "train_ds, validate_ds =  load_train_validate_ds(n_train,n_validation,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = n_train//batch_size\n",
    "print(\"The number of steps per epoch: \", STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTf8ly9PMzA4"
   },
   "source": [
    "### Demonstrate overfitting\n",
    "The simplest way to prevent overfitting is to start with a small model: A model with a small number of learnable parameters (which is determined by the number of layers and the number of units per layer). In deep learning, the number of learnable parameters in a model is often referred to as the model's \"capacity\".\n",
    "\n",
    "Intuitively, a model with more parameters will have more \"memorization capacity\" and therefore will be able to easily learn a perfect dictionary-like mapping between training samples and their targets, a mapping without any generalization power, but this would be useless when making predictions on previously unseen data.\n",
    "\n",
    "Always keep this in mind: deep learning models tend to be good at fitting to the training data, but the real challenge is generalization, not fitting.\n",
    "\n",
    "On the other hand, if the network has limited memorization resources, it will not be able to learn the mapping as easily. To minimize its loss, it will have to learn compressed representations that have more predictive power. At the same time, if you make your model too small, it will have difficulty fitting to the training data. There is a balance between \"too much capacity\" and \"not enough capacity\".\n",
    "\n",
    "To find an appropriate model size, it's best to start with relatively few layers and parameters, then begin increasing the size of the layers or adding new layers until you see diminishing returns on the validation loss.\n",
    "\n",
    "Start with a simple model using only `layers.Dense` as a baseline, then create larger versions, and compare them.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wb9xuHasOywx"
   },
   "source": [
    "## Training procedure\n",
    "Many models train better if you gradually reduce the learning rate during training. Use optimizers.schedules to reduce the learning rate over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGVuWVvKPAKj"
   },
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=STEPS_PER_EPOCH*1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QU1RBjYXfDbR"
   },
   "source": [
    "The code above sets a `schedules.InverseTimeDecay` to hyperbolically decrease the learning rate to 1/2 of the base rate at 1000 epochs, 1/3 at 2000 epochs and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mB4SDJUYRFDP"
   },
   "outputs": [],
   "source": [
    "step = np.linspace(0,100000)\n",
    "lr = lr_schedule(step)\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(step/STEPS_PER_EPOCH, lr)\n",
    "plt.ylim([0,max(plt.ylim())])\n",
    "plt.xlabel('Epoch')\n",
    "_ = plt.ylabel('Learning Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2JZm5jkgi2h"
   },
   "source": [
    "For this tutorial, choose the ADAM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2Q5aBuMfdcX"
   },
   "outputs": [],
   "source": [
    "# Define the Optimizer\n",
    "def get_optimizer():\n",
    "    return tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkms3c7FfbAZ"
   },
   "source": [
    "The training for this tutorial runs for many short epochs. To reduce the logging noise use the `tfdocs.EpochDots` which simply prints a `.` for each epoch, and a full set of metrics every 100 epochs.\n",
    "\n",
    "Next include `callbacks.EarlyStopping` to avoid long and unnecessary training times. Note that this callback is set to monitor the `val_binary_crossentropy`, not the `val_loss`. This difference will be important later.\n",
    "\n",
    "Use `callbacks.TensorBoard` to generate TensorBoard logs for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4bLoEbVERN0_"
   },
   "outputs": [],
   "source": [
    "def get_callbacks(name):\n",
    "    return [\n",
    "    tfdocs.modeling.EpochDots(),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),\n",
    "    tf.keras.callbacks.TensorBoard(logdir/name),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WHZxCKYn9yD3"
   },
   "outputs": [],
   "source": [
    "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
    "shutil.rmtree(logdir, ignore_errors=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V-wc8jBrRRn0"
   },
   "source": [
    "Similarly each model will use the same `Model.compile` and `Model.fit` settings. Choose the binary cross entropy loss function. To read more about this loss https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ydfK02N-RPSp"
   },
   "outputs": [],
   "source": [
    "def compile_and_fit(model, name, optimizer=None, max_epochs=10000):\n",
    "    if optimizer is None:\n",
    "        optimizer = get_optimizer()\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=[\n",
    "                  tf.keras.losses.BinaryCrossentropy(\n",
    "                      from_logits=True, name='binary_crossentropy'),\n",
    "                  'accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "    epochs=max_epochs,\n",
    "    validation_data=validate_ds,\n",
    "    callbacks=get_callbacks(name),\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LlSVVX52RbtJ"
   },
   "source": [
    "## Tiny model\n",
    "Start by training a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tr8HY4ChRduF"
   },
   "outputs": [],
   "source": [
    "FEATURES = 28\n",
    "tiny_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='elu', input_shape=(FEATURES,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DSBbPNdjRvon"
   },
   "outputs": [],
   "source": [
    "compile_and_fit(tiny_model, 'sizes/Tiny')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BMd27fzs_eNC"
   },
   "source": [
    "### View in TensorBoard\n",
    "\n",
    "These models all wrote TensorBoard logs during training.\n",
    "\n",
    "Open an embedded  TensorBoard viewer inside a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-2tUOhhLAHWe"
   },
   "outputs": [],
   "source": [
    " %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VeB_2szu_dde"
   },
   "outputs": [],
   "source": [
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir {logdir}/sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EtJloqceVhmN"
   },
   "source": [
    "## Large model\n",
    "As an exercise, you can create an even larger model, and see how quickly it begins overfitting. Next, let's add to this benchmark a network that has much more capacity, far more than the problem would warrant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LNpb4KQTVpUL"
   },
   "outputs": [],
   "source": [
    "large_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, activation='elu', input_shape=(FEATURES,)),\n",
    "    tf.keras.layers.Dense(512, activation='elu'),\n",
    "    tf.keras.layers.Dense(512, activation='elu'),\n",
    "    tf.keras.layers.Dense(512, activation='elu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmipltqIVvYL"
   },
   "outputs": [],
   "source": [
    "compile_and_fit(large_model, \"sizes/large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir {logdir}/sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QF8eBd_IAi4y"
   },
   "source": [
    "## Strategies to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GFmzTLzAKbKW"
   },
   "source": [
    "## References\n",
    "* https://androidkt.com/split-the-data-into-train-test-dev/\n",
    "* https://www.tensorflow.org/tutorials/keras/overfit_and_underfit\n",
    "* https://www.tensorflow.org/guide/tensor"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Mr6BhtgBq45N",
    "pTf8ly9PMzA4"
   ],
   "name": "Exercise_01_Introduction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_ai",
   "language": "python",
   "name": "venv_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
