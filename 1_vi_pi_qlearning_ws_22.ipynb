{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Your work will be stored in a folder called `drl_ws22` by default to prevent Colab\n",
    "# instance timeouts from deleting your edits.\n",
    "# We do this by mounting your google drive on the virtual machine created in this colab\n",
    "# session. For this, you will likely need to sign in to your Google account and copy a\n",
    "# passcode into a field below\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create paths in your google drive\n",
    "\n",
    "DRIVE_PATH = '/content/gdrive/My\\ Drive/drl_ws22'\n",
    "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
    "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
    "  %mkdir $DRIVE_PATH\n",
    "\n",
    "# the space in `My Drive` causes some issues,\n",
    "# make a symlink to avoid this\n",
    "SYM_PATH = '/content/drl_ws22'\n",
    "if not os.path.exists(SYM_PATH):\n",
    "  !ln -s $DRIVE_PATH $SYM_PATH\n",
    "%cd $SYM_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Install **python** packages\n",
    "\n",
    "%pip install matplotlib numpy tqdm gym==0.21.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing Code\n",
    "\n",
    "All homeworks are self-contained. They can be completed in their respective notebooks.\n",
    "To edit and re-run code, you can therefore simply edit and restart the code cells below.\n",
    "There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window).\n",
    "This file should automatically be synced with your Google Drive. We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout.\n",
    " However, you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "In this homework, we will implement basic planning and reinforcement learning algorithms.\n",
    "We will look at Policy Iteration and Value Iteration, as well as tabular Q-Learning.\n",
    "The algorithms will be evaluated on a gridworld task from OpenAI gym.\n",
    "\n",
    "We start by importing all the necessary python modules and defining some helper functions which you do not need to\n",
    "change.\n",
    "Still, make sure you are aware of what they do.\n",
    "\n",
    "## Hint\n",
    "\n",
    "We will be working with openAI gym DiscreteEnvs which have a property P which is a dictionary of lists, where\n",
    "```\n",
    "P[s][a] == [(transition probability, next state, reward, done), ...]\n",
    "```\n",
    "for a state $s$ and an action $a$. In FrozenLake, each action has a 33% chance of being executed correctly, and a 33%\n",
    "chance for adjacent actions, respectively. In non-terminal states, len(P[s][a]) == 3.\n",
    "This environment is very similar to the one shown in the Optimal Decision Making lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual code\n",
    " This section specifies the actual exercise. It is split into\n",
    " * Import statements\n",
    " * Logging/Recording utility\n",
    " * Runtime arguments\n",
    " * Functional code (classes, functions, ...) and explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Imports and utility\n",
    "\n",
    "import gym\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class ProgressBar:\n",
    "    def __init__(self, num_iterations: int, verbose: bool = True):\n",
    "        if verbose:  # create a nice little progress bar\n",
    "            self.scalar_tracker = tqdm.tqdm(total=num_iterations, desc=\"Scalars\", bar_format=\"{desc}\",\n",
    "                                            position=0, leave=True)\n",
    "            progress_bar_format = '{desc} {n_fmt:' + str(\n",
    "                len(str(num_iterations))) + '}/{total_fmt}|{bar}|{elapsed}<{remaining}'\n",
    "            self.progress_bar = tqdm.tqdm(total=num_iterations, desc='Iteration', bar_format=progress_bar_format,\n",
    "                                          position=1, leave=True)\n",
    "        else:\n",
    "            self.scalar_tracker = None\n",
    "            self.progress_bar = None\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        if self.progress_bar is not None:\n",
    "            formatted_scalars = {key: \"{:.3e}\".format(value[-1] if isinstance(value, list) else value)\n",
    "                                 for key, value in kwargs.items()}\n",
    "            description = (\"Scalars: \" + \"\".join([str(key) + \"=\" + value + \", \"\n",
    "                                                  for key, value in formatted_scalars.items()]))[:-2]\n",
    "            self.scalar_tracker.set_description(description)\n",
    "            self.progress_bar.update(1)\n",
    "\n",
    "# specify the path to save the recordings of this run to.\n",
    "data_path = '/content/drl_ws22/exercise_1'\n",
    "data_path = os.path.join(data_path, time.strftime(\"%d-%m-%Y_%H-%M\"))\n",
    "if not (os.path.exists(data_path)):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "# this function will automatically save your figure into your google drive folder (if correctly mounted!)\n",
    "def save_figure(save_name: str) -> None:\n",
    "    assert save_name is not None, \"Need to provide a filename to save to\"\n",
    "    plt.savefig(os.path.join(data_path, save_name + \".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Runtime arguments and (hyper-)parameters\n",
    "\n",
    "class Args:\n",
    "\n",
    "    # Boilerplate for properly accessing the args\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def __setitem__(self, key, val):\n",
    "        setattr(self, key, val)\n",
    "\n",
    "    gamma = 0.99  #@param {type: \"number\"}\n",
    "    alpha = 0.1  #@param {type: \"number\"}\n",
    "\n",
    "    pi_iters = 100  #@param {type: \"integer\"}\n",
    "    vi_iters = 100  #@param {type: \"integer\"}\n",
    "    ql_iters = 20000  #@param {type: \"integer\"}\n",
    "    t_decay = 10000  #@param {type: \"integer\"}\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Used functions\n",
    "\n",
    "class LearningCurvePlotter:\n",
    "    def __init__(self, env: FrozenLakeEnv, n_eval: int):\n",
    "        \"\"\"This class takes a policy and evaluates it on an environment.\n",
    "        The quality of the policy is recorded in terms of successfully reaching the goal state.\n",
    "\n",
    "        :param env: FozenLake env\n",
    "        :param n_eval: How often to evaluate the policy\n",
    "        \"\"\"\n",
    "        self.n_eval = n_eval\n",
    "        self.env = env\n",
    "        self.sr = list()\n",
    "\n",
    "    def eval_policy(self, pi: np.ndarray, render: bool=False) -> np.ndarray:\n",
    "        \"\"\"Evaluate a policy on an environment and return success rate\n",
    "\n",
    "        :param pi: A policy for env\n",
    "        :param render: Render the policy\n",
    "        :return: The mean success rate of the given policy\n",
    "        \"\"\"\n",
    "\n",
    "        successes = []\n",
    "\n",
    "        for ep in range(self.n_eval):\n",
    "            s = self.env.reset()\n",
    "            for i in range(100):\n",
    "                a = int(pi[s])\n",
    "\n",
    "                s_dash, reward, done, info = self.env.step(a)\n",
    "\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "\n",
    "                if done:\n",
    "                    successes.append(reward)\n",
    "                    break\n",
    "                else:\n",
    "                    s = s_dash\n",
    "        mean_sr = np.mean(successes)\n",
    "        self.sr.append(mean_sr)\n",
    "\n",
    "        return mean_sr\n",
    "\n",
    "    def plot_lc(self):\n",
    "        \"\"\"Plot the learning curve\"\"\"\n",
    "        _, ax = plt.subplots()\n",
    "        ax.plot(self.sr)\n",
    "        ax.set_xlabel(\"Iterations\")\n",
    "        ax.set_ylabel(\"Success Rate\")\n",
    "\n",
    "\n",
    "def get_reward(transition: list):\n",
    "    \"\"\"Transform a reward in the form r(s, a, s') to r(s, a) given a state s and an action a\n",
    "\n",
    "    r(s, a) = E_p(s'|s,a) [r(s, a, s')] =  sum_s'  p(s'|s, a) * r(s, a, s')\n",
    "\n",
    "    :param transition: A list of tuples (p(s'|s, a), s', r(s, a, s'), done) containing transition probabilities and rewards\n",
    "    :return: Reward\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    for tp, _, r, __ in transition:\n",
    "        reward += tp * r\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def plot_value_f(vf: np.ndarray, env: FrozenLakeEnv):\n",
    "    \"\"\"Plot the value function for a given environment.\n",
    "\n",
    "    :param vf: A value function\n",
    "    :param env: An environment\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    shape = (env.nrow, env.ncol)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(vf.reshape(shape))\n",
    "    #\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            text = ax.text(j, i, env.desc[i, j].decode(),\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FrozenLake Environment\n",
    "First, we have a look at the problem we are solving. The agent controls the movement of a character in a grid world.\n",
    "Some tiles of the grid are walkable, and others lead to the agent falling into the water.\n",
    "Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction.\n",
    "The agent is rewarded for finding a walkable path to a goal tile.\n",
    "The episode ends when you reach the goal or fall in a hole.\n",
    "You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "Actions are encoded as integers 0 = left, 1 = down, 2 = right and 3 = up.\n",
    "The states are counted from 0 to env.nS.\n",
    "\n",
    "See https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\n",
    "\n",
    "Execute the next cell to see what a randomly initialized policy does.\n",
    "\n",
    "Note: This environment computes a reward `r(s,a,s')`, where `s'` is the state that follow from executing\n",
    "action `a` in state `s`. In the lecture, we mainly focused on a `r(s,a)` representation instead. To\n",
    "transfer between the two, you can use the provided `get_reward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\")\n",
    "\n",
    "pi_random = np.random.choice(np.arange(env.nA), env.nS)\n",
    "\n",
    "lc_plotter = LearningCurvePlotter(env, 1)\n",
    "lc_plotter.eval_policy(pi_random, render=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TASK 1: Policy Iteration**\n",
    "The first algorithm we will implement is called Policy Iteration.\n",
    "It consists of an inner and an outer loop.\n",
    "The inner loop is called Policy Evaluation and evaluates a given policy on a problem.\n",
    "For every state, it returns the exact value, i.e. the expected return under the current policy.\n",
    "The outer loop is called Policy Improvement and, as the name suggests, takes the current policy's value function\n",
    "and returns an improved version.\n",
    "In Policy Iteration, we first initialize a policy and value function randomly, and then iteratively run Policy\n",
    "Evaluation and Policy Improvement until convergence.\n",
    "\n",
    "The **pseudocode** looks as follows:\n",
    "\n",
    "---\n",
    "- **Init** Initialize $V_{(0)}^{\\pi_0}(s)=0$ for all $s$, $\\pi_0 \\leftarrow$ uniform, $k = 0$\n",
    "\n",
    "- **Repeat**  For $i=1, 2, \\dots$\n",
    "\n",
    "    - `Policy Evaluation`\n",
    "\n",
    "        - **Init** Initialize $V_{(0)}^{\\pi_{i}}(s) = V_{(k)}^{\\pi_{i-1}}(s)$ (i.e. initialize the value function of the new policy with the converged value function of the old policy)\n",
    "\n",
    "        - **Repeat** For $k=1, 2, \\dots$\n",
    "\n",
    "        $V_{(k)}^{\\pi_{i}}(s) = \\sum_a \\pi(a \\mid s)  \\Big( r(s,a) + \\gamma \\sum_{s'} P(s' \\mid s,a) \\, V_{(k - 1)}^{\\pi_{i}}(s') \\Big)$\n",
    "\n",
    "        - **Until convergence**\n",
    "\n",
    "    - `Policy Improvement`\n",
    "\n",
    "         \\begin{align}\n",
    "      \\pi_{(i+1)}(a \\mid s)  = \\begin{cases}1, & \\text{if } a =  \\underset{a'}{\\arg \\max} \\Big( r(s,a') + \\gamma \\sum_{s'} P(s' \\mid s,a') V^{\\pi_i}(s')\\Big)\\\\0 & \\text{else}\\end{cases}\n",
    "    \\end{align}\n",
    "\n",
    "- **Until convergence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 (4 Points)\n",
    "In this first task you have to implement the missing code snippets (marked as *TODO*) of the policy iteration class. The **pseudocode** shown above might be helpful. \n",
    "1. Implement the missing code snippet of the **policy_evaluation** function below. (2 points)\n",
    "2. Implement the missing code snippet of the **policy_improvement** function below. (2 points)\n",
    "\n",
    "After you have finished implementing the corresponding functions, execute the code cell *Run Policy Iteration*. This code cell will automatically save your value function plot and the learning curve plot into your mounted drive folder. You will need to submit these figures together with the notebook as a zip file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    def __init__(self, env: FrozenLakeEnv, gamma: float):\n",
    "        \"\"\"A class for performing Policy Iteration\n",
    "\n",
    "        :param env:\n",
    "        :param gamma:\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.n_states = env.nS\n",
    "        self.n_actions = env.nA\n",
    "\n",
    "        self.lc_plotter = LearningCurvePlotter(env, 100)\n",
    "\n",
    "    def policy_evaluation(self, pi_prob: np.ndarray, v=None) -> np.ndarray:\n",
    "        \"\"\"Perform the Policy Evaluation step given a policy pi and an environment.\n",
    "\n",
    "        :param pi_prob: Action probabilities [num_states, num_actions]\n",
    "        :param v: Initial value function (optional)\n",
    "        :return: value function of the provided policy [num_states]\n",
    "        \"\"\"\n",
    "        print(f'PI_PROB_SHAPE: {pi_prob.shape}')\n",
    "        if v is None:\n",
    "            # Initialize value function for policy evaluation\n",
    "            v = np.zeros(shape=self.n_states)\n",
    "        for pe_iter in range(10000):\n",
    "            # save current estimate\n",
    "            v_prev = np.copy(v)\n",
    "\n",
    "            ## TODO ##\n",
    "            # your code here\n",
    "            # hint: you will need to iterate over states and actions here\n",
    "\n",
    "            # run policy evaluation until convergence\n",
    "            if np.allclose(v, v_prev):\n",
    "                break\n",
    "\n",
    "        return v\n",
    "\n",
    "    def policy_improvement(self, v:np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Perform the Policy Improvement step given a value function.\n",
    "\n",
    "        :param v: Value function of a policy [num_states]\n",
    "        :return: New policy [num_states] and distribution over action probabilities [num_states, num_actions]\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize policy\n",
    "        pi = np.zeros(shape=self.n_states)  # contains the actual actions\n",
    "        pi_prob = np.ones(shape=(self.n_states, self.n_actions))  # contains the action probabilities for each state\n",
    "\n",
    "        ## TODO ##\n",
    "        # your code here\n",
    "        # hint: This again requires you to iterate over states and actions in some way.\n",
    "        # You can use np.argmax() to get the index of the biggest value in an array.\n",
    "\n",
    "\n",
    "        return pi, pi_prob\n",
    "\n",
    "    def policy_iteration(self, n_iter: int) -> (np.array, np.array):\n",
    "        \"\"\"Perform Policy Iteration given a DiscreteEnv environment and a discount factor gamma for n_iter iterations\n",
    "\n",
    "        :param env: An openAI Gym DiscreteEnv object\n",
    "        :param gamma: Discount factor\n",
    "        :param n_iter: Number of PI iterations\n",
    "        :return: Final value function [num_states] and optimal policy [num_states]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        v = np.zeros(shape=self.n_states)\n",
    "        pi = np.zeros(shape=self.n_states)  # contains the actual actions\n",
    "        pi_prob = np.ones(shape=(self.n_states, self.n_actions)) / self.n_actions  # contains the action probabilities for each state\n",
    "\n",
    "        print(\"Iteration |  max|V-Vprev|  | # chg actions | V[0]    \")\n",
    "        print(\"----------+----------------+---------------+---------\")\n",
    "        for pi_iter in range(n_iter):\n",
    "            pi_old = np.copy(pi)\n",
    "            v_old = np.copy(v)\n",
    "\n",
    "            # run policy evaluation\n",
    "            v_init = np.copy(v_old)\n",
    "            v = self.policy_evaluation(pi_prob, v_init)\n",
    "\n",
    "            # run policy improvement\n",
    "            pi, pi_prob = self.policy_improvement(v)\n",
    "\n",
    "            # evaluate policy success rate\n",
    "            self.lc_plotter.eval_policy(pi)\n",
    "\n",
    "            max_diff = np.abs(v - v_old).max()\n",
    "\n",
    "            n_chg_actions = 0 if pi_old is None else (pi != pi_old).sum()\n",
    "            print(\"{:4d}      | {:12.5f}   |   {:4d}        | {:8.3f}\".format(pi_iter, max_diff, n_chg_actions, v[0]))\n",
    "\n",
    "            # if max_diff < 1e-5:\n",
    "            #     break\n",
    "\n",
    "        return v, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Run Policy Iteration \n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", is_slippery=True)\n",
    "\n",
    "p_iteration = PolicyIteration(env, args['gamma'])\n",
    "v_p, pi_p = p_iteration.policy_iteration(args['pi_iters'])\n",
    "\n",
    "plot_value_f(v_p, env)\n",
    "save_figure(\"vf_pi\")\n",
    "p_iteration.lc_plotter.plot_lc()\n",
    "save_figure(\"lc_pi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TASK 2: Value Iteration**\n",
    "Next, we will have a look at Value Iteration.\n",
    "It is very similar to Policy Iteration and only executes one step of policy evaluation.\n",
    "The **pseudocode** looks as follows\n",
    "\n",
    "---\n",
    "- **Init** Initialize $V_{(0)}^\\ast(s)=0$, for all $s$\n",
    "\n",
    "- **Repeat**  For $i=1, 2, \\dots$\n",
    "\n",
    "    $V^\\ast_{(i)}(s) = \\underset{a}{\\max}  \\Big( r(s,a) + \\gamma \\sum_{s'} P(s' \\mid s,a) \\, V^\\ast_{(i-1)}(s') \\Big)$, for all $s$\n",
    "\n",
    "- **Until convergence**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 (4 Points)\n",
    "In this task you have to implement the missing code snippets (marked as *TODO*) of the Value iteration calss. The **psuedocode** shown above might be helpful.\n",
    "1. Implement the missing code snippet of the **value_iteration** function below \n",
    "\n",
    "After you have finished implementing the corresponding functions, execute the code cell *Run Value Iteration*. This code cell will automatically save your value function plot and the learning curve plot into your mounted drive folder. You will need to submit these figures together with the notebook as a zip file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    def __init__(self, env: FrozenLakeEnv, gamma: float):\n",
    "        \"\"\"A class for performing Value Iteration\n",
    "\n",
    "        :param env: A FrozenLakeEnv environment\n",
    "        :param gamma: Discount factor\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.n_states = env.nS\n",
    "        self.n_actions = env.nA\n",
    "\n",
    "        self.lc_plotter = LearningCurvePlotter(env, 100)\n",
    "\n",
    "    def value_iteration(self, n_iter: int) -> (np.ndarray, np.ndarray):\n",
    "        \"\"\"Perform Value Iteration given a DiscreteEnv environment and a discount factor gamma for n_iter iterations.\n",
    "\n",
    "        :param n_iter: Number of VI iterations\n",
    "        :return: A tuple (final value_function [num_states], optimal policy [num_states])\n",
    "        \"\"\"\n",
    "\n",
    "        v = np.zeros(shape=self.n_states)\n",
    "        pi = np.zeros(shape=self.n_states)\n",
    "\n",
    "        print(\"Iteration |  max|V-Vprev|  | # chg actions | V[0]    \")\n",
    "        print(\"----------+----------------+---------------+---------\")\n",
    "        for it in range(0, n_iter):\n",
    "            pi_old = np.copy(pi)\n",
    "            v_old = np.copy(v)\n",
    "\n",
    "            ## TODO ##\n",
    "            # your code here\n",
    "            # hint: Here, you will need to fill the new policy pi and value function v for all states s.\n",
    "            # I.e., you need to update pi[s], v[s] for all s.\n",
    "\n",
    "            # Evaluate policy success rate\n",
    "            self.lc_plotter.eval_policy(pi)\n",
    "\n",
    "            max_diff = np.abs(v - v_old).max()\n",
    "            n_chg_actions = 0 if pi_old is None else (pi != pi_old).sum()\n",
    "            print(\"{:4d}      | {:12.5f}   |   {:4d}        | {:8.3f}\".format(it, max_diff, n_chg_actions, v[0]))\n",
    "\n",
    "            # if max_diff < 1e-5:\n",
    "            #     # assume convergence if the difference is small enough\n",
    "            #     break\n",
    "\n",
    "        return v, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Run Value Iteration \n",
    "\n",
    "# env = gym.make(\"FrozenLake-v1\")\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", is_slippery=True)\n",
    "\n",
    "v_iteration = ValueIteration(env, args['gamma'])\n",
    "v_v, pi_v = v_iteration.value_iteration(args['vi_iters'])\n",
    "\n",
    "plot_value_f(v_v, env)\n",
    "save_figure(\"vf_vi\")\n",
    "v_iteration.lc_plotter.plot_lc()\n",
    "save_figure(\"lc_vi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TASK 3: Q-Learning**\n",
    "In Policy Iteration and Value Iteration, we assume to have knowledge about the problem's underlying dynamics and reward\n",
    "function. These properties are usually not available in practice.\n",
    "Instead, we need to solve the problem by exploiting what we have learned so far and exploring previously unseen situations.\n",
    "An algorithm to solve a problem in this fashion is Q-Learning. The **pseudocode** is given as\n",
    "\n",
    "---\n",
    "- **Init** Initialize $Q_{(0)}(s, a)=0$, for all $s$ and $a$\n",
    "\n",
    "- **Repeat**  For $i=1, 2, \\dots$\n",
    "    - sample an action $a$ using the exploration strategy and get the next state $s'$ and associated reward $r$\n",
    "    - If s' is terminal:\n",
    "        - $\\delta = r(s, a) - Q_{(i-1)}(s, a)$\n",
    "        - Reset environment and sample new initial state $ s' $\n",
    "    - Else:\n",
    "        - $\\delta = r(s, a) + \\gamma \\underset{a'}{\\max} Q_{(i-1)}(s', a') - Q_{(i-1)}(s, a)$\n",
    "\n",
    "    $Q_{(i)}(s, a) = Q_{(i-1)}(s, a) + \\alpha \\delta$ \n",
    "    \n",
    "    Set $ s \\leftarrow s' $\n",
    "\n",
    "- **Until convergence**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 (4 Points)\n",
    "We will implement the missing cod snippet (marked as *TODO*) of the QLearning class. The **pseudoclass** shown above might be helpful.\n",
    "\n",
    "1. Finish the Q-Learning inner loop in the learn function, by implementing the TD-Error and the Q-Learning update rule \n",
    "\n",
    "After you have finished implementing the corresponding functions, execute the code cell *Run QLearning*. This code cell will automatically save your value function plot and the learning curve plot into your mounted drive folder. You will need to submit these figures together with the notebook as a zip file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, env: FrozenLakeEnv, gamma: float, alpha: float, t_decay: int = 10000):\n",
    "        \"\"\"A class for tabular Q-Learning with epsilon-greedy exploration.\n",
    "         The policy is defined implicitly by the Q-function attribute.\n",
    "\n",
    "        :param env: A FrozenLakeEnv environment\n",
    "        :param gamma: Discount factor\n",
    "        :param alpha: Learning rate\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.t_decay = t_decay\n",
    "\n",
    "        self.n_states = env.nS\n",
    "        self.n_actions = env.nA\n",
    "        self.q = np.zeros(shape=(self.n_states, self.n_actions))\n",
    "        self.t = 0\n",
    "\n",
    "        self.lc_plotter = LearningCurvePlotter(env, 100)\n",
    "\n",
    "    def get_greedy_action(self, s: int) -> int:\n",
    "        \"\"\"Return the greedy action given a state. Can be used to evaluate the policy after learning.\n",
    "\n",
    "        :param s: A state id represented as an integer over all state ids.\n",
    "        :return: Return the current best action id, corresponding to the available actions described above.\n",
    "        \"\"\"\n",
    "        return int(np.argmax(self.q[s, :]))\n",
    "\n",
    "    def sample_action(self, s: int) -> int:\n",
    "        \"\"\"Sample an action based given the state id on the epsilon greedy strategy with time decaying exploration.\n",
    "\n",
    "        :param s: A state id represented as an integer over all state ids.\n",
    "        :return: An action id, corresponding to the available actions described above.\n",
    "        \"\"\"\n",
    "        if np.random.random() < np.minimum(0.99, self.t / self.t_decay):\n",
    "            action = self.get_greedy_action(s)\n",
    "\n",
    "        else:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "\n",
    "        self.t += 1\n",
    "        return int(action)\n",
    "\n",
    "    def learn(self, n_iter: int) -> (np.ndarray, np.ndarray):\n",
    "        \"\"\"Perform n_iter iterations of Q-Learning\n",
    "\n",
    "        :param n_iter: Number of learning iterations\n",
    "        :return: The learned Value function and policy as a tuple (final value_function [num_states], optimal policy [num_states])\n",
    "        \"\"\"\n",
    "        s = self.env.reset()\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            a = # TODO\n",
    "\n",
    "            s_dash, reward, done, info = self.env.step(a)\n",
    "\n",
    "            ## TODO ##\n",
    "            # your code here\n",
    "            # hint: use an if-else to handle terminal states (done==True) differently from non-terminal ones.\n",
    "\n",
    "            if done:\n",
    "                s = self.env.reset()\n",
    "            else:\n",
    "                s = s_dash\n",
    "\n",
    "            # evaluate policy success rate\n",
    "            if i % 100 == 0:\n",
    "                self.lc_plotter.eval_policy(np.argmax(self.q, axis=1))\n",
    "\n",
    "        v = np.max(self.q, axis=1)\n",
    "        pi = np.argmax(self.q, axis=1)\n",
    "\n",
    "        return v, pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run QLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Run QLearning\n",
    "\n",
    "# env = gym.make(\"FrozenLake-v1\")\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "\n",
    "np.random.seed(0)  # reset the seed in case of multiple cell executions\n",
    "q_learning = QLearning(env, args['gamma'], args['alpha'], args['t_decay'])\n",
    "v_q, pi_q = q_learning.learn(args['ql_iters'])\n",
    "\n",
    "plot_value_f(v_q, env)\n",
    "save_figure(\"vf_q\")\n",
    "q_learning.lc_plotter.plot_lc()\n",
    "\n",
    "save_figure(\"lc_q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 3.2 (3 Points)\n",
    "You may have noticed that we used the 4x4 version of the environment for the Q-Learning algorithm instead of the 8x8 one. You will find that the 8x8 version does not learn anything. Find out why this is the case and propose a solution for it. Give a short and precise summary of what you would do in 3-5 sentences (3 Points)."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_notebook",
   "language": "python",
   "name": "rl_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
