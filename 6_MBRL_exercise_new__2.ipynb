{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "6_MBRL_Solutions.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H09zLzLtLSw1"
   },
   "source": [
    "#Homework 6: Model Based RL With Deep Dynamics Models (10 Pts)\n",
    "\n",
    "All homeworks are self-contained. They can be completed in their respective notebooks. To edit and re-run code, you can therefore simply edit and restart the code cells below. There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window). This file should automatically be synced with your Google Drive. We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout. However, you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JDp0AADgLU6A",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "34f01482-271b-4aa8-f568-77dbe29ba0d8"
   },
   "source": [
    "# Your work will be stored in a folder called `drl_ws22` by default to prevent Colab\n",
    "# instance timeouts from deleting your edits.\n",
    "# We do this by mounting your google drive on the virtual machine created in this colab\n",
    "# session. For this, you will likely need to sign in to your Google account and copy a\n",
    "# passcode into a field below\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qaGhwNOxLgqC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "90e95874-8a59-4a94-def9-f2e1acbe4599"
   },
   "source": [
    "# Create paths in your google drive\n",
    "DRIVE_PATH = '/content/gdrive/My\\ Drive/drl_ws22'\n",
    "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
    "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
    "    % mkdir $DRIVE_PATH\n",
    "\n",
    "# the space in `My Drive` causes some issues,\n",
    "# make a symlink to avoid this\n",
    "SYM_PATH = '/content/drl_ws22'\n",
    "if not os.path.exists(SYM_PATH):\n",
    "    !ln -s $DRIVE_PATH $SYM_PATH\n",
    "! cd $SYM_PATH"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9JCctqJjLnnP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b01e73f0-3c2d-4359-b7a0-d15ce62a14f5"
   },
   "source": [
    "# Install **python** packages\n",
    "\n",
    "!pip install matplotlib numpy tqdm torch pybullet\n",
    "# for open ai gym\n",
    "!pip install gym==0.17.2\n",
    "\n",
    "!git clone https://github.com/benelot/pybullet-gym lib/pybullet-gym\n",
    "!pip install -e lib/pybullet-gym"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "430EKj9cm5ZK"
   },
   "source": [
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkQ7Vo5914o9"
   },
   "source": [
    "Restart your runtime once the packages are Installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dxzj7M9JMtBD"
   },
   "source": [
    "We start by importing some necessary packages and defining helper functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3L8vmmOmMv5A"
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import pybulletgym\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch, os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# specify the path to save the recordings of this run to.\n",
    "data_path = '/content/drl_ws22/exercise_6'\n",
    "data_path = os.path.join(data_path, time.strftime(\"%d-%m-%Y_%H-%M\"))\n",
    "if not (os.path.exists(data_path)):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "\n",
    "# this function will automatically save your figure into your google drive folder (if correctly mounted!)\n",
    "def save_figure(save_name: str) -> None:\n",
    "    assert save_name is not None, \"Need to provide a filename to save to\"\n",
    "    plt.savefig(os.path.join(data_path, save_name + \".png\"))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Czwxn0NSNJaV"
   },
   "source": [
    "#Set Up The Pybullet Inverted Pendulum Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WycKBzg_NvOz"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UlrqeHoCmlY0"
   },
   "source": [
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment \n",
    "and displaying it.\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XqNF_x0cwyzv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dcdd9dfb-a30c-43da-a068-bef80fc8a45c"
   },
   "source": [
    " env = gym.make('InvertedPendulumSwingupPyBulletEnv-v0')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2hjempk37JG"
   },
   "source": [
    "# Model-Based Reinforcement Learning\n",
    "\n",
    "## Principle\n",
    "We consider the optimal control problem of an MDP with an **unknown deterministic** reward function $r$ and subject to **unknown deterministic** dynamics $s_{t+1} = f(s_t, a_t)$:\n",
    "\n",
    "$$\\max_{(a_0,a_1,\\dotsc)} \\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t)$$\n",
    "\n",
    "In **model-based reinforcement learning**, this problem is solved in **two steps**:\n",
    "1. **Model learning**:\n",
    "We learn a model of the dynamics $f_\\theta \\simeq f$ and reward function $r_\\theta \\simeq r$ through regression on interaction data.\n",
    "2. **Planning**:\n",
    "We leverage the dynamics model $f_\\theta$ to compute the optimal trajectory $$\\max_{(a_0,a_1,\\dotsc)} \\sum_{t=0}^\\infty \\gamma^t r_{\\theta}(\\hat{s}_t,a_t)$$ following the learnt dynamics $\\hat{s}_{t+1} = f_\\theta(\\hat{s}_t, a_t)$.\n",
    "\n",
    "(We can easily extend to stochastic dynamics, but we consider the simpler case in this homework)\n",
    "\n",
    "\n",
    "In this homework you will implement the model-based algorithm proposed in section IV of [this paper](https://arxiv.org/abs/1708.02596) with some differences: \n",
    "\n",
    "1. along with the next environment state, also the reward is learned. To do that another neural network has been used.\n",
    "2. We train on the pybullet gym environment of inverted pendulum.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\overline{\\text { Algorithm } \\mathbf{1} \\text { Model-based Reinforcement Learning }} \\\\\n",
    "&\\hline \\text { 1: gather dataset } \\mathcal{D}_{\\text {RAND }} \\text { of random trajectories } \\\\\n",
    "&\\text { 2: initialize empty dataset } \\mathcal{D}_{\\text {RL }} \\text {, and randomly initialize } f_{\\theta} \\\\\n",
    "&  \\text{ 3: } \\textbf{for} \\text{ iter=1 to max_iter } \\textbf{do}  \\\\\n",
    "&\\quad  \\quad \\quad \\textbf {Model Learning } \\\\\n",
    "&\\text { 4:} \\quad  \\quad \\text{ train } f_{\\theta}(\\mathbf{s}, \\mathbf{a}) \\text{ and } r_{\\theta}(\\mathbf{s}, \\mathbf{a}) \\text { by performing gradient descent on MSE Loss }  \\\\\n",
    "& \\quad \\quad  \\quad \\text { using } \\mathcal{D}_{\\text {RAND }} \\text { and } \\mathcal{D}_{\\text {RL }} \\\\\n",
    "&\\quad  \\quad \\quad \\textbf {Planning and More Experience Collection } \\\\\n",
    "&\\text { 5: } \\quad  \\quad  \\textbf { for } t=1 \\text { to } T \\textbf { do } \\\\\n",
    "&\\text { 6: } \\quad \\quad \\quad \\quad \\text { get agent's current state } \\mathbf{s}_{t} \\\\\n",
    "&\\text { 7: } \\quad \\quad \\quad \\quad \\text { use } f_{\\theta} \\text { and } r_{\\theta} \\text { to estimate optimal action sequence } \\mathbf{A}_{t}^{(H)} \\\\\n",
    "&\\text { 8: } \\quad \\quad \\quad \\quad \\text { execute first action } \\mathbf{a}_{t} \\text { from selected action sequence } \\\\\n",
    "&\\text { 9: } \\quad \\quad \\quad \\quad\\text { Add } \\{s_t,  s_{t+1}, r_t, a_t\\} \\text { to } \\mathcal{D}_{\\text {RL }} \\\\\n",
    "&\\text { 10: } \\quad \\text { end for } \\\\\n",
    "&\\text { 11: end for } \\\\\n",
    "&\\hline\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "You will notice that compared to model free methods, model based methods are more sample efficient. For this environment expect the algorithm to receive reasonable rewards (greater than 600 in about 15 iterations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-WRSNNEOX5w"
   },
   "source": [
    "# Collect Random Trajectories To Train A Dynamics Model and Reward Model (Experience Collection)\n",
    "\n",
    "First, we randomly interact with the environment to produce a batch of experiences \n",
    "\n",
    "$$D = \\{s_t,  s_{t+1}, r_t, a_t\\}_{t\\in[1,N]}$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_AymYXhlOocs"
   },
   "source": [
    "def gather_random_trajectories(num_traj, env):\n",
    "    '''\n",
    "    Run num_traj random trajectories to gather information about the next state and reward.\n",
    "    Data used to train the models in a supervised way.\n",
    "    '''\n",
    "    dataset_random = []\n",
    "\n",
    "    game_rewards = []\n",
    "    with tqdm(total=num_traj, position=0, leave=True) as pbar:\n",
    "        for n in tqdm(range(num_traj),position=0, leave=True):\n",
    "            #env.render()\n",
    "            obs = env.reset()\n",
    "            while True:\n",
    "                sampled_action = env.action_space.sample()\n",
    "                new_obs, reward, done, _ = env.step(sampled_action)\n",
    "\n",
    "\n",
    "                dataset_random.append([obs, new_obs, reward, sampled_action])\n",
    "\n",
    "                obs = new_obs\n",
    "                game_rewards.append(reward)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "            pbar.update()\n",
    "\n",
    "    # print some stats\n",
    "    print('Mean R:', np.round(np.sum(game_rewards) / num_traj, 2), 'Max R:', np.round(np.max(game_rewards), 2),\n",
    "          np.round(len(game_rewards) / num_traj))\n",
    "\n",
    "    return dataset_random\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ed5n0DiPGOZ"
   },
   "source": [
    "# Deep Dynamics Models With Feed Forward Neural Network\n",
    "\n",
    "A dynamics model takes in the state ($s_t$) and action($a_t$) at the current time step and predicts the state ($s_{t+1}$) at the next time step.  \n",
    "\n",
    "**However this function can be difficult to learn when the states $s_t$ and $s_{t+1}$ are too similar and the action has seemingly littl eeffect on the output; this difficulty becomes more pronouncedas the time between states $\\Delta t$ becomes smaller and the state differences do not indicate the underlying dynamics well. Thus we typically a function that predict the differences to the next state. i.e. $\\Delta s_{t+1} = s_{t+1} - s_t = f(s_t,a_t)$. This allows us to learn more accurate models in practice.**\n",
    "\n",
    "In the following block, we define the Dynamics Model Class. Each instance of this class is a Neural Dynamics Model that computes the dynamics function $\\Delta s_{t+1} = f_{\\Theta}(s_t, a_t)$. The input of this Network is a state and action at current time step $s_t$, $a_t$ respectively and the output the differences to the next state at the next time step. Please note that for NN training, we normally feed data in a mini-batch manner. Since each state in current task is a 1st order image tensor (concatenation of $s_t$ and $a_t$), the Dynamics model expects input to be a 2nd order tensor with shape (mini-batch, state_dim + action_dim). And the output is a 2nd order tensor with shape (mini-batch, state_dim)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pPtT-9L7O9CK"
   },
   "source": [
    "###########################Create A Neural Network Dynmics Model####################################\n",
    "class NNDynamicModel(nn.Module):\n",
    "    '''\n",
    "    Model that predict the differnce to next state, given the current state and action\n",
    "    '''\n",
    "    def __init__(self, input_dim, obs_output_dim):\n",
    "      '''\n",
    "      input_dim: state_dim + action_dim\n",
    "      output_dim: state_dim\n",
    "      '''\n",
    "      super(NNDynamicModel, self).__init__()\n",
    "\n",
    "      self.mlp = nn.Sequential(\n",
    "          nn.Linear(input_dim, 512),\n",
    "          nn.BatchNorm1d(num_features=512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512,256),\n",
    "          nn.BatchNorm1d(num_features=256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, obs_output_dim)\n",
    "      )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x.float())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hI_qEzp1Tmvr"
   },
   "source": [
    "# Deep Reward Models With Feed Forward Neural Network\n",
    "\n",
    "A reward model takes in the state ($s_t$) and action($a_t$) at the current time step and predicts reward ($r_{t}$) that we can obtain based on that action.  \n",
    "\n",
    "In the following block, we define the Reward Model Class. Each instance of this class is a Neural Reward Model that computes the reward function $r_{t} = r_{\\theta}(s_t, a_t)$. The input of this Network is a state and action at current time step $s_t$, $a_t$ respectively and the output the rewards at the next time step. Please note that for NN training, we normally feed data in a mini-batch manner. Since each state in current task is a 1st order image tensor (concatenation of $s_t$ and $a_t$), the Dynamics model expects input to be a 2nd order tensor with shape (mini-batch, state_dim + action_dim). And the output is a 2nd order tensor with shape (mini-batch, reward_dim)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aNwMbuFnUTTm"
   },
   "source": [
    "class NNRewardModel(nn.Module):\n",
    "    '''\n",
    "    Model that predict the reward given the current state and action\n",
    "    '''\n",
    "    def __init__(self, input_dim, reward_output_dim):\n",
    "        '''\n",
    "        input_dim: state_dim + action_dim\n",
    "        output_dim: reward_dim\n",
    "        '''\n",
    "        super(NNRewardModel, self).__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(num_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, reward_output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x.float())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6WZSisIVspI"
   },
   "source": [
    "# Training The Dynamics And Reward Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "St4dSNkWO28h"
   },
   "source": [
    "We can now train our models $f_\\theta$ and $r_\\theta$ in a supervised fashion to minimize an MSE loss over our experience batch by stochastic gradient descent:\n",
    "\n",
    "$$L_{Dynamics} = \\frac{1}{|D|}\\sum_{s_t,a_t,s_{t+1}\\in D}||\\Delta s_{t+1}- f_\\theta(s_t, a_t)||^2$$\n",
    "\n",
    "$$L_{Reward} = \\frac{1}{|D|}\\sum_{s_t,a_t,r_{t}\\in D}||r_{t}- r_\\theta(s_t, a_t)||^2$$\n",
    "\n",
    "-----\n",
    "In practice, it’s helpful to normalize the target of a neural network.  So in the code, we’ll train the network to predict a normalized version of the change in state, as in\n",
    "\n",
    "$$L_{Dynamics} = \\frac{1}{|D|}\\sum_{s_t,a_t,s_{t+1}\\in D}||\\text{normalize}(\\Delta s_{t+1})- f_\\theta(s_t, a_t)||^2$$\n",
    "\n",
    "Similarly, we’ll train the network to predict a normalized version of the reward, as in\n",
    "\n",
    "$$L_{Reward} = \\frac{1}{|D|}\\sum_{s_t,a_t,r_{t}\\in D}||\\text{normalize}(r_{t})- r_\\theta(s_t, a_t)||^2$$\n",
    "\n",
    "\n",
    "\n",
    "------\n",
    "Since $f_{\\theta}$ is trained to predict the normalized state difference, you generate thenext prediction with\n",
    "$$\n",
    "\\hat{\\mathbf{s}}_{t+1}=\\mathbf{s}_{t}+\\text { Unnormalize }\\left(f_{\\theta}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right)\n",
    "$$ \\\\\n",
    "\n",
    "You generate the reward with $$\n",
    "\\hat{\\mathbf{r}}_{t}=\\text { Unnormalize }\\left(r_{\\theta}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right)\n",
    "$$\n",
    "\n",
    "## Task 1: Training the Dynamics and Reward Models (3 Pts)\n",
    "We will use the means squarred error (MSE) loss as shown in the equations above:\n",
    "- implement the MSE loss in the *model_MSEloss* function\n",
    "\n",
    "To train the dynamics and reward models, we have to properly prepare the training data. Implement the following steps in the *train_dyna_model*\n",
    "- split the dataset into training (80%) and testing (20%) data\n",
    "- randomly shuffle the training data (hint: you can use *np.random.shuffle* for this)\n",
    "- create the input and output data for the dynamics and reward model. Think what the inputs for each of these networks are. You can see them in the equations above. Do this procedure for both, the testing and the validation data set.\n",
    "\n",
    "\n",
    "** The places you have to fill in your code are marked **\n",
    "\n",
    "**Note** We use sklearn's [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to perform normalization and unnormalization steps, through out this assignment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hDOIWrcmO2LT"
   },
   "source": [
    "def model_MSEloss(y_truth, y_pred, device):\n",
    "    '''\n",
    "    input_1: ground truth values (batch_size x data_dimension)\n",
    "    input_2: predicted values (batch_size x data_dimension)\n",
    "    input_3: device specification for pytorch ('cuda' / 'cpu)\n",
    "\n",
    "    return: mean squared error loss between\n",
    "\n",
    "    Compute the MSE (Mean Squared Error) between y_truth and y_pred\n",
    "    '''\n",
    "\n",
    "    ### Your code starts here ###\n",
    "    return 0\n",
    "    ### Your code ends here ###\n",
    "    "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4lZrey6pan0S"
   },
   "source": [
    "def train_dyna_model(random_dataset, rl_dataset, env_model, rew_model, batch_size, max_model_iter, num_examples_added, ENV_LEARNING_RATE, REW_LEARNING_RATE, device):\n",
    "    '''\n",
    "    Train the two models that predict the next state and the expected reward\n",
    "    '''\n",
    "    print(\"................Training Dynamics and Reward Models With Collected Data So Far.....................\")\n",
    "\n",
    "    env_optimizer = optim.Adam(env_model.parameters(), lr=ENV_LEARNING_RATE)\n",
    "    rew_optimizer = optim.Adam(rew_model.parameters(), lr=REW_LEARNING_RATE)\n",
    "\n",
    "    ###Accessing Inputs and Outputs to the Neural Network From Data Buffer D###\n",
    "    if len(rl_dataset) > 0:\n",
    "        # Concatenate the random dataset with the RL dataset. Used only in the aggregation iterations\n",
    "        d_concat = np.concatenate([random_dataset, rl_dataset], axis=0)\n",
    "    else:\n",
    "        d_concat = np.array(random_dataset)\n",
    "\n",
    "\n",
    "    ### >>>>>>>>>Your code starts here<<<<<<<<< ###\n",
    "\n",
    "    # TODO: Split the dataset into train(80%) and test(20%)\n",
    "    #D_train =\n",
    "    #D_valid =\n",
    "\n",
    "    print(\"len(D):\", len(d_concat), 'len(Dtrain)', len(D_train))\n",
    "\n",
    "    # TODO: Shuffle the dataset\n",
    "    #D_train =\n",
    "\n",
    "\n",
    "    # TODO: Create the input and output for the train dynamics and reward models by accessing D_train and D_test\n",
    "    #X_train =\n",
    "    #y_rew_train =\n",
    "    #y_next_state_train =\n",
    "    #y_diff_train =\n",
    "\n",
    "    # Create the input and output array for the validation\n",
    "    #X_valid =\n",
    "    #y_rew_valid =\n",
    "    #y_next_state_valid =\n",
    "    #y_diff_valid =\n",
    "\n",
    "    ###>>>>>>>>>>>> Your code Ends here<<<<<<<<<< ###\n",
    "\n",
    "    ####Standardize The Inputs And Outputs ###\n",
    "\n",
    "    # Standardize the input features by removing the mean and scaling to unit variance\n",
    "    input_scaler = StandardScaler()\n",
    "    X_train = input_scaler.fit_transform(X_train)\n",
    "    X_valid = input_scaler.transform(X_valid)\n",
    "\n",
    "    # Standardize the outputs by removing the mean and scaling to unit variance\n",
    "\n",
    "    env_output_scaler = StandardScaler()\n",
    "    y_diff_train = env_output_scaler.fit_transform(y_diff_train)\n",
    "    y_diff_valid = env_output_scaler.transform(y_diff_valid)\n",
    "\n",
    "    rew_output_scaler = StandardScaler()\n",
    "    y_rew_train = rew_output_scaler.fit_transform(y_rew_train)\n",
    "    y_rew_valid = rew_output_scaler.transform(y_rew_valid)\n",
    "\n",
    "    # store all the scalers in a variable to later uses\n",
    "    standardizers = (input_scaler, env_output_scaler, rew_output_scaler)\n",
    "\n",
    "    losses_env = []\n",
    "    losses_rew = []\n",
    "\n",
    "    # go through max_model_iter supervised iterations\n",
    "    with tqdm(total=max_model_iter, position=0, leave=True) as pbar:\n",
    "      for it in tqdm(range(max_model_iter),position=0, leave=True):\n",
    "          # create mini batches of size batch_size\n",
    "          for mb in range(0, len(X_train), batch_size):\n",
    "\n",
    "              if len(X_train) > mb+BATCH_SIZE:\n",
    "                  X_mb = X_train[mb:mb+BATCH_SIZE]\n",
    "\n",
    "                  y_diff_mb = y_diff_train[mb:mb+BATCH_SIZE]\n",
    "                  y_rew_mb = y_rew_train[mb:mb+BATCH_SIZE]\n",
    "\n",
    "                  # Add gaussian noise with mean 0 and variance 0.0001 as in the paper\n",
    "                  X_mb += np.random.normal(loc=0, scale=0.001, size=X_mb.shape)\n",
    "\n",
    "                  ## Optimization of the 'env_model' neural net\n",
    "\n",
    "                  env_optimizer.zero_grad()\n",
    "                  # forward pass of the model to compute the output\n",
    "                  pred_state = env_model(torch.tensor(X_mb).to(device))\n",
    "                  # compute the MSE loss\n",
    "                  loss = model_MSEloss(y_diff_mb, pred_state, device)\n",
    "\n",
    "                  if it == (max_model_iter - 1):\n",
    "                      losses_env.append(loss.cpu().detach().numpy())\n",
    "\n",
    "                  # backward pass\n",
    "                  loss.backward()\n",
    "                  # optimization step\n",
    "                  env_optimizer.step()\n",
    "\n",
    "\n",
    "                  ## Optimization of the 'rew_model' neural net\n",
    "                  rew_optimizer.zero_grad()\n",
    "                  # forward pass of the model to compute the output\n",
    "                  pred_rew = rew_model(torch.tensor(X_mb).to(device))\n",
    "                  # compute the MSE loss\n",
    "                  loss = model_MSEloss(y_rew_mb, pred_rew, device)\n",
    "\n",
    "                  if it == (max_model_iter - 1):\n",
    "                      losses_rew.append(loss.cpu().detach().numpy())\n",
    "                  # backward pass\n",
    "                  loss.backward()\n",
    "                  # optimization step\n",
    "                  rew_optimizer.step()\n",
    "\n",
    "      # Evalute the models every 10 iterations and print the losses\n",
    "      if it % 10 == 0:\n",
    "          env_model.eval()\n",
    "          rew_model.eval()\n",
    "\n",
    "          pred_state = env_model(torch.tensor(X_valid).to(device))\n",
    "          pred_rew = rew_model(torch.tensor(X_valid).to(device))\n",
    "          env_model.train(True)\n",
    "          rew_model.train(True)\n",
    "\n",
    "          valid_env_loss = model_MSEloss(y_diff_valid, pred_state, device)\n",
    "          valid_rew_loss = model_MSEloss(y_rew_valid, pred_rew, device)\n",
    "\n",
    "          print('..', it, valid_env_loss.cpu().detach().numpy(), valid_rew_loss.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "    ## Evaluate the MSE losses\n",
    "\n",
    "    env_model.eval()\n",
    "    rew_model.eval()\n",
    "\n",
    "    pred_state = env_model(torch.tensor(X_valid).to(device))\n",
    "    pred_rew = rew_model(torch.tensor(X_valid).to(device))\n",
    "    env_model.train(True)\n",
    "    rew_model.train(True)\n",
    "\n",
    "    valid_env_loss = model_MSEloss(y_diff_valid, pred_state, device)\n",
    "    valid_rew_loss = model_MSEloss(y_rew_valid, pred_rew, device)\n",
    "\n",
    "    return np.mean(losses_env), np.mean(losses_rew), valid_env_loss.cpu().detach().numpy(), valid_rew_loss.cpu().detach().numpy(), standardizers\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jh_C5RZ7bhQx"
   },
   "source": [
    "# Planning With Dynamics And Reward Models\n",
    "\n",
    "Given the learned dynamics and reward models, we now want to select and execute actions that minimize a cost function (long term rewards). Ideally, you would calculate these actions by solving the following optimization:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}_{t}^{*}=\\arg \\max _{\\mathbf{a}_{t: \\infty}} \\sum_{t^{\\prime}=t}^{\\infty} r_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right) \\text { where } \\hat{\\mathbf{s}}_{t^{\\prime}+1}=\\hat{\\mathbf{s}}_{t^{\\prime}}+f_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right)\n",
    "$$\n",
    "\n",
    "However, solving the above equation is impractical for two reasons:\n",
    "- planning over an infinite sequence of actions is impossible and\n",
    "- the learned dynamics model is imperfect, so using it to plan in such an open-loop manner will lead to accumulating errors over time and planning far into the future will become very inaccurate.\n",
    "\n",
    "Instead, we will solve the following gradient-free optimization problem:\n",
    "$$\n",
    "\\mathbf{A}^{*}=\\arg \\max _{\\left\\{\\mathbf{A}^{(0)}, \\ldots, \\mathbf{A}^{(K-1)}\\right\\}} \\sum_{t^{\\prime}=t}^{t+H-1} r_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right) \\text { s.t. } \\hat{\\mathbf{s}}_{t^{\\prime}+1}=\\hat{\\mathbf{s}}_{t^{\\prime}}+f_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right)\n",
    "$$\n",
    "in which $\\mathbf{A}^{(k)}=\\left(a_{t}^{(k)}, \\ldots, a_{t+H-1}^{(k)}\\right)$ are each a random action sequence of length $H$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOFAOWMKb4Rj"
   },
   "source": [
    "## Planner 1: Random Shooting Based Planner\n",
    "\n",
    "We will now use a simple random shooting method to solve the following gradient-free optimization problem :\n",
    "$$\n",
    "\\mathbf{A}^{*}=\\arg \\min _{\\left\\{\\mathbf{A}^{(0)}, \\ldots, \\mathbf{A}^{(K-1)}\\right\\}} \\sum_{t^{\\prime}=t}^{t+H-1} r_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right) \\text { s.t. } \\hat{\\mathbf{s}}_{t^{\\prime}+1}=\\hat{\\mathbf{s}}_{t^{\\prime}}+f_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right)\n",
    "$$\n",
    "in which $\\mathbf{A}^{(k)}=\\left(a_{t}^{(k)}, \\ldots, a_{t+H-1}^{(k)}\\right)$ are each a random action sequence of length $H$.\n",
    "\n",
    "**Random Shooting**: The simplest gradient-free optimizer simply generates $N$ independent random action sequences $\\left\\{A_{0} \\ldots A_{N}\\right\\}$, where each sequence $A_{i}=\\left\\{a_{t}^{i} \\ldots a_{t+H-1}^{i}\\right\\}$ is of length $H$ action. Given a reward function $r(s, a)$ that defines the task, and given future state predictions $\\hat{s}_{t+1}=f_{\\theta}\\left(\\hat{s}_{t}, a_{t}\\right)+\\hat{s}_{t}$ from the learned dynamics model $f_{\\theta}$, the optimal action sequence $A_{i^{*}}$ is selected to be the one corresponding to the sequence with highest predicted return: $i^{*}=\\arg \\max _{i} \\sum_{t^{\\prime}=t}^{t+H-1} r\\left(\\hat{s}_{t^{\\prime}}, a_{t^{\\prime}}^{i}\\right) .$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5yaL550Mb7sc"
   },
   "source": [
    "def random_shooting_based_control(env_model, rew_model, real_obs, num_sequences, horizon_length, sample_action, standardizers, device):\n",
    "    '''\n",
    "    Use a random-sampling shooting method, generating random action sequences. The first action with the highest reward of the entire sequence is returned.\n",
    "    env_model: the Neural Network model of the environment\n",
    "    rew_model: the Neural Network model of the environment\n",
    "    real_obs: the observations last seen from the real physical environment\n",
    "    num_sequences: number of sequences to generate\n",
    "    horizon_length: the planning horizon H\n",
    "    sample_action: the function which is used to sample the action. We usually use the env.action_space.sample function.\n",
    "    standardizers: a tuple containing the standardizers which were created during training the models. Needed to unnormalize.\n",
    "    device: device specification for pytorch ('cuda' / 'cpu)\n",
    "    '''\n",
    "    input_scaler, env_output_scaler, rew_output_scaler = standardizers\n",
    "\n",
    "    m_obs = np.array([real_obs for _ in range(num_sequences)])\n",
    "\n",
    "    # array that contains the cumilative rewards for all the sequence\n",
    "    cumilative_rewards = np.zeros((num_sequences, 1))\n",
    "    first_sampled_actions = []\n",
    "\n",
    "    env_model.eval()\n",
    "    rew_model.eval()\n",
    "\n",
    "    ## Create a batch of size 'num_sequences' (number of trajectories) to roll the models 'horizon_length' times.\n",
    "    ## i.e. roll a given number of trajectories in a single batch (to increase speed)\n",
    "\n",
    "    # sampled actions for each sequence\n",
    "    sampled_action_sequences = []\n",
    "    for _ in range(num_sequences):\n",
    "      sampled_action_sequence = [sample_action() for _ in range(horizon_length)]\n",
    "      sampled_action_sequences.append(sampled_action_sequence)\n",
    "\n",
    "    sampled_action_sequences = np.array(sampled_action_sequences)\n",
    "\n",
    "\n",
    "    for t in range(horizon_length):\n",
    "      # select action for time step t for each sequence\n",
    "      sampled_actions = sampled_action_sequences[:,t,:]\n",
    "      # scale the input\n",
    "      models_input = input_scaler.transform(np.concatenate([m_obs, sampled_actions], axis=1))\n",
    "      # compute the next state for each sequence\n",
    "      pred_obs = env_model(torch.tensor(models_input).to(device))\n",
    "      # and the reward\n",
    "      pred_rew = rew_model(torch.tensor(models_input).to(device))\n",
    "\n",
    "      # inverse scaler transofrmation\n",
    "      pred_obs = env_output_scaler.inverse_transform(pred_obs.cpu().detach().numpy())\n",
    "      # and add previous observation\n",
    "      m_obs = pred_obs + m_obs\n",
    "\n",
    "      assert(pred_rew.cpu().detach().numpy().shape == cumilative_rewards.shape)\n",
    "\n",
    "      # sum of the expected rewards\n",
    "      cumilative_rewards += pred_rew.cpu().detach().numpy()\n",
    "\n",
    "      if t == 0:\n",
    "        first_sampled_actions = sampled_actions\n",
    "\n",
    "    env_model.train(True)\n",
    "    rew_model.train(True)\n",
    "\n",
    "    # Best the position of the sequence with the higher reward\n",
    "    arg_best_reward = np.argmax(cumilative_rewards)\n",
    "    best_sum_reward = cumilative_rewards[arg_best_reward].squeeze()\n",
    "    # take the first action of this sequence\n",
    "    best_action = first_sampled_actions[arg_best_reward]\n",
    "\n",
    "    return best_action, best_sum_reward"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8r_3mKZ1b8Rd"
   },
   "source": [
    "# Cross Entropy Method (CEM) for Planning (5 Pts)\n",
    "The random shooting approach has been shown to achieve success on continuous control tasks with learned models, but it has numerous drawbacks: it scales poorly with the dimension of both the planning horizon and the action space, and it often is insufficient for achieving high task performance since a sequence of actions sampled at random often does not directly lead to meaningful behavior. Therefor, the Cross Entropy Method is a favorable planning approach in model based reinforcement learning.\n",
    "\n",
    "We already got known to CEM in the stochastic search homework. However, here, we consider a different application.\n",
    "\n",
    "We will use CEM to solve the following gradient-free optimization problem :\n",
    "$$\n",
    "\\mathbf{A}^{*}=\\arg \\min _{\\left\\{\\mathbf{A}^{(0)}, \\ldots, \\mathbf{A}^{(K-1)}\\right\\}} \\sum_{t^{\\prime}=t}^{t+H-1} r_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right) \\text { s.t. } \\hat{\\mathbf{s}}_{t^{\\prime}+1}=\\hat{\\mathbf{s}}_{t^{\\prime}}+f_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right)\n",
    "$$\n",
    "in which $\\mathbf{A}^{(k)}=\\left(a_{t}^{(k)}, \\ldots, a_{t+H-1}^{(k)}\\right)$ are each a random action sequence of length $H$\n",
    "\n",
    "**Cross-Entropy Method** (`CEM`) is an optimization algorithm applicable to problems that are both **combinatorial** and **continuous**, which is our case: find the best performing sequence of actions.\n",
    "\n",
    "The Cross-entropy method (CEM) approach, begins as the random shooting approach, but then does this sampling for multiple iterations $m \\in\\{0 \\ldots M\\}$ at each time step. The top $J$ highest-scoring action sequences from each iteration are used to update and refine the mean and variance of the sampling distribution for the next iteration, as follows:\n",
    "\n",
    "\\begin{aligned}\n",
    "A_{i} &=\\left\\{a_{0}^{i} \\ldots a_{H-1}^{i}\\right\\} \\text {, where } a_{t}^{i} \\sim \\mathcal{N}\\left(\\mu_{t}^{m}, \\Sigma_{t}^{m}\\right) \\forall i \\in N, t \\in 0 \\ldots H-1 \\\\\n",
    "A_{\\text {elites }} &=\\operatorname{sort}\\left(A_{i}\\right)[-J:] \\\\\n",
    "\\mu_{t}^{m+1} &=  \\operatorname{mean}\\left(A_{\\text {elites }}\\right) \\quad \\forall t \\in 0 \\ldots H-1 \\\\\n",
    "\\Sigma_{t}^{m+1} &= \\operatorname{var}\\left(A_{\\text {elites }}\\right)\\quad  \\forall t \\in 0 \\ldots H-1\n",
    "\\end{aligned}\n",
    "\n",
    "After $M$ iterations, the optimal actions are selected to be the resulting mean of the action distribution.\n",
    "Note that, since our model is imperfect and things will never go perfectly according to plan, we adopt a model predictive control (MPC) approach.\n",
    "The MPC planner replans at every time step similar to previous section with random shooting planner. Students may refer to [this paper](/https://arxiv.org/pdf/1909.11652.pdf).\n",
    "\n",
    "## Task 2: Implementing the CEM for Planning\n",
    "In the following task you will implement the CEM following the standard procedure. In each optimization iteration, your code will execute the steps mentioned before, which we detail in the following.\n",
    "For each optimization step, do:\n",
    "- sample a sequence of actions $A_{i}$ from the Gaussian distribution from the iteration before\n",
    "- prepare the data for inputting to the dynamics and rewards model by standardizing it\n",
    "- perform the inference of the reward model to obtain a reward prediction\n",
    "- perform the inference via the dynamics model to obtain the observation difference prediction\n",
    "- unnormalize the predicted observation difference\n",
    "- obtain the observation of the next step by adding the predicted observation to the current observation\n",
    "- sum the predicted reward to the rewards from the steps before\n",
    "\n",
    "After each optimization step, do:\n",
    "- obtain the top 10 action sequences $A_{i}$  which lead to the highes cumulative reward\n",
    "- update the search distribution by calculating the sample mean and the sample standard deviation using the elite samples\n",
    "**NOTE:** In the last homework, we have calculate the full covariance of the distribution. Here we only consider the standard deviation of each dimension of the search space.\n",
    "\n",
    "We have provided you with comments guiding you through the parts you need to implement."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "W3x-Bvy1cA5_"
   },
   "source": [
    "def cem_based_control(env_model, rew_model, real_obs, num_sequences, horizon_length, sample_action, standardizers, device):\n",
    "    '''\n",
    "    Use a cem method for planning\n",
    "    env_model: the Neural Network model of the environment\n",
    "    rew_model: the Neural Network model of the environment\n",
    "    real_obs: the observations last seen from the real physical environment\n",
    "    num_sequences: number of sequences to generate\n",
    "    horizon_length: the planning horizon H\n",
    "    sample_action: the function which is used to sample the action. We usually use the env.action_space.sample function.\n",
    "    standardizers: a tuple containing the standardizers which were created during training the models. Needed to unnormalize.\n",
    "    device: device specification for pytorch ('cuda' / 'cpu)\n",
    "    '''\n",
    "    optimisation_iters = 3\n",
    "\n",
    "    input_scaler, env_output_scaler, rew_output_scaler = standardizers\n",
    "\n",
    "    first_sampled_actions = []\n",
    "\n",
    "    env_model.eval()\n",
    "    rew_model.eval()\n",
    "\n",
    "    ## Create a batch of size 'num_sequences' (number of trajectories) to roll the models 'horizon_length' times.\n",
    "    ## i.e. roll a given number of trajectories in a single batch (to increase speed)\n",
    "\n",
    "    # sampled actions for each sequence each sequence is of length H.\n",
    "    sampled_action_sequences = []\n",
    "    for _ in range(num_sequences):\n",
    "        sampled_action_sequence = [sample_action() for _ in range(horizon_length)]\n",
    "        sampled_action_sequences.append(sampled_action_sequence)\n",
    "\n",
    "    sampled_action_sequences = np.array(sampled_action_sequences)\n",
    "    action_dim = sampled_action_sequences.shape[-1]\n",
    "    ### >>>>>>>>>Your code starts here<<<<<<<<< ###\n",
    "\n",
    "    action_mean, action_std_dev =  # TODO: Calculate the mean and standard deviation of the sampled action sequences\n",
    "\n",
    "    for _ in range(optimisation_iters):\n",
    "        '''\n",
    "        In this section you implement an MPC with CEM method.  In each optimization iteration,\n",
    "        you sample actions sequences from a normal distribution, Calculate the cost for each\n",
    "        sequence using the learnt dyanmics and reward models. We will use a MPC similar to previous\n",
    "        section to replan at every timestep.\n",
    "        '''\n",
    "        sampled_action_sequences =  # TODO: Sample from a Gaussian Using The Means and Standard Deviations\n",
    "        # array that contains the cumilative_rewards for all the sequence, set to zero intially before an optimization iteration\n",
    "        cumilative_rewards = np.zeros((num_sequences, 1))\n",
    "        m_obs = np.array([real_obs for _ in range(num_sequences)])\n",
    "\n",
    "        for t in range(horizon_length):\n",
    "            # sampled actions for each sequence\n",
    "            sampled_actions = sampled_action_sequences[:, t, :]\n",
    "            # scale the input\n",
    "            models_input = input_scaler.transform(np.concatenate([m_obs, sampled_actions], axis=1))\n",
    "\n",
    "            \n",
    "            pred_obs = # TODO compute the differences to the next state using the dynamics model\n",
    "            \n",
    "            pred_rew = # TODO compute the reward using the reward model\n",
    "            \n",
    "            pred_obs_unnormlized = # TODO inverse scaler transofrmation (Unnormalize the predicted differences)\n",
    "            \n",
    "            m_obs = # TODO and add pred_unnormalized_observation to the previous observation\n",
    "\n",
    "            assert (pred_rew.cpu().detach().numpy().shape == cumilative_rewards.shape)\n",
    "\n",
    "            # sum of the expected rewards\n",
    "            cumilative_rewards #TODO #keep on adding the rewards obtained so far\n",
    "\n",
    "            if t == 0:\n",
    "                first_sampled_actions = sampled_actions\n",
    "            \n",
    "        # TODO Select Top K Action Sequences (lets call them elite_sequences) that gave the highest cumilative reward\n",
    "\n",
    "\n",
    "        #TODO: Recalculate the mean and variances using the elite (topk) action sequences\n",
    "\n",
    "    ### >>>>>>>>>Your code ends here<<<<<<<<< ###\n",
    "    env_model.train(True)\n",
    "    rew_model.train(True)\n",
    "\n",
    "    # Best the position of the sequence with the higher reward\n",
    "    arg_best_reward = np.argmax(cumilative_rewards)\n",
    "    best_sum_reward = cumilative_rewards[arg_best_reward].squeeze()\n",
    "    # take the first action of this sequence\n",
    "    best_action = first_sampled_actions[arg_best_reward] #you can also choose the best action as the mean of the elites from last optimization iteration. \n",
    "                                                          # However we do a greedy step in the end where the best among the elites from last iteration is chosen.\n",
    "\n",
    "    return best_action, best_sum_reward\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 3: What is a Model Predictive Controller (MPC)? (2 Pts)\n",
    "\n",
    "Since our dynamics and reward models can be imperfect and things will never go perfectly ac-ording to plan, we adopt a model predictive control (MPC) approach. Explain in few lines whats the basic priniciple behind an MPC.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RzYWX5HcYVs"
   },
   "source": [
    "# Main Loop\n",
    "\n",
    "Under Default Hyperparameters, Run the Main MBRL Loop with two planners Random Shooting and CEM. You will compare the reward plots with both the planners.\n",
    "\n",
    "\\begin{aligned}\n",
    "&\\overline{\\text { Algorithm } \\mathbf{1} \\text { Model-based Reinforcement Learning }} \\\\\n",
    "&\\hline \\text { 1: gather dataset } \\mathcal{D}_{\\text {RAND }} \\text { of random trajectories } \\\\\n",
    "&\\text { 2: initialize empty dataset } \\mathcal{D}_{\\text {RL }} \\text {, and randomly initialize } f_{\\theta} \\\\\n",
    "&  \\text{ 3: } \\textbf{for} \\text{ iter=1 to max_iter } \\textbf{do}  \\\\\n",
    "&\\quad  \\quad \\quad \\textbf {Model Learning } \\\\\n",
    "&\\text { 4:} \\quad  \\quad \\text{ train } f_{\\theta}(\\mathbf{s}, \\mathbf{a}) \\text{ and } r_{\\theta}(\\mathbf{s}, \\mathbf{a}) \\text { by performing gradient descent on MSE Loss }  \\\\\n",
    "& \\quad \\quad  \\quad \\text { using } \\mathcal{D}_{\\text {RAND }} \\text { and } \\mathcal{D}_{\\text {RL }} \\\\\n",
    "&\\quad  \\quad \\quad \\textbf {Planning and More Experience Collection } \\\\\n",
    "&\\text { 5: } \\quad  \\quad  \\textbf { for } t=1 \\text { to } T \\textbf { do } \\\\\n",
    "&\\text { 6: } \\quad \\quad \\quad \\quad \\text { get agent's current state } \\mathbf{s}_{t} \\\\\n",
    "&\\text { 7: } \\quad \\quad \\quad \\quad \\text { use } f_{\\theta} \\text { and } r_{\\theta} \\text { to estimate optimal action sequence } \\mathbf{A}_{t}^{(H)} \\\\\n",
    "&\\text { 8: } \\quad \\quad \\quad \\quad \\text { execute first action } \\mathbf{a}_{t} \\text { from selected action sequence } \\\\\n",
    "&\\text { 9: } \\quad \\quad \\quad \\quad\\text { Add } \\{s_t,  s_{t+1}, r_t, a_t\\} \\text { to } \\mathcal{D}_{\\text {RL }} \\\\\n",
    "&\\text { 10: } \\quad \\text { end for } \\\\\n",
    "&\\text { 11: end for } \\\\\n",
    "&\\hline\n",
    "\\end{aligned}\n",
    "\n",
    "**Students should get two reward plots, one with the random shooting based planner (set planner = 'random_shooting') and another one with CEM planner (set planner = 'cem'). Expect equal or greater than 500 reward for both cases in 20 iterations. Please submit the saved figures for both planners with the default parameters given here.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9S1ymuhOySCN"
   },
   "source": [
    "# 'cuda' or 'cpu'\n",
    "device = 'cuda'\n",
    "planner = 'cem' #Set this to 'cem' if you want use CEM Planner"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Main loop hyper\n",
    "# Do Not Change This Block\n",
    "# Do not change This Block\n",
    "\n",
    "AGGR_ITER = 20 #Number Of Outer iterations \n",
    "STEPS_PER_AGGR = 500 #Minimum Number of steps of experiences to add to buffer in each iteration\n",
    "\n",
    "# Number of Randon Trajectories (Experiences) To Collect for $D_{random}$\n",
    "NUM_RAND_TRAJECTORIES = 100\n",
    "\n",
    "\n",
    "# Supervised Model Hyperp\n",
    "ENV_LEARNING_RATE = 1e-3\n",
    "REW_LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 2000\n",
    "TRAIN_ITER_MODEL = 55\n",
    "\n",
    "# Controller Hyperp\n",
    "HORIZION_LENGTH = 15\n",
    "NUM_ACTIONS_SEQUENCES = 200\n",
    "\n",
    "save_video_test = True\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "date_time = \"{}_{}.{}.{}\".format(now.day, now.hour, now.minute, now.second)\n",
    "env = gym.make('InvertedPendulumSwingupPyBulletEnv-v0')\n",
    "obs = env.reset()\n",
    "\n",
    "#Step 1:  gather the dataset of random sequences\n",
    "print(\">>>>>>>>>>>>>>Gathering Random Trajectories to train Dynamics and Reward Models\")\n",
    "rand_dataset = gather_random_trajectories(NUM_RAND_TRAJECTORIES, env)\n",
    "\n",
    "rl_dataset = []\n",
    "mean_rew_list = []\n",
    "\n",
    "#Step 2: Initialize the models and itarate over model learning , planning and on policy experience collection\n",
    "env_model = NNDynamicModel(env.action_space.shape[0] + env.observation_space.shape[0],\n",
    "                          env.observation_space.shape[0]).to(device)\n",
    "rew_model = NNRewardModel(env.action_space.shape[0] + env.observation_space.shape[0], 1).to(device)\n",
    "\n",
    "game_reward = 0\n",
    "num_examples_added = len(rand_dataset)\n",
    "\n",
    "for n_iter in range(AGGR_ITER):\n",
    "\n",
    "  # supervised training of the dataset (random and rl if it exists)\n",
    "  train_env_loss, train_rew_loss, valid_env_loss, valid_rew_loss, standardizers = train_dyna_model(rand_dataset,\n",
    "                                                                                          rl_dataset,\n",
    "                                                                                          env_model,\n",
    "                                                                                          rew_model,\n",
    "                                                                                          BATCH_SIZE,\n",
    "                                                                                          TRAIN_ITER_MODEL,\n",
    "                                                                                          num_examples_added,\n",
    "                                                                                          ENV_LEARNING_RATE,\n",
    "                                                                                          REW_LEARNING_RATE,\n",
    "                                                                                          device)\n",
    "  print('{} >> Eloss:{:.4f} EV loss:{:.4f} -- Rloss:{:.4f} RV loss:{:.4f}'.format(n_iter, train_env_loss,\n",
    "                                                                                  valid_env_loss,\n",
    "                                                                                  train_rew_loss,\n",
    "                                                                                  valid_rew_loss))\n",
    "  env = wrap_env(gym.make('InvertedPendulumSwingupPyBulletEnv-v0'))\n",
    "  obs = env.reset()\n",
    "\n",
    "  num_examples_added = 0\n",
    "  game_reward = 0\n",
    "  game_pred_rews = []\n",
    "  rews = []\n",
    "\n",
    "  while num_examples_added < STEPS_PER_AGGR:\n",
    "      while True:\n",
    "          tt = time.time()\n",
    "          # Execute the control to roll the sequences and pick the first action of the sequence with the higher reward\n",
    "          if planner == 'random_shooting':\n",
    "              action, pred_rew = random_shooting_based_control(env_model, rew_model, obs, NUM_ACTIONS_SEQUENCES,\n",
    "                                                    HORIZION_LENGTH, env.action_space.sample, standardizers, device)\n",
    "          elif planner == 'cem':\n",
    "              action, pred_rew = cem_based_control(env_model, rew_model, obs, NUM_ACTIONS_SEQUENCES,\n",
    "                                                    HORIZION_LENGTH, env.action_space.sample, standardizers, device)\n",
    "          else:\n",
    "              raise ValueError(\"planner must be random_shooting/cem\")\n",
    "          game_pred_rews.append(pred_rew)\n",
    "\n",
    "          # one step in the environment with the action returned by the controller\n",
    "          new_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "          input_scaler, env_output_scaler, rew_output_scaler = standardizers\n",
    "\n",
    "          ## Compute the reward and print some stats\n",
    "          models_input = input_scaler.transform([np.concatenate([obs, action])])\n",
    "          rew_model.eval()\n",
    "          p_rew = rew_model(torch.tensor(models_input).to(device))\n",
    "          rew_model.train(True)\n",
    "          unnorm_rew = rew_output_scaler.inverse_transform(p_rew.cpu().detach().numpy())\n",
    "          if num_examples_added == 0:\n",
    "              print('Steps taken with MPC Planner:', num_examples_added + 1, end='->')\n",
    "          elif num_examples_added % 25 == 0:\n",
    "              print(num_examples_added + 1, end='->')\n",
    "          rl_dataset.append([obs, new_obs, reward, action])\n",
    "\n",
    "          num_examples_added += 1\n",
    "          obs = new_obs\n",
    "          game_reward += reward\n",
    "\n",
    "          # if the environment is done, reset it and print some stats\n",
    "          if done:\n",
    "              print(num_examples_added + 1, end='->')\n",
    "              obs = env.reset()\n",
    "              \n",
    "              rews.append(game_reward)\n",
    "              game_reward = 0\n",
    "              game_pred_rews = []\n",
    "              break\n",
    "\n",
    "  print('#########################Total Episodic Reward Obtained With Current Policy At Iteration',n_iter,' = ', rews)\n",
    "  mean_rew_list.append(rews)\n",
    "  env.close()\n",
    "  show_video()\n",
    "\n",
    "## Plot the Reward Curves\n",
    "fig, ax = plt.subplots(1, figsize=(10,6))\n",
    "sns.set_style(\"darkgrid\")\n",
    "idxs = range(len(mean_rew_list))\n",
    "plt.plot(idxs, mean_rew_list)\n",
    "if planner == \"random_shooting\":\n",
    "    plt.title('Random Shooting')\n",
    "else:\n",
    "    plt.title('CEM Control')\n",
    "plt.ylabel('Rewards')\n",
    "plt.xlabel('Iterations')\n",
    "save_figure('Deep_Dynamics_Reward_'+planner)\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FRIS_WMayXkc",
    "outputId": "9893daf4-af50-4953-9a5a-d614140b807f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jNngsO5A4iR"
   },
   "source": []
  }
 ]
}
