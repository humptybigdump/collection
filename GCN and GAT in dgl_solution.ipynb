{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network Basic "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Neural Network (GNN) is a generalisation of Deep Neural Network (DNN) on graph-strucutred. In this session, we will explain common graph neural networks such as GCN and GAT, and how to use graph neural networks to accomplish node classification and graph classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GCN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 GCN in Matrix form "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please what you have learned in each layer of the GCN. Given a graph with an adjacency matrix $A$ and a node feature matrix $X$, if we use $W$ to denote the parameters in the GCN layer and $H$ to denote the features in the hidden layer, then the operation in a layer of GCN can be expressed as $H=\\text{ReLU}(AXW)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please import toolkits \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from sklearn.metrics import f1_score\n",
    "import scipy.sparse as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"GCN layer，Refer to https://github.com/tkipf/pygcn\n",
    "    \n",
    "    Params\n",
    "    ----------\n",
    "    in_features : dim of input feature\n",
    "    out_features : output channel of feature (the same as out_channel in CNN)\n",
    "    with_bias: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, with_bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features)) # Create weight matrix in shape (in_features, out_features)\n",
    "        if with_bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self): \n",
    "        \"\"\"Initializing your weight params through any distribution\"\"\"\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"Forward\"\"\"\n",
    "        # Your Code\n",
    "        if x.data.is_sparse:\n",
    "            # sparse matrix \n",
    "            support = torch.spmm(x, self.weight) # XW\n",
    "        else:\n",
    "            support = torch.mm(x, self.weight) # XW\n",
    "        output = torch.spmm(adj, support) # AXW\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias # AXW + b\n",
    "        else:\n",
    "            return output # AXW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\" two layers GCN\n",
    "    \n",
    "    Params\n",
    "    ----------\n",
    "    nfeat : input dim of feature\n",
    "    nhid : dim of hidden neuron\n",
    "    nclass : number of classes\n",
    "    dropout : dropout probability (less than 0.3)\n",
    "    with_bias: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout=0.5, with_bias=True):\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.nfeat = nfeat\n",
    "        self.hidden_sizes = [nhid]\n",
    "        self.nclass = nclass\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid, with_bias=with_bias)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass, with_bias=with_bias)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # The implementation of GCN is the same as CNN\n",
    "        # Your architecture should be input -> gc1 -> Act -> dropout -> gc2 -> softmax/logsoftmax\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"initializing \"\"\"\n",
    "        self.gc1.reset_parameters()\n",
    "        self.gc2.reset_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Classification using GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "dataset = Planetoid(root='./data', name='Cora') \n",
    "data = dataset[0]\n",
    "adj = to_scipy_sparse_matrix(data.edge_index)\n",
    "features = data.x\n",
    "labels = data.y   \n",
    "labels.max()\n",
    "adj.row\n",
    "adj.col\n",
    "adj.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize： \n",
    "\n",
    "$A \\leftarrow A + I$\n",
    "\n",
    "$\\hat{A}= D^{-1/2}{A}D^{-1/2}$ \n",
    "\n",
    "Laplacian Matrix: \n",
    "$L = D^{-\\frac12}(D-A)D^{-\\frac12} = I - D^{-\\frac12}AD^{-\\frac12}$  \n",
    "\n",
    "$h=Lf=(D-A)f=Df-Af$ \n",
    "\n",
    "$h[i]=\\sum_{v_j \\in \\mathcal{N}(v_i)}(f[i]-f[j])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(mx):\n",
    "    \"\"\"Standarize：A' = (D + I)^-1/2 * ( A + I ) * (D + I)^-1/2\n",
    "    \"\"\"\n",
    "    mx = mx + sp.eye(mx.shape[0])\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1/2).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    mx = mx.dot(r_mat_inv)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 2708)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_norm = normalize_adj(adj)\n",
    "adj_norm.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"transform scipy.sparse into torch 's sparse tensor\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    sparserow=torch.LongTensor(sparse_mx.row).unsqueeze(1)\n",
    "    sparsecol=torch.LongTensor(sparse_mx.col).unsqueeze(1)\n",
    "    sparseconcat=torch.cat((sparserow, sparsecol),1)\n",
    "    sparsedata=torch.FloatTensor(sparse_mx.data)\n",
    "    return torch.sparse.FloatTensor(sparseconcat.t(),sparsedata,torch.Size(sparse_mx.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_norm = sparse_mx_to_torch_sparse_tensor(adj_norm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implemetation with one example, good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 7])\n",
      "tensor([[-1.6163, -2.1519, -2.2419,  ..., -1.7424, -1.9223, -2.1807],\n",
      "        [-1.3507, -2.2947, -2.0696,  ..., -1.7330, -1.8976, -2.1724],\n",
      "        [-1.4260, -2.2138, -2.4253,  ..., -1.6594, -1.8275, -2.3899],\n",
      "        ...,\n",
      "        [-1.5953, -1.5008, -2.6109,  ..., -2.5341, -1.6702, -2.3943],\n",
      "        [-1.7057, -2.1982, -2.1837,  ..., -1.9552, -1.8260, -2.2074],\n",
      "        [-1.7169, -2.2505, -2.1469,  ..., -1.9051, -1.8991, -2.0919]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nclass = labels.max().item()+1\n",
    "model = GCN(nfeat=features.shape[1], nhid=16, nclass=nclass)\n",
    "output = model(features, adj_norm)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 GCN in PyTorch Geometric \n",
    "Compare with matrix form GCN, adj is equivalent to edge_index and edge_weight while node features is called x.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    \"\"\" Two Layers' GCN\n",
    "    \n",
    "    Params\n",
    "    ----------\n",
    "    nfeat : dimension of input \n",
    "    nhid : dim of hidden neurons\n",
    "    nclass : number of classes ground truth\n",
    "    dropout : dropout probability\n",
    "    with_bias: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout=0.5, with_bias=True):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(nfeat, nhid, bias=with_bias, activation=F.relu)\n",
    "        self.conv2 = GCNConv(nhid, nclass, bias=with_bias)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize\"\"\"\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, data, features=None):\n",
    "        \"\"\"Forward Your code\"\"\"\n",
    "        x, edge_index = data.x, data.edge_index \n",
    "        x = F.relu(self.conv1(x, edge_index)) \n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.8455, -1.9122, -2.0024,  ..., -1.9261, -1.8196, -2.0608],\n",
      "        [-1.9529, -1.8418, -2.0281,  ..., -1.9211, -1.8361, -2.0128],\n",
      "        [-1.9458, -1.9027, -2.0004,  ..., -1.9212, -1.8071, -2.0033],\n",
      "        ...,\n",
      "        [-1.9190, -1.9139, -2.0323,  ..., -1.8697, -1.8866, -2.0042],\n",
      "        [-2.0191, -1.9901, -2.0172,  ..., -1.7597, -1.9056, -1.9327],\n",
      "        [-1.9957, -1.9738, -2.0045,  ..., -1.7931, -1.9316, -1.9390]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([2708, 7])\n"
     ]
    }
   ],
   "source": [
    "nclass = labels.max().item()+1\n",
    "model = GCN(nfeat=features.shape[1], nhid=16, nclass=nclass)\n",
    "output = model(data)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN Variant\n",
    "\n",
    "\\begin{equation}\n",
    "e_{ij} = a(\\mathbf{W_l}\\overrightarrow{h_i}, \\mathbf{W_r} \\overrightarrow{h_j})\n",
    "\\end{equation}\n",
    "\n",
    "Masked Attention:\n",
    "softmax function:\n",
    "\\begin{equation}\n",
    "\\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{ij} = \\frac{\\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i} + \\overrightarrow{a_r}^T\\mathbf{W_r}\\overrightarrow{h_j}\\Big)\\Big)}{\\sum_{k\\in \\mathcal{N}_i} \\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i} + \\overrightarrow{a_r}^T\\mathbf{W_r}\\overrightarrow{h_k}\\Big)\\Big)}\n",
    "\\end{equation}\n",
    "\n",
    "Now, we use the normalized attention coefficients to compute a linear combination of the features corresponding to them. These aggregated features will serve as the final output features for every node.\n",
    "\n",
    "\\begin{equation}\n",
    "h_i' = \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij} \\mathbf{W_r} \\overrightarrow{h_j}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "class GAT(nn.Module):\n",
    "    \"\"\" Two layers' GAT.\n",
    "    \n",
    "    Params\n",
    "    ----------\n",
    "    nfeat : dim of input features\n",
    "    nhid : dim of hidden neurons\n",
    "    nclass : output of classes\n",
    "    heads: number of head in attention mechanism\n",
    "    output_heads: output head\n",
    "    dropout : dropout probability\n",
    "    with_bias: with or without bias\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, heads=8, output_heads=1, dropout=0.5, with_bias=True):\n",
    "\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.conv1 = GATConv(\n",
    "            nfeat,\n",
    "            nhid,\n",
    "            heads=heads,\n",
    "            dropout=dropout,\n",
    "            bias=with_bias)\n",
    "\n",
    "        self.conv2 = GATConv(\n",
    "            nhid * heads,\n",
    "            nclass,\n",
    "            heads=output_heads,\n",
    "            concat=False,\n",
    "            dropout=dropout,\n",
    "            bias=with_bias)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize\n",
    "        \"\"\"\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.8640, -1.7439, -1.7616, -1.7624, -1.8578, -1.7678],\n",
      "        [-1.8386, -1.7945, -1.6146, -1.9597, -1.6796, -1.9076],\n",
      "        [-1.5514, -2.1524, -1.6537, -1.8335, -2.0381, -1.6588],\n",
      "        ...,\n",
      "        [-1.8890, -1.2048, -2.4622, -2.2169, -1.7755, -1.6851],\n",
      "        [-1.9284, -1.6651, -1.8490, -1.8591, -1.7979, -1.6788],\n",
      "        [-2.1232, -1.3792, -1.9038, -1.9034, -1.7307, -1.8750]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([2708, 6])\n"
     ]
    }
   ],
   "source": [
    "gat = GAT(nfeat=features.shape[1],\n",
    "      nhid=8, heads=8,\n",
    "      nclass=nclass)\n",
    "output = gat(data)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, lr=0.01, weight_decay=5e-4, epochs=200):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    labels = data.y\n",
    "    train_mask = data.train_mask\n",
    "    best_loss_val = 100\n",
    "\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # only training nodes will be used to calculate loss \n",
    "        loss = F.nll_loss(output[train_mask], labels[train_mask]) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch {}, training loss: {}'.format(i, loss.item()))\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    \"\"\"Evaluate GAT performance on test set.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_mask = data.test_mask\n",
    "    labels = data.y \n",
    "    output = model(data) \n",
    "    loss_test = F.nll_loss(output[test_mask], labels[test_mask])\n",
    "    preds = output[test_mask].argmax(1) \n",
    "    acc_test = preds.eq(labels[test_mask]).cpu().numpy().mean() \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test))\n",
    "    return preds, output, acc_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 1.9424370527267456\n",
      "Epoch 10, training loss: 0.7486507296562195\n",
      "Epoch 20, training loss: 0.25204676389694214\n",
      "Epoch 30, training loss: 0.10805542021989822\n",
      "Epoch 40, training loss: 0.06834225356578827\n",
      "Epoch 50, training loss: 0.044421691447496414\n",
      "Epoch 60, training loss: 0.048820436000823975\n",
      "Epoch 70, training loss: 0.0476372055709362\n",
      "Epoch 80, training loss: 0.03384850546717644\n",
      "Epoch 90, training loss: 0.029627133160829544\n"
     ]
    }
   ],
   "source": [
    "model = GCN(nfeat=features.shape[1], nhid=16, nclass=nclass)\n",
    "device = 'cpu' # device ='cuda'\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "train(model, data, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.6074 accuracy= 0.8080\n"
     ]
    }
   ],
   "source": [
    "preds, output, acc = test(model, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Classification using GCN\n",
    "In Graph Classification, each label is related to the whole graph but not the embedding vector of each node. We use pooling and a linear layer to generate this label.\n",
    "\n",
    "## Dataset [Enzymes](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.TUDataset.html#torch_geometric.datasets.TUDataset)\n",
    "In ENZYMES dataset, there are 6 classes as ground truth, 18 continuous node features and three different kinds of node types as feature matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip\n",
      "Extracting data/ENZYMES/ENZYMES/ENZYMES.zip\n",
      "Processing...\n",
      "Done!\n",
      "/Users/chenshao/miniconda3/envs/gnn/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "dataset = TUDataset(root='./data/ENZYMES', name='ENZYMES', use_node_attr=True)\n",
    "dataset = dataset.shuffle()\n",
    "train_ratio = 0.8 \n",
    "test_ratio = 0.2\n",
    "train_dataset = dataset[: int(train_ratio*len(dataset))]\n",
    "test_dataset = dataset[-int(test_ratio*len(dataset)):]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 50], x=[14, 21], y=[1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    \"\"\" 3 layers' GCN + Linar layers\"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout=0.2):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.gc1 = GCNConv(nfeat, nhid)\n",
    "        self.gc2 = GCNConv(nhid, nhid)\n",
    "        self.gc3 = GCNConv(nhid, nhid)\n",
    "        self.lin = nn.Linear(nhid, nclass) # output of the whole graph\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.gc1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc2(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training) \n",
    "        x = self.gc3(x, edge_index)\n",
    "        x = global_mean_pool(x, batch=data.batch) \n",
    "        x = self.lin(x) \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeat = dataset.num_node_features\n",
    "nclass = dataset.num_classes\n",
    "nhid = 64\n",
    "device = 'cpu'\n",
    "model = GCN(nfeat, nhid, nclass).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, lr=0.001, epochs=1000):\n",
    "    \"\"\"train loop\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loss_all = 0\n",
    "        for data in (train_loader): \n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_model = model(data)\n",
    "            loss = F.nll_loss(y_model, data.y)\n",
    "            loss.backward()\n",
    "            loss_all += loss.item() * data.num_graphs\n",
    "            optimizer.step()\n",
    "        loss_train = loss_all / len(train_loader.dataset) \n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch: {:03d}, Loss: {:.7f}'.format(epoch, loss_train))\n",
    "\n",
    "@torch.no_grad()            \n",
    "def test(model, loader):\n",
    "    \"\"\"test loop\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "\n",
    "        _, pred = model(data).max(dim=1)\n",
    "        correct += float(pred.eq(data.y).sum().item())\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 2.0274307\n",
      "Epoch: 100, Loss: 1.5137738\n",
      "Epoch: 200, Loss: 1.2263177\n",
      "Epoch: 300, Loss: 0.9777098\n",
      "Epoch: 400, Loss: 0.7704745\n",
      "Epoch: 500, Loss: 0.6072058\n",
      "Epoch: 600, Loss: 0.4968923\n",
      "Epoch: 700, Loss: 0.3625646\n",
      "Epoch: 800, Loss: 0.3085780\n",
      "Epoch: 900, Loss: 0.2164070\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6333333333333333"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "619993b1dd7dc5a8ffd0ce1e4939e3790e46a66a33a4b854e6e47a9bd061fee8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
