{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Examples and Calculations with Convolutions\n"
      ],
      "metadata": {
        "id": "kpbowDKaAY4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch # Pytorch is one of the most used machine learning frameworks in academia\n",
        "import torch.nn as nn # nn contains the neural network functions of pytorch\n",
        "import torchvision # For Image Processing\n",
        "from torchvision import datasets, models, transforms\n",
        "import os # load data\n",
        "import numpy as np # work with matrices\n",
        "from PIL import Image # Pillow is a commonly used Image Processing Library \n",
        "import matplotlib.pyplot as plt #for visualization\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "JPY1UcEcBBWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss Functions.\n",
        "\n",
        "In Training we need Loss / Error Functions that define \"how well our network is currently performing\". We use backpropagation with gradient descent to minimize the loss and increase the performance of our network.\n",
        "\n",
        "\n",
        "The most commonly used loss functions are Mean Squared Error and Crossentropy.\n",
        "\n",
        "- MSE is used for regressive tasks (e.g. how big is a bounding box)\n",
        "- Crossentropy is used for classification (e.g. which class is inside bounding box)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aq_L7JK2bbfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MSE\n",
        "$L_\\text{MSE}=\\frac{1}{n}\\sum_{i=1}^n(\\text{prediction}_i-\\text{label}_i)^2$\n",
        "\n",
        "Let's say we have Prediction\n",
        "\n",
        "[2 3 4]\n",
        "\n",
        "and label\n",
        "\n",
        "[2 4  1]\n",
        "\n",
        "The MSE is calculated as \n",
        "\n",
        "$\\frac{(2-2)^2 + (3-4)^2+(4-1)^2}{3}$\n",
        "= $\\frac{0^2 + (-1)^2+3^2}{3} = \\frac{10}{3} $"
      ],
      "metadata": {
        "id": "l7SZKzskwltp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = torch.tensor([2.0, 3.0, 4.0])\n",
        "label = torch.tensor([2.0, 4.0, 1.0])\n",
        "mse_criterion = nn.MSELoss()\n",
        "mse_loss = mse_criterion(prediction, label)\n",
        "mse_loss"
      ],
      "metadata": {
        "id": "q2ujbc9ud49v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crossentropy Loss\n",
        "$L_\\text{CE}=-\\sum_{i=1}^n\\text{label}_i \\cdot \\text{log} (\\text{prediction}_i)$\n",
        "\n",
        "Softmax is always applied beforehand to prediction\n",
        "\n",
        "$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{k} e^{z_k}}$\n",
        "\n",
        "\n",
        "E.g. we have $ \\text{prediction} = [4 \\quad 2 \\quad 1]$ and $\\text{label} = [1 \\quad 0 \\quad 0]$\n",
        "\n",
        "Then Softmax calculates\n",
        "\n",
        "$[ \\frac{e^4}{e^4+e^2+e^1} \\quad \\frac{e^2}{e^4+e^2+e^1}  \\quad \\frac{e^1}{e^4+e^2+e^1}]$\n",
        "\n",
        "$[0.844 \\quad  0.114 \\quad 0.042]$\n",
        "\n"
      ],
      "metadata": {
        "id": "HG94ZJz8gHQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sm = nn.Softmax(dim=1)\n",
        "prediction = torch.tensor([[4.0, 2.0, 1.0],])\n",
        "\n",
        "softmaxed_pred = sm(prediction)\n",
        "print(\"softmax of [4 2 0] is \", softmaxed_pred)"
      ],
      "metadata": {
        "id": "ay4zpqVxoDup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the crossentropy loss is\n",
        "\n",
        "$L_\\text{CE}= -(1 \\cdot \\text{log}(0.844) + 0 \\cdot \\text{log}(0.114) + 0 \\cdot \\text{log}(0.042)) = -\\text{log}(0.844) = 0.169$\n"
      ],
      "metadata": {
        "id": "hqPsSuIiruCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = torch.tensor([[1.0, 0.0, 0.0]])\n",
        "ce_criterion = nn.CrossEntropyLoss() # This function automatically applies softmax\n",
        "ce_loss = ce_criterion(prediction, label)\n",
        "print(\"Crossentropy loss is\", ce_loss)"
      ],
      "metadata": {
        "id": "cQD12Gumrxhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parameters**"
      ],
      "metadata": {
        "id": "e3abXtWmw05m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say we want to classify images into 5 possible classes.\n",
        "The images have size $32\\times32$ and are in RGB.\n",
        "For this classification we want to use 2 Convolutional Layer and one fully connected layer.\n",
        "- The first convolutional layer has 16 filters, stride 2 and a 5$\\times$5 kernel\n",
        "- The second convolutional layer has 32 filters, stride 1, a 3$\\times3$ kernel\n",
        "and padding to keep the same feature map resolution\n",
        "\n",
        "Calculate the number of trainable parameters!"
      ],
      "metadata": {
        "id": "K54pUEOSCL3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate parameters of convolutional Layers\n",
        "Number of parameters per conv.layer can be calculated as follows:\n",
        "\n",
        "$\\text{number parameters} =\\text{numberFilters} \\times \n",
        "(\\text{kernelWidth} \\times \\text{kernelHeight} \\times \\text{inputChannels} + 1)$\n",
        "\n",
        "For the first convolutional layer we get:\n",
        "\n",
        "$16 \\cdot (5 \\cdot 5 \\cdot 3 + 1) = 1216 \\text{ Parameters}$ \n",
        "\n",
        "For the second convolutional layer we get:\n",
        "\n",
        "$32 \\cdot  (3 \\cdot 3 \\cdot 16 + 1) = 4640 \\text{ Parameters}$\n"
      ],
      "metadata": {
        "id": "oVrVvwnrDL5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate parameters of fully connected layer\n",
        "\n",
        "The number of parameters in convolutional layers is independent of the input  resolution. But for the fully connected layer we need to know the resolution of the incoming feature map.\n",
        "\n",
        "We can calculate the resolution after a convolution with:\n",
        "\n",
        "$\\text{new resolution} = ⌊\\frac{\\text{prev feature map size} - \\text{kernel size} }{stride} + 1⌋$\n",
        "\n",
        "Dimension of first featuremap (original image):\n",
        "\n",
        " $\\quad 32\\times32\\times3$\n",
        "\n",
        "Resolution after first convolutional layer:\n",
        "\n",
        "$\\quad ⌊\\frac{32-5}{2}+1⌋ = ⌊14.5⌋ = 14$\n",
        "\n",
        "Dimension of feature map after first convolutional layer:\n",
        "\n",
        "$ \\quad 14 \\times 14 \\times 16$\n",
        "\n",
        "Second convolution uses padding to keep the resolution identical but increases number of filters. So the dimension after the second convolutional layer is:\n",
        "\n",
        "$ \\quad 14 \\times 14 \\times 32$\n",
        "\n",
        "Now we can calculate the number of trainable parameters of the fully connected layer:\n",
        "\n",
        "$\\text{number parameters} = (\\text{input features} + 1) \\times \\text{number neurons} $\n",
        "\n",
        "which is:\n",
        "\n",
        "$\\quad (14 \\cdot 14 \\cdot 32 + 1) \\cdot 5 = 31365$\n"
      ],
      "metadata": {
        "id": "aHAJodsWGcko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we have\n",
        "\n",
        "1216 + 4640 + 31365 = 37221 parameter.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yEYN_vXyLxDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "              in_channels= 3,\n",
        "              out_channels=16,\n",
        "              kernel_size=5,\n",
        "              stride=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(\n",
        "              in_channels= 16,\n",
        "              out_channels=32,\n",
        "              kernel_size=3,\n",
        "              stride=1,\n",
        "              padding=\"same\",\n",
        "              padding_mode = \"zeros\"),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(\n",
        "               in_features=14*14*32,\n",
        "               out_features=5),\n",
        "        nn.Softmax()  \n",
        ")\n",
        "\n",
        "\n",
        "sum_of_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"this model has {sum_of_trainable_params} parameters\")"
      ],
      "metadata": {
        "id": "GZsyZZ9_GvUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning\n",
        "\n",
        "Using pre-trained models for new tasks to increase training speed.\n",
        "We will use a model trained on ImageNet to classify images into classes that don't exist in ImageNet."
      ],
      "metadata": {
        "id": "cjujuZopolGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip alienvspredator.zip"
      ],
      "metadata": {
        "id": "KhKPehkHtp76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "alien_root = \"alienvspredator/train/alien/\"\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(Image.open(alien_root + sorted(os.listdir(alien_root))[i]))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F-w--_zpwjSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "predator_root = \"alienvspredator/train/predator/\"\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(Image.open(predator_root + sorted(os.listdir(predator_root))[i]))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "14es1W4kx_cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Pytorch Dataset.\n",
        "We use the normalization of ImageNet for our images and crop them to 224x224.\n",
        "We use Batchsize 32."
      ],
      "metadata": {
        "id": "65713fn2zSGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "data_transforms = {\n",
        "    'train':\n",
        "        transforms.Compose([\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            normalize]),\n",
        "    'validation':\n",
        "        transforms.Compose([\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            normalize])}\n",
        "\n",
        "image_datasets = {\n",
        "    'train':\n",
        "        datasets.ImageFolder('alienvspredator/train', data_transforms['train']),\n",
        "    'validation':\n",
        "        datasets.ImageFolder('alienvspredator/validation', data_transforms['validation'])}\n",
        "\n",
        "dataloaders = {\n",
        "    'train':\n",
        "        torch.utils.data.DataLoader(\n",
        "            image_datasets['train'],\n",
        "            batch_size=32,\n",
        "            shuffle=True,\n",
        "            num_workers=1),\n",
        "    'validation':\n",
        "        torch.utils.data.DataLoader(\n",
        "            image_datasets['validation'],\n",
        "            batch_size=32,\n",
        "            shuffle=False,\n",
        "            num_workers=1)}"
      ],
      "metadata": {
        "id": "JHZoVhFpqAD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Pretrained Model"
      ],
      "metadata": {
        "id": "8nWjdjLIzXzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"RUNNING ON \", device) # should be cuda\n",
        "\n",
        "def get_model_and_optimizer(pretrained=True):\n",
        "    # load model architecture from pytorch\n",
        "    # if pretrained == true, then also load imagenet weights. otherwise use kaiming initialization\n",
        "    model = models.efficientnet_b0(pretrained=pretrained).to(device)\n",
        "\n",
        "    if pretrained:\n",
        "        # Don't change weights of pretrained convolutional filters\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # create a new classier\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Linear(1280, 128),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(128, 2)).to(device)\n",
        "\n",
        "    # use adam and give it the trainable parameters\n",
        "    if pretrained:\n",
        "        optimizer = torch.optim.Adam(model.classifier.parameters())\n",
        "    else:\n",
        "        optimizer = torch.optim.Adam(model.parameters())\n",
        "    return model, optimizer\n"
      ],
      "metadata": {
        "id": "ClGfwP-nzWyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, num_epochs=3):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in dataloaders['train']:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(image_datasets[\"train\"])\n",
        "\n",
        "        print(f\"epoch loss is = {epoch_loss}\")\n",
        "    return model\n",
        "\n",
        "tl_model, optimizer = get_model_and_optimizer()\n",
        "tl_model_trained = train_model(tl_model, optimizer, num_epochs=5)"
      ],
      "metadata": {
        "id": "JbBiFfzx0wF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(model):\n",
        "    with torch.no_grad():\n",
        "        a_valid = \"alienvspredator/validation/alien/\"\n",
        "        p_valid = \"alienvspredator/validation/predator/\"\n",
        "        validation_img_paths = [a_valid + x for x in sorted(os.listdir(a_valid))]\n",
        "        validation_img_paths += [p_valid + x for x in sorted(os.listdir(p_valid))]\n",
        "\n",
        "        img_list = []\n",
        "        for img_path in validation_img_paths:\n",
        "            i = Image.open(img_path)\n",
        "            if i.mode == \"RGB\":\n",
        "                img_list.append(i)\n",
        "                \n",
        "        validation_batch = torch.stack([data_transforms['validation'](img).to(device)\n",
        "                                        for img in img_list])\n",
        "\n",
        "        pred_logits_tensor = model(validation_batch)\n",
        "        sm = nn.Softmax(dim=1)\n",
        "        pred_probs = sm(pred_logits_tensor).cpu().data.numpy()\n",
        "        alien_argmax = (-pred_probs[:,0]).argsort()[:5]\n",
        "\n",
        "        pred_argmax = (-pred_probs[:,1]).argsort()[:5]\n",
        "\n",
        "\n",
        "\n",
        "        fig, axs = plt.subplots(2, 5, figsize=(20, 5))\n",
        "\n",
        "        for idx, i in enumerate(alien_argmax):\n",
        "            img = img_list[i]\n",
        "            ax = axs[0,idx]\n",
        "            ax.axis('off')\n",
        "            ax.set_title(\"{:.0f}% Alien, {:.0f}% Predator\".format(100*pred_probs[i,0],\n",
        "                                                                100*pred_probs[i,1]))\n",
        "            ax.imshow(img)\n",
        "\n",
        "        for idx, i in enumerate(pred_argmax):\n",
        "            img = img_list[i]\n",
        "            ax = axs[1,idx]\n",
        "            ax.axis('off')\n",
        "            ax.set_title(\"{:.0f}% Alien, {:.0f}% Predator\".format(100*pred_probs[i,0],\n",
        "                                                                100*pred_probs[i,1]))\n",
        "            ax.imshow(img)\n",
        "\n",
        "validation(tl_model_trained)"
      ],
      "metadata": {
        "id": "JXrvzhXf3mOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training should be much slower and worse if we don't use pretrained weights. Let's try it with default Kaiming initialization."
      ],
      "metadata": {
        "id": "P_wmPyVSCUvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "default_model, optimizer = get_model_and_optimizer(False)\n",
        "default_model_trained = train_model(default_model, optimizer, num_epochs=5)\n",
        "validation(default_model_trained)"
      ],
      "metadata": {
        "id": "TVRmYwn2CbRT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}