{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38b65e-7a85-4fa6-810e-8eaf6016c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download: https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c820b312-f0ed-4623-8fdf-e518dd2c3108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# print(os.getcwd())\n",
    "# print(os.listdir())\n",
    "os.path.isfile('./aclImdb_v1.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d61268-93c5-4443-9fe4-ac26864b1415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "# tar = tarfile.open('./aclImdb.tar.gz', 'r')\n",
    "# tar.extractall('')\n",
    "# tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "214165e8-cb4f-47a2-b492-5d582563533d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "raw_trains = glob.glob('./aclImdb/train/pos/*.txt')\n",
    "len(raw_trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1cbc97e-d4bc-4a11-9c81-bee2b658cb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./aclImdb/train/pos\\\\0_9.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_trains[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6c7008-03a5-490d-a48e-533c458d7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# def read_text_files(directory):\n",
    "#     texts = []\n",
    "#     labels = []\n",
    "#     for sentiment in ['pos', 'neg']:\n",
    "#         sentiment_dir = os.path.join(directory, sentiment) \n",
    "#         for filepath in glob.iglob(os.path.join(sentiment_dir, '*.txt')):\n",
    "#             with open(filepath, 'r', encoding='utf-8') as file:\n",
    "#                 if len(texts) < 1000:   \n",
    "#                     texts.append(file.read())\n",
    "#                     labels.append(1 if sentiment == 'pos' else 0)\n",
    "#     return texts, labels\n",
    "\n",
    "# train_texts, train_labels = read_text_files('./aclImdb/train/')\n",
    "# test_texts, test_labels = read_text_files('./aclImdb/test/')\n",
    "# print(len(train_texts), len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "049eebb1-50f0-495c-8d4b-b34b779cf924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import random\n",
    "\n",
    "def read_text_files(directory, num_samples=1000):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        sentiment_dir = os.path.join(directory, sentiment)\n",
    "        files = glob.glob(os.path.join(sentiment_dir, '*.txt'))\n",
    "        \n",
    "        # Select random files\n",
    "        random_files = random.sample(files, min(num_samples, len(files)))\n",
    "        \n",
    "        for filepath in random_files:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                texts.append(file.read())\n",
    "                labels.append(1 if sentiment == 'pos' else 0)\n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = read_text_files('./aclImdb/train/')\n",
    "test_texts, test_labels = read_text_files('./aclImdb/test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9a463-5ae5-456f-866b-5b3a41f74577",
   "metadata": {},
   "source": [
    "<span style=\"color: gray; font-family: Helvetica; font-size: 30px;\">Wall time vs. CPU time:</span>\n",
    "\n",
    "### Wall time\n",
    "    The total time taken to complete a process from beginning to end, including all waiting times. On the other hand, CPU time represents the actual time the CPU spends executing a process, excluding any waiting periods for I/O operations or other processes. (baking analogy: gathering the ingredients until taking the cake out-> Wall time)\n",
    "\n",
    "    In a multiprocessor environment, CPU time can surpass wall time due to parallel execution. Wall time provides a sense of the total duration your program will run, while CPU time shows how much processing power your program utilizes. Both these times are essential to measure for understanding program performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae9bc7d4-3796-4e97-9d33-69e9f70c9b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d4b6f8-e36c-46b6-ab4d-9cf6a2bb8118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train_texts in bytes:  16552\n",
      "Total size of train_texts in kilobytes:  2652.63\n",
      "Total size of train_texts in megabytes:  2.5904550552368164\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"Size of train_texts in bytes: \", sys.getsizeof(train_texts))\n",
    "\n",
    "total_size = sys.getsizeof(train_texts) + sum(sys.getsizeof(i) for i in train_texts)\n",
    "\n",
    "total_size_kb = total_size / 1024\n",
    "total_size_mb = total_size_kb / 1024\n",
    "\n",
    "print(f\"Total size of train_texts in kilobytes: \", \"{:.2f}\".format(total_size_kb))\n",
    "print(\"Total size of train_texts in megabytes: \", total_size_mb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b30dd9-a9ed-4ea3-850a-8ed0b643e943",
   "metadata": {},
   "source": [
    "<span style=\"color: gray; font-family: Helvetica; font-size: 30px;\"> Getting the size of objects in Python:\n",
    " \n",
    "### sys.getsizeof(train_texts)\n",
    "     returns the size of the train_texts object itself, which includes the overhead of the list object and any additional attributes or metadata associated with it. It does not take into account the size of the elements contained within the list.\n",
    "\n",
    "### total_size\n",
    "    calculated by summing the size of train_texts obtained from sys.getsizeof(train_texts) with the size of each individual element within train_texts obtained using sys.getsizeof(i) in a generator expression. This provides a more accurate estimation of the total size, considering the memory used by both the list object and its elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9954ae01-2f76-4400-b65e-067745137dce",
   "metadata": {},
   "source": [
    "<span style=\"color: gray; font-family: Helvetica; font-size: 30px;\">Formatting floating-point numbers\n",
    "\n",
    "### {}: \n",
    "    These curly braces are used to enclose the field that will be replaced with the formatted value\n",
    "    \n",
    "### :: \n",
    "    It indicates the start of the format specification.\n",
    "    \n",
    "### .3f: \n",
    "    This is the format specifier itself. It consists of two parts:\n",
    "    \n",
    "### .: \n",
    "    The dot specifies the precision or the number of decimal places.\n",
    "    \n",
    "### 3f: \n",
    "    The f is the type specifier for a floating-point number, and 3 represents the number of decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dc1eaf-12eb-40e0-ae64-5f1162383a29",
   "metadata": {},
   "source": [
    "<span style=\"color: gray; font-family: Helvetica; font-size: 30px;\"> Preporcessing:\n",
    "\n",
    "<span style=\"color: black; font-family: Helvetica; font-size: 15px;\"> To clean and standardize the input, preprocessing is a crucial step when working with text data.\n",
    "\n",
    "### Tokenization: \n",
    "    This is the process of breaking down the text into individual words or tokens. We use nltk.tokenize.word_tokenize for this purpose. This function splits the text by whitespace and punctuation to create a list of individual words.\n",
    "\n",
    "### Lower casing: \n",
    "    We convert all the text to lower case. This is done to ensure that the same word in different cases (for example, \"Movie\" and \"movie\") is treated as the same word.\n",
    "\n",
    "### Punctuation Removal: \n",
    "    This step removes all punctuation from the text. Punctuation doesn't usually add much meaning to the text and can be removed. We check if each token is a punctuation symbol using string.punctuation.\n",
    "\n",
    "### Stop words Removal: \n",
    "    Stop words are common words like 'is', 'the', 'and', etc. that do not carry important meaning and are often removed from the text. We use a predefined list of stop words from NLTK, which can be accessed using nltk.corpus.stopwords.words('english').\n",
    "\n",
    "### Lemmatization: \n",
    "    This is the process of reducing a word to its base or root form. For example, 'running' is transformed to 'run', 'better' is changed to 'good'. It is a more advanced form of stemming because it considers the context and part of speech of the word. We use WordNetLemmatizer from NLTK for lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c24bacf-0655-4982-b512-9df79dc2563c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Xamani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Xamani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Xamani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c6782d1-1ae0-41dc-b88c-a74d98751b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation = string.punctuation + \"''\"\n",
    "def preprocess_text(text):\n",
    "    # Tokenize and lower case\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove punctuation and filter out stopwords, then lemmatize\n",
    "    processed_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in string.punctuation and token not in stop_words]\n",
    "    \n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5245d79a-b799-4fd6-b7f0-fb5f3b915074",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [preprocess_text(text) for text in train_texts]\n",
    "test_tokens = [preprocess_text(text) for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3185490-02f9-48ff-b3ba-0863f5c74fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47074c6e-4e1a-40bc-88c6-6cb9e55e5320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jim Carrey is good as usual, and even though there are quite a few \"Jim Carrey moments\", it's definitely not a \"Jim Carrey movie\".<br /><br />It's targeted mostly at children, and I managed to enjoy it as such movie. It was promoted in Israel as another Jim Carrey movie, so those who expected a weird over the top comedy were disappointed.<br /><br />The movie has nice moments and works well as a movie for kids. I can't say I LOVED this movie, but then again I'm not its target audience!\n"
     ]
    }
   ],
   "source": [
    "print(train_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69c04a86-9489-499b-ae01-b9e9f94e9e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jim carrey is good as usual, and even though there are quite a few \"jim carrey moments\", it's definitely not a \"jim carrey movie\".<br /><br />it's targeted mostly at children, and i managed to enjoy it as such movie. it was promoted in israel as another jim carrey movie, so those who expected a weird over the top comedy were disappointed.<br /><br />the movie has nice moments and works well as a movie for kids. i can't say i loved this movie, but then again i'm not its target audience!\n"
     ]
    }
   ],
   "source": [
    "print(train_texts[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e7ddde9-057c-48d6-ab5e-ea6592bbb8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jim', 'carrey', 'is', 'good', 'as', 'usual', ',', 'and', 'even', 'though', 'there', 'are', 'quite', 'a', 'few', '``', 'jim', 'carrey', 'moments', \"''\", ',', 'it', \"'s\", 'definitely', 'not', 'a', '``', 'jim', 'carrey', 'movie', \"''\", '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'it', \"'s\", 'targeted', 'mostly', 'at', 'children', ',', 'and', 'i', 'managed', 'to', 'enjoy', 'it', 'as', 'such', 'movie', '.', 'it', 'was', 'promoted', 'in', 'israel', 'as', 'another', 'jim', 'carrey', 'movie', ',', 'so', 'those', 'who', 'expected', 'a', 'weird', 'over', 'the', 'top', 'comedy', 'were', 'disappointed.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'movie', 'has', 'nice', 'moments', 'and', 'works', 'well', 'as', 'a', 'movie', 'for', 'kids', '.', 'i', 'ca', \"n't\", 'say', 'i', 'loved', 'this', 'movie', ',', 'but', 'then', 'again', 'i', \"'m\", 'not', 'its', 'target', 'audience', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(train_texts[0].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a63709d3-becd-469a-b9e4-34e491a56255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\'\\''"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cb2c42b-f8bd-456e-bdfc-2c9a87aa5f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jim', 'carrey', 'good', 'usual', 'even', 'though', 'quite', '``', 'jim', 'carrey', 'moment', \"'s\", 'definitely', '``', 'jim', 'carrey', 'movie', 'br', 'br', \"'s\", 'targeted', 'mostly', 'child', 'managed', 'enjoy', 'movie', 'promoted', 'israel', 'another', 'jim', 'carrey', 'movie', 'expected', 'weird', 'top', 'comedy', 'disappointed.', 'br', 'br', 'movie', 'nice', 'moment', 'work', 'well', 'movie', 'kid', 'ca', \"n't\", 'say', 'loved', 'movie', \"'m\", 'target', 'audience']\n"
     ]
    }
   ],
   "source": [
    "print([lemmatizer.lemmatize(token) for token in word_tokenize(train_texts[0].lower()) if token not in string.punctuation and token not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ea7ceb6-2fe5-4e04-b9a3-8a3777209c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation = string.punctuation + \"--\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "641d4f62-14cf-46bc-b212-11dca87da2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\'\\'--'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ffcfb45-7d0e-4fa0-a35b-d77c7230e430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jim', 'carrey', 'good', 'usual', 'even', 'though', 'quite', '``', 'jim', 'carrey', 'moment', \"'s\", 'definitely', '``', 'jim', 'carrey', 'movie', 'br', 'br', \"'s\", 'targeted', 'mostly', 'child', 'managed', 'enjoy', 'movie', 'promoted', 'israel', 'another', 'jim', 'carrey', 'movie', 'expected', 'weird', 'top', 'comedy', 'disappointed.', 'br', 'br', 'movie', 'nice', 'moment', 'work', 'well', 'movie', 'kid', 'ca', \"n't\", 'say', 'loved', 'movie', \"'m\", 'target', 'audience']\n"
     ]
    }
   ],
   "source": [
    "print([lemmatizer.lemmatize(token) for token in word_tokenize(train_texts[0].lower()) if token not in string.punctuation and token not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92011203-fa0a-4ccf-a0e0-c34548522d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jim', 'carrey', 'good', 'usual', 'even', 'though', 'quite', '``', 'jim', 'carrey', 'moment', \"'s\", 'definitely', '``', 'jim', 'carrey', 'movie', 'br', 'br', \"'s\", 'targeted', 'mostly', 'child', 'managed', 'enjoy', 'movie', 'promoted', 'israel', 'another', 'jim', 'carrey', 'movie', 'expected', 'weird', 'top', 'comedy', 'disappointed', 'br', 'br', 'movie', 'nice', 'moment', 'work', 'well', 'movie', 'kid', 'ca', \"n't\", 'say', 'loved', 'movie', \"'m\", 'target', 'audience']\n"
     ]
    }
   ],
   "source": [
    "print([lemmatizer.lemmatize(token.replace('.', '')) for token in word_tokenize(train_texts[0].lower()) if token not in string.punctuation and token not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "895b80db-60a5-464b-b351-1081044b3a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jim', 'carrey', 'good', 'usual', 'even', 'though', 'quite', '``', 'jim', 'carrey', 'moment', \"'s\", 'definitely', '``', 'jim', 'carrey', 'movie', 'br', 'br', \"'s\", 'targeted', 'mostly', 'child', 'managed', 'enjoy', 'movie', 'promoted', 'israel', 'another', 'jim', 'carrey', 'movie', 'expected', 'weird', 'top', 'comedy', 'disappointed', 'br', 'br', 'movie', 'nice', 'moment', 'work', 'well', 'movie', 'kid', 'ca', \"n't\", 'say', 'loved', 'movie', \"'m\", 'target', 'audience']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print([lemmatizer.lemmatize(re.sub(r\"\\.{1,}\", \"\", token)) for token in word_tokenize(train_texts[0].lower()) if token not in string.punctuation and token not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d979f6-5530-42f1-a2cc-badd47549dcc",
   "metadata": {},
   "source": [
    "<span style=\"color: gray; font-family: Helvetica; font-size: 30px;\"> Compute Term Frequencies (TF)\n",
    "\n",
    "    In this step we calculate how often each word appears in each document. We start by initializing an empty dictionary tf_dict. Then for each token in the list of tokens, we check if it's already in tf_dict. If it is, we increment its count by one. If it's not, we add it to tf_dict with a count of one. In the end, the keys will be the unique words in the tokens, and the values will be the counts of each word.\n",
    "\n",
    "    The list comprehension at the end applies this function to each list of tokens in train_tokens and test_tokens, giving us the term frequencies for each document in our train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dbdc64f-8ff4-44a6-93b9-bc94be1ee6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(tokens):\n",
    "    tf_dict = {}\n",
    "    for token in tokens:\n",
    "        if token in tf_dict:\n",
    "            tf_dict[token] += 1\n",
    "        else:\n",
    "            tf_dict[token] = 1\n",
    "    return tf_dict\n",
    "\n",
    "train_tf = [compute_tf(tokens) for tokens in train_tokens]\n",
    "test_tf = [compute_tf(tokens) for tokens in test_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30f3bd83-8303-4d6d-8d22-5ac8ec243041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jim': 4, 'carrey': 4, 'good': 1, 'usual': 1, 'even': 1, 'though': 1, 'quite': 1, '``': 2, 'moment': 2, \"'s\": 2, 'definitely': 1, 'movie': 6, 'br': 4, 'targeted': 1, 'mostly': 1, 'child': 1, 'managed': 1, 'enjoy': 1, 'promoted': 1, 'israel': 1, 'another': 1, 'expected': 1, 'weird': 1, 'top': 1, 'comedy': 1, 'disappointed.': 1, 'nice': 1, 'work': 1, 'well': 1, 'kid': 1, 'ca': 1, \"n't\": 1, 'say': 1, 'loved': 1, \"'m\": 1, 'target': 1, 'audience': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_tf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d976b1e-b206-4498-8f37-9c02dc4b96dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movie', 6), ('jim', 4), ('carrey', 4), ('br', 4), ('``', 2), ('moment', 2), (\"'s\", 2), ('good', 1), ('usual', 1), ('even', 1), ('though', 1), ('quite', 1), ('definitely', 1), ('targeted', 1), ('mostly', 1), ('child', 1), ('managed', 1), ('enjoy', 1), ('promoted', 1), ('israel', 1), ('another', 1), ('expected', 1), ('weird', 1), ('top', 1), ('comedy', 1), ('disappointed.', 1), ('nice', 1), ('work', 1), ('well', 1), ('kid', 1), ('ca', 1), (\"n't\", 1), ('say', 1), ('loved', 1), (\"'m\", 1), ('target', 1), ('audience', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(train_tf[0].items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcddd755-ffe4-4ffd-a3f6-7698a4f43188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_tf(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "train_tf = [compute_tf(tokens) for tokens in train_tokens]\n",
    "test_tf = [compute_tf(tokens) for tokens in test_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa69f459-91cc-4827-885d-81cf9ea1d9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'movie': 6, 'jim': 4, 'carrey': 4, 'br': 4, '``': 2, 'moment': 2, \"'s\": 2, 'good': 1, 'usual': 1, 'even': 1, 'though': 1, 'quite': 1, 'definitely': 1, 'targeted': 1, 'mostly': 1, 'child': 1, 'managed': 1, 'enjoy': 1, 'promoted': 1, 'israel': 1, 'another': 1, 'expected': 1, 'weird': 1, 'top': 1, 'comedy': 1, 'disappointed.': 1, 'nice': 1, 'work': 1, 'well': 1, 'kid': 1, 'ca': 1, \"n't\": 1, 'say': 1, 'loved': 1, \"'m\": 1, 'target': 1, 'audience': 1})\n"
     ]
    }
   ],
   "source": [
    "print(train_tf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa0b6ec-e4e8-44b2-abab-26c15d318022",
   "metadata": {},
   "source": [
    "<span style=\"color: gray; font-family: Helvetica; font-size: 30px;\"> Compute Inverse Document Frequency (IDF)\n",
    "\n",
    "    In this step we compute how many documents contain each word. Then, use this to compute the IDF for each word. We compute the Inverse Document Frequency (IDF) for each unique word across all documents. IDF gives more weight to the words that are less frequent in the entire corpus (set of all documents) which often are more informative than the frequently occurring words. It can be calculated as the log of the total number of documents divided by the number of documents containing the word. The purpose of using the logarithm is to dampen the effect of the idf; without it, the idf would give too much weight to very rare words.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb1e0ad4-d94b-41b8-be0a-644a780b4d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_idf(documents):\n",
    "    idf_dict = dict()\n",
    "    total_docs = len(documents)\n",
    "\n",
    "    # Count number of documents that contain each word\n",
    "    for doc in documents:\n",
    "        for word in set(doc):  # Using set to remove duplicates in each document\n",
    "            if word in idf_dict:\n",
    "                idf_dict[word] += 1\n",
    "            else:\n",
    "                idf_dict[word] = 1\n",
    "\n",
    "    # Compute IDF for each word\n",
    "    for word, val in idf_dict.items():\n",
    "        idf_dict[word] = math.log(total_docs / float(val))\n",
    "    \n",
    "    return idf_dict\n",
    "\n",
    "idf = compute_idf(train_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4207df40-2bb3-4941-ba4f-9acab6ef0363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26959\n"
     ]
    }
   ],
   "source": [
    "print(len(idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1524ba9d-8e03-4cbd-a39f-bb048cac90c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.961845129926823"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf['underrated']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aba7ed-7c8d-419b-83f2-360138df8a57",
   "metadata": {},
   "source": [
    "    In this function, we first create an empty dictionary idf_dict. Then for each document in documents (which are our tokenized and preprocessed training documents), we iterate over the set of unique words in the document. If a word is in idf_dict, we increment its value by one (because we found another document containing the word). If a word is not in idf_dict, we add it with a value of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8e14ff4-396c-472d-b91c-88cdfe4bdeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "def compute_idf(documents):\n",
    "    idf = defaultdict(int)\n",
    "    total_docs = len(documents)\n",
    "\n",
    "    for doc in documents:\n",
    "        for word in set(doc):\n",
    "            idf[word] += 1\n",
    "\n",
    "    for word, val in idf.items():\n",
    "        idf[word] = math.log(total_docs / float(val))\n",
    "    \n",
    "    return idf\n",
    "\n",
    "idf = compute_idf(train_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7680659-a262-43fd-b173-82335c53e7d9",
   "metadata": {},
   "source": [
    "<span style=\"color: gray; font-family: Helvetica; font-size: 30px;\"> Compute TF-IDF\n",
    "    \n",
    "    Finally, we can compute the TF-IDF for each word in each document. The TF-IDF is simply the product of the TF and IDF and stands for Term Frequency-Inverse Document Frequency. It is numerical statistic used to reflect how important a word is to a document in a collection or corpus. It's a commonly used technique for transforming text into meaningful representations, which can be used for various data mining and machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2cfbef4-3162-4ec4-b5f1-8ea9389bceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(tf, idf):\n",
    "    tfidf = {}\n",
    "    for word, val in tf.items():\n",
    "        if word in idf:\n",
    "            tfidf[word] = val * idf[word]\n",
    "        else:\n",
    "            tfidf[word] = 0.0  # Assign a default value for unseen words\n",
    "    return tfidf\n",
    "\n",
    "train_tfidf = [compute_tfidf(tf, idf) for tf in train_tf]\n",
    "test_tfidf = [compute_tfidf(tf, idf) for tf in test_tf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d9c9b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actor': 1.6528674703614363,\n",
       " 'dark': 3.158251203051766,\n",
       " 'film': 3.2631928670651456,\n",
       " 'truly': 2.995732273553991,\n",
       " 'believable': 3.490028595368771,\n",
       " 'well': 1.2928040180325517,\n",
       " 'cast': 2.132842318406951,\n",
       " 'quality': 3.2064533048696435,\n",
       " 'camera': 2.864704011147587,\n",
       " 'work': 1.7573580425107227,\n",
       " 'make': 2.1842666451408013,\n",
       " 'feel': 2.067512970814562,\n",
       " 'screenplay': 3.912023005428146,\n",
       " 'intense': 4.767689115485866,\n",
       " 'wander': 5.991464547107982,\n",
       " 'plot': 1.6270928476728213,\n",
       " 'one': 1.1312677205219714,\n",
       " 'want': 1.810942288644829,\n",
       " 'watch': 3.0056563547516846,\n",
       " 'second': 2.610469872763346,\n",
       " 'time': 0.9675840262617057,\n",
       " 'new': 2.107841016201534,\n",
       " 'perspective': 4.4654082436129325,\n",
       " 'gained': 6.502290170873972,\n",
       " 'ending': 2.583622622727158,\n",
       " 'showed': 4.017383521085972,\n",
       " 'small': 2.796881414808826,\n",
       " 'group': 6.437751649736401,\n",
       " 'patron': 15.201804919084164,\n",
       " 'gadsden': 0.0,\n",
       " \"'s\": 0.3516874024276933,\n",
       " 'center': 4.268697949366879,\n",
       " 'cultural': 5.035953102080546,\n",
       " 'art': 3.2314546070750607,\n",
       " 'ever': 1.7690199822585655,\n",
       " 'eager': 5.2030071867437115,\n",
       " 'discus': 5.654992310486769,\n",
       " 'person': 2.788718104169665,\n",
       " 'called': 3.2188758248682006,\n",
       " 'next': 2.8051119139453413,\n",
       " 'day': 2.1541650878757723,\n",
       " 'say': 1.5726239393113841,\n",
       " 'still': 1.7010051059595908,\n",
       " '``': 0.7732732250392302,\n",
       " 'bothered': 4.55638002181866,\n",
       " 'put': 2.3434070875143007,\n",
       " '18': 5.521460917862246,\n",
       " 'age': 3.2314546070750607,\n",
       " 'restriction': 0.0,\n",
       " 'would': 1.0541170487815585,\n",
       " 'youth': 4.422848629194137,\n",
       " 'real': 2.006191079940243,\n",
       " 'portrayal': 3.7090821614314557,\n",
       " 'ugly': 4.422848629194137,\n",
       " 'situation': 3.1465551632885744,\n",
       " 'set': 2.30759763481759,\n",
       " 'stage': 3.6888794541139363,\n",
       " 'great': 1.3783261914707137,\n",
       " 'conversation': 4.710530701645918}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tfidf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610bcad2-deed-4955-b838-3fdfb8bc96c4",
   "metadata": {},
   "source": [
    "### TF \n",
    "    (Term Frequency) is a measure of how frequently a term occurs in a document. In the simplest case, it's the raw count of a term in a document. If a word occurs frequently in a document, it's likely to be more important to the document.\n",
    "\n",
    "### IDF (Inverse Document Frequency) \n",
    "    is a measure of how important a term is across the entire corpus (the entire collection of all documents). The intuition here is that words that appear in many different documents are likely less meaningful (i.e., they're probably common words like \"the\" or \"and\"), so they're given lower weight. IDF is computed as the logarithm of the number of total documents divided by the number of documents containing the word.\n",
    "    \n",
    "    By multiplying these two numbers together, we get the TF-IDF score for a word in a document. The result is that words that are common in a single document but rare across other documents have a high TF-IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c80129da-bb49-4d59-9b71-80289b19def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(documents, max_features=5000):\n",
    "    idf_dict = {}\n",
    "    total_docs = len(documents)\n",
    "    word_counts = {}\n",
    "\n",
    "    for doc in documents:\n",
    "        for word in set(doc): \n",
    "            if word in idf_dict:\n",
    "                idf_dict[word] += 1\n",
    "            else:\n",
    "                idf_dict[word] = 1\n",
    "                \n",
    "            # Count frequency of each word\n",
    "            if word in word_counts:\n",
    "                word_counts[word] += 1\n",
    "            else:\n",
    "                word_counts[word] = 1\n",
    "\n",
    "    # max_features with highest document frequency\n",
    "    top_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:max_features]\n",
    "\n",
    "    # Compute IDF for each top word\n",
    "    for word, _ in top_words:\n",
    "        idf_dict[word] = math.log(total_docs / float(idf_dict[word]))\n",
    "    \n",
    "    # Only keep the words, not their frequencies\n",
    "    top_words = [word for word, _ in top_words]\n",
    "    \n",
    "    return idf_dict, top_words\n",
    "\n",
    "idf, vocabulary = compute_idf(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd46b9f6-4156-4ea5-ad9b-3ad48248d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tfidf(tfidf_dicts, vocabulary):\n",
    "    normalized_tfidf = []\n",
    "\n",
    "    for tfidf in tfidf_dicts:\n",
    "        # Initialize a vector for this document\n",
    "        tfidf_vector = [0] * len(vocabulary)\n",
    "        \n",
    "        # Compute L2 norm\n",
    "        l2_norm = np.sqrt(sum([value**2 for value in tfidf.values()]))\n",
    "\n",
    "        # Normalize tf-idf values and add them to the vector\n",
    "        for word, value in tfidf.items():\n",
    "            if word in vocabulary:\n",
    "                index = vocabulary.index(word)\n",
    "                tfidf_vector[index] = value / l2_norm\n",
    "                \n",
    "        normalized_tfidf.append(tfidf_vector)\n",
    "    \n",
    "    return normalized_tfidf\n",
    "\n",
    "train_tfidf_normalized = normalize_tfidf(train_tfidf, vocabulary)\n",
    "test_tfidf_normalized = normalize_tfidf(test_tfidf, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77ece6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.018386908794721084,\n",
       " 0.06975342084295526,\n",
       " 0.01249632573114062,\n",
       " 0,\n",
       " 0.057048995461369406,\n",
       " 0,\n",
       " 0,\n",
       " 0.04042824441264822,\n",
       " 0,\n",
       " 0.02577963248462714,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.029745118782690653,\n",
       " 0,\n",
       " 0,\n",
       " 0.03379516781796117,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.04110993561456676,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.04593906666013953,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.05047880110050221,\n",
       " 0,\n",
       " 0,\n",
       " 0.05111753054071422,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.053434114021872835,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.05488676455451426,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.05631199287510394,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.05980263473556123,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.06413449629312022,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.12949921325035596,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0660250294757978,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.06824024848563527,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.07206324961883297,\n",
       " 0,\n",
       " 0,\n",
       " 0.07226989878169725,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.07420715401965303,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.07629954907775924,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0793785134069188,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.08801135571913747,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.09591327484269653,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.09643093498559228,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.09695905361351222,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf_normalized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaea87e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
