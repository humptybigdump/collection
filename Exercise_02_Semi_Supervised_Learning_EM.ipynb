{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Models/Mixture Models - Expectation Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the likelihood of a sample given the parameters of the gaussian\n",
    "def gaussian(mean: float, std: float, x: np.array):\n",
    "    if abs(std) < 1e-12: std = 1e-12\n",
    "    return 1/(std*np.sqrt(2*math.pi))*np.exp(-0.5*((x-mean)/std)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the posterior of sample belonging to a distribution\n",
    "def marginal_likelihood(likelihoods: np.array, priors: np.array):\n",
    "        return np.sum(np.multiply(likelihoods, priors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the posterior of sample belonging to a distribution\n",
    "def posterior(likelihood: float, prior: float, marginal_likelihood: float):\n",
    "        return likelihood*prior/marginal_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM:\n",
    "    def __init__(self, n_components: int, samples: np.array, labeled_samples: np.array, labeled = False, n_iterations: int = 10):\n",
    "        self.n_components = n_components\n",
    "        self.n_iterations = n_iterations\n",
    "        self.samples = samples\n",
    "        self.labeled_samples = labeled_samples         \n",
    "        \n",
    "        # stds for each component\n",
    "        self.stds = np.ones(n_components)\n",
    "        # means for each component - sampling uniform random\n",
    "        self.means = np.random.uniform(low=np.min(samples), high=np.max(samples), size=n_components)\n",
    "        if labeled:\n",
    "            self.means = np.mean(labeled_samples, axis=1)\n",
    "        \n",
    "        # priors of the sample being from the respective component\n",
    "        self.priors = np.ones(n_components)/n_components\n",
    "        # likelihoods of the sample belonging to the respective component\n",
    "        self.lhoods = np.zeros([n_components,len(samples)])\n",
    "        # posteriors of the sample being from the respective component\n",
    "        self.posts = np.zeros([n_components,len(samples)])\n",
    "        \n",
    "        self.fig, self.axs = plt.subplots(n_iterations, figsize=[15,n_iterations*3])\n",
    "               \n",
    "    def fit(self):\n",
    "        for i in range(self.n_iterations):\n",
    "            self.plot(i)        \n",
    "            # caculate the likelihood of the sample belonging to the respective component\n",
    "            for c in range(self.n_components):\n",
    "                self.lhoods[c,:] = gaussian(self.means[c], self.stds[c], self.samples)\n",
    "            \n",
    "            # caculate the posterior of the sample belonging to the respective component\n",
    "            for c in range(self.n_components):\n",
    "                for s in range(len(self.samples)):\n",
    "                    marginal = marginal_likelihood(self.lhoods[:,s], self.priors)\n",
    "                    self.posts[c,s] = posterior(self.lhoods[c,s],self.priors[c], marginal)\n",
    "            self.priors = np.sum(self.lhoods, axis=1)\n",
    "\n",
    "            # caculate the new mean and variance\n",
    "            for c in range(self.n_components):\n",
    "                self.means[c] = np.sum(np.multiply(self.posts[c,:],self.samples))/np.sum(self.posts[c,:])\n",
    "                self.stds[c] = np.sum(np.multiply(self.posts[c,:],np.square(self.samples-self.means[c])))/np.sum(self.posts[c,:])\n",
    "                \n",
    "    def plot(self, i):\n",
    "        clrs = np.array(['r', 'b', 'g'])\n",
    "        min_val = np.min(self.samples)\n",
    "        max_val = np.max(self.samples)\n",
    "        x = np.linspace(min_val, max_val, 1000)\n",
    "        \n",
    "        labels = np.argmax(self.posts,axis=0)\n",
    "        col = np.where(labels==0,clrs[0],np.where(labels==1,clrs[1],clrs[2]))\n",
    "        \n",
    "        self.axs[i].scatter(self.samples, np.zeros(len(self.samples)), c=col)\n",
    "\n",
    "                \n",
    "        for c in range(self.n_components):\n",
    "            self.axs[i].plot(x, gaussian(self.means[c], self.stds[c], x), c=clrs[c])\n",
    "            #print(f\"mean of component {c} is {self.means[c]}\")\n",
    "            #print(f\"std  of component {c} is {self.stds[c]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "\n",
    "n_labled = 2\n",
    "# create three gaussian distributions\n",
    "class_a = np.random.normal(-5, 1, n_samples)\n",
    "class_b = np.random.normal(0, 1, n_samples)\n",
    "class_c = np.random.normal(5, 1, n_samples)\n",
    "# random number generator\n",
    "rng = np.random.default_rng()\n",
    "# sample labeled data\n",
    "labeled_a = rng.choice(class_a,size=n_labled)\n",
    "labeled_b = rng.choice(class_b,size=n_labled)\n",
    "labeled_c = rng.choice(class_c,size=n_labled)\n",
    "\n",
    "labeled_samples = np.array([labeled_a, labeled_b, labeled_c])\n",
    "\n",
    "samples = np.concatenate((class_a, class_b, class_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generated from Three Gaussians\n",
    "\n",
    "Gaussians consist of:\n",
    "* mean\n",
    "* variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data distribution of the samples\n",
    "plt.scatter(class_a, np.zeros(n_samples), facecolors='none', edgecolors='black')\n",
    "plt.scatter(class_b, np.zeros(n_samples), facecolors='none', edgecolors='black')\n",
    "plt.scatter(class_c, np.zeros(n_samples), facecolors='none', edgecolors='black')\n",
    "\n",
    "plt.scatter(labeled_a, np.zeros(n_labled), c=\"red\")\n",
    "plt.scatter(labeled_b, np.zeros(n_labled), c=\"blue\")\n",
    "plt.scatter(labeled_c, np.zeros(n_labled), c=\"green\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation Maximization\n",
    "* place n Gaussians\n",
    "* caculate the likelihood of the sample belonging to the respective component\n",
    "* caculate the posterior of the sample belonging to the respective component\n",
    "* $Posterior \\propto Likelihood \\times Prior$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = EM(3, samples, labeled_samples, False, 4)\n",
    "\n",
    "model.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
