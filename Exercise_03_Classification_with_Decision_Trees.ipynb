{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Decision Trees are non-probabilistic classifiers. As they rely on labled data, they belong to the class of supervised learning models. Decision trees can be used both for classification as well as regression. This exercise will focus on the classification part.\n",
    "\n",
    "The documentation for Decision Trees in scikit-learn can be found [here](https://scikit-learn.org/stable/modules/tree.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test datasets from the CSV files\n",
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_float_values(data: pd.DataFrame, attribute: str):\n",
    "    \"\"\"Add missing float values using mean and standard deviation\"\"\"\n",
    "    mean = data[attribute].mean()\n",
    "    std = data[attribute].std()\n",
    "    null_count = data[attribute].isnull().sum()\n",
    "    random_list = np.random.randint(mean - std, mean + std, size=null_count)  \n",
    "    data_copy = data[attribute].copy()\n",
    "    data_copy[np.isnan(data_copy)] = [random_list]\n",
    "    data[attribute] = data_copy\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nan values\n",
    "train[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\n",
    "# add missing ages\n",
    "train = add_missing_float_values(train, \"Age\")\n",
    "# create age bins\n",
    "train.loc[train['Age'] <= 14, 'Age_bin'] = 0\n",
    "train.loc[(train['Age'] > 14) & (train['Age'] <= 30), 'Age_bin'] = 1\n",
    "train.loc[(train['Age'] > 30) & (train['Age'] <= 40), 'Age_bin'] = 2\n",
    "train.loc[(train['Age'] > 40) & (train['Age'] <= 50), 'Age_bin'] = 3\n",
    "train.loc[(train['Age'] > 50) & (train['Age'] <= 60), 'Age_bin'] = 4\n",
    "train.loc[ train['Age'] > 60, 'Age_bin'] = 5\n",
    "train['Age_bin'] = train['Age_bin'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.info()\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_target = \"Survived\"\n",
    "train = train[[\"Pclass\", \"Embarked\", \"Sex\", \"Survived\", \"Age_bin\"]]\n",
    "test = train.sample(frac=0.2,random_state=200) #random state is a seed value\n",
    "train=train.drop(index=test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "### Representation\n",
    "* Each node represents a split of the data on an attribute\n",
    "* Each edge represents an attribute value\n",
    "* Each leaf represents the classification result\n",
    "\n",
    "### ID3 Algorithm\n",
    "\n",
    "1. Compute the overall best attribute (i.e. using information gain)\n",
    "  1. Compute the entropy for attribute\n",
    "  2. Compute entropy for all attribute values\n",
    "  3. Weight entropy values for each attribute value by weight of attribute value\n",
    "2. Assign best attribute to the next node\n",
    "3. Create edges for all attribute values from node and distribute data according to attribute value split\n",
    "4. Repeat until no further splits are possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ToDo**: Complete the function f_entropy\n",
    "\n",
    "Entropy H: $$H(S) = \\sum_{x \\in X}{-p(x) \\log_{2}p(x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_entropy(probabilities: pd.DataFrame):\n",
    "    \"\"\"Calculate the entropy for given probabilities.\"\"\"\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ToDo**: Complete the function probability_attribute_value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_attribute_value(\n",
    "    data: pd.DataFrame,\n",
    "    attribute: str,\n",
    "    attribute_value: any,\n",
    "    target: str,\n",
    "):\n",
    "    \"\"\"Calculate the probability for an attribute value to classify the target.\"\"\"\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ToDo**: Complete the function f_information_gain.\n",
    "\n",
    "Information Gain IG: $$IG(S,A) = H(S) - \\sum_{v \\in V(A)}{ \\frac{|S_v|}{|S|} H(S_v)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_information_gain(data: pd.DataFrame, attribute: str, target: str):\n",
    "    \"\"\"Calculate the information gain for a specific attribute given the target.\"\"\"\n",
    "    entropies, weights = entropy_attribute_value(data, attribute, target)\n",
    "    weighted_attribute_value_entropy = 0\n",
    "    for attribute_value, entropy in entropies.items():\n",
    "        weighted_attribute_value_entropy += weights[attribute_value] * entropy\n",
    "    return f_entropy(probability_attribute_values(data[target])) - weighted_attribute_value_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def probability_attribute_values(attribute_values: pd.DataFrame):\n",
    "    \"\"\"Calculate the probability for all attribute values.\"\"\"\n",
    "    return attribute_values.value_counts() / len(attribute_values)\n",
    "\n",
    "def entropy_attribute_value(data: pd.DataFrame, attribute: str, target: str):\n",
    "    \"\"\"Calculate the entropies for all values of a given attribute as well as the weight of the attribute value.\"\"\"\n",
    "    entropies = {}\n",
    "    for attribute_value in data[attribute].unique():\n",
    "        probabilities = probability_attribute_value(data, attribute, attribute_value, target)\n",
    "        entropies[attribute_value] = f_entropy(probabilities)\n",
    "    return entropies, dict(data[attribute].value_counts() / len(data[attribute]))\n",
    "\n",
    "\n",
    "def entropies_attributes(data: pd.DataFrame, target: str):\n",
    "    \"\"\"Calculate the entropies for all attributs.\"\"\"\n",
    "    entropies = {}\n",
    "    for attribute in data.columns:\n",
    "        if attribute == target:\n",
    "            continue\n",
    "        entropies[attribute] = entropy_attribute_value(data, attribute, target)[0]\n",
    "    return entropies\n",
    "\n",
    "def f_information_gains(data: pd.DataFrame, target: str):\n",
    "    \"\"\"Calculate the information gain for each attribute, excluding the target.\"\"\"\n",
    "    information_gains = {}\n",
    "    for attribute in data.columns:\n",
    "        if attribute == target:\n",
    "            continue\n",
    "        information_gains[attribute] = f_information_gain(data, attribute, target)\n",
    "    return information_gains\n",
    "\n",
    "\n",
    "def max_information_gain_attribute(data: pd.DataFrame, target: str):\n",
    "    \"\"\"Returns the attribute with the maximum information gain\"\"\"\n",
    "    information_gain = f_information_gains(data, target)\n",
    "    return max(information_gain, key=information_gain.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ToDo**: Calculate the entropy and information gain for all attributes. Which attribute should be the first node in the tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = probability_attribute_values(train[classification_target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node \n",
    "The Node class is used to build up the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Node with associated decision tree information\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    parent: Node\n",
    "        The reference to the parent of this node.\n",
    "    children: Node[]\n",
    "        The list of references to the children of this node.\n",
    "    in_attribute_value : any\n",
    "        The value of the parent attribute that led to this node.\n",
    "    attribute : any\n",
    "        The attribute the data is split on in this node.\n",
    "    prediction : any\n",
    "        The predicted class value at of a leaf node.\n",
    "    depth : int\n",
    "        The depth of the node; used for printing the tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attribute: str, in_attribute_value: any, parent: any):\n",
    "        self.attribute = attribute\n",
    "        self.in_attribute_value = in_attribute_value\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.prediction = None\n",
    "        self.depth = 0 if parent is None else parent.depth + 1\n",
    "\n",
    "    def add_child(self, child: any):\n",
    "        \"\"\"Appends the given child to the list of children.\"\"\"\n",
    "        self.children.append(child)\n",
    "\n",
    "    def find_child(self, attribute_value: any):\n",
    "        \"\"\"\"Returns the child with the given attribute_value.\"\"\"\n",
    "        for child in self.children:\n",
    "            if child.in_attribute_value == attribute_value:\n",
    "                return child\n",
    "        raise KeyError\n",
    "\n",
    "    def predict(self, sample: pd.Series):\n",
    "        \"\"\"Returns the prediction of the leaf node for the given sample.\"\"\"\n",
    "        node = self\n",
    "        while node.prediction is None:\n",
    "            node = node.find_child(sample[node.attribute])\n",
    "        return node.prediction\n",
    "\n",
    "    def set_prediction(self, data: pd.DataFrame, node_data: pd.DataFrame, target: str):\n",
    "        \"\"\"Sets the prediction of a node, given the attribute and node_data\"\"\"\n",
    "        values, counts = np.unique(node_data[target], return_counts=True)\n",
    "        if len(counts) > 0:\n",
    "            self.prediction = values[np.argmax(counts)]\n",
    "        # if the attribute value has no instances of the target class, choose the most likely class over all attribute values\n",
    "        elif len(counts) == 0:\n",
    "            values, counts = np.unique(data[target], return_counts=True)\n",
    "            self.prediction = values[np.argmax(counts)]\n",
    "\n",
    "    def to_str(self):\n",
    "        \"\"\"Converts a node to a string for printing.\"\"\"\n",
    "        return \"{}{}:{} --> {}\".format(\n",
    "            self.parent.depth * \"\\t\",\n",
    "            self.parent.attribute,\n",
    "            self.in_attribute_value,\n",
    "            self.prediction,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3 Algorithm\n",
    "\n",
    "1. Compute the overall best attribute (i.e. using information gain)\n",
    "  1. Compute the entropy for attribute\n",
    "  2. Compute entropy for all attribute values\n",
    "  3. Weight entropy values for each attribute value by weight of attribute value\n",
    "2. Assign best attribute to the next node\n",
    "3. Create edges for all attribute values from node and distribute data according to attribute value split\n",
    "4. Repeat until no further splits are possible\n",
    "\n",
    "**ToDos**:\n",
    "1. select the attribute with the maximum information gain\n",
    "2. create a data set for determining the maximum information gain after the split by the parent attribute value\n",
    "3. create a tree node with the attribute and an in_attribute_value of parent_attribute_value\n",
    "4. recursively grow the tree deeper\n",
    "\n",
    "ToDos are marked with `###TODO`, the code won't work before you managed to finish all ToDos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"DecisionTree with associated methods.\n",
    "    Attributes\n",
    "    ----------\n",
    "    root: Node\n",
    "        The root node of the tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        base_data: pd.DataFrame,\n",
    "        target: str,\n",
    "        log: bool = False,\n",
    "        parent: Node = None,\n",
    "        data: pd.DataFrame = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This method fits a decision tree. A decision tree consists of three different kind of nodes.\n",
    "            1. the rood node\n",
    "            The root node is the attribute with the maximum information gain over all attributes.\n",
    "            2. the tree nodes\n",
    "            The tree nodes are the remaining attributes with descending information gain.\n",
    "            3. the leaf nodes\n",
    "            The leaf nodes encode the prediction of the respective branch of the tree.\n",
    "        \"\"\"\n",
    "        if data is None:\n",
    "            data = base_data\n",
    "        if not self.is_pure(data, target):\n",
    "            # create a root node\n",
    "            if parent is None:\n",
    "                # remove pure attributes from the data frame for the rest of the tree\n",
    "                data = self.remove_pure_attributes(data, target)\n",
    "                ### TODO select the attribute with the maximum information gain\n",
    "                attribute = \"Sex\"\n",
    "                parent = Node(attribute, None, None)\n",
    "\n",
    "            # create child nodes with in_attribute_values for all attribute values of the parent\n",
    "            for parent_attribute_value in base_data[parent.attribute].unique():\n",
    "                ### TODO create a data set for determining the maximum information gain after the split by the parent attribute value\n",
    "                node_data = data\n",
    "                # remove pure attributes from the data frame for the rest of the tree\n",
    "                node_data = self.remove_pure_attributes(node_data, target)\n",
    "                # grow a tree node\n",
    "                if not self.is_pure(node_data, target):\n",
    "                    ### TODO select the attribute with the maximum information gain\n",
    "                    attribute = \"Sex\"\n",
    "                    ### TODO create a tree node with the attribute and an in_attribute_value of parent_attribute_value\n",
    "                    node = None\n",
    "                    # add the tree node to the children of the parent\n",
    "                    parent.add_child(node)\n",
    "                    # set the prediction to the most likely\n",
    "                    node.set_prediction(data, node_data, target)\n",
    "                    if log:\n",
    "                        print(node.to_str())\n",
    "                    ### TODO recursively grow the tree deeper\n",
    "                # grow a leaf node\n",
    "                elif self.is_pure(node_data, target):\n",
    "                    # create a leaf node with no attribute and an in_attribute_value of parent_attribute_value\n",
    "                    node = Node(None, parent_attribute_value, parent)\n",
    "                    # add the leaf node to the children of the parent\n",
    "                    parent.add_child(node)\n",
    "                    # set the prediction to the most likely\n",
    "                    node.set_prediction(data, node_data, target)\n",
    "                    if log:\n",
    "                        print(node.to_str())\n",
    "        self.root = parent\n",
    "        return parent\n",
    "\n",
    "    def predict(self, data: pd.DataFrame, target: str):\n",
    "        \"\"\"Predicts the class of each sample of the dataframe\"\"\"\n",
    "        result = []\n",
    "        for index, row in data.iterrows():\n",
    "            result.append(self.root.predict(row) == data[target][index])\n",
    "        return np.mean(result), 1 / len(data[target].unique())\n",
    "\n",
    "    def keep_attribute_value(self, data: pd.DataFrame, attribute: str, attribute_value: any):\n",
    "        \"\"\"Returns only the selected attribute values\"\"\"\n",
    "        return data[data[attribute] == attribute_value].reset_index(drop=True)\n",
    "\n",
    "    def is_pure(self, data, target):\n",
    "        \"\"\"Checks whether data is pure, e.g. all attributes have only a single value\"\"\"\n",
    "        for attribute in data.columns:\n",
    "            if attribute == target:\n",
    "                continue\n",
    "            if len(np.unique(data[attribute])) > 1:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def remove_pure_attributes(self, data: pd.DataFrame, target: str):\n",
    "        \"\"\"Removes all attributes that are pure.\"\"\"\n",
    "        for attribute in data.columns:\n",
    "            if attribute == target:\n",
    "                continue\n",
    "            if len(data[attribute].unique()) == 1:\n",
    "                data.drop(attribute, inplace=True, axis=1)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decision_tree = DecisionTree()\n",
    "decision_tree.fit(train, classification_target, True)\n",
    "accuracy, chance = decision_tree.predict(train, classification_target)\n",
    "\n",
    "print(\"Mean accuracy: {:.2} vs Chance: {:.2}\".format(accuracy, chance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# encode the data numerically\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "if \"Embarked\" in train.columns:\n",
    "    label_encoder.fit(train[\"Embarked\"])\n",
    "    train[\"Embarked_int\"] = label_encoder.transform(train[\"Embarked\"])\n",
    "    test[\"Embarked_int\"] = label_encoder.transform(test[\"Embarked\"])\n",
    "    train = train.drop(columns=\"Embarked\")\n",
    "    test = test.drop(columns=\"Embarked\")\n",
    "if \"Sex\" in train.columns:    \n",
    "    label_encoder.fit(train[\"Sex\"])\n",
    "    train[\"Sex_int\"] = label_encoder.transform(train[\"Sex\"])\n",
    "    test[\"Sex_int\"] = label_encoder.transform(test[\"Sex\"])\n",
    "    train = train.drop(columns=\"Sex\")\n",
    "    test = test.drop(columns=\"Sex\")\n",
    "\n",
    "# create training and test data\n",
    "X = train.drop(columns=classification_target)\n",
    "Y = train[classification_target]\n",
    "X_test = test.drop(columns=classification_target)\n",
    "Y_test = test[classification_target]\n",
    "\n",
    "# create the tree\n",
    "decision_tree = tree.DecisionTreeClassifier(criterion=\"entropy\",max_depth=len(X.columns)-1)\n",
    "# fit the tree\n",
    "decision_tree.fit(X,Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for index, row in X_test.iterrows():\n",
    "    result.append(decision_tree.predict(np.array(row).reshape(1, -1)) == Y_test[index])\n",
    "print(\"Mean accuracy: {:.2} vs Chance: {:.2}\".format(np.mean(result), chance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize the tree\n",
    "import graphviz\n",
    "feature_names = list(X.columns)\n",
    "dot_data = tree.export_graphviz(decision_tree, out_file=None, feature_names=feature_names,filled=True) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"decision_tree\")\n",
    "fig, ax = plt.subplots(figsize=(50, 24))\n",
    "tree.plot_tree(decision_tree);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
