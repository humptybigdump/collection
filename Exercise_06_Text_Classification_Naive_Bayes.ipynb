{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHsXe6_pLBUJ"
   },
   "source": [
    "# Tweets Classification using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekN7txKLsTbQ"
   },
   "source": [
    "## Naive Bayes Classifier\n",
    "This is a simple (naive) classification method based on Bayes rule. It relies on a very simple representation of the document (called the bag of words representation)\n",
    "Imagine we have 2 classes ( positive and negative ), and our input is a text representing a review of a movie. We want to know whether the review was positive or negative. So we may have a bag of positive words (e.g. love, amazing,hilarious, great), and a bag of negative words (e.g. hate, terrible).\n",
    "\n",
    "\n",
    "We may then count the number of times each of those words appears in the document, in order to classify the document as positive or negative.\n",
    "\n",
    "This technique works well for topic classification; say we have a set of academic papers, and we want to classify them into different topics (computer science, biology, mathematics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dN0cYz_otcDJ"
   },
   "source": [
    "### Bayes’ Rule applied to Tweets and Classes\n",
    "\n",
    "* For a tweet $d$ and a class $c$, and using Bayes’ rule,\n",
    "\n",
    "$$P( c | d ) = \\frac{P( d | c )  P( c )} {P( d )}$$\n",
    "\n",
    "\n",
    "**What do we mean by the term $P( d | c )$?**\n",
    "\n",
    "Let’s represent the tweet as a set of features (words or tokens) $\\{x_1, x_2, x_3, \\ldots \\}$\n",
    "\n",
    "We can then re-write $P( d | c )$ as:\n",
    "$$P( d | c ) = P( x_1, x_2, x_3, … , x_n | c )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02m98bxEyNci"
   },
   "source": [
    "**What about $P( c )$? How do you calculate it?**\n",
    "\n",
    "$P( c )$ is the total probability of a class. => How often does this class occur in total?\n",
    "\n",
    "\n",
    "E.g., in the case of classes positive and negative, we would be calculating the probability that any given review is positive or negative without actually analyzing the current input document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HB3W4mKiyQ13"
   },
   "source": [
    "**Do you need to calculate $P( d )$?**  \n",
    "Since all probabilities have $P( d )$ as their denominator, we can eliminate the denominator, and simply compare the different values of the numerator:\n",
    "\n",
    "$$P( c | d ) =P( d | c )  P( c ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum a Posteriori (MAP) Hypothesis\n",
    "$$c = \\arg\\max_{c\\in C} P( d | c )  P( c ) $$\n",
    "\n",
    "Under what conditions Maximum Likelihood (ML) rule for detection is same as MAP rule?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFGj4-8pwRZ5"
   },
   "source": [
    "## Sentiment Analysis\n",
    "In this exercise, you will be using Naive Bayes for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one.\n",
    "\n",
    "1. Train a naive bayes model on a sentiment analysis task\n",
    "2. Test using your model\n",
    "3. Compute ratios of positive words to negative words\n",
    "4. Do some error analysis\n",
    "5. Predict on your own tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHTHG_S3w9t5"
   },
   "source": [
    "## Natural Language Toolkit\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xwxnAsa865q"
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewsLk1tJ8m0V"
   },
   "source": [
    "### Example\n",
    "Tokenization is the process by which a large quantity of text is divided into smaller parts called tokens. These tokens are very useful for finding patterns and are considered as a base step for stemming and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFVOo7tv8LAy"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # This tokenizer divides a text into a list of sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QC571nx77U3"
   },
   "outputs": [],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning ... Arthur didn't feel very good.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ox9671i595ku"
   },
   "source": [
    "### NLTK Stopword List\n",
    "So stopwords are words that are very common in human language but are generally not useful because they represent particularly common words such as “the”, “of”, and “to”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_EHi-OU942T"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFzSvGjx-VAa"
   },
   "source": [
    "## Import the Data\n",
    "Download the sample tweets from the NLTK package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeIbN7-T-L_N"
   },
   "outputs": [],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpqeI2G4_CHF"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKZb0SBG_HtX"
   },
   "source": [
    "This will import three datasets from NLTK that contain various tweets to train and test the model:\n",
    "\n",
    "* negative_tweets.json: 5000 tweets with negative sentiments\n",
    "* positive_tweets.json: 5000 tweets with positive sentiments\n",
    "* tweets.20150430-223406.json: 20000 tweets with no sentiments\n",
    "\n",
    "Next, create variables for positive_tweets and negative_tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WRH6ilmF-2Wc"
   },
   "outputs": [],
   "source": [
    "# get the sets of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHnd70JE-2a5"
   },
   "outputs": [],
   "source": [
    "# avoid assumptions about the length of all_positive_tweets\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "532DGXTNzrgS"
   },
   "source": [
    "## Process the Data\n",
    "For any machine learning project, once you've gathered the data, the first step is to process it to make useful inputs to your model.\n",
    "\n",
    "1. Eliminate handles and URLs\n",
    "2. Tokenize the string into words. \n",
    "3. Remove stop words like \"and, is, a, on, etc.\"\n",
    "4. Stemming- or convert every word to its stem. Like dancer, dancing, danced, becomes 'danc'. You can use porter stemmer to take care of this. \n",
    "5. Convert all your words to lower case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoKQrTPDAZJA"
   },
   "outputs": [],
   "source": [
    "custom_tweet = test_pos[5]\n",
    "# print tweet\n",
    "print(custom_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVo2hdFKA5BM"
   },
   "source": [
    "the function `process_tweet()` does this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWYOBCihAk4X"
   },
   "outputs": [],
   "source": [
    "from utils import process_tweet\n",
    "# print cleaned tweet\n",
    "print(process_tweet(custom_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHhXInsDCTc-"
   },
   "source": [
    "## Feature Extraction\n",
    "\n",
    "**Feature extraction** refers to the process of transforming raw data into numerical features that can be processed while preserving the information in the original data set. It yields better results than applying machine learning directly to the raw data.\n",
    "\n",
    "What would be your guess as to which features are suitable to represent text documents? \n",
    "\n",
    "* Assign a real number to each word in the English dictionary and replace each text with the corresponding number. \n",
    "* Create a list of possible words and compare it with the words in each of your texts. You will end up with a feature vector with zeros and ones whose size corresponds to the number of possible words.\n",
    "* Count how many times each word from the texts occurs in each category (positive and negative), and then add these numbers for each of your texts in each category.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRlraJeUMXPz"
   },
   "source": [
    "### Feature Extraction with Frequencies\n",
    "You have to encode each tweet as a 3-dimesional vector. To do so, you have to create a dictionary to map the word, and the class it appeared in (positive or negative) to the number of times that word appeared in its corresponding class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c64_jtdNDCrF"
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGrhLzH_DCKu"
   },
   "outputs": [],
   "source": [
    "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "ys = [1, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SmEa9aZDGIk"
   },
   "source": [
    "| Vocabulary  | PosFreq  | NegFreq  |\n",
    "|---|---|---|\n",
    "| happi   | 1  | 0  |\n",
    "| trick | 0  | 1  |\n",
    "|  sad |  0 |  1 |\n",
    "|  tire | 0  | 2  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryJT__GrH1AM"
   },
   "outputs": [],
   "source": [
    "def count_tweets(result, tweets, ys):\n",
    "    '''\n",
    "    Input:\n",
    "        result: a dictionary that will be used to map each pair to its frequency\n",
    "        tweets: a list of tweets\n",
    "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
    "    Output:\n",
    "        result: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            # define the key, which is the word and label tuple\n",
    "            pair = (word,y)\n",
    "\n",
    "            # if the key exists in the dictionary, increment the count\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "\n",
    "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BY094e5qH7fO"
   },
   "outputs": [],
   "source": [
    "result = {}\n",
    "freqs = count_tweets(result, tweets, ys)\n",
    "print(freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zttjhBr76tM"
   },
   "source": [
    "define `lookup` function to get the positive frequencies and the negative frequencies for a specific word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZ2LPg7iOWJP"
   },
   "outputs": [],
   "source": [
    "def lookup(freqs, word, label):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        word: the word to look up\n",
    "        label: the label corresponding to the word\n",
    "    Output:\n",
    "        n: the number of times the word with its corresponding label appears.\n",
    "    '''\n",
    "    n = 0  # freqs.get((word, label), 0)\n",
    "\n",
    "    pair = (word, label)\n",
    "    if (pair in freqs):\n",
    "        n = freqs[pair]\n",
    "\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TsWu65AOZV6"
   },
   "outputs": [],
   "source": [
    "word = 'happi'\n",
    "label = 0\n",
    "lookup(freqs, word, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classificiation:\n",
    "$$D_{NB} = \\arg \\max _{D_j \\in \\{D_{neg}, D_{pos} \\}} P(D_{j}) \\prod_{i}^m P(W_{i}|D_{j})\\tag{3}$$\n",
    "\n",
    "To do inference, you can compute the following: \n",
    "$$\\frac {P(D_{pos})}{P(D_{neg})} \\prod_{i}^m \\frac {P(W_{i}|D_{pos})}{ P(W_{i}|D_{neg})} > 1 $$\n",
    "\n",
    "As $m$ gets larger, we can get numerical flow issues, so we introduce the $\\log$, which gives you the following equation: \n",
    "\n",
    "$$\\log \\frac {P(D_{pos})}{P(D_{neg})} + \\sum_{i}^m  \\log \\frac {P(W_{i}|D_{pos})}{ P(W_{i}|D_{neg})} > 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usNUS04d2FVr"
   },
   "source": [
    "#### Prior and Logprior:\n",
    "The prior probability represents the underlying probability in the target population that a tweet is positive versus negative. In other words, if we had no specific information and blindly picked a tweet out of the population set, what is the probability that it will be positive versus that it will be negative? That is the \"prior\".\n",
    "\n",
    "\n",
    "To train a Naive Bayes classifier:\n",
    "- The first part of training a naive bayes classifier is to identify the number of classes that you have.\n",
    "- You will create a probability for each class.\n",
    "$P(D_{pos})$ is the probability that the document is positive.\n",
    "$P(D_{neg})$ is the probability that the document is negative.\n",
    "Use the formulas as follows and store the values in a dictionary:\n",
    "\n",
    "$$P(D_{pos}) = \\frac{D_{pos}}{D}\\tag{1}$$\n",
    "\n",
    "$$P(D_{neg}) = \\frac{D_{neg}}{D}\\tag{2}$$\n",
    "\n",
    "Where $D$ is the total number of documents, or tweets in this case, $D_{pos}$ is the total number of positive tweets and $D_{neg}$ is the total number of negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MU3iDdr3PumC"
   },
   "outputs": [],
   "source": [
    "# Calculate D, the number of documents\n",
    "D = len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zlyOgqbPw_R"
   },
   "outputs": [],
   "source": [
    "# Calculate D_pos, the number of positive documents \n",
    "D_pos = (len(list(filter(lambda x: x > 0, train_y))))\n",
    "print(\"a priori P(Dpos) = \", D_pos/D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8uvI--pPzjI"
   },
   "outputs": [],
   "source": [
    "# Calculate D_neg, the number of negative documents\n",
    "D_neg = (len(list(filter(lambda x: x <= 0, train_y))))\n",
    "print(\"a priori P(Dneg) = \", D_neg/D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \\text{Logprior} =  \\log \\frac {P(D_{pos})}{P(D_{neg})} = \\log(P(D_{pos})) - \\log(P(D_{neg}))  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior = np.log(D_pos) - np.log(D_neg)\n",
    "print(\"logprior = %0.2f\" %\n",
    "      (logprior))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gqNeYdHIw66"
   },
   "source": [
    "#### Positive and Negative Probability of a Word\n",
    "To compute the positive probability and the negative probability for a specific word in the vocabulary, we'll use the following inputs:\n",
    "\n",
    "- $freq_{pos}$ and $freq_{neg}$ are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.\n",
    "- $N_{pos}$ and $N_{neg}$ are the total number of positive and negative words for all documents (for all tweets), respectively.\n",
    "- $V$ is the number of unique words in the entire set of documents, for all classes, whether positive or negative.\n",
    "\n",
    "We'll use these to compute the positive and negative probability for a specific word using this formula:\n",
    "\n",
    "$$P(W|D_{pos}) =  P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
    "$$P(W|D_{neg}) = P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHyV49GbJMze"
   },
   "source": [
    "##### Create `freqs` dictionary\n",
    "- Given your `count_tweets` function, you can compute a dictionary called `freqs` that contains all the frequencies.\n",
    "- In this `freqs` dictionary, the key is the tuple (word, label)\n",
    "- The value is the number of times it has appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJby4iyR1_xt"
   },
   "outputs": [],
   "source": [
    "# Build the freqs dictionary for later uses\n",
    "freqs = count_tweets({}, train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nLkdyutKqZe"
   },
   "source": [
    "You can compute the number of unique words that appear in the `freqs`dictionary to get your $V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7YwYZN65vVp"
   },
   "outputs": [],
   "source": [
    "# calculate V, the number of unique words in the vocabulary\n",
    "vocab = set([pair[0] for pair in freqs.keys()])\n",
    "V = len(vocab)\n",
    "print(\"The number of unique words in the vocabulary =\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avAKyRd6Kica"
   },
   "source": [
    "Using `freqs` dictionary, you can also compute the total number of positive words and total number of negative words \n",
    " and \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qloy2N6s50ce"
   },
   "outputs": [],
   "source": [
    "# calculate N_pos and N_neg\n",
    "N_pos = N_neg = 0\n",
    "for pair in freqs.keys():\n",
    "    # if the label is positive (greater than zero)\n",
    "    if pair[1] > 0:\n",
    "\n",
    "        # Increment the number of positive words by the count for this (word, label) pair\n",
    "        N_pos += freqs[pair]\n",
    "\n",
    "    # else, the label is negative\n",
    "    else:\n",
    "\n",
    "        # increment the number of negative words by the count for this (word,label) pair\n",
    "        N_neg += freqs[pair]\n",
    "\n",
    "print(\"N_pos = \", N_pos)\n",
    "print(\"N_neg = \", N_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r66HWS9oLVZi"
   },
   "source": [
    "you can iterate over each word in the vocabulary, use your `lookup` function to get the positive frequencies, $freq_{pos}$, and the negative frequencies, $freq_{neg}$, for that specific word.\n",
    "- Compute the positive probability of each word $P(W_{pos})$, negative probability of each word $P(W_{neg})$ using equations 4 & 5.\n",
    "\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
    "\n",
    "The log likelihood of a specific word\n",
    "\n",
    "- You can then compute the loglikelihood: $log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right) \\tag{6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYLqQ4NFK-s3"
   },
   "outputs": [],
   "source": [
    "# the log likelihood of you Naive bayes equation\n",
    "loglikelihood = {}\n",
    "# For each word in the vocabulary...\n",
    "for word in vocab:\n",
    "    # get the positive and negative frequency of the word\n",
    "    freq_pos = lookup(freqs,word,1)\n",
    "    freq_neg = lookup(freqs,word,0)\n",
    "\n",
    "    # calculate the probability that each word is positive, and negative\n",
    "    p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "    p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "    \n",
    "    # calculate the log likelihood of the word\n",
    "    loglikelihood[word] = np.log(p_w_pos/p_w_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3rh3evDLeEf"
   },
   "outputs": [],
   "source": [
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlaKQY8pSafW"
   },
   "source": [
    "## Test your naive bayes\n",
    "We can test the naive bayes function by making predicting on some tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsuUqvMMSdfV"
   },
   "outputs": [],
   "source": [
    "my_tweet = 'she smiled and was happy.'\n",
    "word_l = process_tweet(my_tweet)\n",
    "print(word_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bn_Yuu7cSknE"
   },
   "outputs": [],
   "source": [
    "# initialize probability to logprior\n",
    "p = logprior\n",
    "\n",
    "for word in word_l:\n",
    "    # check if the word exists in the loglikelihood dictionary\n",
    "    if word in loglikelihood:\n",
    "        # add the log likelihood of that word to the probability\n",
    "        p += loglikelihood[word]\n",
    "\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KOMpRrHOTfsv"
   },
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # process the tweet to get a list of words\n",
    "    word_l = process_tweet(tweet)\n",
    "\n",
    "    # initialize probability to logprior\n",
    "    p = logprior\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNw8R52KS20O"
   },
   "outputs": [],
   "source": [
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    # print( '%s -> %f' % (tweet, naive_bayes_predict(tweet, logprior, loglikelihood)))\n",
    "    p = naive_bayes_predict(tweet,logprior, loglikelihood)\n",
    "    # print(f'{tweet} -> {p:.2f} ({p_category})')\n",
    "    print(f'{tweet} -> {p:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUpb-DR0Tqbt"
   },
   "outputs": [],
   "source": [
    "your_tweet = 'you are sad and not happy :('\n",
    "naive_bayes_predict(your_tweet,logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_tweet = 'you are sad :('\n",
    "naive_bayes_predict(your_tweet,logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter words by Ratio of positive to negative counts\n",
    "Some words have more positive counts than others, and can be considered \"more positive\". Likewise, some words can be considered more negative than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"bad\"\n",
    "pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use lookup() to find positive counts for the word (denoted by the integer 1)\n",
    "pos_neg_ratio['positive'] = lookup(freqs,word,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use lookup() to find negative counts for the word (denoted by integer 0)\n",
    "pos_neg_ratio['negative'] = lookup(freqs,word,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_ratio['ratio'] = (pos_neg_ratio['positive'] + 1)/(pos_neg_ratio['negative'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_neg_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text Classification in NLP — Naive Bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
