{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Science and AI for Energy Systems** \n",
    "\n",
    "Karlsruhe Institute of Technology\n",
    "\n",
    "Institute of Automation and Applied Informatics\n",
    "\n",
    "Summer Term 2024\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise V: Central Limit Theorem and Superstatistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.special import gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem V.2 (programming) -- Stable distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After examining some properties of Gaussian random variables, we want to take a look into non-Gaussian distributions, i.e. in the exercise we consider stable distributions. We start with writing a function for the probability density depending on the stability, skewness, scale and location parameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**(a) The characteristic function $\\varphi(t,\\alpha, \\beta, c, \\mu)$ of a stable probability distribution is given by**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_fun_stable(t,alpha, beta, c, mu):\n",
    "    return np.exp(1j*t*mu - ((np.abs(c*t))**(alpha))*\n",
    "            (1-1j*beta*np.sign(t)*np.tan(np.pi*alpha/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for $\\alpha \\neq 1$.<br>\n",
    "Now we can denote a formula for the probability density function (PDF) of the stable distribution. The PDF is given as a Fourier Transform of the characteristic function:\n",
    "\\begin{align*}p(x)=\\frac{1}{2\\pi}\\int_{-\\infty}^{+\\infty}\\varphi(t)e^{-ixt}dt.\\end{align*} \n",
    "Write a function for the PDF using *scipy.integrate.quad*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a separate function for the integrand and then write a function for the integration, using scipy.integrate.quad'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Write a separate function for the integrand and then write a function for the integration, using scipy.integrate.quad'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**(b) Consider the parameters $c = 1, \\mu = 0$. Plot the PDF for each combination of $\\alpha \\in \\{0.4,1.5\\}$ and $\\beta \\in \\{0,0.5,0.75\\}$, for $x \\in [-5,5]$ with a step size of $0.05$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**(c) Instead of our own implementation we can also use *scipy.stats.levy\\_stable*. Plot the PDF using *scipy.stats.levy\\_stable.pdf* with the same parameters as in (b). What do you observe?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**(d) In the following step we draw samples from the a stable distribution. Consider the parameters $\\alpha = 0.5, \\beta = 0.5, c = 1, mu = 0$. Draw 10000 realization from the distribution using *scipy.stats.levy\\_stable.rvs*. In the next step, plot a histogram of the samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'choose a suitable number of bins for the histogram'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''choose a suitable number of bins for the histogram'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem V.3 (PROGRAMMING) â€“ Q-GAUSSIAN DISTRIBUTIONS AND PARAMETER FITTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We continue looking into non-Gaussian, i.e. q-Gaussian distributions. We consider a dataset of power grid frequency data with a 1-second time resolution, the dataset *frequency\\_sample\\_2015\\_ex5.csv* can be found in [https://bwsyncandshare.kit.edu/s/QPySS7eZCWjSjYP](https://bwsyncandshare.kit.edu/s/QPySS7eZCWjSjYP).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**(a) Plot a histogram of the data, both with linear and logarithmic scale on the y-axis, and calculate the kurtosis with *scipy.stats.kurtosis* (use *fisher=False*).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Plot the histograms and calculate the kurtosis:'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/frequency_sample_2015_ex5.csv').values.reshape(-1)\n",
    "'''Plot the histograms and calculate the kurtosis:'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**(b) The probability density function of a q-Gaussian distribution is given as**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We use beta > 0 in this definition'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def q_Gauss_pdf(x,q,beta,mu):\n",
    "    if q==1:\n",
    "        constant = np.sqrt(np.pi)\n",
    "    elif 1<q<3:\n",
    "        constant=np.sqrt(np.pi)*gamma((3-q)/(2*(q-1)))/(np.sqrt(q-1)*gamma(1/(q-1)))\n",
    "    else:\n",
    "        constant = 2*np.sqrt(np.pi)*gamma(1/(1-q))/((3-q)*(1-q)*gamma((3-q)/(2*(1-q))))\n",
    "    \n",
    "    pdf=np.sqrt(beta)/constant*(1+(1-q)*(-beta*(x-mu)**2))**(1/(1-q))\n",
    "    return pdf\n",
    "\n",
    "'''We use beta > 0 in this definition'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We assume beta > 0 in this definition.<br>\n",
    "Now we aim to write a function for fitting the distribution, i.e. for a maximum log-likelihood estimation of the parameters $q$, with fixed $\\beta$ and $\\mu$, and initial parameter value for the maximization, $q_0$. Maximize the log-likelihood \n",
    "\\begin{align*}\\log(l_q(x_1,\\ldots,x_n)) = \\sum_{i=1}^n\\log(p(x_i|q,\\beta,mu))\\end{align*}\n",
    "with respect to $q$, where $x_i$ are the data points. For the optimization you can use *scipy.minimize*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**(c) Normalize the frequency data by subtracting the mean and dividing by the standard deviation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized = (data-np.mean(data))/np.std(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the normalized frequency data with the following probability distributions:**\n",
    "-  **Gaussian distribution $\\rightarrow$ output: mean $\\mu$ and variance $\\sigma^2$,**\n",
    "- **q-Gaussian distribution for fixed $\\beta=1$, $\\mu=0$ and initial parameter value $q_0=1.2$ $\\rightarrow$ output: $q$.**\n",
    "\n",
    "**For the normal distribution you can use *scipy.stats.norm.fit* and for the q-Gaussian distribution use your own function from (b).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Plot the resulting probability density functions (PDFs) in a figure together with the histogram of the normalized frequency data. Use a logarithmic scaling for the y-axis. Compare the result to the figure on slide 35 in the lecure 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUse \\nx = np.arange(np.min(data_normalized),np.max(data_normalized),0.01) as the x-axis for the plots\\nof the PDFs, with q_Gauss_pdf(x,...) and sc.stats.norm.pdf(x,...)\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Use \n",
    "x = np.arange(np.min(data_normalized),np.max(data_normalized),0.01) as the x-axis for the plots\n",
    "of the PDFs, with q_Gauss_pdf(x,...) and sc.stats.norm.pdf(x,...)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem V.4 (programming) -- Superstatistics - Examples for a synthetic frequency dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We consider the dataset *time\\_series\\_superstatistics.csv* which contains a time series consisting of several shorter time series linked together. The time series is constructed as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''You don't need to tun the code! It is just for illustration purposes'''\n",
    "\n",
    "damping=0.00211224\n",
    "T= 20000 \n",
    "logNormalFit=(0.5228696759042721, -6.139031040500482, 115.71274435712661)\n",
    "#set some further parameters: Initial condition very close to zero and time resolution of the simulation\n",
    "oneSecond=10\n",
    "initialFrequency=10**-10\n",
    "t = np.linspace(0, T, oneSecond*T+1)\n",
    "delta_t = np.diff(t)[0]\n",
    "#define the deterministic and the probabilistic contributions to the frequency dynamics\n",
    "def bulkFrequency(y, t):\n",
    "    omega = y\n",
    "    dydt = -damping*omega\n",
    "    return dydt\n",
    "\n",
    "# define function for solving the stochastic differential equation \n",
    "def frequencyTrajectory(eps,initialFrequency):\n",
    "    omega = np.zeros(len(t))\n",
    "    omega[0]=initialFrequency\n",
    "    dW = np.random.normal(0,1,len(t)) * np.sqrt(delta_t) # noise dynamics\n",
    "    for i in range(1,len(t)):\n",
    "        omega[i] = omega[i-1] - delta_t * damping*omega[i-1] + eps* dW[i] #  noise dynamics with noise amplitude epsilons\n",
    "    sol = omega \n",
    "    return sol\n",
    "\n",
    "'''An alternative method using the library sdeint for solving the stochastic differential equation would be:'''\n",
    "# def frequencyTrajectory(eps,initialFrequency):\n",
    "#     def noiseDynamicsDirect(x, t):\n",
    "#         #define noise process\n",
    "#         return eps\n",
    "#     sol = sdeint.itoint(bulkFrequency,noiseDynamicsDirect, initialFrequency, t)\n",
    "#     return sol.flatten()\n",
    "\n",
    "#initialize list of frequency measurements\n",
    "aggregatedTrajectory=[]\n",
    "#draw several random realizations from the log-normal distribution to then use as noise amplitudes, similar to the Mathematica distribution, see also https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.lognorm.html\n",
    "numberOfGaussians = 500\n",
    "randomBetas= sc.stats.lognorm(*logNormalFit).rvs(size=numberOfGaussians)\n",
    "epsilons=np.sqrt(2*damping/randomBetas) # effective friction: randomBetas = 2*damping/epsilons^2 \n",
    "#run process in series: to use previous end point of trajectory as new initial condition\n",
    "for eps in epsilons:\n",
    "    trajectory=frequencyTrajectory(eps,initialFrequency)\n",
    "    initialFrequency=trajectory[-1]\n",
    "    aggregatedTrajectory.append(trajectory[:-1])\n",
    "#rescale to frequencies\n",
    "frequency=(50+1/(2*np.pi)*np.array(aggregatedTrajectory).flatten())[0::oneSecond]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dynamics of the time series is given by a constant damping and a variable noise amplitude $\\epsilon$ which is changing over time, i.e. each data snippet has a different noise amplitude. The \"effective friction\" $\\beta = (2*damping)/ \\epsilon^2$ is distributed with respect to a log-normal distribution.<br>\n",
    "The list of \"randomBetas\" which was used for the construction of the time series can be found in *randomBetas.txt* in [https://bwsyncandshare.kit.edu/s/QPySS7eZCWjSjYP](https://bwsyncandshare.kit.edu/s/QPySS7eZCWjSjYP).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**(a) Plot a histogram of the data, both with linear and logarithmic scale on the y-axis, and calculate the kurtosis with *scipy.stats.kurtosis* (use *fisher=False*).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Plot the histograms of the synthetic power grid frequency time series and calculate the kurtosis:'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency = pd.read_csv('data/time_series_superstatistics.csv').values.reshape(-1)\n",
    "'''Plot the histograms of the synthetic power grid frequency time series and calculate the kurtosis:'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**(b) We want to derive the superstatistics from the time series which is subject to the probability density function (PDF)\n",
    "\\begin{align*}\n",
    "p(x)= \\int_0^{\\infty}f(\\beta)p(x|\\beta),\n",
    "\\end{align*} \n",
    "where $f(\\beta)$ is the distribution of the effective friction $\\beta$. For details, see slides 32 and 33 of lecture 5. <br>\n",
    "At first we can derive the local kurtosis $\\kappa(\\Delta t)$ and the \"long time scale\" $T$, for which $\\kappa(\\Delta t = T) \\approx 3$, i.e. the case for which a locally Gaussian kurtosis arises. <br>\n",
    "The formula for the average kurtosis $\\kappa(\\Delta t)$ is given by**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageKappa(data,DeltaT):\n",
    "    #make sure that negative calls return still a number\n",
    "    if DeltaT<1:\n",
    "        return 0\n",
    "    meanData=np.mean(data); #here we use a global mean but a local mean for each data snippet of length DeltaT would also be ok. Results do not change too much\n",
    "    tMax=len(data);\n",
    "    nominator=sum((data[0:DeltaT]-meanData)**4)/DeltaT\n",
    "    denominator=sum((data[0:DeltaT]-meanData)**2)/DeltaT\n",
    "    sumOfFractions=nominator/(denominator**2);\n",
    "\n",
    "    for i in range(0,tMax-DeltaT):\n",
    "        nominator=nominator+((data[i+DeltaT]-meanData)**4-(data[i]-meanData)**4)/DeltaT;\n",
    "        denominator=denominator+((data[i+DeltaT]-meanData)**2-(data[i]-meanData)**2)/DeltaT;\n",
    "        sumOfFractions = sumOfFractions + nominator/(denominator**2);\n",
    "    return sumOfFractions/(tMax-DeltaT) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run *averageKappa(data=frequency, DeltaT = T)* with \"long time scale $T=20000$. The output value should be $\\approx 3$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "averageKappa(frequency,DeltaT=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**(c) Now complete in a similar way as the function *averageKappa* the following function for calculating the distribution of $\\beta$, \n",
    "\\begin{align*}\\beta(t_0) = \\frac{1}{<x^2>_{t_0,T}-<x>^2_{t_0,T}}\\end{align*} \n",
    "for $t_0 = 0,\\ldots,length(data)-T$, depending on the dataset and $T$:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def betaList(data,T):\n",
    "    tMax=len(data)\n",
    "    xSquareMean=sum(data[0:T]**2)/T # here: t_0 = 0\n",
    "    xMean=sum(data[0:T])/T # here: t_0 = 0\n",
    "    betaValues=[1/(xSquareMean-xMean**2)]\n",
    "    # calculate the local averages for every t_0 = 1,...,tMax-T\n",
    "    for i in range(0,tMax-T):\n",
    "    #???  # Complete the function simmilar as in the function \"averageKappa\"\n",
    "    return np.array(betaValues) # this list should contain lenth(data)-T+1 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: We denote $<...>_{t_0,\\Delta t} = \\frac{1}{\\Delta t}\\int_{t_0}^{t_0+\\Delta t}...~dt$ as local averages.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**(d) Use your formula from (c) to calculate the values of  $\\beta$ for $T=20000$ and for the dataset $data = (frequency-50) \\cdot 2\\pi$ (this represents the angulare velocity which is also used for the construction of the dataset), i.e. calculate \n",
    "\\begin{align*}betaList\\left(\\text{data} = (frequency-50)\\cdot 2\\pi,\\text{T} = 20000\\right).\\end{align*}  \n",
    "<br>\n",
    "    Then fit a log-normal distribution to the distribution, use *scipy.stats.lognorm.fit* in order to receive the parameters for the log-normal distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**(e) Plot the following results in one figure:**\n",
    "- **histogram of the output distribution from the *betaList*-function from (d),**\n",
    "- **the probability density function given the parameters that were calculated in (d), use *scipy.stats.lognorm.pdf*,**\n",
    "- **histogram of the randomBetas in *randomBetas.txt*.**\n",
    "\n",
    "**Compare your result to the figure at the bottom in slide 36 of lecture 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Use x = np.arange(np.min(beta_list),np.max(beta_list),0.1) as x-axis for the plots:'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Use x = np.arange(np.min(beta_list),np.max(beta_list),0.1) as x-axis for the plots:'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**(f) Normalize the time series as in Exercise 3. Fit the distribution of the normalized time series as in Exercise 3 with a normal distribution\n",
    "    and a q-Gaussian distribution via the Maximum Likelihood method. <br>\n",
    "For the normal distribution you can use *scipy.stats.norm.fit* \n",
    "    and for the q-Gaussian use your own function from Exercise V.3, using $\\beta=1,\\mu=0$ and $q_0=1.2$. Then plot the PDFs together with the histogram of the time series values. Use a logarithmic scaling for the y-axis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the frequency and fit the data with the distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histograms and the PDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sindy311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
