{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "The goal of this exercise is to implement a simple model-based reinforcement learning algorithm.  First, we will learn a dynamics function to model observed state transitions, and then we will use model decision timing planning to maximize predicted rewards [paper](https://arxiv.org/pdf/1708.02596.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we install some necessary packages to visualise the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install graphviz\n",
    "!pip install pydot\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing an RL cycle using OpenAI GYM\n",
    "`Gym` is a toolkit for developing and comparing reinforcement learning algorithms. `Gym` has a lot of built-in environments like the cartpole, pendulum,... In this [link](https://gym.openai.com/envs/), you can find a list of all defined environments.\n",
    "\n",
    "<img src=img/rl.png width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "print(\"the shape of the observation space: \", obs_dim)\n",
    "print(\"the shape of the action space: \", act_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space of our system contains 3 Measurements $ [\\cos(\\theta), \\sin(\\theta), \\dot{\\theta}] $. This task aims to control the pendulum to its rest position using motor torque $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Policy\n",
    "\n",
    "The following code lets the RL agent plays for four episodes in which Agent makes 100 moves while the game is rendered at each step and prints the accumulated reward for each game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play 4 games\n",
    "number_episodes = 4\n",
    "number_moves    = 100\n",
    "for i in range(number_episodes):\n",
    "    # initialize the environment\n",
    "    env.reset()\n",
    "    done = False\n",
    "    game_rew = 0  # accumulated reward\n",
    "    for j in range(number_moves):\n",
    "        # choose a random action\n",
    "        action = env.action_space.sample()\n",
    "        # take a step in the environment\n",
    "        new_obs, rew, done, info = env.step(action)\n",
    "        game_rew += rew\n",
    "        env.render()\n",
    "        # when is done, print the cumulative reward of the game and reset the environment\n",
    "        if done:\n",
    "            print(\"Done\")\n",
    "            break\n",
    "    print('Episode %d finished, reward:%d, the lenght of the episode:%d'% (i, game_rew,j))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is initialized by calling `reset()`. After doing so, the cycle loops 10 times. In each iteration, `env.action_space.sample()` samples a random action, executes it in the environment with `env.step()`, and displays the result with the `render()` method; that is, the current state of the game, as in the preceding screenshot. In the end, the environment is closed by calling `env.close()`.  Indeed, the `step()` method returns four variables that provide information about the interaction with the environment; namely, Observation, Reward, Done, and Info.\n",
    "\n",
    "Whenever `done` is True, this means that the episode has terminated and that the environment should be reset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instance attributes  `low` and `high` return the minimum and maximum values of the observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The minimum value of the observation space :\", env.observation_space.low)\n",
    "print(\"The maximum value of the observation space :\", env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with TF 2.X (Recap)\n",
    "\n",
    "As a recap of what we  used in the last exercises \n",
    "\n",
    "```python\n",
    "## Load Dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "## Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "## Define the loss function\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "## Create the optimizer by minimizing the loss using the Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "## Compile the model\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_object,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "## Train the model\n",
    "model.fit(x_train, \n",
    "          y_train,\n",
    "          epochs=3,\n",
    "          validation_data=(x_test, y_test),\n",
    "          verbose=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Based Reinforcement Learning with TF 2.X\n",
    "Model-Based Reinforcement Learning consists primarily of two steps:\n",
    "1. Learn a dynamics model\n",
    "2. Plan optimal action sequence using the model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "if(int(tf.__version__[0]) <= 1):\n",
    "    print('tensorflow {} detected; Please install tensorflow >= 2.0.0'.format(tf.__version__))\n",
    "else:\n",
    "    print('tensorflow {} detected'.format(tf.__version__))\n",
    "    \n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import ml2_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed:\n",
    "SEED = 999\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics Model\n",
    "We parameterize our learned dynamics function $f_\\theta (s_t, a_t)$ as a deep neural network, where the parameter vector $\\theta$ represents the network's weights. \n",
    "\n",
    "We don't want to learn a network to predict the next state $s_{t+1}$, given the current state and the current action $s_t, a_t$.  This function can be challenging to learn when the states $s_t$  and $s_{t+1}$ are too similar, and the action has seemingly little effect on the output. This difficulty becomes more evident as the time between states $∆t$ becomes small.\n",
    "\n",
    "Note that increasing this $∆t$ increases the information available from each data point and can help with dynamics learning and planning using the learned dynamics model. However, increasing $∆t$ also increases the discretization and complexity of the underlying continuous-time dynamics, making the learning process more difficult.\n",
    "\n",
    "We will learn a neural network dynamics model encodes the change in state that occurs as a result of executing the action $a_t$from state $s_t$ of the form:\n",
    "$$\\hat{\\Delta}_{t+1} = f_\\theta (s_t, a_t)$$\n",
    "such that\n",
    "$$ s_{t+1} =  s_t + \\hat{\\Delta}_{t+1} $$\n",
    "\n",
    "We will train $f_\\theta$ in a standard supervised learning setup, by performing gradient descent on the following objective:\n",
    "$$L(\\theta) =   \\sum_{(s_t, a_t,s_{t+1} ) \\in D}  \\lVert (s_{t+1} − s_t) − f_\\theta(s_t, a_t)\\rVert_2^2$$\n",
    "$$L(\\theta) =   \\sum_{(s_t, a_t,s_{t+1} ) \\in D}  \\lVert \\Delta_{t+1} − \\hat{\\Delta}_{t+1}\\rVert_2^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model:\n",
    "We will implement a neural network dynamics model and train it using a fixed dataset consisting of rollouts collected by a random policy.\n",
    "<img src=img/5.png width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(input_layer,hidden_layers, output_layer, activation=tf.tanh, last_activation=None):\n",
    "    input_shape = (input_layer)\n",
    "    # generate input vector\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, name='model_input')\n",
    "    x = inputs\n",
    "    # generate hidden layers\n",
    "    for filters in hidden_layers:\n",
    "        x = tf.keras.layers.Dense(filters,\n",
    "                                   activation=activation)(x)\n",
    "    \n",
    "    # generate output vector\n",
    "    output = tf.keras.layers.Dense(units=output_layer,\n",
    "                                   activation=last_activation, \n",
    "                                   name='modle_output')(x)\n",
    "    # generate the model\n",
    "    dynamic = tf.keras.models.Model(inputs,\n",
    "                output,\n",
    "                name='dynamic')\n",
    "    return dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_layer = obs_dim + act_dim\n",
    "# number of units per layer\n",
    "hidden_layers = [64,64]\n",
    "output_layer = obs_dim\n",
    "# Define the model\n",
    "dynamic = mlp(input_layer,hidden_layers, output_layer)\n",
    "dynamic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model(dynamic, to_file='mlp-model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training:\n",
    "Model takes as input the current state, next state and compute both the actual state difference and the predicted state difference and predicted next state, and returns the loss and optimizer for training the dynamics model.\n",
    "\n",
    "1. The loss function is the mean-squared-error between the normalized state difference and normalized predicted state difference\n",
    "2. Use Adam optimizer with learning_rate to minimize the loss \n",
    "3. Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "# Create the optimizer by minimizing the loss using the Adam optimizer with learning rate\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "dynamic.compile(optimizer=optimizer,\n",
    "                loss=loss_object,\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use callbacks `TensorBoard` to generate TensorBoard logs to visualize the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(name):\n",
    "    return [\n",
    "        tf.keras.callbacks.TensorBoard(logdir/name),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting training data:\n",
    "\n",
    "model_buffer is an instance of the FullBuffer class that contains the samples generated by the environment, and generate_random_dataset creates two partitions for training and validation, which are then returned by calling get_training_batch and get_valid_batch.\n",
    "1. Random Policy: to generate Date for the model\n",
    "2. Gather Rollouts and save them in buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_rollouts(env, num_rollouts, max_rollout_length, render = False):\n",
    "    dataset = ml2_utils.Dataset()\n",
    "    for _ in range(num_rollouts):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        t = 0\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            # Random policy\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            done = done or (t >= max_rollout_length)\n",
    "            dataset.add(state, action, next_state, reward, done)\n",
    "\n",
    "            state = next_state\n",
    "            t += 1\n",
    "            \n",
    "    if render:\n",
    "        env.close()\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "num_init_random_rollouts=250\n",
    "max_rollout_length=500\n",
    "render =False\n",
    "print('Gathering random dataset')\n",
    "random_dataset = gather_rollouts(env,num_init_random_rollouts, max_rollout_length,render)\n",
    "print(\"The state mean: \", random_dataset.state_mean)\n",
    "print(\"The state std: \",  random_dataset.state_std)\n",
    "print(\"The action mean: \",random_dataset.action_mean)\n",
    "print(\"The action std: \", random_dataset.action_std)\n",
    "print(\"shape of the random dataset: \", random_dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the statistical data of the random data set, because we will use it many more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary \n",
    "args = {}\n",
    "args['state_mean'] =random_dataset.state_mean\n",
    "args['state_std'] =random_dataset.state_std\n",
    "args['action_mean']=random_dataset.action_mean\n",
    "args['action_std'] =random_dataset.action_std\n",
    "args['delta_state_mean'] =random_dataset.delta_state_mean\n",
    "args['delta_state_std'] =random_dataset.delta_state_std\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_init_random_rollouts_valid=10\n",
    "max_rollout_length=500\n",
    "render =False\n",
    "print('Gathering random dataset')\n",
    "valid_dataset  = gather_rollouts(env,num_init_random_rollouts_valid, max_rollout_length,render)\n",
    "print(\"The state mean: \", valid_dataset.state_mean)\n",
    "print(\"The state std: \", valid_dataset.state_std)\n",
    "print(\"The action mean: \", valid_dataset.action_mean)\n",
    "print(\"The action std: \", valid_dataset.action_std)\n",
    "print(\"shape of the validation dataset: \", valid_dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dynamics Function\n",
    "\n",
    "1. Normalize both the states and actions in this buffer\n",
    "2. Concatenate the normalized state and action\n",
    "3. Pass the concatenated, normalized state-action tensor through a neural network. The resulting output is the normalized predicted difference between the next state and the current state\n",
    "4. Compute the actual state difference\n",
    "5. Normalize the state difference\n",
    "6. return the normalized state difference as labels and the normalized state-action tensor as features\n",
    "\n",
    "**Note in order to produce the predicted next state you need to unnormalize the delta state prediction, and add it to the current state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_dataset(states, actions, rewards, next_states, dones):\n",
    "    #### Define the Features\n",
    "    # Normalize both the states and actions in this buffer\n",
    "    states_norm  = ml2_utils.normalize(states, args[\"state_mean\"] ,  args[\"state_std\"])\n",
    "    actions_norm = ml2_utils.normalize(actions,args[\"action_mean\"], args[\"action_std\"])\n",
    "    # Concatenate the normalized state and action\n",
    "    input_layer  = tf.concat([states_norm, actions_norm], axis=1)\n",
    "    \n",
    "    #### Define the Labels\n",
    "    # the actual state difference\n",
    "    diff = next_states - states\n",
    "    # Normalize it by using the statistics random_dataset and normalize function\n",
    "    diff_norm = ml2_utils.normalize(diff, args[\"delta_state_mean\"], args[\"delta_state_std\"])\n",
    "    yield input_layer,diff_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tf = tf.data.Dataset.from_generator(tf_dataset, \n",
    "                                            output_types =(tf.float64,tf.float64),\n",
    "                                            output_shapes = (tf.TensorShape([None,4]), tf.TensorShape([None,3])),\n",
    "                                            args = (random_dataset.list_2_np()),\n",
    "                                            ).unbatch()\n",
    "\n",
    "batch_size = 200\n",
    "batched_dataset_tf = dataset_tf.batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset_tf = tf.data.Dataset.from_generator(tf_dataset, \n",
    "                                            output_types =(tf.float64,tf.float64),\n",
    "                                            output_shapes = (tf.TensorShape([None,4]), tf.TensorShape([None,3])),\n",
    "                                            args = (valid_dataset.list_2_np())\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = pathlib.Path.home() / '.keras' /\"tensorboard_logs\"\n",
    "# Delete an entire directory tree\n",
    "shutil.rmtree(logdir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "name = \"model\"\n",
    "\n",
    "dynamic.fit(batched_dataset_tf,\n",
    "            epochs=12,\n",
    "            validation_data= valid_dataset_tf,\n",
    "            callbacks=get_callbacks(name),\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View in TensorBoard\n",
    "Open an embedded  TensorBoard viewer inside a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dynamic,state,action,args):\n",
    "    # Normalize both the state and action\n",
    "    states_norm = ml2_utils.normalize(state, args[\"state_mean\"],   args[\"state_std\"])\n",
    "    actions_norm = ml2_utils.normalize(action, args[\"action_mean\"],args[\"action_std\"])\n",
    "    # Concatenate the normalized state and action\n",
    "    # Batch Case\n",
    "    if len(actions_norm.shape)>1:\n",
    "        input_layer =  tf.concat([states_norm, actions_norm], axis=1)\n",
    "    else:\n",
    "        input_layer = tf.concat([states_norm, actions_norm], axis=0)\n",
    "        input_layer = tf.expand_dims(input_layer,0)\n",
    "    # Pass the concatenated, normalized state-action tensor through a neural network. \n",
    "    # The resulting output is the normalized predicted difference between the next state and the current state\n",
    "    pred_diff_norm = dynamic.predict(input_layer)\n",
    "    # Compute the actual state difference\n",
    "    pred_diff = ml2_utils.unnormalize(pred_diff_norm, args[\"delta_state_mean\"],args[\"delta_state_std\"])\n",
    "    # The next State\n",
    "    next_state = state +  pred_diff\n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model for H-step in the future. We run first a random action sequence on the real system and save the resulted trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon= 15\n",
    "# Reset the environment:\n",
    "init_state = env.reset()\n",
    "# Lists to save the predicted observations and used actions\n",
    "state_seq = [init_state]\n",
    "used_action_seq= []\n",
    "state = init_state\n",
    "# Start the episode\n",
    "for i in range(horizon):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done , _ = env.step(action)\n",
    "    # append the next observations and used actions to lists to plot them\n",
    "    state_seq.append(state)\n",
    "    used_action_seq.append(action)\n",
    "    env.render()\n",
    "env.close()\n",
    "# convert to numpy array\n",
    "state_seq = np.asarray(state_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a the same action sequence on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = init_state\n",
    "# Lists to save the predicted observations and used actions\n",
    "pred_state_seq = [init_state]\n",
    "# Start the episode\n",
    "for action in used_action_seq:\n",
    "    next_state = predict(dynamic,state,action,args)  \n",
    "    # \n",
    "    state = next_state[0]\n",
    "    # append the next observations to list to plot them\n",
    "    pred_state_seq.append(state)\n",
    "# convert to numpy array\n",
    "pred_state_seq = np.asarray(pred_state_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gripper position evaluation\n",
    "#resulting_states_list = np.rollaxis(np.array(resulting_states_list), 1)\n",
    "fig1, (ax1, ax2, ax3) = plt.subplots(figsize=(20,30), nrows=3, ncols=1)\n",
    "\n",
    "# plot the predicted state\n",
    "ax1.plot(np.arange(horizon+1), pred_state_seq[:,0], 'o-',label='model prediction state 0')\n",
    "ax2.plot(np.arange(horizon+1), pred_state_seq[:,1], 'o-',label='model prediction state 1')\n",
    "ax3.plot(np.arange(horizon+1), pred_state_seq[:,2], 'o-',label='model prediction state 2')\n",
    "\n",
    "# plot real values\n",
    "ax1.plot(np.arange(horizon+1), state_seq[:,0], 'o-',label='real state 0')\n",
    "ax2.plot(np.arange(horizon+1), state_seq[:,1], 'o-',label='real state 1')\n",
    "ax3.plot(np.arange(horizon+1), state_seq[:,2], 'o-',label='real state 2')\n",
    "\n",
    "# plot the used action\n",
    "# plot the predicted state\n",
    "ax3.plot(np.arange(horizon), used_action_seq[:], 'o-',label='used action')\n",
    "\n",
    "# set axis lables\n",
    "for ax in (ax1, ax2, ax3):\n",
    "    ax.set_xlabel('step',fontsize='x-large')\n",
    "ax1.set_ylabel('Cos(θ)' ,fontsize='x-large')\n",
    "ax2.set_ylabel('sin(θ)' ,fontsize='x-large')\n",
    "ax3.set_ylabel('d(θ)'   ,fontsize='x-large')\n",
    "\n",
    "# plot legend\n",
    "ax1.legend(loc='best',fontsize='x-large')\n",
    "ax2.legend(loc='best',fontsize='x-large')\n",
    "ax3.legend(loc='best',fontsize='x-large')\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Selection\n",
    "Given the learned dynamics model, we now want to select and execute actions that maximize a known reward function (Decision-Time Planning)\n",
    "$$ a^*_t = \\arg \\min_{a_t} \\sum_{t'=t}^{t+H-1} r(\\hat{s}_{t'},a_{t'})$$\n",
    "$$\\text{s.t.}\\; \\hat{s}_{t'+1} = \\hat{s}_{t'} + f_\\theta ( \\hat{s}_{t'}, a_t)$$\n",
    "\n",
    "<img src=https://imgur.com/lJA1kXQ.png width=\"400\">\n",
    "\n",
    "\n",
    "However, solving this Equation is impractical because the learned dynamics model is imperfect, so using it to plan in such an open-loop manner will lead to accumulating errors over time and planning far into the future will become very inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will solve this equation using the sampling method (gradient-free optimization), where we will sample $k$ random action sequences of length $H$, later we will use the model to predict the future states by taking each of these action sequences, then we will evaluate the reward with each candidate action sequence, and the last step will be to  select the best action sequence and return the first action in that sequence.\n",
    "\n",
    "<img src=https://imgur.com/6gJcbv4.png width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_random_action_selection= 3000\n",
    "mpc_horizon=10\n",
    "action_dim       = env.action_space.shape[0]\n",
    "action_space_low = env.action_space.low\n",
    "action_space_high= env.action_space.high \n",
    "action_sequences = tf.random.uniform(\n",
    "            shape=[num_random_action_selection, mpc_horizon, action_dim],\n",
    "            minval=action_space_low,\n",
    "            maxval=action_space_high,\n",
    "            dtype=tf.float64\n",
    "        )\n",
    "print(\"The Shape of Actions: \", action_sequences.shape)\n",
    "print(\"The first sequence: \", action_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Cost\n",
    "costs = tf.zeros(num_random_action_selection, dtype=tf.float64)\n",
    "print(\"The Shape of costs: \", costs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function: \n",
    "\n",
    "We try to stabilize the pendulum in its rest position. Therefore, we define the cost function to achieve two goals:\n",
    "1. Theta should be zero (rest position).\n",
    "2. The rotation speed should also be damped. When our pendulum reaches its rest position, and the rotation speed is higher than zero, it will not stay there.\n",
    "3. The torque should be as small as possible because we do not want to consume infinite energy to reach our goal.\n",
    "\n",
    "* **Note:** We can also approximate the cost function with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_fun(states,actions):\n",
    "    cos_th = states[:,0]\n",
    "    sin_th = states[:,1]\n",
    "    th_dot = states[:,2]\n",
    "    th = np.arctan2(sin_th,cos_th)\n",
    "    th_normalize = (((th+np.pi) % (2*np.pi)) - np.pi)\n",
    "    action = np.clip(actions,-2.0, 2.0)[0]\n",
    "    costs = (th_normalize ** 2 + .1 * th_dot ** 2 + .001 * (action ** 2))\n",
    "    return costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan in the Model:\n",
    "We have only one initial state and 3000 action sequence candidates. We will use our model to estimate the predicted state sequence for each of these candidates. Then we can use the cost function to calculate the value of each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our model to process one sample and a batch of samples. Using batch of samples makes finding an optimal trajecotry more efficent. Therfore we will use  we a batch of samples. For example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch of the first action in all action sequence candidates.\n",
    "action1 = action_sequences[:, 0, :]\n",
    "print(action1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same initial state for 3000 trajectories \n",
    "states = tf.stack([init_state] * num_random_action_selection)\n",
    "\n",
    "print(states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(mpc_horizon):\n",
    "    #  if t = 1 then the first actions a1 batch   \n",
    "    actions      = action_sequences[:, t, :]\n",
    "    next_states  = predict(dynamic,states,actions,args) \n",
    "    # calculate the cost\n",
    "    costs +=cost_fun(states, actions)\n",
    "    #     \n",
    "    states = next_states\n",
    "    \n",
    "# convert to numpy array\n",
    "pred_state_seq = np.asarray(pred_state_seq)\n",
    "# optimal sequence of actions\n",
    "action_seq  = action_sequences[tf.argmin(costs)]\n",
    "# the first action that minimizes the cost function\n",
    "# optimal_seq = action_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs[146]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the optimal action sequence in the model (for evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to save the predicted observations and used actions\n",
    "state_seq = [init_state]\n",
    "state = init_state\n",
    "# Start the episode\n",
    "for action in action_seq:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done , _ = env.step(action)\n",
    "    # append the next observations and used actions to lists to plot them\n",
    "    state_seq.append(state)\n",
    "    env.render()\n",
    "env.close()\n",
    "# convert to numpy array\n",
    "state_seq = np.asarray(state_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = init_state\n",
    "# Lists to save the predicted observations and used actions\n",
    "pred_state_seq = [init_state]\n",
    "# Start the episode\n",
    "for action in action_seq:\n",
    "    next_state = predict(dynamic,state,action,args)  \n",
    "    # \n",
    "    state = next_state[0]\n",
    "    # append the next observations to list to plot them\n",
    "    pred_state_seq.append(state)\n",
    "# convert to numpy array\n",
    "pred_state_seq = np.asarray(pred_state_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gripper position evaluation\n",
    "#resulting_states_list = np.rollaxis(np.array(resulting_states_list), 1)\n",
    "fig1, (ax1, ax2, ax3) = plt.subplots(figsize=(20,30), nrows=3, ncols=1)\n",
    "\n",
    "# plot the predicted state\n",
    "ax1.plot(np.arange(mpc_horizon+1), pred_state_seq[:,0], 'o-',label='model prediction state 0')\n",
    "ax2.plot(np.arange(mpc_horizon+1), pred_state_seq[:,1], 'o-',label='model prediction state 1')\n",
    "ax3.plot(np.arange(mpc_horizon+1), pred_state_seq[:,2], 'o-',label='model prediction state 2')\n",
    "\n",
    "# plot real values\n",
    "ax1.plot(np.arange(mpc_horizon+1), state_seq[:,0], 'o-',label='real state 0')\n",
    "ax2.plot(np.arange(mpc_horizon+1), state_seq[:,1], 'o-',label='real state 1')\n",
    "ax3.plot(np.arange(mpc_horizon+1), state_seq[:,2], 'o-',label='real state 2')\n",
    "\n",
    "# plot the used action\n",
    "# plot the predicted state\n",
    "ax3.plot(np.arange(mpc_horizon), action_seq[:], 'o-',label='used action')\n",
    "\n",
    "# set axis lables\n",
    "for ax in (ax1, ax2, ax3):\n",
    "    ax.set_xlabel('step',fontsize='x-large')\n",
    "ax1.set_ylabel('Cos(θ)' ,fontsize='x-large')\n",
    "ax2.set_ylabel('sin(θ)' ,fontsize='x-large')\n",
    "ax3.set_ylabel('d(θ)'   ,fontsize='x-large')\n",
    "\n",
    "# plot legend\n",
    "ax1.legend(loc='best',fontsize='x-large')\n",
    "ax2.legend(loc='best',fontsize='x-large')\n",
    "ax3.legend(loc='best',fontsize='x-large')\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replanning:\n",
    "<img src=img/4.png width=\"400\">\n",
    "\n",
    "1. Execute the first planned action a_t and observe the next state s_{t+1}\n",
    "2. Use model again to optimize the action sequenc a_{t+1},..., a_{t+H}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDO\n",
    "for i in range(50):\n",
    "    actions = tf.random.uniform(\n",
    "            shape=[num_random_action_selection, mpc_horizon, action_dim],\n",
    "            minval=action_space_low,\n",
    "            maxval=action_space_high,\n",
    "            dtype=tf.float64\n",
    "        )\n",
    "    init_state = state\n",
    "    states = tf.stack([init_state] * num_random_action_selection)\n",
    "    costs = 0\n",
    "    \n",
    "    # find the optimal action sequence \n",
    "    for t in range(mpc_horizon):\n",
    "        # Normailize the state and the action\n",
    "        states_norm = ml2_utils.normalize(states, random_dataset.state_mean,   random_dataset.state_std)\n",
    "        actions_norm = ml2_utils.normalize(actions[:, t, :], random_dataset.action_mean,random_dataset.action_std)\n",
    "        input_layer = tf.concat([states_norm, actions_norm], axis=1)\n",
    "        # The resulting output is the normalized predicted difference between the next state and the current state\n",
    "        pred_diffs_norm = dynamic.predict(input_layer)\n",
    "        # calculate the cost\n",
    "        costs +=cost_fun(states, actions[:, t, :])                    \n",
    "        # The next State\n",
    "        next_states = states + ml2_utils.unnormalize(pred_diffs_norm, random_dataset.delta_state_mean,random_dataset.delta_state_std)\n",
    "        states = next_states\n",
    "    #action_seq  = actions[tf.argmax(costs)]\n",
    "    print(\"the cost of best action sequence: \", tf.reduce_min(costs))\n",
    "    # the action that minimizes the cost function\n",
    "    best_action = actions[tf.argmin(costs)][0]\n",
    "    # run the actions on the real system\n",
    "    \n",
    "    state, reward, done , _ = env.step(best_action)\n",
    "    env.render()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_teaching",
   "language": "python",
   "name": "venv_teaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
