{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Machine Vision - Assignment 8: ViT Implementation\n",
    "\n",
    "This exercise will follow assignment 7 in structure. However, its goal is to get used to implement state-of-the-art architectures based on reading a paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Preparation\n",
    "\n",
    "##### German Traffic Sign Recognition Benchmark\n",
    "\n",
    "The German Traffic Sign Recognition Benchmark [(GTSRB)](https://benchmark.ini.rub.de/) is a competition that was held at the IJCNN 2011. In this competition images of traffic signs should be classified.\n",
    "You will implement your own neural network to classify a subset of the GTSRB dataset. This subset consists of `12` different classes, which are shown in the figures below. However, you are free to extend your solution to the full dataset.\n",
    "\n",
    "\n",
    "|---|------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|--------------------------------|--------------------------------|--------------------------------|\n",
    "|  ![Class 0](res/images/0.png) | ![Class 1](res/images/6.png) | ![Class 2](res/images/16.png) | ![Class 3](res/images/17.jpg) | ![Class 4](res/images/19.png) | ![Class 5](res/images/22.jpg) | ![Class 6](res/images/28.png) | ![Class 7](res/images/29.png) | ![Class 8](res/images/32.png) | ![Class 9](res/images/33.png) | ![Class 10](res/images/38.png) | ![Class 11](res/images/40.png) |\n",
    "<br></br>\n",
    "\n",
    "In order to simplify this exercise, the raw GTSRB images are already transformed into a dataset, where each image has the shape of `[C,H,W]` (Height x Width x Channels) with values ranging from `0-1`.\n",
    "Furthermore, the dataset is split into a train-, validation- and test-dataset, where the train- and validation-datasets are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 12\n",
    "\n",
    "train_ds = torchvision.datasets.ImageFolder(\"data_train\", transform=v2.Compose([\n",
    "    v2.RandAugment(),\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "]))\n",
    "\n",
    "val_ds = torchvision.datasets.ImageFolder(\"data_val\", transform=v2.Compose([\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "]))\n",
    "\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=8, shuffle=True)\n",
    "val_dl = DataLoader(dataset=val_ds, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Which means, that each label is a vector of 12 entries, where only the entry of the class has the value $1$ and all others values are $0$The `torchvision.datasets.ImageFolder` is a simple way to represent classification datasets. For more information you can read it up here: [ImageFolder](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html)\n",
    "\n",
    "Furthermore, the standard pytorch dataloader class is used to create an iterator based on an `torch.utils.data.Dataset` class.\n",
    "Each iteration, the dataloader returns a batch of `x = [Bx3x32x32]` images and `y = [Bx12]` class labels, where B is the batch size.\n",
    "\n",
    "There are different approaches to encode the class label. For further information you can read this blog entry [integer- or one-hot-encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/).\n",
    "In this exercise the labels are encoded in the integer format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_batch, y_batch = next(iter(train_dl))\n",
    "\n",
    "# @student print the image and label shape of a batch\n",
    "print(f\"image shape: {x_batch.shape}\")\n",
    "print(f\"label shape: {y_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @student show one image of the batch and its label\n",
    "print(f\"label: {y_batch[1]}\")\n",
    "\n",
    "plt.imshow(x_batch[0].permute(1,2,0))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\n",
    "\n",
    "We will follow the steps of the ground braking paper \"AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\" [Link to Publication](https://arxiv.org/abs/2010.11929)\n",
    "which generalized the idea of language transformer models to vision tasks.\n",
    "\n",
    "![vit_architecture](res/images/vit.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Method\n",
    "\n",
    "Before starting to implement the ViT please read the publication and shortly describe each of these steps:\n",
    "<br />\n",
    "\n",
    "##### Step 1: Transform Images into Patches\n",
    "\n",
    "##### Step 2: Linear embed Patches into Tokens (Embeddings)\n",
    "\n",
    "##### Step 3: Add Position embedding to Tokens (Embeddings)\n",
    "\n",
    "##### Step 4: Feed Tokens into multiple Transformer Encoder Layer\n",
    "\n",
    "##### Step 5: Pool the Result and feed into Classification Head"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# hint: you can easily implement the patching of an image using einops. https://einops.rocks/1-einops-basics/\n",
    "# However, it is also possible using numpy / for loops\n",
    "from einops import rearrange\n",
    "def transform_batch_into_patches(image_batch, patch_size):\n",
    "    # @student: implement\n",
    "    ...\n",
    "\n",
    "print(x_batch.shape)\n",
    "batch_patches = transform_batch_into_patches(x_batch, patch_size=4)\n",
    "print(batch_patches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make sure tokens are extracted correctly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img0_patches = batch_patches[0]\n",
    "\n",
    "rows = 8\n",
    "cols = 8\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(4, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, patch in enumerate(img0_patches):\n",
    "    # @ student: reshape the flattened patch back to an image\n",
    "    patch_ = patch.reshape(4,4,3)\n",
    "    axes[i].imshow(patch_)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 2, 3, 4, 5\n",
    "\n",
    "The embedding from patches to tokens is a part of our model, hence we will start with the implementation of our model in this step.\n",
    "To ease the implementation, an already working Transformer class is available in `transformer.py`.\n",
    "Please use it until you can train your model to convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformer import Transformer\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                 image_size, patch_size,\n",
    "                 transformer_dim, transformer_depth, transformer_heads=4, transformer_dim_head=32\n",
    "     ):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # @student: Layers for step 2\n",
    "\n",
    "        # @student: Layers for step 3\n",
    "        # Hint: Use the learned position embedding (using a nn.Parameter)\n",
    "\n",
    "        # @student: Layers for step 4\n",
    "\n",
    "        # @student: Layers for step 5\n",
    "\n",
    "    def forward(self, image_batch):\n",
    "        # @student: (step 1) Transform Images into Patches\n",
    "\n",
    "        # @student: (step 2) Linear embed Patches into Tokens (Embeddings)\n",
    "\n",
    "        # @student: (step 3) Add Position embedding to Tokens (Embeddings)\n",
    "\n",
    "        # @student: (step 4) Feed Tokens (Embeddings) into multiple Transformer Encoder Layer\n",
    "        # hint: instead of a cls token you can use the mean over the tokens as final embedding per image (Simple ViT) https://arxiv.org/abs/2205.01580\n",
    "\n",
    "        # @student: (step 5)\n",
    "        return ...\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = VisionTransformer(\n",
    "    image_size=32, patch_size=4,\n",
    "    transformer_dim=48, transformer_depth=6, transformer_heads=8, transformer_dim_head=16\n",
    ")\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "def eval(model, dl, with_cm=False):\n",
    "    y_labels = []\n",
    "    pred_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, labels) in enumerate(dl):\n",
    "            logits = model(imgs)\n",
    "\n",
    "            # @student: calculate class predictions based on the logits\n",
    "            preds = torch.argmax(torch.softmax(logits, dim=-1), dim=-1)\n",
    "\n",
    "            pred_labels.extend(preds.detach().numpy())\n",
    "            y_labels.extend(labels.detach().numpy())\n",
    "\n",
    "    if with_cm:\n",
    "        cm = confusion_matrix(y_labels, pred_labels)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n",
    "        plt.show()\n",
    "    model.train()\n",
    "    print(f\"f1: {f1_score(y_labels, pred_labels, average='macro')}, p: {precision_score(y_labels, pred_labels, average='macro')}, r: {recall_score(y_labels, pred_labels, average='macro')}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below is the training process of the model.\n",
    "If your machine supports gpu acceleration uncomment line 6."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    print(f\"epoch {epoch+1}\")\n",
    "    pbar = tqdm(enumerate(train_dl))\n",
    "    running_loss = []\n",
    "    for i, (imgs, labels) in pbar:\n",
    "        # @student: uncomment if your machine has GPU support\n",
    "        #imgs, labels = imgs.to(\"cuda\"), labels.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(imgs)\n",
    "        loss = loss_fn(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss.append(loss.detach().numpy())\n",
    "        pbar.set_description(f\"loss {np.mean(running_loss):.3} - \" )\n",
    "        pbar.update()\n",
    "    eval(model, val_dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### From Logits to Labels\n",
    "\n",
    "Your network will output logits and not the final predictions.\n",
    "Hence, you further need to calculate the predicted label based on the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval(model, val_dl, with_cm=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once you feel confident with your model evaluate it one last time on the test set.\n",
    "The test set will be uploaded during the exercise session."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @student: final check of your fully trained model\n",
    "test_ds = torchvision.datasets.ImageFolder(\"data_test\", transform=v2.Compose([\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "]))\n",
    "test_dl = DataLoader(dataset=test_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "eval(model, test_dl)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
