{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer*: This notebook borrows from different sources including the IBMs tutorial on [quantum generative adversarial networks](https://learn.qiskit.org/course/machine-learning/quantum-generative-adversarial-networks)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from qiskit import QuantumCircuit, Aer\n",
    "from qiskit.visualization import plot_histogram\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.circuit.library import TwoLocal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our real distribution based on Bell states\n",
    "\n",
    "In one of our first lectures we got to know to the Bell state – a maximally entangled quantum state. This state can be constructed by appplying a Hadamard gate followed by a CNOT gate. The state we are interested to model with our quantum generative adversarial network is therefore $$|\\psi\\rangle = \\frac{1}{\\sqrt{2}}(|00\\rangle + |11\\rangle).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the constants we will need for the selected example\n",
    "REAL_DISTRIBUTION_NQUBITS = 2  # Number of qubits needed to model real distribution\n",
    "DISCRIMINATOR_NQUBITS = REAL_DISTRIBUTION_NQUBITS + 1  # Number of qubits for discriminator\n",
    "GENERATOR_NLAYERS = 2  # Number of layers used for the generator\n",
    "EPOCHS = 100  # Number of epochs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circuit creating our real distribution\n",
    "real_circuit = QuantumCircuit(REAL_DISTRIBUTION_NQUBITS)\n",
    "real_circuit.h(0)\n",
    "real_circuit.cx(0, 1);\n",
    "real_circuit.draw(\"mpl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the variational quantum generator and discriminator\n",
    "\n",
    "The research is still ongoing on how to find and define a proper ansatz for generator and discriminator. So most hyperparameters are still driven by heuristics. \n",
    "\n",
    "### Variational quantum generator\n",
    "\n",
    "For our variational quantum generator we need to ensure that ansatz has enough capacity and expressibility to fully reproduce the real quantum state $|\\psi\\rangle$. So in the following we use an ansatz based on successive $R_Y$ and $R_Z$ gates as well as entangling $CZ$ gates. This ansatz is expressive enough to properly represent the Bell state we are trying to model.\n",
    "\n",
    "For this, we can use the [`TwoLocal`](https://qiskit.org/documentation/stable/0.39/stubs/qiskit.circuit.library.TwoLocal.html) ansatz provided by Qiskit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TwoLocal(\n",
    "    REAL_DISTRIBUTION_NQUBITS,\n",
    "    ['ry', 'rz'],  # Parameterized single qubit rotations\n",
    "    'cz',  # Entangling gate\n",
    "    'full', # Entanglement structure: all to all\n",
    "    reps=GENERATOR_NLAYERS, # Number of layers\n",
    "    parameter_prefix='θ_g',\n",
    "    name='Generator')\n",
    "generator = generator.decompose() # decompose into standard gates\n",
    "generator.draw(\"mpl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational quantum discriminator\n",
    "\n",
    "For the ansatz of the discrimator we create a custom ansatz and define our own [`ParameterVector`](https://qiskit.org/documentation/stable/0.39/stubs/qiskit.circuit.ParameterVector.html?highlight=parametervector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_weights = ParameterVector('θ_d', 12)\n",
    "discriminator = QuantumCircuit(3, name=\"Discriminator\")\n",
    "discriminator.barrier()\n",
    "discriminator.h(0)\n",
    "\n",
    "for i in range(DISCRIMINATOR_NQUBITS):\n",
    "    discriminator.rx(discriminator_weights[3 * i + 0], i)\n",
    "    discriminator.ry(discriminator_weights[3 * i + 1], i)\n",
    "    discriminator.rz(discriminator_weights[3 * i + 2], i)\n",
    "\n",
    "discriminator.cx(0, 2)\n",
    "discriminator.cx(1, 2)\n",
    "discriminator.rx(discriminator_weights[9], 2)\n",
    "discriminator.ry(discriminator_weights[10], 2)\n",
    "discriminator.rz(discriminator_weights[11], 2)\n",
    "discriminator.draw(\"mpl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the QGAN\n",
    "\n",
    "We now got all the components we need:\n",
    "\n",
    "- the real distribution,\n",
    "- the generator creating our fake data, and\n",
    "- the discriminator.\n",
    "\n",
    "We can now construct the two circuits that we presented in the slides already. The first feeds the real data into our discriminator. The second feeds the generated state into the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_discriminator_circuit = QuantumCircuit(DISCRIMINATOR_NQUBITS)\n",
    "real_discriminator_circuit.compose(real_circuit, inplace=True)\n",
    "real_discriminator_circuit.compose(discriminator, inplace=True)\n",
    "real_discriminator_circuit.draw(\"mpl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_discriminator_circuit = QuantumCircuit(DISCRIMINATOR_NQUBITS)\n",
    "generator_discriminator_circuit.compose(generator, inplace=True)\n",
    "generator_discriminator_circuit.compose(discriminator, inplace=True)\n",
    "generator_discriminator_circuit.draw(\"mpl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the cost function\n",
    "\n",
    "Remember the minimax decision rule we have presented earlier:\n",
    "\n",
    "$$\\min_{\\vec\\theta_G}\\max_{\\vec\\theta_D}(\\Pr(D(\\vec\\theta_D, \\mathrm{real}=|\\mathrm{real}\\rangle) + \\Pr(D(\\vec\\theta_D, G(\\vec\\theta_G)) = |\\mathrm{fake}\\rangle))).$$\n",
    "\n",
    "Based on this, we can define a loss function for the discriminator and the generator. For the discriminator we get:\n",
    "\n",
    "$$\\mathrm{Cost}_D = \\Pr(D(\\vec\\theta_D, G(\\vec\\theta_G)) = |\\mathrm{real}\\rangle) - \\Pr(D(\\vec\\theta_D, \\mathrm{real})=|\\mathrm{real}\\rangle).$$\n",
    "\n",
    "Minimising $\\mathrm{Cost}_D$ entails maximising the probability of correctly classifying real data while minimising the probability of wrongly classifying fake data as real.\n",
    "\n",
    "The generator's cost function can be the negative of the cost of the discriminator. However, the optimal strategy is to maximise the probability that the discriminator miclassifies fake data as real. Thus, the term concerning the real quantum state can be omited:\n",
    "\n",
    "$$\\mathrm{Cost}_G=-\\Pr(D(\\vec\\theta_D, G(\\vec\\theta_G))=|\\mathrm{real}\\rangle).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GPARAMS = generator.num_parameters\n",
    "N_DPARAMS = discriminator.num_parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementign the cost functions above, please note the order of qubits in qiskit. We want to ensure to sum up all states where the last qubit is one, that is the probability of a sample being classified as $|\\mathrm{real}\\rangle = |1\\rangle$, thus $|XX1\\rangle$. This means that we need to consider the states being $|1XX\\rangle$ due to the reverse ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use Statevector to retrieve statevector of given circuit\n",
    "from qiskit.quantum_info import Statevector\n",
    "\n",
    "def discriminator_cost(disc_params):\n",
    "    \"\"\"Discriminator cost function for the optimizer to minimize.\"\"\"\n",
    "    # .numpy() method extracts numpy array from TF tensor\n",
    "    curr_params = np.append(disc_params.numpy(), gen_params.numpy())\n",
    "    gendisc_probs = Statevector(\n",
    "        generator_discriminator_circuit.bind_parameters(curr_params)).probabilities()\n",
    "    realdisc_probs = Statevector(\n",
    "        real_discriminator_circuit.bind_parameters(disc_params.numpy())).probabilities()\n",
    "    # Get total prob of measuring |1> on last qubit\n",
    "    prob_fake_true = np.sum(gendisc_probs[0b100:])\n",
    "    # Get total prob of measuring |1> on last qubit\n",
    "    prob_real_true = np.sum(realdisc_probs[0b100:])\n",
    "    cost = prob_fake_true - prob_real_true\n",
    "    return cost\n",
    "\n",
    "def generator_cost(gen_params):\n",
    "    \"\"\"Generator cost function for the optimizer to minimize.\"\"\"\n",
    "    # .numpy() method extracts numpy array from TF tensor\n",
    "    curr_params = np.append(disc_params.numpy(), gen_params.numpy())\n",
    "    state_probs = Statevector(\n",
    "        generator_discriminator_circuit.bind_parameters(curr_params)).probabilities()\n",
    "    # Get total prob of measuring |1> on q2\n",
    "    prob_fake_true = np.sum(state_probs[0b100:])\n",
    "    cost = -prob_fake_true\n",
    "    return cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the goodness of our whole model\n",
    "\n",
    "The Kullback-Leibler divergence is a measure that is used to measure the distance between two distributions. We therefore define a helper function to calculate the Kullback-Leibler divergence between the model and target distribution. This is commonly done to track the generator's progress when training GANs. A lower KL divergence indicates that the two distributions are similar, with a score of $0$ implying equivalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kl_div(model_distribution: dict, target_distribution: dict):\n",
    "    \"\"\"Gauge model performance using Kullback Leibler Divergence\"\"\"\n",
    "    kl_div = 0\n",
    "    for bitstring, p_data in target_distribution.items():\n",
    "        if np.isclose(p_data, 0, atol=1e-8):\n",
    "            continue\n",
    "        if bitstring in model_distribution.keys():\n",
    "            kl_div += (p_data * np.log(p_data)\n",
    "                 - p_data * np.log(model_distribution[bitstring]))\n",
    "        else:\n",
    "            kl_div += p_data * np.log(p_data) - p_data * np.log(1e-6)\n",
    "    return kl_div"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our quantum GAN\n",
    "\n",
    "### Compiling our models for training\n",
    "\n",
    "For simplicity we use the [`CircuitQNN`](https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.CircuitQNN.html) that compiles the parmaeterised quantum circuit and handles calculation of gradients.\n",
    "\n",
    "Please note that the ordering of parameters is internally done alphabetically. So in case you renamed the parameter vectors, please ensure that you adapt the ranges of parameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.utils import QuantumInstance\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN\n",
    "\n",
    "# define quantum instances (statevector and sample based)\n",
    "qi_sv = QuantumInstance(Aer.get_backend('aer_simulator_statevector'))\n",
    "\n",
    "# specify QNN to update generator weights\n",
    "generator_qnn = CircuitQNN(\n",
    "    generator_discriminator_circuit,  # parameterized circuit\n",
    "    # frozen input arguements (discriminator weights)\n",
    "    generator_discriminator_circuit.parameters[:N_DPARAMS],\n",
    "    # differentiable weights (generator weights)\n",
    "    generator_discriminator_circuit.parameters[N_DPARAMS:],\n",
    "    sparse=True, # returns sparse probability vector\n",
    "    quantum_instance=qi_sv)\n",
    "\n",
    "# specify QNNs to update discriminator weights\n",
    "discriminator_fake_qnn = CircuitQNN(\n",
    "    generator_discriminator_circuit, # parameterized circuit\n",
    "    # frozen input arguments (generator weights)\n",
    "    generator_discriminator_circuit.parameters[N_DPARAMS:],\n",
    "    # differentiable weights (discriminator weights)\n",
    "    generator_discriminator_circuit.parameters[:N_DPARAMS],\n",
    "    sparse=True, # get sparse probability vector\n",
    "    quantum_instance=qi_sv)\n",
    "\n",
    "discriminator_real_qnn = CircuitQNN(\n",
    "    real_discriminator_circuit, # parameterized circuit\n",
    "    [], # no input parameters\n",
    "    # differentiable weights (discriminator weights)\n",
    "    generator_discriminator_circuit.parameters[:N_DPARAMS],\n",
    "    sparse=True, # get sparse probability vector\n",
    "    quantum_instance=qi_sv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we use TensorFlow Keras and create an instance of the ADAM optimiser to optimise the models of the generator and discriminator. The ADAM optimiser is widely used in classical machine learning and usally outperforms vanilla gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # to serialize and deserialize variables\n",
    "# Initialize parameters\n",
    "init_gen_params = np.random.uniform(\n",
    "    low=-np.pi,\n",
    "    high=np.pi,\n",
    "    size=(N_GPARAMS,))\n",
    "init_disc_params = np.random.uniform(\n",
    "    low=-np.pi,\n",
    "    high=np.pi,\n",
    "    size=(N_DPARAMS,))\n",
    "gen_params = tf.Variable(init_gen_params)\n",
    "disc_params = tf.Variable(init_disc_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at our starting distribution that is generated by our generator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_gen_circuit = generator.bind_parameters(init_gen_params)\n",
    "init_prob_dict = Statevector(init_gen_circuit).probabilities_dict()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax1 = plt.subplots(1, 1, sharey=True)\n",
    "ax1.set_title(\"Initial generator distribution\")\n",
    "plot_histogram(init_prob_dict, ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Adam optimizer from Keras\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.02)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to track metrics while training\n",
    "best_gen_params = tf.Variable(init_gen_params)\n",
    "gloss = []\n",
    "dloss = []\n",
    "kl_div = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "While training classical GANs it is not uncommon to train an unbalanced number of steps between the two networks. Currently we use a 5:1 ratio that has been determined through trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCRIMINATOR_UPDATE_STEPS = 5 # N discriminator updates per generator update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_HEADERS = \"Epoch | Generator cost | Discriminator cost | KL Div. |\"\n",
    "print(TABLE_HEADERS)\n",
    "for epoch in range(EPOCHS):\n",
    "    #--- Quantum discriminator parameter updates ---#\n",
    "    for disc_train_step in range(DISCRIMINATOR_UPDATE_STEPS):\n",
    "        # Partial derivatives wrt θ_D\n",
    "        d_fake = discriminator_fake_qnn.backward(gen_params, disc_params\n",
    "                                       )[1].todense()[0, 0b100:]\n",
    "        d_fake = np.sum(d_fake, axis=0)\n",
    "        d_real = discriminator_real_qnn.backward([], disc_params\n",
    "                                       )[1].todense()[0, 0b100:]\n",
    "        d_real = np.sum(d_real, axis=0)\n",
    "        # Recall Cost_D structure\n",
    "        grad_dcost = [d_fake[i] - d_real[i] for i in range(N_DPARAMS)]\n",
    "        grad_dcost = tf.convert_to_tensor(grad_dcost)\n",
    "        # Update disc params with gradient\n",
    "        discriminator_optimizer.apply_gradients(zip([grad_dcost],\n",
    "                                                    [disc_params]))\n",
    "        # Track discriminator loss\n",
    "        if disc_train_step % DISCRIMINATOR_UPDATE_STEPS == 0:\n",
    "            dloss.append(discriminator_cost(disc_params))\n",
    "\n",
    "    #--- Quantum generator parameter updates ---#\n",
    "    for gen_train_step in range(1):\n",
    "        # Compute partial derivatives of prob(fake|true) wrt each\n",
    "        # generator weight\n",
    "        grads = generator_qnn.backward(disc_params, gen_params)\n",
    "        grads = grads[1].todense()[0][0b100:]\n",
    "        # Recall Cost_G structure and the linearity of\n",
    "        # the derivative operation\n",
    "        grads = -np.sum(grads, axis=0)\n",
    "        grads = tf.convert_to_tensor(grads)\n",
    "        # Update gen params with gradient\n",
    "        generator_optimizer.apply_gradients(zip([grads], [gen_params]))\n",
    "        gloss.append(generator_cost(gen_params))\n",
    "\n",
    "    #--- Track KL and save best performing generator weights ---#\n",
    "    # Create test circuit with updated gen parameters\n",
    "    gen_checkpoint_circuit = generator.bind_parameters(gen_params.numpy())\n",
    "    # Retrieve probability distribution of current generator\n",
    "    gen_prob_dict = Statevector(gen_checkpoint_circuit).probabilities_dict()\n",
    "    # Constant real probability distribution\n",
    "    real_prob_dict = Statevector(real_circuit).probabilities_dict()\n",
    "    current_kl = calculate_kl_div(gen_prob_dict, real_prob_dict)\n",
    "    kl_div.append(current_kl)\n",
    "    if np.min(kl_div) == current_kl:\n",
    "        # New best\n",
    "        # serialize & deserialize to simply ensure zero links\n",
    "        best_gen_params = pickle.loads(pickle.dumps(gen_params))\n",
    "    if epoch % 10 == 0:\n",
    "        # print table every 10 epochs\n",
    "        for header, val in zip(TABLE_HEADERS.split('|'),\n",
    "                              (epoch, gloss[-1], dloss[-1], kl_div[-1])):\n",
    "            print(f\"{val:.3g} \".rjust(len(header)), end=\"|\")\n",
    "        print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising progress and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (loss, kl) = plt.subplots(2, sharex=True,\n",
    "                               gridspec_kw={'height_ratios': [0.75, 1]},\n",
    "                               figsize=(6,4))\n",
    "fig.suptitle('QGAN training stats')\n",
    "fig.supxlabel('Training step')\n",
    "loss.plot(range(len(gloss)), gloss, label=\"Generator loss\")\n",
    "loss.plot(range(len(dloss)), dloss, label=\"Discriminator loss\",\n",
    "          color=\"C3\")\n",
    "loss.legend()\n",
    "loss.set(ylabel='Loss')\n",
    "kl.plot(range(len(kl_div)), kl_div, label=\"KL Divergence (zero is best)\",\n",
    "        color=\"C1\")\n",
    "kl.set(ylabel='KL Divergence')\n",
    "kl.legend()\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test circuit with new parameters\n",
    "gen_checkpoint_circuit = generator.bind_parameters(\n",
    "    best_gen_params.numpy())\n",
    "gen_prob_dict = Statevector(gen_checkpoint_circuit).probabilities_dict()\n",
    "real_prob_dict = Statevector(real_circuit).probabilities_dict() # constant\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,3))\n",
    "plot_histogram(gen_prob_dict, ax=ax1)\n",
    "ax1.set_title(\"Trained generator distribution\")\n",
    "plot_histogram(real_prob_dict, ax=ax2)\n",
    "ax2.set_title(\"Real distribution\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model already approximates the Bell state quite well.\n",
    "\n",
    "However, due to the competing nature of the two models, the training can become quite fragile and might fail to converge. This is especially true for QGANs due to the lack of best practices. This is still ongoing research and we might often suffer from effects such as vanishing gradients caused by a discriminator that is too good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dc5d5bdbdf7113047e58213ff4dd76f18871061ce7c2250ed15efe3e9d22e94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
