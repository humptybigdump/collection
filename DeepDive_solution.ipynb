{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Classroom\n",
    "Contact and Copyright: Mark.schutera@mailbox.com\n",
    "\n",
    "\n",
    "\n",
    "I am happy to have you. In the following, you will interactively experience and explore the world of artificial intelligence, machine learning, or, even more specifically, deep learning. You will get to know...\n",
    "- the general design of a deep learning pipeline\n",
    "- the basic principles behind deep learning \n",
    "- the fundamental neural network architecture\n",
    "- the process of training a neural network\n",
    "- the deployment of your trained neural network\n",
    "\n",
    "After this course, you will know the basics of...\n",
    "- the programming language python\n",
    "- the deep learning packages tensorflow and Keras\n",
    "- the mathematics and approaches within deep learning\n",
    "- many more\n",
    "\n",
    "This course will enable you to set up your own deep learning projects, train your own models on your own data. You will be well on your way to become a deep learning engineer, machine learning researcher, deploying the power of deep learning to generate progress, make the world a better place and solve the pressing problems of today and tomorrow. This sounds exciting - it is! And you are precisely in the right place - Buckle Up!\n",
    "\n",
    "<img src=\"graphics/welcome.svg\" width=\"500\"><br>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be emojis along the way. These are for virtual lectures with this notebook. They help your lecturer stay aware of how people progress. Once you reach a smiley, please post it within the chat function of your communication software.\n",
    "\n",
    "üòç Smiling Face with Heart-Eyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Getting comfortable in the driver's seat!\n",
    "\n",
    "The interface you are seeing in front of you is called jupyter notebook, an excellent interactive way to present and communicate code. Think of it as your cockpit. The nice thing is that in the back, we already got your engine preheated and ready to rumble, waiting for you to kick the pedal to the metal. \n",
    "\n",
    "In principle, this notebook is structured in cells, which get highlighted once you select them. There are two types of cells. The markdown cells (such as this one) are for everything that is not code. Here you will find explanations, descriptions, figures, and many more. The other type of cell is for code. Here you can interactively change code and run it within your browser. \n",
    "\n",
    "Navigate to the next cell by using the down-arrow or click on the next cell...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Great job. This is an interactive python cell with an easy print function. Feel free to change and play around with it.\n",
    "\n",
    "print('Ready Player One!')\n",
    "\n",
    "# A hashtag is used to comment on stuff that is not executed as code. This is useful if you want to add an explanation or further relevant information to your code.\n",
    "# Run a cell by clicking the Run button above, or by Ctrl + Enter (on your german keyboard, this is Strg + Enter) \n",
    "\n",
    "# Notice the output below the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We will use this cell to import all the packages you will need in the following - think of it as turning on all your systems\n",
    "# in your cockpit\n",
    "\n",
    "# This makes sure that if you change code in your external scripts, they will be updated\n",
    "\n",
    "\n",
    "import checker\n",
    "import generator\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "import importlib\n",
    "importlib.reload(checker)\n",
    "importlib.reload(generator)\n",
    "\n",
    "\n",
    "# now go ahead and Run the cell. This might take a while...\n",
    "# while the cell is running, you will see ln[*] next to it. Once it finished, you will see the number of execution\n",
    "# In case you want to interrupt the run of a cell, press Ctrl + C (on your german keyboard, this is Strg + C) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hint: \n",
    "Below code cells you sometimes find, what we call hints. So in case you get stuck, or are in search of additional information this is the place for additional elaboration on the topic or links. <br>\n",
    "- [print()](https://docs.python.org/3/library/functions.html#print) <br>\n",
    "- [import](https://docs.python.org/3/reference/simple_stmts.html#import)\n",
    "\n",
    "##### Side Quest:\n",
    "Another feature is the side quests. These are things you could try, think about, and tease your brain with. They are not necessary for you to learn the fundamentals. They are nevertheless, valuable for your holistic understanding of the subject. Also, they are fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Making sure you are still on track\n",
    "\n",
    "Next, welcome to the world of what we call sanity checks.\n",
    "Every now and then, you will find sanity checks below your interactive code cells. \n",
    "These are to make sure that your code does what it should (this is usually called a [unit test](https://en.wikipedia.org/wiki/Unit_testing)).\n",
    "Once your code passed the unit test, it is safe for you to go ahead with the notebook.\n",
    "So do not touch the code within a sanity check cell. Instead, just run it and fix your code in the respective cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write a function that adds a to b and returns c\n",
    "\n",
    "def add(a,b):\n",
    "    c = a+b\n",
    "    # the code goes here flag has to be removed and indicates that you need to pick your brain and write some code.\n",
    "    \n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "An addition in python can be done by simply writing a '+' sign connecting the two input arguments. An alternative is to use a dedicated function that takes an array of values. <br>\n",
    "- [sum([a, b])](https://docs.python.org/3/library/functions.html#sum) <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# After writing the add function in the cell above, go ahead and run this cell.\n",
    "\n",
    "'''\n",
    " ____   __   __ _  __  ____  _  _     ___  _  _  ____  ___  __ _ \n",
    "/ ___) / _\\ (  ( \\(  )(_  _)( \\/ )   / __)/ )( \\(  __)/ __)(  / )\n",
    "\\___ \\/    \\/    / )(   )(   )  /   ( (__ ) __ ( ) _)( (__  )  ( \n",
    "(____/\\_/\\_/\\_)__)(__) (__) (__/     \\___)\\_)(_/(____)\\___)(__\\_)\n",
    "\n",
    "Do not touch, just run!\n",
    "'''\n",
    "checker.test_add(add)\n",
    "\n",
    "\n",
    "# If your code is working correctly, you will be prompted with the message \"Everything passed, \n",
    "# you are ready to go.\"\n",
    "# This means you can continue with the next cell\n",
    "\n",
    "# If there is a bug in your code, you will be prompted with an AssertionError, \n",
    "# outlining how your code failed.\n",
    "\n",
    "# Go ahead, break your code and see what's happening to get a feeling for the sanity checks. \n",
    "# Then feel free to move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how to drive this thing, you are ready for some real action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talking about brains\n",
    "\n",
    "Your brain is the most astonishing multi-purpose computer around. And neuroscientists and engineers alike have been trying to understand the mechanics of brains, to apply their findings to other fields, such as computer science. This is how the idea of artificial neural networks came into being. <br>\n",
    "\n",
    "Your brain comprises approximately 80 billion neurons, or brain cells, which are heavily interconnected with what are called synapses. Neurons communicate with each other by sending electrical impulses through these synapses. If a neuron receives enough of these messages, the neuron itself sends an impulse to the neurons it is connected with. In this way, the brain can process complex data, extract features, and recognize a pattern. <br>\n",
    "\n",
    "And this is not yet, the most exciting ability of the brain. Your brain is further able to learn, adjusting to newly perceived data, patterns, and behavior. This happens by either strengthening or weakening the connection or synapses between neurons, influencing their interplay. <br>\n",
    "\n",
    "[Deep learning a machine learning revolution (slide)](slides/slide_machinelearningrevolution.pdf) <br>\n",
    "[Machine learning to deep learning and where the difference is (slide)](slides/slide_machinelearningtodeeplearning.pdf)\n",
    "<img src=\"graphics/first_pot.svg\" width=\"700\"><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üôÉ Upside-Down Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A computational model\n",
    "\n",
    "As of today, the brain is not fully understood, yet the very idea behind neural networks can be mold into a mathematical and computational model. For the purpose of explainability, we further reduce our neural network to a single neuron with two inputs, $x_0$ and $x_1$, that travel along synapses with connection factors $w_0$ and $w_1$. The neuron then sends an output $y$ with respect to the activation function $f($**x**$, $**W**$)$. <br>\n",
    "\n",
    "To make things easier for our silicon-based computers, it is usually assumed that inputs are sent simultaneously. There is only one direction in which the information flow (meaning no loops within the network). In deep learning speech, we refer to this as [feed forward](https://en.wikipedia.org/wiki/Feedforward_neural_network). <br>\n",
    "\n",
    "To understand the principle concept, you will now build this simplified model of a neuron. We will then teach this neuron to model an underlying function $(a * x_1 + b * x_2)^2 = y$. We would thus expect the weights to converge onto a and b. <br>\n",
    "\n",
    "#### Set up your neuron for a first run\n",
    "Define an input vector **x** with two input values and a fully connected weight matrix **W** with two synapses. The ground truth should be defined as 1.\n",
    "\n",
    "\\begin{align}\n",
    "\t\\textbf{x} =\\begin{bmatrix}\n",
    "\t0.2 \\\\ 0.4 \n",
    "\t\\end{bmatrix}\n",
    "\\end{align}\t\n",
    "\n",
    "\\begin{align}\n",
    "\t\\textbf{W} =\\begin{bmatrix}\n",
    "\t0.1 \\\\ 0.5 \n",
    "\t\\end{bmatrix}\n",
    "\\end{align}\t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setting up the inputs and the weight matrix \n",
    "# (do not forget to run code cells that have entries that are used in later cells)\n",
    "\n",
    "x = np.array([[0.2], [4]])\n",
    "W = np.array([[0.1], [0.5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to write functions that (1) determine the signals' strength to our neuron and (2) that create the appropriate and accumulated output signal based on the signal accumulation. This split into two functions is done to make our next step more straightforward, but for now, go ahead and implement them.\n",
    "\n",
    "\\begin{align}\n",
    "\t f_{(1)}(\\textbf{x,W})=  \\textbf{W*x}, \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\t f_{(2)}(\\textbf{x,W})= ||f_{(1)}||^2.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculating the strength of the signals to the neuron\n",
    "def accumulation(x,W):\n",
    "    q = np.dot(W, x)\n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "For this step the dot product could be used. The numpy package which we already imported as np provides such functionality. <br>\n",
    "- [np.dot(a, b)](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculating the activation of the neuron\n",
    "def activation(q): \n",
    "    y = np.sum(q*q)\n",
    "   \n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "The output after the activation (squaring the inputs) needs to be accumulated or summed up. Squaring in this case is easiest with the multiplication operator *. <br>\n",
    "- [np.sum()](https://numpy.org/doc/stable/reference/generated/numpy.sum.html) <br>\n",
    "\n",
    "##### Side Quest:\n",
    "Feel free to play around with the activation function, what type of activation function might be more meaningful. When thinking of how a neuron only reacts after a certain threshold is reached and how it is limited when it comes to the output strength. \n",
    "\n",
    "[Activation functions and their characteristics (slide)](slides/slide_activationfunction.pdf)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some space for you to write down your thoughts on the side quest questions \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let me put the things together for you (if the sanity checker breaks, check your accumulation and activation code), \n",
    "# allowing you to see your neuron in action for the first time.\n",
    "# This is what we call a forward pass.\n",
    "def forwardPass(x, W):\n",
    "    q = accumulation(x, W)\n",
    "    y = activation(q)\n",
    "    \n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    " ____   __   __ _  __  ____  _  _     ___  _  _  ____  ___  __ _ \n",
    "/ ___) / _\\ (  ( \\(  )(_  _)( \\/ )   / __)/ )( \\(  __)/ __)(  / )\n",
    "\\___ \\/    \\/    / )(   )(   )  /   ( (__ ) __ ( ) _)( (__  )  ( \n",
    "(____/\\_/\\_/\\_)__)(__) (__) (__/     \\___)\\_)(_/(____)\\___)(__\\_)\n",
    "\n",
    "Do not touch, just run!\n",
    "'''\n",
    "checker.test_forwardPass(forwardPass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make your neuron learn \n",
    "The forward pass is what you want your neuron and your whole neural network to do. But only once it is trained.\n",
    "Meaning, the weights and biases must be determined so that the neuron outputs what we would expect.\n",
    "\n",
    "Let's remember our task. We want the model to model an underlying function.\n",
    "\n",
    "This model behavior is learned by iteratively adjusting the weights of the neural network, such that the current prediction gets closer to the expected output, which is available as a label or ground truth $\\tilde{\\textbf{y}} = (a * x_1 + b * x_2)^2$.\n",
    "\n",
    "Now that we know the general idea let's implement the training procedure step by step.\n",
    "Let's start at the bottom of the neural network, at the output layer, or, more precisely, the prediction $\\hat{\\textbf{y}}$.\n",
    "To begin, we need to get to know how far our neural network was actually off.\n",
    "\n",
    "\n",
    "#### Objective function and Loss\n",
    "The offset of a neural network is determined by an objective function, which then yields a loss. The design of an objective function is crucial for the training process, as the weight parameters converge faster if the objective function models the error well. This said, a less suitable yet correct objective function will still take you to the finish line. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# implement the mean squared error as objective function\n",
    "def objectiveFunction(y_pred, y):\n",
    "    loss = np.square(y_pred-y)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "In our implementation, the loss is going to be calculated for a single sample a time. Numpy has your back with a square function <br>\n",
    "- [np.square()](https://numpy.org/doc/stable/reference/generated/numpy.square.html) <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    " ____   __   __ _  __  ____  _  _     ___  _  _  ____  ___  __ _ \n",
    "/ ___) / _\\ (  ( \\(  )(_  _)( \\/ )   / __)/ )( \\(  __)/ __)(  / )\n",
    "\\___ \\/    \\/    / )(   )(   )  /   ( (__ ) __ ( ) _)( (__  )  ( \n",
    "(____/\\_/\\_/\\_)__)(__) (__) (__/     \\___)\\_)(_/(____)\\___)(__\\_)\n",
    "\n",
    "Do not touch, just run!\n",
    "'''\n",
    "checker.test_objectiveFunction(objectiveFunction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation\n",
    "Now that we know about the loss, we need to update the weights and parameters so that the next time the neural network sees the same input, it is slightly less off. This update step is called backpropagation and is one of the core concepts of deep learning and neural networks.\n",
    "\n",
    "[Backpropagation](https://cs231n.github.io/optimization-2/) computes the gradients of the nested operations of your neural network through recursive application of chain rule. Starting from the calculated loss, this allows to trace back which weight had which share of the error, and more importantly, in which direction the weight has to be adjusted to reduce the error. \n",
    "\n",
    "So let's determine the partial derivatives of our neuron. We aim to determine the gradient of our function \n",
    "$f(\\textbf{x,W})$ dependent on the input values of our network $\\textbf{x}$, and the network's parameters $\\textbf{W}$.\n",
    "\n",
    "\\begin{align}\n",
    "\t\\frac{\\partial f(\\textbf{x,W})}{\\partial \\textbf{W}}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial f(\\textbf{x,W})}{\\partial \\textbf{x}}\n",
    "\\end{align}\n",
    "\n",
    "As we can not influence the input to reduce the loss, this partial derivative with respect to $\\textbf{x}$ is waved.\n",
    "By applying the chain rule, we can now propagate the error through the local derivatives at each operation and neuron. Starting from the output layer, working our way up to the input layer.\n",
    "\n",
    "\\begin{align}\n",
    "\t\\frac{\\partial f(\\textbf{x,W})}{\\partial \\textbf{W}} = \\frac{\\partial f(\\textbf{q})}{\\partial \\textbf{q}} * \\frac{\\partial \\textbf{q}}{\\partial \\textbf{W}} \n",
    "\\end{align}\n",
    "\n",
    "With\n",
    "\\begin{align}\n",
    "\tf(\\textbf{q})= ||\\textbf{q}||^2 = q_{1}^2 + q_{2}^2,\n",
    "\\end{align}\n",
    "\n",
    "we get\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial f(\\textbf{q})}{\\partial \\textbf{q}} = 2 * q_{i}.\n",
    "\\end{align}\n",
    "\n",
    "Great, we have the first part, now let's concentrate on the part before the activation function where\n",
    "\n",
    "\\begin{align}\n",
    "\t\\textbf{q} = \\textbf{W} * \\textbf{x},\n",
    "\\end{align}\n",
    "\n",
    "and thus \n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial q_{i}}{\\partial \\textbf{W}} = \\textbf{x}.\n",
    "\\end{align}\n",
    "\n",
    "Putting things together, we get the gradients with respect to the weights $\\textbf{W}$\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial f(\\textbf{x},\\textbf{W})}{\\partial \\textbf{W}} = 2 * q_i * \\textbf{x}^{T} * 2 * (\\hat{\\textbf{y}} - \\tilde{\\textbf{y}})\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implement the gradientFunction with respect to the weights W (see the above cell)\n",
    "def gradientFunction (q, x, y_pred, y):\n",
    "    deltaW = (4*q*(y_pred-y)) * (x.T)\n",
    "    \n",
    "    return deltaW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hint: \n",
    "You will need the transposed of an array. <br>\n",
    "- [np.ndarray.T](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.T.html) <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " ____   __   __ _  __  ____  _  _     ___  _  _  ____  ___  __ _ \n",
    "/ ___) / _\\ (  ( \\(  )(_  _)( \\/ )   / __)/ )( \\(  __)/ __)(  / )\n",
    "\\___ \\/    \\/    / )(   )(   )  /   ( (__ ) __ ( ) _)( (__  )  ( \n",
    "(____/\\_/\\_/\\_)__)(__) (__) (__/     \\___)\\_)(_/(____)\\___)(__\\_)\n",
    "\n",
    "Do not touch, just run!\n",
    "'''\n",
    "checker.test_gradientFunction(gradientFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The weight update\n",
    "Now that we know from the gradient in which direction we need to change the weights, we will do just that.\n",
    "The direction put aside, we need to specify how much the weights will be changed. In optimization, this is usually called step size. In neural network training, this is called the learning rate.\n",
    "This is arguably the most critical parameter. It is pretty tricky to set: Too small the learning rate slows down the convergence, too large speeds up convergence, but it is risky as this might prevent the training from full convergence. \n",
    "\n",
    "\\begin{align}\n",
    "    \\textbf{W}_{t+1} = \\textbf{W}_{t} - learningRate * \\partial \\textbf{W}_{t}.\n",
    "\\end{align}\n",
    "\n",
    "[Learning rate too large, too small - to convergence (slide)](slides/slide_learningrate.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update the weights, control the magnitude of the change with the learning rate parameter. \n",
    "def update (W, deltaW, learningRate):\n",
    "    W = W - learningRate * deltaW\n",
    "    \n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " ____   __   __ _  __  ____  _  _     ___  _  _  ____  ___  __ _ \n",
    "/ ___) / _\\ (  ( \\(  )(_  _)( \\/ )   / __)/ )( \\(  __)/ __)(  / )\n",
    "\\___ \\/    \\/    / )(   )(   )  /   ( (__ ) __ ( ) _)( (__  )  ( \n",
    "(____/\\_/\\_/\\_)__)(__) (__) (__/     \\___)\\_)(_/(____)\\___)(__\\_)\n",
    "\n",
    "Do not touch, just run!\n",
    "'''\n",
    "checker.test_update(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the things together for the training of your first neural network\n",
    "We are getting excitingly close to our first neural network training. The underlying model your neural network is going to depict is $\\tilde{\\textbf{y}} = (a * x_1 + b * x_2)^2$. The convenient thing is our neural network architecture, which is pretty much the same as this function. As such that $w_1$ converges to the value of $a$ and $w_2$ to $b$. As we understand the workings and mechanics within a neural network, this allows for easier tracking and interpretability while training the network. \n",
    "\n",
    "Of course, in practice, the underlying models and pattern will be way more difficult, but starting small allows us to make the first step in a safe environment. \n",
    "\n",
    "Remember, you can stop a cell from executing with the squared stop button above and rerun it with the triangle Run button.\n",
    "\n",
    "<img src=\"graphics/machine.svg\" width=\"700\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üòâ Winking Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Learning rate\n",
    "learningRate = 1e-5\n",
    "\n",
    "# Weight initialization\n",
    "W = np.array([[0.1, 0.8]])\n",
    "\n",
    "# Define target parameters of your function\n",
    "a = 0.5\n",
    "b = 0.6\n",
    "\n",
    "\n",
    "for updates in range(1, 1000):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # This is where we use a preimplemented generator function to generate a new dataset pair \n",
    "    x1, x2, y = generator.generateDataPair(a, b)\n",
    "    x = np.array([[x1], [x2]])\n",
    "    display('This is update step: ' + str(updates))\n",
    "    display('Target weights are: [[' + str(a) + ' ' + str(b) + ']]')\n",
    "    display('Current weights are: ' + str(W))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = forwardPass(x, W) #'''your code goes here'''\n",
    "    \n",
    "    \n",
    "    # Calculate the loss of your neural network's prediction\n",
    "    loss = objectiveFunction(y_pred,y)  #'''your code goes here'''\n",
    "    display(\"Training loss: \" + str(np.round(loss, 2)))\n",
    "    \n",
    "    \n",
    "    # Backpropagation to calculate the gradient\n",
    "    q = accumulation(x, W) #'''your code goes here'''\n",
    "    deltaW = gradientFunction(q, x, y_pred, y) #'''your code goes here'''\n",
    "    \n",
    "    \n",
    "    # Update the weights of your neural network\n",
    "    W = update(W, deltaW, learningRate) #'''your code goes here'''\n",
    "\n",
    "    \n",
    "    \n",
    "    display(\"Current prediction: \" +  str(np.round(y_pred, 2)) + '  (' + str(np.round(y, 2)) + ')')\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "In this case, there won't be a sanity check, as you will find out whether your network works if your current weights converge onto your target weights (which are the same as $a$ and $b$. <br>\n",
    "\n",
    "\n",
    "##### Side Quest:\n",
    "Feel free to play around with your neural network, and try to answer the questions below to get a feeling for the training mechanics.\n",
    "Great places to start are:\n",
    "- The learning rate (What happens for learning rates that are too large or very small? [Hint - Exploding Gradients](https://www.deeplearningbook.org/contents/optimization.html))\n",
    "- The weight initialization (What happens if the initialized weights are the same as the target parameters? [Hint - Parameter Initialization](https://www.deeplearningbook.org/contents/optimization.html))\n",
    "- Once you worked your way through the rest of the notebook, feel free to come back. And try to implement another activation function, network architecture, and adjust the modules above to suit your design purpose.\n",
    "- What impact does the amount of update steps have on the result?\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some space for you to write down your thoughts on the side quest questions \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What have we learned\n",
    "\n",
    "Your neural network has learned to depict an underlying function. \n",
    "You have learned how to set up a neural network training: Ranging from forwardpass to backpropagation. Without actively noticing, you touched on even more of the fundamental principles of deep learning and its nuts and bolts.\n",
    "\n",
    "Nevertheless, this was quite some work to get a single neuron to learn a rather disenchanting function, wasn't it?\n",
    "\n",
    "Luckily, python packages happen to do all of the brain-teasing and gradient crushing for us. Below we will import the most important ones and check their version, making sure you have the right ones. In case you do not have the expected version, update your packages in the anaconda prompt like this:\n",
    "\n",
    "<img src=\"graphics/tryCoffee.svg\" width=\"500\"><br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade tensorflow==2.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get to concentrate on the training hyperparameters and process and the neural network architecture design. Not to forget adding layers - juhuuu!\n",
    "\n",
    "<br>\n",
    "\n",
    "In the following, you will be introduced to...\n",
    "- .. [tensorflow](https://www.tensorflow.org/) \n",
    "- .. [keras](https://keras.io/)\n",
    "\n",
    "While you will clamber on the \"Hello World!\" of deep learning, the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is this joke that once you have added this import line to your code, \n",
    "# you are allowed to sell your product telling everyone that you are using AI. Life is easy, sometimes.\n",
    "import tensorflow as tf        \n",
    "print('Tensorflow version:', tf.__version__, '(Expected 2.4.0)')\n",
    "\n",
    "\n",
    "\n",
    "# Keras is a model-level library, meaning that it is built upon tensorflow (using it as a backend) - allowing for\n",
    "# high-level building blocks. Making it even easier to design neural networks.\n",
    "# We will call it as tf.keras\n",
    "\n",
    "# The tf and k abbreviations are best practice (same for numpy np and pandas pd), \n",
    "# since you do not want to type T E N S O R F L O W all over your code.\n",
    "# They are prevalent all over the industry and academia in a way that you'll risk a fight if you import them differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hint: \n",
    "The [space versus tabs](https://www.youtube.com/watch?v=SsoOG6ZeyUI&feature=emb_logo) war is a birthday party in comparison to breaking import guidelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the MNIST dataset\n",
    "\n",
    "Before any data science or machine learning project, it is essential to get to know your data. This will enable you to detect issues, noise, pitfalls and understand what your model will learn at the end of the day. With the MNIST dataset, you will be working with a beautiful, cleaned, easy to understand, low-memory, large-scale dataset. \n",
    "Full disclaimer, MNIST is excellent for learning and research purposes, yet this is not what you can expect in real-life. \n",
    "\n",
    "This initially becomes obvious as MNIST is so commonly used that the Keras packages got our back with loading the data in one line. We are loading pairs of samples and ground truth annotations for both the training set (for training our model) and the test set (for testing our model). \n",
    "\n",
    "<img src=\"graphics/jars.svg\" width=\"650\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üòÖ Grinning Face with Sweat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MNIST dataset in one line\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Printing the shape\n",
    "print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('x_test:', x_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "# Plotting data samples\n",
    "print('\\n Plot of the first 25 samples in the MNIST training set')\n",
    "numbers_to_display = 25\n",
    "num_cells = math.ceil(math.sqrt(numbers_to_display))\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(numbers_to_display):\n",
    "    plt.subplot(num_cells, num_cells, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(y_train[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the data arrays already gave away that we are using way more samples (60k) for training than for testing (10k). A typical ratio is 80% training and 20% testing as a rule of thumb. Actually, the 80% training is further split into the real training and a validation set, but that will not bother us here. \n",
    "\n",
    "[Deep dive on dataset generation, split and pitfalls (slide)](slides/slide_datasetgeneration.pdf)\n",
    "\n",
    "The other thing the shape gave away is the samples' dimensions $28x28x1$ and the dimension of the ground truth $1$.\n",
    "\n",
    "Along with the shape, we also plot the first 25 samples of the MNIST training set together with their ground truth labels (annotations). We can start to guess what the dataset is all about - it is a dataset purposed for digit recognition.\n",
    "\n",
    "\n",
    "### Data preprocessing\n",
    "We already talked quite a bit about [activation functions](https://cs231n.github.io/neural-networks-1/) (or non-linearity) and how they take a single, usually accumulated, number and perform a specific fixed mathematical operation on it. There are quite different ones, but you will soon find out that there emerges a shared pattern within deep neural networks that makes them beneficial in practice.\n",
    "\n",
    "- They usually map any incoming value so that they are asymptotically bounded from above and below. For example, the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) does map to a range of values between $0$ and $1$.\n",
    "They are usually most sensitive around zero because they tend to saturate to the bounds of the mapping for large numbers.\n",
    "\n",
    "Thus we can speed up the training process by preprocessing our data so that the first layer's activation function has an easier time picking up the pattern.\n",
    "\n",
    "For the MNIST, this is quickly done by normalizing the grayscale values $[0, 255]$ to a range between $[0, 1]$. Remember to do the preprocessing for both the training and the test samples.\n",
    "\n",
    "\n",
    "<img src=\"graphics/standardizedBeans.svg\" width=\"700\"><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_normalized = x_train/255 #'''your code goes here'''  \n",
    "x_test_normalized = x_test/255 #'''your code goes here''' \n",
    "\n",
    "# in the next step, we also need to reshape our input to fit our input layer later on. \n",
    "# This is due to keras expecting a definition for how many channels your input sample has, as we \n",
    "# deal with gray scale this is 1.\n",
    "x_train= x_train_normalized.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test_normalized.reshape(-1, 28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " ____   __   __ _  __  ____  _  _     ___  _  _  ____  ___  __ _ \n",
    "/ ___) / _\\ (  ( \\(  )(_  _)( \\/ )   / __)/ )( \\(  __)/ __)(  / )\n",
    "\\___ \\/    \\/    / )(   )(   )  /   ( (__ ) __ ( ) _)( (__  )  ( \n",
    "(____/\\_/\\_/\\_)__)(__) (__) (__/     \\___)\\_)(_/(____)\\___)(__\\_)\n",
    "\n",
    "Do not touch, just run!\n",
    "'''\n",
    "checker.test_normalize(x_train, x_train_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "In python a division can be done like this in python:  $3, 5~/~ 2 == 2.5$ <br>\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the neural network architecture\n",
    "\n",
    "Remember how much work it was to set up a single neuron - right. Now enjoy how Keras is helping you with this. We will call our neural network [marvin](https://en.wikipedia.org/wiki/Marvin_the_Paranoid_Android). Feel free to change it to whatever you like, in case you are not okay with it.\n",
    "\n",
    "Our model will follow some design requirements, it will be [sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential?version=stable) or it other words, feed forward, meaning that once a value passed through a unit, the unit will not see a value during that forward pass again. Or in other words, we do not have any recurrence in the architecture (feedback loops and so on). \n",
    "\n",
    "That's a lot to tackle, there will be quite some hyperparameters. This is for the visual learner - need a sandbox before we get started? I got your back [tensorflow sandbox](http://playground.tensorflow.org)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚ÄúDon't blame you,\" said Marvin and counted five hundred and ninety-seven thousand million sheep \n",
    "# before falling asleep again a second later.‚Äù\n",
    "marvin = tf.keras.models.Sequential()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding layers to Marvin is as easy as adding a layer while handing over that layer's specification. In the end, the layers will be stacked following the order in which your code gets compiled.\n",
    "\n",
    "Initially, we need an input layer, which can take in our input samples. Go ahead and set up a 2D convolutional layer, with an input shape fitting our image samples, with a kernel size of 5, 8 channels or filters, a stride of 1, and an activation function that does identity mapping, to begin with.\n",
    "\n",
    "[Convolutions? What?! would you mind explaining? - Yes, here you go. (slide)](slides/slide_convolutions.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marvin = tf.keras.models.Sequential()\n",
    "\n",
    "\n",
    "# Design the input layer\n",
    "marvin.add(tf.keras.layers.Convolution2D( #'''your layer type goes here'''  \n",
    "    input_shape=(28, 28, 1), #'''your input shape goes here'''  \n",
    "    kernel_size=5, #'''your kernel size goes here'''  \n",
    "    filters=8, #'''your number of filters goes here'''  \n",
    "    strides=1, #'''your stride goes here'''  \n",
    "    activation=tf.keras.activations.relu #'''your activation function goes here'''  \n",
    "))\n",
    "\n",
    "\n",
    "# Add your hidden layers here.\n",
    "marvin.add(tf.keras.layers.Convolution2D(\n",
    "    kernel_size=5,\n",
    "    filters=16,\n",
    "    strides=1,\n",
    "    activation=tf.keras.activations.sigmoid\n",
    "))\n",
    "\n",
    "\n",
    "# To prepare the outputs of your last hidden layer up your model you will need to \n",
    "# flatten all its outputs\n",
    "marvin.add(tf.keras.layers.Flatten())\n",
    "\n",
    "\n",
    "# Then it is about time to output your prediction, remember that there are 10 classes in our\n",
    "# MNIST digit classification problem\n",
    "marvin.add(tf.keras.layers.Dense(\n",
    "    units=10,\n",
    "    activation=tf.keras.activations.softmax\n",
    "))\n",
    "\n",
    "\n",
    "marvin.summary()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "Find out about Keras activation functions [here](https://keras.io/api/layers/activations/) and about layers [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers). The input shape only needs to be defined for the input layer. Using softmax in your output layer will give you the pleasant effect of a probability distribution for your prediction. The output of softmax transformation has all values non-negative and sum to 1.\n",
    "\n",
    "<br>    \n",
    "\n",
    "Keep on designing your model until your model summary is the same as this one:\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the neural network training\n",
    "\n",
    "Great, that's it. Take a moment to realize how quickly and with how much ease (maybe not during the first time) you designed this architecture with $67k$ parameters. Also, within the blink of an eye, you created a neural network that has more neurons than a box jellyfish ($17.5k$ neurons), a sea slug ($18k$), coming in behind the fruitfly ($250k$). To put things into perspective, humans come in second with ($8.6\\times10^{10}$) neurons, only surpassed by the African elephant. Find a full list [here](https://en.wikipedia.org/wiki/List_of_animals_by_number_of_neurons). \n",
    "\n",
    "Next, we need to train Marvin to assign a class prediction when we hand him over a sample of a digit. \n",
    "\n",
    "Remember, when we set up our loss function and learning rate? That's precisely what we are going to do here as well, yet again with the help of Keras.\n",
    "\n",
    "\n",
    "<img src=\"graphics/blackCoffee.svg\" width=\"500\"><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your learning rate and optimizer\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "\n",
    "marvin.compile(\n",
    "    optimizer=sgd,\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "Make sure you understand the difference between a [metric](https://keras.io/api/metrics/) (only tells you how good your neural network performs) and a [loss](https://keras.io/api/losses/) (tells you how your model performance, but also directly tells your model how to update its weights to improve performance). <br>\n",
    "\n",
    "[The loss is at loss - metric does the trick (slide)](slides/slide_objectivefunctionandmetric.pdf) <br>\n",
    "[Why would I use SGD? (slide)](slides/slide_sgd.pdf)\n",
    "\n",
    "\n",
    "##### Side Quest:\n",
    "Feel free to play around with your neural network and the learning configuration.\n",
    "Great places to start are:\n",
    "- The [optimizer](https://keras.io/api/optimizers/), the losses, the metrics.\n",
    "- Adding layers, changing Kernel sizes, strides, and activation functions.\n",
    "- You might have guessed the learning rate. Truth be told, playing around with each hyperparameter will get you some experience and intuition for its behavior. \n",
    "\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some space for you to write down your thoughts on the side quest questions and experiments\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, the model\n",
    "\n",
    "Now it is time to plug our data into Marvin and let him learn to recognize the digit within an image sample. Besides plugging in the data, and the labels, we need to let Keras know how long we want to train Marvin. In deep learning, training on each sample of the training set precisely ones is called an epoch.\n",
    "Obviously, the longer you train, the better your model will fit the underlying pattern. Then you can also set how many samples Marvin will see during each update step (this is called batch size). The more samples, the better the gradient approximation, but at some point, you will also run into hardware limitations.\n",
    "\n",
    "[Praise the minibatch (slide)](slides/slide_minibatch.pdf)\n",
    "\n",
    "Yet, remember that it is the unseen data of your validation set that machine learning approaches are after. This is also what lets deep learning stand out from pure optimization.\n",
    "\n",
    "[Seen data, unseen data, optimization, regularization. These slides will take home the point (slide)](slides/slide_regularization.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get ready for your first training run.\n",
    "marvin.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=8,\n",
    "    validation_data=(x_test, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "Even if you stop the training, the model's current state will be saved as long as your juypter notebook session is active. Also, there is more to specify for training within the fit function of Keras. Check it out [here](https://keras.io/api/models/model_training_apis/) <br>\n",
    "\n",
    "##### Side Quest:\n",
    "Reach at least a validation performance of $95 \\%$ or higher before moving on. To achieve this, you are free to change whatever you want within the architecture and training process. Obviously, better performance is an excellent achievement.\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# after the training finishes, we will also save Marvin in Keras style (HDF5), so we do not have to \n",
    "# train him again\n",
    "# every time we start our computer. Obviously, by changing the model_name, you can also save different\n",
    "# configurations of Marvin. The name has to be a string, like this: 'name'\n",
    "model_name = 'marvin.h5' # ''' Your model name goes here'''\n",
    "Marvin.save(model_name, save_format='h5')\n",
    "\n",
    "# It is best practice to indicate what configuration changes you did within the name, so you know\n",
    "# which model you need to load already from its name\n",
    "# Let's say instead of a learning rate of 0.001 you used 0.1, your naming could then look like:\n",
    "# 'marvin_lr01.h5'\n",
    "\n",
    "print('Success! You saved Marvin as: ', model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluating Marvin\n",
    "\n",
    "Yes, you are right. The job is basically done. Marvin has been trained, and you also made sure that the model recognizes more than $95 \\%$ of the unseen samples correctly. Would you say that you know what Marvin learned or what exactly he is capable of in the end?\n",
    "\n",
    "#### Side Quest:\n",
    "Try to answer the following questions:\n",
    "\n",
    "- Which digit is hardest for Marvin to predict correctly?\n",
    "- Which two digits does Marvin confuse most often?\n",
    "- Which digit does he perform best in?\n",
    "- How do digits look that Marvin was not able to predict correctly? Can you see why?\n",
    "\n",
    "These questions are hard to answer and clarify that an in depth analysis of the model's performance is paramount. I like a mix of visualization (which helps my intuition) and number crushing (which helps to write excellent reports and prove my intuition with numbers).\n",
    "\n",
    "So in the following, we will go ahead and answer some of the questions mentioned above. Along the way, you will learn some strategies to evaluate your neural network.\n",
    "\n",
    "\n",
    "<img src=\"graphics/stars.svg\" width=\"500\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üòÜ Grinning Squinting Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some space for you to write down your thoughts on the side quest questions and experiments\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Intuitive Approach\n",
    "\n",
    "We will start with the intuition building approach. The following code will plot the image samples. The caption below is Marvin's prediction. If he is right, the sample will get a greenish touch. If he is wrong, a reddish one. Try to answer the above questions with the results from the intuitive plot you will generate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load a saved marvin configuration you want to evaluate\n",
    "model_name = 'marvin.h5'\n",
    "marvin_reloaded = tf.keras.models.load_model(model_name)\n",
    "\n",
    "# Let Marvin predict on the test set, so we have some data to evaluate his performance.\n",
    "predictions = marvin_reloaded.predict([x_test])\n",
    "\n",
    "# Remember that the prediction of Marvin is a probability distribution over all ten-digit classes\n",
    "# We want him to assign the digit class with the highest probability to the sample.\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "#pd.DataFrame(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot for the intuitive approach\n",
    "\n",
    "numbers_to_display = 196\n",
    "num_cells = math.ceil(math.sqrt(numbers_to_display))\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for plot_index in range(numbers_to_display):    \n",
    "    predicted_label = predictions[plot_index]\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    color_map = 'Greens' if predicted_label == y_test[plot_index] else 'Reds'\n",
    "    plt.subplot(num_cells, num_cells, plot_index + 1)\n",
    "    plt.imshow(x_test_normalized[plot_index].reshape((28, 28)), cmap=color_map)\n",
    "    plt.xlabel(predicted_label)\n",
    "\n",
    "plt.subplots_adjust(hspace=1, wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some space for you to write down your thoughts on the side quest questions and experiments\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### The Empirical Number Crushing Approach\n",
    "\n",
    "Now that you intuitively answered the questions, let's do some data analysis on the results and see how much you were off and where you were right and picked up patterns in Marvin's capabilities. \n",
    "\n",
    "#### Hint: \n",
    "Usually, visualizations to building your intuition are most beneficial during your first training runs on problems you do not know what to expect from your neural network. In visualizations, you might also be able to pick up wrong label assignments (think somebody labeled all \"8\"s as \"9\"s), ambiguous or overlapping classes (think somebody labeled all \"6\"s and \"9\"s as the same class). These things do not usually happen in beautifully cleaned datasets such as MNIST. But I promise you, you will be surprised how messy these public datasets out there are. And further, I promise you that if you are building your own datasets, these bugs will be there too and want to be found.\n",
    "\n",
    "Now for the analytical approach, we will look at one of the most basic strategies, the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) or error matrix. This table shows the actual class of the sample (horizontal), plotted over which class it was predicted (vertical). To make things clearer, numbers in the diagonal are true positives, meaning Marvin predicted them correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = tf.math.confusion_matrix(y_test, predictions)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 7))\n",
    "sn.heatmap(\n",
    "    confusion_matrix,\n",
    "    annot=True,\n",
    "    linewidths=.7,\n",
    "    fmt=\"d\",\n",
    "    square=True,\n",
    "    ax=ax,\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some space for you to write down your thoughts on the side quest questions and experiments\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to say Goodbye\n",
    "Well, actually not quite yet. Now, you are capable of training neural networks, you learned about the fundamentals, and you also can evaluate your model's performance professionally. Next, I want to point out some things you can look into along your path to become a machine learning engineer or deep learning researcher.\n",
    "\n",
    "[Training your models will soon be way more difficult - take these sanity checks to make sure you stay on track. (slide)](slides/slide_sanitycheck.pdf)\n",
    "\n",
    "#### In this notebook\n",
    "You can definitely come back to this notebook again and again. Play with the hyperparameters, try to follow through the notebook with other datasets. There are quite a few [directly supported](https://www.tensorflow.org/datasets/catalog/overview) by tensorflow, so they can easily plug into this notebook. Or try to load a dataset from disk and write your own data pipeline.\n",
    "\n",
    "Come back and try working on other layers, such as Dropout or Batchnorm. Get to know regularization strategies, such as augmentation, and try to implement them here as well. Think of this notebook as your bridgehead to the world of deep learning. This is the place to break things and try stuff.\n",
    "\n",
    "#### Literature\n",
    "Reading helps to get to know new approaches, different ideas, and dive deeper and build up your knowledge. I recommend the following books to start with. From there, start to go with the actual papers.\n",
    "\n",
    "[To start with](http://www.deeplearningbook.org/) Deep Learning Book - Ian Goodfellow and Yoshua Bengio and Aaron Courville\n",
    "Abstract: The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in particular. The online version of the book is now complete and will remain available online for free.\n",
    "\n",
    "[To understand the fundamentals](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf) Pattern Recognition and Machine Learning - Bishop Abstract: ThisnewtextbookreÔ¨Çectstheserecentdevelopmentswhileprovidingacomprehensive introduction to the Ô¨Åelds of pattern recognition and machine learning. It is aimed at advanced undergraduates or Ô¨Årst year Ph.D. students and researchers and practitioners and assumes no previous knowledge of pattern recognition or machine learning concepts.\n",
    "\n",
    "[To get practice](https://www.coursera.org/courses?query=andrew%20ng) Deep Learning and Machine Learning Courses on Coursera - Andrew Ng Abstract: In these courses, you will learn the foundations of Deep Learning, understand how to build neural networks, and learn how to lead successful machine learning projects. You will learn about Convolutional networks, RNNs, LSTM, Adam, Dropout, BatchNorm, Xavier/He initialization, and more. You will work on case studies from healthcare, autonomous driving, sign language reading, music generation, and natural language processing. You will master not only the theory but also see how it is applied in industry. You will practice all these ideas in Python and in TensorFlow, which we will teach.\n",
    "Link: \n",
    "\n",
    "[From beginning to mastery](http://cs231n.stanford.edu/) CS231n: Convolutional Neural Networks for Visual Recognition - Andrej Karpathy Abstract: Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications is a visual recognition task such as image classification, localization, and detection. Recent developments in neural networks (aka ‚Äúdeep learning‚Äù) have significantly advanced the performance of these state-of-the-art visual recognition systems. This course is a deep dive into details of the deep learning architectures focusing on learning end-to-end models for these tasks, particularly image classification. During the 10-week course, students will learn to implement, train, and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision. The final assignment will involve training a multi-million parameter convolutional neural network and applying it on the largest image classification dataset (ImageNet). We will focus on teaching how to set up image recognition, the learning algorithms (e.g., backpropagation), practical engineering tricks for training and fine-tuning the networks, and guide the students through hands-on assignments and a final course project. Much of the background and materials of this course will be drawn from the ImageNet Challenge. There is also a YouTube [lecture](https://www.youtube.com/watch?v=NfnWJUyUJYU&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC). \n",
    "\n",
    "\n",
    "#### Mailing list\n",
    "There is so much more out there - different neural network architecture, different learning paradigms, not to mention endless real-world application possibilities.\n",
    "Every now and then, I'll share another part of this world. If you want to keep posted, write an e-mail to mark.schutera@kit.edu with the subject: \"AppliedDeepLearningSchool Newsletter\" - I promise there will be only a little spam.\n",
    "\n",
    "#### Someone deleted the internet, or a link broke in the notebook? \n",
    "You wanted to know more, click a link - and it was dead? That happens, as the internet and especially software is a living thing. In that case, I would be delighted if you'd let me know so that stuff gets fixed for the ones after you. You noticed a bug, a typo, an error, something? Send me an e-mail to mark.schutera@mailbox.org subject: \"AppliedDeepLearningSchool Bug Hunt\".\n",
    "Any other feedback is also highly appreciated.\n",
    "\n",
    "#### Can you use this code for a school or university project?\n",
    "Sure thing, go ahead. Just make sure to cite it appropriately. If you are unsure on how to do this, again, feel free to reach out. I can not wait to see you burning through school and university competitions with deep learning applications. \n",
    "\n",
    "\n",
    "<img src=\"graphics/droplet.svg\" width=\"650\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üòá Smiling Face with Halo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some space for you to write down your thoughts\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
