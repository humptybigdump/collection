{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2lgHznLKCik",
    "outputId": "bb1871b4-cb9e-4130-a95f-2284550e4168"
   },
   "outputs": [],
   "source": [
    "# Google Colab\n",
    "!pip3 install box2d-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Machine\n",
    "!pip3 install torch\n",
    "!pip3 install gym\n",
    "!pip3 install box2d-py\n",
    "!pip3 install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDnRyak4HXqD"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3tQa0MvCDrG",
    "outputId": "33d274cb-48c8-4fcb-88f6-b62ea1f2ad5b"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "print(\"PyTorch:\\t{}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 31\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zhf2UmjcHgPm"
   },
   "source": [
    "## GYM Environments\n",
    "* `CartPole-v1`\n",
    "<img src=\"cartpole.jpg\"\n",
    "     alt=\"World\"\n",
    "     width=\"500\" />\n",
    "\n",
    "\n",
    "* `LunarLander-v2`\n",
    "<img src=\"LunarLander.png\"\n",
    "     alt=\"World\"\n",
    "     width=\"500\" />\n",
    "[source](https://shiva-verma.medium.com/solving-lunar-lander-openaigym-reinforcement-learning-785675066197)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGAwAzv1HmqZ"
   },
   "outputs": [],
   "source": [
    "env_1 = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gjk55Io5YdVi",
    "outputId": "fbb2b509-4fc2-43d8-b910-a79e6b9c16ad"
   },
   "outputs": [],
   "source": [
    "env_1.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space A:\n",
    "* left\n",
    "* right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list = [\"left\", \"right\"]\n",
    "print(env_1.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation (State) Space S:\n",
    "* position of cart\n",
    "* velocity of cart\n",
    "* angle of pole\n",
    "* rotation rate of pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env_1.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition:\n",
    "A transition is a tuple consisting of \n",
    "* the current state,\n",
    "* the action used, \n",
    "* the received reward , \n",
    "* and the new state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFNndUKBIIRP"
   },
   "source": [
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/300px-Reinforcement_learning_diagram.svg.png).\n",
    "\n",
    "Start the game (Episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j4fyun3jILKq",
    "outputId": "833ba7ef-9d98-47c8-bcb6-5154befa3247"
   },
   "outputs": [],
   "source": [
    "state_0 = env_1.reset()\n",
    "print(state_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make one interaction with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env_1.action_space.sample()\n",
    "print(\"action\" , action_list[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env_1.step(action) \n",
    "print(\"reward\", reward)\n",
    "print(\"done\", done) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode = 3\n",
    "horizon   = 200\n",
    "\n",
    "# Set up lists to hold results\n",
    "for i_episode in range(n_episode):\n",
    "    states  = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    state = env_1.reset()\n",
    "    for t in range(horizon):\n",
    "        env_1.render()\n",
    "        time.sleep(0.05)\n",
    "        action = env_1.action_space.sample()\n",
    "        state, reward, done, info = env_1.step(action)\n",
    "        ## Add the transition to the lists\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "    print(\"total Reward\", np.sum(rewards))\n",
    "# Convert lists to numpy arrays\n",
    "states =  np.array(states)\n",
    "actions=  np.array(actions)\n",
    "rewards=  np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltY9Qgi4l_VX"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the States "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kD_mYYCNm3T_",
    "outputId": "46c9b74d-2122-48b1-aa0c-9f69ee79d789"
   },
   "outputs": [],
   "source": [
    "duration = np.arange(states.shape[0])\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mXJ684o6mBRe",
    "outputId": "a87f04d1-00c8-440d-f3a0-7f65c2042265"
   },
   "outputs": [],
   "source": [
    "# plot results (States + Reward)\n",
    "fig1, (ax1, ax2, ax3,ax4,ax5) = plt.subplots(figsize=(25,45), nrows=5, ncols=1)\n",
    "\n",
    "\n",
    "# plot ensemble predictions\n",
    "ax1.plot(duration, states[ :, 0], label='Pos_x' , marker='o')\n",
    "ax2.plot(duration, states[ :, 1], label='Vel_x' , marker='o')\n",
    "ax3.plot(duration, states[ :, 2], label='Ang' , marker='o')\n",
    "ax4.plot(duration, states[ :, 3], label='Vel_ang' , marker='o')\n",
    "ax5.plot(duration, rewards[:], label='reward ', marker='o')\n",
    "\n",
    "# set title\n",
    "ax1.set_title('cart position in x axis')\n",
    "ax2.set_title('cart velocity x axis')\n",
    "ax3.set_title('pole angle')\n",
    "ax4.set_title('pole angular velocity')\n",
    "ax5.set_title('reward')\n",
    "\n",
    "# plot legend\n",
    "for ax in (ax1, ax2, ax3,ax4,ax5):\n",
    "    ax.legend(loc='best', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.hist(actions)\n",
    "plt.title(\"actions histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy (Deep RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q20Tnt5fCEz3"
   },
   "outputs": [],
   "source": [
    "class policy_estimator():\n",
    "    def __init__(self, env):\n",
    "        self.n_inputs = env.observation_space.shape[0]\n",
    "        self.n_outputs = env.action_space.n  \n",
    "        # Define network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.n_inputs, 256), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, self.n_outputs),\n",
    "            nn.Softmax(dim=-1))\n",
    "    \n",
    "    def predict(self, state):\n",
    "        action_probs = self.network(torch.FloatTensor(state))\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = policy_estimator(env_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxIQrzjSHQG-"
   },
   "source": [
    "### Discount Rewards\n",
    "\n",
    "<img src=\"https://imgur.com/g3mYTzn.png\" width=\"400\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47KO0jMeGjQn"
   },
   "source": [
    "\n",
    "### Causality\n",
    "$\\sum_{t'=t}^H R(s_{t'},a_{t'})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "takes a list of rewards {r_0, r_1, ..., r_t', ... r_T},\n",
    "and returns a list where the entry in each index t' is\n",
    "sum_{t'=t}^T gamma^(t'-t) * r_{t'}\n",
    "\"\"\"\n",
    "gamma = 0.9\n",
    "# We are in the state 1,1\n",
    "rewards = [1,1,1,1,1,1]\n",
    "print(1 + 0.9 + 0.9**2 + 0.9**3 + 0.9**4 + 0.9**5)\n",
    "print(0.9 + 0.9**2 + 0.9**3 + 0.9**4 + 0.9**5)\n",
    "print(0.9**2 + 0.9**3 + 0.9**4 + 0.9**5)\n",
    "print(0.9**3 + 0.9**4 + 0.9**5)\n",
    "print(0.9**4 + 0.9**5)\n",
    "print(0.9**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    # Reverse the array direction for cumsum and then\n",
    "    # revert back to the original order\n",
    "    #r = r[::-1].cumsum()[::-1]\n",
    "    r = np.array([gamma**i * rewards[i] for i in range(len(rewards))])\n",
    "    r = r[::-1].cumsum()[::-1]\n",
    "    return r - r.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9_lEPEjMFiD"
   },
   "source": [
    "## Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.99\n",
    "num_episodes = 1200 # run agent for this many episodes\n",
    "batch_size= 10\n",
    "lr = 0.001 # learning rate for actor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(policy.network.parameters(), \n",
    "                        lr=lr)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNnrfaWwMSq5"
   },
   "source": [
    "## Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = np.arange(env_1.action_space.n)\n",
    "action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMoyCqp4KyCU",
    "outputId": "fb3d1661-d7d6-4a85-fa13-33d0a43d5326"
   },
   "outputs": [],
   "source": [
    "# Set up lists to hold results\n",
    "total_rewards = []\n",
    "batch_rewards = []\n",
    "batch_actions = []\n",
    "batch_states  = []\n",
    "batch_counter = 1\n",
    "ep = 0\n",
    "epoch = 0\n",
    "while ep < num_episodes:\n",
    "    s_0 = env_1.reset()\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    done = False\n",
    "    while done == False:\n",
    "        # Get actions and convert to numpy array\n",
    "        action_probs = policy.predict(s_0).detach().numpy()\n",
    "        action = np.random.choice(action_space, p=action_probs)\n",
    "        s_1, r, done, _ = env_1.step(action)\n",
    "        states.append(s_0)\n",
    "        rewards.append(r)\n",
    "        actions.append(action)\n",
    "        s_0 = s_1\n",
    "        \n",
    "        # If done, batch data\n",
    "        if done:\n",
    "            batch_rewards.extend(discount_rewards(rewards, gamma))\n",
    "            batch_states.extend(states)\n",
    "            batch_actions.extend(actions)\n",
    "            batch_counter += 1\n",
    "            total_rewards.append(sum(rewards))\n",
    "            \n",
    "            # If batch is complete, update network\n",
    "            if batch_counter == batch_size:\n",
    "                print(\"Epoch:\", epoch)\n",
    "                optimizer.zero_grad()\n",
    "                state_tensor = torch.FloatTensor(batch_states)\n",
    "                reward_tensor = torch.FloatTensor(batch_rewards)\n",
    "                # Actions are used as indices, must be \n",
    "                # LongTensor\n",
    "                action_tensor = torch.LongTensor(batch_actions)\n",
    "                # Calculate loss\n",
    "                logprob = torch.log(policy.predict(state_tensor))\n",
    "                selected_logprobs = reward_tensor * torch.gather(logprob, 1,action_tensor.unsqueeze(1)).squeeze()\n",
    "                loss = -selected_logprobs.mean()\n",
    "                \n",
    "                # Calculate gradients\n",
    "                loss.backward()\n",
    "                # Apply gradients\n",
    "                optimizer.step()\n",
    "                \n",
    "                batch_rewards = []\n",
    "                batch_actions = []\n",
    "                batch_states = []\n",
    "                batch_counter = 1\n",
    "                epoch += 1\n",
    "                \n",
    "            avg_rewards = np.mean(total_rewards[-100:])\n",
    "            # Print running average\n",
    "            print(\"Episode: \", ep + 1)\n",
    "            print(\"Average of last 100 Episode:\", avg_rewards)\n",
    "            ep += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results\n",
    "plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rewards\n",
    "episode = np.arange(len(total_rewards))\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(episode, total_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode = 3\n",
    "horizon   = 200\n",
    "\n",
    "# Set up lists to hold results\n",
    "for i_episode in range(n_episode):\n",
    "    states  = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    state = env_1.reset()\n",
    "    for t in range(horizon):\n",
    "        env_1.render()\n",
    "        time.sleep(0.05)\n",
    "        # Get actions and convert to numpy array\n",
    "        action_probs = policy.predict(state).detach().numpy()\n",
    "        #action = np.random.choice(action_space, p=action_probs)\n",
    "        action = np.argmax(action_probs)\n",
    "        state, reward, done, info = env_1.step(action)\n",
    "        ## Add the transition to the lists\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "    print(\"total Reward\", np.sum(rewards))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "states =  np.array(states)\n",
    "actions=  np.array(actions)\n",
    "rewards=  np.array(rewards)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.hist(actions)\n",
    "plt.title(\"actions histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_2 = gym.make('LunarLander-v2')\n",
    "env_2.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4txLgnSgIc9t"
   },
   "source": [
    "### Action Space A:\n",
    "* Four discrete actions available: \n",
    "  * do nothing, \n",
    "  * fire left orientation engine, \n",
    "  * fire main engine, \n",
    "  * fire right orientation engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vi0o3ZHcIh_z",
    "outputId": "e639d141-7dec-458a-a4fb-5d3fd25dcd7c"
   },
   "outputs": [],
   "source": [
    "action_list = [\"do nothing\", \"fire left orientation engine\", \"fire main engine\" , \"fire right orientation engine\"]\n",
    "print(env_2.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cf0qwXqohUdi"
   },
   "source": [
    "### Observation (State) Space S:\n",
    "* position in x axis and y axis(hieght)\n",
    "* x,y axis velocity \n",
    "* lander angle and angular velocity\n",
    "* left and right contact points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Ijy6spkh0Md",
    "outputId": "465f6a6a-9efe-4ce8-be9e-6130eeaf06c8"
   },
   "outputs": [],
   "source": [
    "print(env_2.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_9tDj8aj4wK"
   },
   "source": [
    "### Reward Function R(s,a):\n",
    "* Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. \n",
    "* If lander moves away from landing pad it loses reward back. \n",
    "* Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. \n",
    "* Each leg ground contact is +10. \n",
    "* Firing main engine is -0.3 points each frame. \n",
    "* Solved is 200 points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ok35emJkhce"
   },
   "source": [
    "### Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6efj45sWkgbE",
    "outputId": "3b6cf5db-8154-4f78-de94-176a7e89a3e2"
   },
   "outputs": [],
   "source": [
    "from utils import on_environment\n",
    "\n",
    "n_episode = 3\n",
    "horizon   = 200\n",
    "policy = \"random\"\n",
    "\n",
    "states,actions,rewards = on_environment(env_2,policy,n_episode,horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = np.arange(states.shape[0])\n",
    "\n",
    "# plot results (States + Reward)\n",
    "fig1, (ax1, ax2, ax3,ax4,ax5,ax6,ax7,ax8,ax9) = plt.subplots(figsize=(25,45), nrows=9, ncols=1)\n",
    "\n",
    "\n",
    "# plot ensemble predictions\n",
    "ax1.plot(duration, states[ :, 0], label='Pos_x' , marker='o')\n",
    "ax2.plot(duration, states[ :, 1], label='Pos_y' , marker='o')\n",
    "ax3.plot(duration, states[ :, 2], label='Vel_x' , marker='o')\n",
    "ax4.plot(duration, states[ :, 3], label='Vel_y' , marker='o')\n",
    "ax5.plot(duration, states[ :, 4], label='Ang' , marker='o')\n",
    "ax6.plot(duration, states[ :, 5], label='Vel_ang' , marker='o')\n",
    "ax7.plot(duration, states[ :, 6],label='Cont_rig', marker='o')\n",
    "ax8.plot(duration, states[ :, 7], label='Cont_lef ' , marker='o')\n",
    "ax9.plot(duration, rewards[:], label='reward ', marker='o')\n",
    "\n",
    "# set title\n",
    "ax1.set_title('lander position in x axis')\n",
    "ax2.set_title('lander position in y axis(hieght)')\n",
    "ax3.set_title('lander velocity x axis')\n",
    "ax4.set_title('lander velocity y axis')\n",
    "ax5.set_title('lander angle')\n",
    "ax6.set_title('lander angular velocity')\n",
    "ax7.set_title('right contact points')\n",
    "ax8.set_title('left contact points')\n",
    "ax9.set_title('reward')\n",
    "\n",
    "# plot legend\n",
    "for ax in (ax1, ax2, ax3,ax4,ax5,ax6,ax7,ax8,ax9):\n",
    "    ax.legend(loc='best', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "nDggTbz09UEy",
    "outputId": "cddbe0e2-f39e-4a6d-c5ce-f936b080aa7c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.hist(actions)\n",
    "plt.title(\"actions histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpZ3gWrTHd8D"
   },
   "source": [
    "## Define Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzpQ3DONsk9k"
   },
   "outputs": [],
   "source": [
    "policy = policy_estimator(env_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ub4f28wJMIrq"
   },
   "outputs": [],
   "source": [
    "args = dict()\n",
    "args[\"gamma\"]=0.99\n",
    "args[\"num_episodes\"] = 1200 # run agent for this many episodes\n",
    "args[\"batch_size\"]= 10\n",
    "args[\"lr\"] = 0.001 # learning rate for actor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbVktwTQPtcj"
   },
   "source": [
    "### Optimizaer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_oz5GZDPswJ"
   },
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(policy.network.parameters(), \n",
    "                        lr=args[\"lr\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import reinforce\n",
    "policy,total_rewards = reinforce(env_2,policy,optimizer,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "jYeJA-jqKHLy",
    "outputId": "eed5850e-bcea-4348-ef8e-df24fd8ac9d1"
   },
   "outputs": [],
   "source": [
    "# plot rewards\n",
    "episode = np.arange(len(total_rewards))\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(episode, total_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode = 3\n",
    "horizon   = 200\n",
    "states,actions,rewards = on_environment(env_2,policy,n_episode,horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AWN0N6kif19Z",
    "outputId": "3347236f-fdfe-4194-9781-a332a78dc422"
   },
   "outputs": [],
   "source": [
    "# plot results (States + Reward)\n",
    "fig1, (ax1, ax2, ax3,ax4,ax5,ax6,ax7,ax8,ax9) = plt.subplots(figsize=(25,45), nrows=9, ncols=1)\n",
    "\n",
    "duration = np.arange(states.shape[0])\n",
    "\n",
    "# plot ensemble predictions\n",
    "ax1.plot(duration, states[ :, 0], label='Pos_x' , marker='o')\n",
    "ax2.plot(duration, states[ :, 1], label='Pos_y' , marker='o')\n",
    "ax3.plot(duration, states[ :, 2], label='Vel_x' , marker='o')\n",
    "ax4.plot(duration, states[ :, 3], label='Vel_y' , marker='o')\n",
    "ax5.plot(duration, states[ :, 4], label='Ang' , marker='o')\n",
    "ax6.plot(duration, states[ :, 5], label='Vel_ang' , marker='o')\n",
    "ax7.plot(duration, states[ :, 6],label='Cont_rig', marker='o')\n",
    "ax8.plot(duration, states[ :, 7], label='Cont_lef ' , marker='o')\n",
    "ax9.plot(duration, rewards[:], label='reward ', marker='o')\n",
    "\n",
    "# set title\n",
    "ax1.set_title('lander position in x axis')\n",
    "ax2.set_title('lander position in y axis(hieght)')\n",
    "ax3.set_title('lander velocity x axis')\n",
    "ax4.set_title('lander velocity y axis')\n",
    "ax5.set_title('lander angle')\n",
    "ax6.set_title('lander angular velocity')\n",
    "ax7.set_title('right contact points')\n",
    "ax8.set_title('left contact points')\n",
    "ax9.set_title('reward')\n",
    "\n",
    "# plot legend\n",
    "for ax in (ax1, ax2, ax3,ax4,ax5,ax6,ax7,ax8,ax9):\n",
    "    ax.legend(loc='best', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "mbDLBKZnf8GV",
    "outputId": "5844b2b7-1f10-4bce-e301-c414469db365"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.hist(actions)\n",
    "plt.title(\"actions histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJ7-Ch3jmQ5F"
   },
   "source": [
    "## Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XODIy0mfPnn",
    "outputId": "169ceccd-33eb-430c-b3c6-1711be77685d"
   },
   "outputs": [],
   "source": [
    " !pip install stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WiNG4AuudNiy",
    "outputId": "890dee8c-5238-4777-d6ab-d18c48da60bf"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "policy = A2C(\"MlpPolicy\", env_2, learning_rate=0.007, verbose=1)\n",
    "policy.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ivo4ZxpqmZak",
    "outputId": "e6d32eb2-1e6b-4469-f974-64192c573f7b"
   },
   "outputs": [],
   "source": [
    "n_episode = 3\n",
    "horizon   = 250\n",
    "\n",
    "# Set up lists to hold results\n",
    "\n",
    "for i_episode in range(n_episode):\n",
    "    states  = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    state = env_2.reset()\n",
    "    for t in range(horizon):\n",
    "        env_2.render()\n",
    "        time.sleep(0.05)\n",
    "        # Get actions and convert to numpy array\n",
    "        action, _states = policy.predict(state)\n",
    "        state, reward, done, info = env_2.step(action)\n",
    "        ## Add the transition to the lists\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "    \n",
    "    print(\"total Reward\", np.sum(rewards))\n",
    "        \n",
    "# Convert lists to numpy arrays\n",
    "states =  np.array(states)\n",
    "actions=  np.array(actions)\n",
    "rewards=  np.array(rewards)\n",
    "env_2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sHBdSHN_m4Qk",
    "outputId": "9029551e-6955-46fe-8c88-6708f249732e"
   },
   "outputs": [],
   "source": [
    "# plot results (States + Reward)\n",
    "fig1, (ax1, ax2, ax3,ax4,ax5,ax6,ax7,ax8,ax9) = plt.subplots(figsize=(25,45), nrows=9, ncols=1)\n",
    "\n",
    "duration = np.arange(states.shape[0])\n",
    "\n",
    "# plot ensemble predictions\n",
    "ax1.plot(duration, states[ :, 0], label='Pos_x' , marker='o')\n",
    "ax2.plot(duration, states[ :, 1], label='Pos_y' , marker='o')\n",
    "ax3.plot(duration, states[ :, 2], label='Vel_x' , marker='o')\n",
    "ax4.plot(duration, states[ :, 3], label='Vel_y' , marker='o')\n",
    "ax5.plot(duration, states[ :, 4], label='Ang' , marker='o')\n",
    "ax6.plot(duration, states[ :, 5], label='Vel_ang' , marker='o')\n",
    "ax7.plot(duration, states[ :, 6],label='Cont_rig', marker='o')\n",
    "ax8.plot(duration, states[ :, 7], label='Cont_lef ' , marker='o')\n",
    "ax9.plot(duration, rewards[:], label='reward ', marker='o')\n",
    "\n",
    "# set title\n",
    "ax1.set_title('lander position in x axis')\n",
    "ax2.set_title('lander position in y axis(hieght)')\n",
    "ax3.set_title('lander velocity x axis')\n",
    "ax4.set_title('lander velocity y axis')\n",
    "ax5.set_title('lander angle')\n",
    "ax6.set_title('lander angular velocity')\n",
    "ax7.set_title('right contact points')\n",
    "ax8.set_title('left contact points')\n",
    "ax9.set_title('reward')\n",
    "\n",
    "# plot legend\n",
    "for ax in (ax1, ax2, ax3,ax4,ax5,ax6,ax7,ax8,ax9):\n",
    "    ax.legend(loc='best', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "sZOUR3dvnS1_",
    "outputId": "e4c880aa-7bf6-4fa7-d5a8-23e10732228e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.hist(actions)\n",
    "plt.title(\"actions histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C328wFgFnffE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled23.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "venv_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
