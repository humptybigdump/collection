{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Klassifikation mit CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dieser Übung steht der MNIST Datensatz im Mittelpunkt. Dieser Datensatz enhält eine Klassifikationsaufgabe mit Bildern handschriftlicher Ziffern. Zunächst wird ein einfacher Klassifikator auf dem Datensatz trainiert und evaluiert. Im weiteren Verlauf wird dann ein einfaches CNN zur Klassifikation verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Der MNIST Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset can be automatically downloaded from within TensorFlow\n",
    "import tensorflow.keras.datasets.mnist as keras_mnist\n",
    "(x_train, y_train), (x_test, y_test) = keras_mnist.load_data()\n",
    "\n",
    "# In this source, there is no separated validation data, so we have to split the training set\n",
    "x_train, x_val = np.split(x_train, [50000])\n",
    "y_train, y_val = np.split(y_train, [50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Shape: (50000, 28, 28)\n",
      "Validation Set Shape: (10000, 28, 28)\n",
      "Test Set Shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Set Shape: \" + str(x_train.shape))\n",
    "print(\"Validation Set Shape: \" + str(x_val.shape))\n",
    "print(\"Test Set Shape: \" + str(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.concatenate((x_train[:6]), axis=1))\n",
    "plt.gray()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 6 training labels: \n",
      "[5 0 4 1 9 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"First 6 training labels: \\n\" + str(y_train[:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 6 training lables as one-hot encoded vectors:\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Each line represents one label.\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "print(\"First 6 training lables as one-hot encoded vectors:\\n\", str(y_train[:6]))\n",
    "print(\"Each line represents one label.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], x_val.shape[2], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_mnist_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "keras_mnist_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "keras_mnist_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving MNIST - ein erstes einfaches NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der letzte Abschnitt diente im Wesentlichen zur Visualisierung der einzelnen Schritte, die beim Laden der Trainingsdaten durchgeführt werden. Nachfolgend wird der MNIST Datensatz aus tensorflow_datasets verwendet. Dort wurden die gezeigten Schritte bereits beim Laden durchgeführt. Ein Vorteil ist dabi auch, dass der Datensatz direkt als dataset Objekt vorliegt. Es ist zwar grundsätzlich nicht notwendig, dass ein Dataset Objekt aus den Daten erzeugt wird - die Daten könnnen auch in Form eines numpy-Arrays an das Model übergeben werden - allerdings ist das Arbeiten mit einem Dataset deutlich komfortabler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die in tensorflow_datasets verfügbaren Datensätze werden auch Metadaten mitgeliefert, die einige Informationen zu den Datensätzen enthalten. Diese können z. B. beim Laden gleich mit abgefragt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    version=3.0.1,\n",
       "    description='The MNIST database of handwritten digits.',\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    total_num_examples=70000,\n",
       "    splits={\n",
       "        'test': 10000,\n",
       "        'train': 60000,\n",
       "    },\n",
       "    supervised_keys=('image', 'label'),\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da eine Normalisierung der Daten nicht für jedes Modell zwingend ist, und sowohl der Datentyp als auch das Verfahren nicht einheitlich sein müssen, kann dmieser Schritt nicht bereits durch tensorflow_datasets durchgeführt werden. Zu diesem Zweck wird eine Transformationsfunktion bestimmt, die jedes Datum im Datensatz durchlaufen muss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  # Normalizes images: uint8 -> float32, [0,255] -> [0,1]\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Danach werden Eigenschaften des Datensatzes wie die Batch-Size oder die Menge der im Voraus zu ladenden Daten festgelegt. Hier kann auch schon die Anzahl der Trainingsepochen festegelegt werden. In diesem Beispiel wird diese erst innerhalb der Trainingsfunktion festgelegt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mittels der Sequential API von Keras wird danach ein einfaches Model definiert, das MNIST erfolgreich klassifizieren soll. Die Sequential API von Keras bietet die schnellste Möglichkeit, einfache Netze mit nur einer Ein- und einer Ausgabe zu definieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.Dense(128,activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ist das Model definiert, kann es anschließend kompiliert werden. Hierbei kann dem Model auch eine Loss-Funktion und ein Optimizer zugewiesen werden. Zusätzlich können Metriken definiert werden, die beim Training des Netzes berechnet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Anschluss kann das Model dann trainiert werden. Da die Daten als Dataset vorliegen, können sie direkt als ganzes übergeben werden. Andernfalls müssten an der Stelle Bilder und Label in getrennten numpy-Arrays übergeben werden. Zusätzlich wird an dieser Stelle auch die Anzahl der Epochen festgelegt, die das Model trainiert werden soll. Als Validationsdatensatz wird an dieser Stelle AUSNAHMSWEISE der Testdatensatz verwendet, um die Güte der einzelnen Modelle zu visualisieren. Eine Auswahl der Modelle anhand dieser Daten wäre nicht zulässig!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3566 - accuracy: 0.9013 - val_loss: 0.1967 - val_accuracy: 0.9439\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1684 - accuracy: 0.9523 - val_loss: 0.1353 - val_accuracy: 0.9620\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1214 - accuracy: 0.9649 - val_loss: 0.1109 - val_accuracy: 0.9691\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0942 - accuracy: 0.9730 - val_loss: 0.0942 - val_accuracy: 0.9713\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0764 - accuracy: 0.9781 - val_loss: 0.0849 - val_accuracy: 0.9752\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0641 - accuracy: 0.9814 - val_loss: 0.0782 - val_accuracy: 0.9766\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0533 - accuracy: 0.9847 - val_loss: 0.0748 - val_accuracy: 0.9788\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0465 - accuracy: 0.9865 - val_loss: 0.0764 - val_accuracy: 0.9771\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0379 - accuracy: 0.9893 - val_loss: 0.0712 - val_accuracy: 0.9787\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0340 - accuracy: 0.9908 - val_loss: 0.0681 - val_accuracy: 0.9805\n"
     ]
    }
   ],
   "source": [
    "history = model_seq.fit(\n",
    "    ds_train,\n",
    "    epochs=10,\n",
    "    validation_data=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST mit CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit 97% Accuracy war das einfache Neuronale Netz schon recht gut. Die Vermutung liegt allerdings nahe, dass ein CNN hier noch deutlich bessere Werte erreichen kann. Deshalb wird nachfolgend ein einfaches CNN mit 2 Conv und 2 Pooling Schichten definiert. Das Model wird ebenfalls mit der Sequential API von Keras definiert. Zu Demonstrationszwecken wird für dieses Model allerdings zunächst ein leeres Model erstellt und die einzelnen Layer erst nach und nach hinzugefügt. Auch hierbei muss dem ersten Layer durch den input_shape Parameter mitgeteilt werden, welche Form die Eingabedaten haben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq_cnn = tf.keras.Sequential(name=\"Sequential_CNN\")\n",
    "model_seq_cnn.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "model_seq_cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model_seq_cnn.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_seq_cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model_seq_cnn.add(tf.keras.layers.Dropout(0.25))\n",
    "model_seq_cnn.add(tf.keras.layers.Flatten())\n",
    "model_seq_cnn.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model_seq_cnn.add(tf.keras.layers.Dropout(0.5))\n",
    "model_seq_cnn.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential_CNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 225,034\n",
      "Trainable params: 225,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_seq_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq_cnn.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.3144 - accuracy: 0.9024 - val_loss: 0.0595 - val_accuracy: 0.9808\n",
      "Epoch 2/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1059 - accuracy: 0.9679 - val_loss: 0.0374 - val_accuracy: 0.9876\n",
      "Epoch 3/6\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 0.0795 - accuracy: 0.9763 - val_loss: 0.0372 - val_accuracy: 0.9884\n",
      "Epoch 4/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0664 - accuracy: 0.9801 - val_loss: 0.0291 - val_accuracy: 0.9894\n",
      "Epoch 5/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0575 - accuracy: 0.9828 - val_loss: 0.0287 - val_accuracy: 0.9910\n",
      "Epoch 6/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0521 - accuracy: 0.9838 - val_loss: 0.0267 - val_accuracy: 0.9908\n"
     ]
    }
   ],
   "source": [
    "history = model_seq_cnn.fit(\n",
    "    ds_train,\n",
    "    epochs=6,\n",
    "    validation_data=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der Functional API von Keras können auch Netze definiert werden, die deutlich komplexer sind als einfache sequentielle Netze. So können z. B. Netze mit mehreren Eingaben oder mehreren Ausgaben erstellt werden. Es können natürlich auch einfache Netze wie die bisher gesehenen definiert werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = tf.keras.Input(shape=(28, 28, 1))\n",
    "x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(img_input)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.Dropout(0.25)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "model_func = tf.keras.Model(img_input, x, name=\"Functional_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Functional_Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 225,034\n",
      "Trainable params: 225,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_func.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.3095 - accuracy: 0.9038 - val_loss: 0.0623 - val_accuracy: 0.9798\n",
      "Epoch 2/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1065 - accuracy: 0.9682 - val_loss: 0.0412 - val_accuracy: 0.9860\n",
      "Epoch 3/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0781 - accuracy: 0.9760 - val_loss: 0.0339 - val_accuracy: 0.9888\n",
      "Epoch 4/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0661 - accuracy: 0.9805 - val_loss: 0.0273 - val_accuracy: 0.9897\n",
      "Epoch 5/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0550 - accuracy: 0.9837 - val_loss: 0.0287 - val_accuracy: 0.9895\n",
      "Epoch 6/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0501 - accuracy: 0.9856 - val_loss: 0.0274 - val_accuracy: 0.9906\n"
     ]
    }
   ],
   "source": [
    "history = model_func.fit(\n",
    "    ds_train,\n",
    "    epochs=6,\n",
    "    validation_data=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = tf.keras.Input(shape=(960, 1280, 3))\n",
    "\n",
    "# Block 1\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3),\n",
    "                          activation='relu',\n",
    "                          padding='same',\n",
    "                          name='block1_conv1')(img_input)\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3),\n",
    "                          activation='relu',\n",
    "                          padding='same',\n",
    "                          name='block1_conv2')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "# Block 2\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3),\n",
    "                          activation='relu',\n",
    "                          padding='same',\n",
    "                          name='block2_conv1')(x)\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3),\n",
    "                          activation='relu',\n",
    "                          padding='same',\n",
    "                          name='block2_conv2')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "# Block 3\n",
    "x = tf.keras.layers.Conv2D(256, (3, 3),\n",
    "                          activation='relu',\n",
    "                          padding='same',\n",
    "                          name='block3_conv1')(x)\n",
    "x = tf.keras.layers.Conv2D(256, (3, 3),\n",
    "                          activation='relu',\n",
    "                          padding='same',\n",
    "                          name='block3_conv2')(x)\n",
    "x = tf.keras.layers.Conv2D(256, (3, 3),\n",
    "                          activation='relu',\n",
    "                          padding='same',\n",
    "                          name='block3_conv3')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "# Block 4\n",
    "x = tf.keras.layers.Conv2D(512, (3, 3),\n",
    "                          activation='relu',\n",
    "                          padding='same',\n",
    "                          name='block4_conv1')(x)\n",
    "x = tf.keras.layers.Conv2D(512, (3, 3),\n",
    "                          activation='relu',\n",
    "                          padding='same',\n",
    "                          name='block4_conv2')(x)\n",
    "x = tf.keras.layers.Conv2D(512, (3, 3),\n",
    "                          activation='relu',\n",
    "                          padding='same',\n",
    "                          name='block4_conv3')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "# Block 5\n",
    "x = tf.keras.layers.Conv2D(512, (3, 3),\n",
    "                          activation='relu',\n",
    "                          padding='same',\n",
    "                          name='block5_conv1')(x)\n",
    "x = tf.keras.layers.Conv2D(512, (3, 3),\n",
    "                          activation='relu',\n",
    "                          padding='same',\n",
    "                          name='block5_conv2')(x)\n",
    "x = tf.keras.layers.Conv2D(512, (3, 3),\n",
    "                          activation='relu',\n",
    "                          padding='same',\n",
    "                          name='block5_conv3')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(4096, 7, name=\"HDTLR/conv_1\", padding=\"same\", activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5, name=\"HDTLR/drop_conv_1\")(x)\n",
    "x = tf.keras.layers.Conv2D(4096, 1, name=\"HDTLR/conv_2\", padding=\"same\", activation='relu')(x)\n",
    "split = tf.keras.layers.Dropout(0.5, name=\"HDTLR/drop_conv_2\")(x)\n",
    "\n",
    "classification = tf.keras.layers.Conv2D(832, 1, name=\"HDTLR/conv_class\", activation='relu')(split)\n",
    "bb = tf.keras.layers.Conv2D(256, 1, name=\"HDTLR/conv_bb\", padding=\"same\", activation='relu')(split)\n",
    "\n",
    "model_det = tf.keras.Model(img_input, [classification, bb], name='detection_mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"detection_mod\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 960, 1280, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 960, 1280, 64 1792        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 960, 1280, 64 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 480, 640, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 480, 640, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 480, 640, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 240, 320, 128 0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 240, 320, 256 295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 240, 320, 256 590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 240, 320, 256 590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 120, 160, 256 0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 120, 160, 512 1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 120, 160, 512 2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 120, 160, 512 2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 60, 80, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 60, 80, 512)  2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 60, 80, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 60, 80, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 30, 40, 512)  0           block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "HDTLR/conv_1 (Conv2D)           (None, 30, 40, 4096) 102764544   block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "HDTLR/drop_conv_1 (Dropout)     (None, 30, 40, 4096) 0           HDTLR/conv_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "HDTLR/conv_2 (Conv2D)           (None, 30, 40, 4096) 16781312    HDTLR/drop_conv_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "HDTLR/drop_conv_2 (Dropout)     (None, 30, 40, 4096) 0           HDTLR/conv_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "HDTLR/conv_class (Conv2D)       (None, 30, 40, 832)  3408704     HDTLR/drop_conv_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "HDTLR/conv_bb (Conv2D)          (None, 30, 40, 256)  1048832     HDTLR/drop_conv_2[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 138,718,080\n",
      "Trainable params: 138,718,080\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_det.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition eigener Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras bietet bereits eine Vielzahl vordefinierter Layer. Sollten über diese Layer hinaus weitere Layer benötigt werden, ist es in TensorFlow natürlich nach wie vor möglich, eigene Layer zu definieren. Hier wurde beispielhaft eine vereinfachte Version des Conv2D Layers erstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import nn_ops\n",
    "\n",
    "class My_Conv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation=None,\n",
    "                 data_format=None,\n",
    "                 trainable=True,\n",
    "                 name=None,\n",
    "                 **kwargs):\n",
    "        super(My_Conv2D, self).__init__(\n",
    "            trainable=trainable,\n",
    "            name=name,\n",
    "            activity_regularizer=tf.python.keras.regularizers.get(None),\n",
    "            **kwargs)\n",
    "        self.filters = filters\n",
    "        self.activation = tf.python.keras.activations.get(activation)\n",
    "        self.kernel_size = tf.python.keras.utils.conv_utils.normalize_tuple(\n",
    "            kernel_size, 2, 'kernel_size')\n",
    "        self.dilation_rate = tf.python.keras.utils.conv_utils.normalize_tuple(\n",
    "            1, 2, 'dilation_rate')\n",
    "        self.strides = tf.python.keras.utils.conv_utils.normalize_tuple(1, 2, 'strides')\n",
    "        self.padding = tf.python.keras.utils.conv_utils.normalize_padding(padding)\n",
    "        self.data_format = tf.python.keras.utils.conv_utils.normalize_data_format(None)\n",
    "        self.kernel_initializer = tf.python.keras.initializers.get('glorot_uniform')\n",
    "        self.kernel_regularizer = tf.python.keras.regularizers.get(None)\n",
    "        self.kernel_constraint = tf.python.keras.constraints.get(None)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        input_shape = tf.python.framework.tensor_shape.TensorShape(input_shape)\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "        if input_shape.dims[channel_axis].value is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "        input_dim = int(input_shape[channel_axis])\n",
    "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel',\n",
    "            shape=kernel_shape,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            trainable=True,\n",
    "            dtype=self.dtype)\n",
    "        self.bias = None\n",
    "        \n",
    "        if self.padding == 'causal':\n",
    "            op_padding = 'valid'\n",
    "        else:\n",
    "            op_padding = self.padding\n",
    "        if not isinstance(op_padding, (list, tuple)):\n",
    "            op_padding = op_padding.upper()\n",
    "\n",
    "        \n",
    "        self._convolution_op = nn_ops.Convolution(\n",
    "            input_shape,\n",
    "            filter_shape=self.kernel.shape,\n",
    "            dilation_rate=self.dilation_rate,\n",
    "            strides=self.strides,\n",
    "            padding=op_padding,\n",
    "            data_format=tf.python.keras.utils.conv_utils.convert_data_format(self.data_format,\n",
    "                                                       2 + 2))\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self._convolution_op(inputs, self.kernel)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend kann dieser Layer analog zu den in Keras definierten Layern für den Aufbau des Models verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = tf.keras.Input(shape=(28, 28, 1))\n",
    "x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(img_input)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = My_Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.Dropout(0.25)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "model_func_layer = tf.keras.Model(img_input, x, name=\"Functional_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func_layer.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.2997 - accuracy: 0.9082 - val_loss: 0.0627 - val_accuracy: 0.9815\n",
      "Epoch 2/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1064 - accuracy: 0.9690 - val_loss: 0.0424 - val_accuracy: 0.9852\n",
      "Epoch 3/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0801 - accuracy: 0.9763 - val_loss: 0.0398 - val_accuracy: 0.9864\n",
      "Epoch 4/6\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 0.0654 - accuracy: 0.9801 - val_loss: 0.0330 - val_accuracy: 0.9888\n",
      "Epoch 5/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0577 - accuracy: 0.9827 - val_loss: 0.0276 - val_accuracy: 0.9909\n",
      "Epoch 6/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0498 - accuracy: 0.9848 - val_loss: 0.0286 - val_accuracy: 0.9908\n"
     ]
    }
   ],
   "source": [
    "history = model_func_layer.fit(\n",
    "    ds_train,\n",
    "    epochs=6,\n",
    "    validation_data=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition eines eigenen Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobald man an die Grenzen der Functional API stößt, hat man noch die Möglichkeit, ein eigenes Model als Subklasse der Keras Klasse Model zu erstellen. Hiermit ist es dann möglich, einzelne Methoden der Klasse zu überschreiben und so an beliebiger Stelle ein selbst gewähltes Verhalten zu definieren. Im nachfolgenden Beispiel wird zunächst im Kostruktor das Model erstellt. Zusätzlich wird die train_step Methode überschrieben, um das Verhalten in den einzelnen Schritten selbst definieren zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Model(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, name=\"My_Model\", **kwargs):\n",
    "        img_input = tf.keras.Input(shape=(28, 28, 1))\n",
    "        x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(img_input)\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        x = tf.keras.layers.Dropout(0.25)(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        x = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "        model = tf.keras.Model(img_input, x)\n",
    "        super(My_Model, self).__init__(inputs=model.inputs, outputs=model.outputs, name=name, **kwargs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "\n",
    "        # watch gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der erstellten Klasse kann nun ein Model generiert werden. Mit Aufruf der Summary zeigt sich, dass auch aus dieser Methode die selbe Architektur resultiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"My_Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 225,034\n",
      "Trainable params: 225,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model = My_Model()\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.3039 - accuracy: 0.9065 - val_loss: 0.0587 - val_accuracy: 0.9807\n",
      "Epoch 2/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1030 - accuracy: 0.9695 - val_loss: 0.0390 - val_accuracy: 0.9865\n",
      "Epoch 3/6\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0764 - accuracy: 0.9771 - val_loss: 0.0325 - val_accuracy: 0.9882\n",
      "Epoch 4/6\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0639 - accuracy: 0.9811 - val_loss: 0.0295 - val_accuracy: 0.9897\n",
      "Epoch 5/6\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0560 - accuracy: 0.9828 - val_loss: 0.0292 - val_accuracy: 0.9894\n",
      "Epoch 6/6\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0507 - accuracy: 0.9843 - val_loss: 0.0262 - val_accuracy: 0.9908\n"
     ]
    }
   ],
   "source": [
    "history = my_model.fit(\n",
    "    ds_train,\n",
    "    epochs=6,\n",
    "    validation_data=ds_test\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
