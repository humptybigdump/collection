{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e42763e",
   "metadata": {},
   "source": [
    "# Stochastic Simulation\n",
    "\n",
    "*Winter Semester 2023/24*\n",
    "\n",
    "16.02.2024\n",
    "\n",
    "Prof. Sebastian Krumscheid<br>\n",
    "Asstistant: Stjepan Salatovic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5187ec",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">\n",
    "Exercise sheet 12\n",
    "</h3>\n",
    "\n",
    "---\n",
    "\n",
    "<h1 align=\"center\">\n",
    "Markov Chain Monte Carlo\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f636d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.special import gamma\n",
    "from ipywidgets import interact\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82b0a91-6d0f-436d-bc1e-bfc75cc60fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=14)          # Controls default text sizes\n",
    "plt.rc('axes', titlesize=16)     # Fontsize of the axes title\n",
    "plt.rc('axes', labelsize=14)     # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=12)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=12)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=14)    # legend fontsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d52ade-5638-4c42-87cb-4d5d7992f83d",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "In many applications of interest, it is not uncommon to encounter the need for sampling from a multi-modal distribution $f$. The theory developed so far can be directly applicable to these types of distributions.\n",
    "However, in practice, sampling from these distributions using MCMC can be computationally challenging,  as we will investigate in this problem. Throughout this exercise, we will consider the bi-modal distribution\n",
    "$$\\tag{1}\n",
    "f(x;\\gamma,x_0)=\\frac{e^{-\\gamma (x^2-x_0)^2}}{Z}, \\quad \\gamma>0,\n",
    "$$\n",
    "where $Z$ is some normalizing constant. Depending on the values of $\\gamma$ and $x_0$, designing a sampling strategy to properly sample from (1) can become challenging. Intuitively, if both peaks are too far apart, using a random walk Metropolis  (RWM) might not work, as it is possible for the sampler to get stuck on one of the peaks if the _step-size_ is too small. Conversely, a RWM with very large _steps_ might tend to reject quite often, thus rendering the whole sampling procedure inefficient. We begin by verifying this. Implement the RWM algorithm using as proposal distribution the density of the normal distribution $q(x,y)=\\mathcal{N}(x,\\sigma^2)$ and target distribution $f(x;\\gamma,x_0)$ for $\\gamma=1$,  $x_0=1,4,9,25$ and different choices of $\\sigma$. Discuss the quality of your samples by analyzing the trace-plots (one realization of the chain), autocorrelation functions and histograms of the chains obtained.\n",
    "\n",
    "**Remark:** Have a look at [`plt.acorr`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.acorr.html) for plotting the autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "273e2baf-f476-498b-91e2-7277bab70829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: float, gamma: float, x0: float) -> float:\n",
    "    \"\"\"\n",
    "    Computes the density of a bi-modal distribution.\n",
    "\n",
    "    Args:\n",
    "        x (float): The point at which to evaluate the density.\n",
    "        gamma (float): The gamma parameter of the distribution.\n",
    "        x0 (float): The x0 parameter of the distribution.\n",
    "\n",
    "    Returns:\n",
    "        float: The density of the bi-modal distribution at point x.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09847719-5db3-4856-9ed1-9827539b2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RWM(sigma: float, gamma: float, x0: float, N: int=10000) -> np.array:\n",
    "    \"\"\"\n",
    "    Implements the Random Walk Metropolis (RWM) algorithm.\n",
    "\n",
    "    Args:\n",
    "        sigma (float): The standard deviation of the proposal distribution.\n",
    "        gamma (float): The gamma parameter of the target distribution.\n",
    "        x0 (float): The x0 parameter of the target distribution.\n",
    "        N (int, optional): The number of samples to generate. Defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "        np.array: An array of samples generated by the RWM algorithm.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f683348-46e9-498a-add7-b9fb9f22f3a9",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Ideally, we would like to obtain (approximately) i.i.d samples from a target distribution $f$ using Markov Chain Monte Carlo (MCMC) algorithms. \n",
    "One practical way of doing so is via _sub-sampling_ (also called _batch sampling_), which is implemented to reduce or eliminate correlation between the successive values in the Markov chain.\n",
    "That is, instead of considering the entire chain $\\{X_n\\colon n\\ge 0\\}$, say, this technique sub-samples the chain with a batch size $k>1$, so that only the values $\\{X_{kn}\\colon n\\ge 0\\}$ are considered.\n",
    "If the covariance $\\text{Cov}_f(X_0,X_n)$ vanishes as $n\\to\\infty$, then the idea of sub-sampling is quite natural since $X_{kn}$ and $X_{k(n+1)}$ can be considered to be approximately independent for $k$ sufficiently big; estimating such a $k$ may be\n",
    "difficult in practice though.\n",
    "While sub-sampling provides a way of generating (approx.) i.i.d. samples from $f$ and may thus be useful assessing the convergence of a MCMC method, it necessarily leads to an efficiency loss.\n",
    "Let $\\{X_n\\in \\mathbb{R}^d \\colon n\\ge 0\\}$ be a Markov chain with a unique stationary \tdistribution $f$, and $X_0 \\sim f$ (i.e., the chain is at equilibrium). Take $\\phi\\colon \\mathbb{R}^d\\to\\mathbb{R}$ such that $\\mathbb{E}_f\\bigl({\\lvert \\phi\\rvert}^2\\bigr)<\\infty$ and consider two estimators for $\\mu=\\mathbb{E}_f(\\phi)$, namely one that uses the entire Markov chain ($\\hat\\mu$) and one based on sub-sampling ($\\hat\\mu_{k}$) using only every $k$-th value:\n",
    "\\begin{equation*}\n",
    "\\hat\\mu = \\frac{1}{Nk}\\sum_{n=1}^{Nk}\\phi(X_n)\\;,\\quad\\text{and}\\quad \\hat\\mu_{k} = \\frac{1}{N}\\sum_{n=1}^N\\phi(X_{nk})\\;.\n",
    "\\end{equation*}\n",
    "Show that the variance of $\\hat\\mu$ satisfies $\\mathbb{V}_f(\\hat\\mu)\\le \\mathbb{V}_f(\\hat\\mu_{k})$ for every $k>1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb06053-01f7-4c7d-9817-b8c83bfbd073",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "At every iteration of the general Metropolis-Hastings algorithm, a\n",
    "new candidate state $\\boldsymbol{Y}_{n+1}$ is proposed by sampling\n",
    "$\\boldsymbol{Y}_{n+1} \\sim q(\\boldsymbol{X}_{n},\\cdot)$, given the\n",
    "current state $\\boldsymbol{X}_{n}$. Here,\n",
    "$q(\\boldsymbol{x},\\boldsymbol{y})$ is the so-called proposal\n",
    "density. Consider now the case where the proposal does not depend on\n",
    "the current state, that is\n",
    "$q(\\boldsymbol{x},\\boldsymbol{y}) \\equiv q(\\boldsymbol{y})$, so that the proposed candidate is $\\boldsymbol{Y}_{n+1} \\sim q$. This particular\n",
    "Markov Chain Monte Carlo (MCMC) variant is sometimes called\n",
    "_independent Metropolis-Hastings algorithm_ with fixed proposal\n",
    "(or simply _independence sampler_). Let's denote the target\n",
    "density by $f$. As such, this MCMC variant appears very similar to the\n",
    "Accept-Reject method for sampling from $f$ (cf. Lab 02)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e582e-e192-49c7-b49c-be4ff14ef146",
   "metadata": {},
   "source": [
    "1. Suppose there exists a positive constant $C$ such that\n",
    "\t$f(\\boldsymbol{x})\\le C q(\\boldsymbol{x})$ for any\n",
    "\t$\\boldsymbol{x}\\in\\text{supp}(f)=\\{\\boldsymbol{x}\\in\\mathbb{R}^d\\colon\n",
    "\tf(\\boldsymbol{x})>0\\}$. Show that the expected acceptance probability of the independent\n",
    "\t\tMetropolis-Hastings algorithm is _at least_ $\\frac{1}{C}$\n",
    "\t\twhenever the chain is stationary. How does this compare to the\n",
    "\t\texpected acceptance probability of an Accept-Reject method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737939bc-3387-497c-89a9-e3a70fce52a8",
   "metadata": {},
   "source": [
    "2. Let us compare the independent Metropolis-Hastings algorithm\n",
    "\tand the Accept-Reject method in some more detail by an\n",
    "\texample. Specifically, the goal is to sample from a Gamma\n",
    "\tdistribution with shape parameter $\\alpha$ and scale parameter\n",
    "\t$\\beta$, denoted by $\\text{Gamma}(\\alpha,\\beta)$, so that the target\n",
    "\tPDF reads\n",
    "\t$$f(x) \\equiv f(x;\\alpha,\\beta) = \\beta^\\alpha x^{\\alpha -1}\n",
    "\te^{-\\beta x}/\\Gamma(\\alpha) \\mathbb{I}_{\\{x\\ge 0\\}},$$ where\n",
    "\t$\\Gamma$ denotes the Gamma function.\n",
    "\n",
    "    **Hint:** [`scipy.special.gamma`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.gamma.html)\n",
    "\n",
    "    1. Implement the Accept-Reject method to sample from\n",
    "\t\t$\\text{Gamma}(\\alpha,1)$ for $\\alpha>1$, using the PDF of the\n",
    "\t\t$\\text{Gamma}(a,b)$ distribution with $a = [\\alpha]$ as auxiliary\n",
    "\t\tdensity (here $[\\alpha]$ denotes the integer part of\n",
    "\t\t$\\alpha$). Show that $b = [\\alpha]/\\alpha$ is the optimal choice for $b$.\n",
    "\n",
    "\n",
    "        **Hint:** Recall that\n",
    "    \t\t\t\t$\\sum_{k=1}^K \\xi_k \\sim \\mathrm{Gamma}(K,\\beta)$ for\n",
    "    \t\t\t\t$K\\in\\mathbb{N}$, if\n",
    "    \t\t\t\t$\\xi_k\\overset{\\mathrm{i.i.d.}}{\\sim}\n",
    "    \t\t\t\t\\mathrm{Gamma}(1,\\beta)\\equiv \\mathrm{Exp}(\\beta)$.\n",
    "   2. Use your Accept-Reject method to generate $m$ random numbers\n",
    "    $X_1,\\dots ,X_m$ with each $X_i\\sim\\text{Gamma}(\\alpha,1)$, when\n",
    "    using $n=5000$ random variables $Y_1,\\dots ,Y_n$ from the auxiliary\n",
    "    $\\text{Gamma}([\\alpha],[\\alpha]/\\alpha)$ distribution. Notice that\n",
    "    $m$ is a random variable, which is smaller than $n$ due to\n",
    "    rejections. Perform the simulations for $\\alpha = 4.85$.\n",
    "\n",
    "    3. Implement the independent Metropolis-Hastings algorithm using\n",
    "    as proposal $q$ the PDF of the\n",
    "    $\\text{Gamma}([\\alpha],[\\alpha]/\\alpha)$ distribution.\n",
    "\n",
    "    4. Use the same sample $Y_1,\\dots ,Y_n$ used within the\n",
    "    Accept-Reject method, now in the corresponding Metropolis-Hastings\n",
    "    algorithm to generate $n=5000$ realizations of the target\n",
    "    distribution $\\text{Gamma}(\\alpha,1)$ with $\\alpha = 4.85$.\n",
    "\n",
    "    5. Compare both methods with respect to:\n",
    "        1. their acceptance rates,\n",
    "        2. their estimates for the mean of the $\\text{Gamma}(4.85,1)$\n",
    "        distribution, which is $4.85$,\n",
    "        3. the correctness of the target distribution.\n",
    "\n",
    "\n",
    "        Discuss your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e84e39-7498-4068-b8ec-a0f761af56c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: np.array , a: float, b: float) -> np.array:\n",
    "    \"\"\"\n",
    "    Evaluates the PDF of a Gamma distribution with parameters a and b.\n",
    "\n",
    "    Args:\n",
    "        x (np.array): An array of values for which to compute the PDF of the specified Gamma distribution.\n",
    "        a (float): The shape parameter of the Gamma distribution.\n",
    "        b (float): The rate parameter of the Gamma distribution.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Computed PDF values for the input values\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fc93951-3584-40e5-890f-c2420c6caf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_C(alpha: float, a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Optimal value for `C`: optimization of the ratio `f(x; alpha, 1) / f(x; a, b)`.\n",
    "    \n",
    "    Args:\n",
    "        alpha (float): Parameter of the target distribution function `f`.\n",
    "        a (float): The shape parameter of the Gamma distribution.\n",
    "        b (float): The rate parameter of the Gamma distribution.\n",
    "\n",
    "    Returns:\n",
    "        float: Optimal value for `C\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d168d2e5-b756-47c5-bfa5-3053cc22e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acceptance_rejection(alpha: float, n: int = 5000) -> tuple:\n",
    "    \"\"\"\n",
    "    Performs acceptance-rejection sampling to generate samples from a target distribution.\n",
    "\n",
    "    Args:\n",
    "        alpha (float): Parameter of the target distribution function `f`.\n",
    "        n (int, optional): Number of samples to generate. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the generated samples (numpy array) and the acceptance probability (float).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f604d2a-da3c-4cfc-9ce3-3ca9d4d4ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings(alpha: float, n: int = 5000, initial_value: float = 4) -> tuple:\n",
    "    \"\"\"\n",
    "    Performs Metropolis-Hastings sampling to generate samples from a target distribution.\n",
    "\n",
    "    Args:\n",
    "        alpha (float): Parameter of the target distribution function `f`.\n",
    "        n (int, optional): Number of samples to generate. Defaults to 5000.\n",
    "        initial_value (float, optional): Initial value of the Markov chain. Defaults to 4.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the generated samples (numpy array) and the acceptance rate (float).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f347ea-10e2-4581-a9e3-a08efe97cb46",
   "metadata": {},
   "source": [
    "## Exercise 4 \n",
    "\n",
    "Let $X\\subset\\mathbb{R}^d$ and $P_i:X\\times\\mathcal{B}(X)\\rightarrow[0,1]$, $i=1\\dots,m$ be a Markov transition kernel on $X$ with $\\mathcal{B}(X)$ the associated $\\sigma-$algebra. \n",
    "\n",
    "1. Given $a_1,\\dots,a_m\\in \\mathbb{R}^+$, such that $\\sum_{i=1}^{m}a_i=1$, show that $P(x,A)=\\sum_{i=1}^{m} a_i P_i(x,A)$ is a Markov kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e066b44b-845a-4ae5-8f3d-f104937a1edb",
   "metadata": {},
   "source": [
    "2. Now consider $a_1,\\dots,a_m\\in \\mathbb{R}$, such that $\\sum_{i=1}^{m}a_i=1$ (i.e, not necessarily positive weights). Construct an affine combination of kernels $P(x,A)=\\sum_{i=1}^{m}\\alpha_iP_i(x,A)$ that is still Markovian.\n",
    "\n",
    "\t **Hint:** Consider a kernel $P$, for which there exists a measure $\\nu$ on $(X, \\mathcal{B}(X))$ such that $\\forall x \\in X$, $P_1(x, A) \\geq \\epsilon \\nu(A)$ $\\forall A \\in \\mathcal{B}(X)$, $\\forall x \\in X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1049143c-e9cf-4470-b6bb-a55136260452",
   "metadata": {},
   "source": [
    "3. Suppose that a measure $\\pi:\\mathcal{B}\\rightarrow[0,1]$ is invariant for each kernel $P_i$. Show that it is also invariant for $P=\\sum_{i=1}^m a_i P_i$, where  $a_1,\\dots,a_m\\in \\mathbb{R}^+$, such that $\\sum_{i=1}^{m}a_i=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10825e83-f710-4e0a-9b58-567668440411",
   "metadata": {},
   "source": [
    "# Exercise 5 (optional, no solution)\n",
    "\n",
    "Consider a Markov chain $\\{X_n\\} \\sim \\text{Markov}(\\pi,P)$ on a discrete state space $\\mathcal{X}$ at equilibrium, with $P$ irreducible, and $\\pi$ the unique invariant probability measure of $P$. \n",
    "Let $l^2_{\\pi}$ be the Hilbert space $l^2_{\\pi} = \\{f:\\mathcal{X} \\to \\mathbb{R}: \\sum_{i\\in \\mathcal{X}} f(i)^2 \\pi_i < \\infty\\}$ with inner product $(f,g)_{l^2_\\pi} = \\sum_{i \\in \\mathcal{X}} f(i)g(i)\\pi_i$, and $l^2_{\\pi,0} = \\{f \\in l^2_\\pi: \\mathbb{E}_\\pi[f]=0\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f0e49-7645-49e0-afa6-3f44733ece15",
   "metadata": {},
   "source": [
    "1. Show that if $(P,\\pi)$ are in detailed balance, then $(Pf,g)_{l^2_\\pi} = (f,Pg)_{l^2_\\pi}$ for any $f,g \\in l^2_\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d95ba00-105f-4f90-a44b-8ca21751c09e",
   "metadata": {},
   "source": [
    "2. Show that $\\mathbb{E}[f(X_n)f(X_m)]=(P^{m-n}f,f)_{l^2_\\pi}$ for any $f \\in l^2_\\pi$ and $m>n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b2173a-bb33-4093-9618-c151ae67fa08",
   "metadata": {},
   "source": [
    "3. Consider now the estimator $$\\hat{\\mu}_N = \\frac{1}{N} \\sum_{n=1}^N f(X_n)$$ of $\\mu = \\mathbb{E}_\\pi[f]$ under the assumption that $f \\in l^2_\\pi$. Show that $\\mathbb{E}_\\pi[\\hat{\\mu}_N] = \\mu$, and $$\\mathbb{V}ar[\\hat{\\mu}_N] = \\frac{1}{N} \\sum_{l=0}^N c_l (P^l \\tilde{f}, \\tilde{f})_{l^2_\\pi},$$ with $\\tilde{f} = f - \\mathbb{E}_{\\pi}[f] \\in l^2_{\\pi,0}$ and\n",
    "\\begin{align}\n",
    "c_{l,N} = \\begin{cases} 1, \\quad l=0 \\\\ 2(1-\\frac{l}{N}), \\quad l>0 \\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e505e463-4064-48d8-99c3-5b871fe1330b",
   "metadata": {},
   "source": [
    "4. Conclude that the asymptotic variance $\\mathbb{V}(f,p) \\coloneqq \\lim_{N \\to \\infty} N \\mathbb{V}ar_{\\pi}(\\hat{\\mu}_N)$ satisfies $\\mathbb{V}(f,p) = ((2(I-P)^{-1}-I)\\tilde{f},\\tilde{f})_{l^2_\\pi}$ if \n",
    "$$\n",
    "\\tag{3}\n",
    "\\sup_{g \\in l^2_{\\pi,0}} \\frac{(Pg,g)_{l^2_\\pi}}{\\|g\\|_{l^2_\\pi}} = \\beta < 1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641ca80-e385-4600-a27f-cfa7ddc272b1",
   "metadata": {},
   "source": [
    "5. Consider now the two irreducible transition matrices $P_1$ and $P_2$, both in detailed balance with $\\pi$ and satisfying (3) for some $\\beta_1,\\beta_2$.\n",
    "Show that if $(P_1)_{ij} \\geq (P_2)_{ij} \\forall i \\neq j$, then \n",
    "\\begin{align}\n",
    "\\mathbb{V}(f,P_1) \\leq \\mathbb{V}(f,P_2),\n",
    "\\end{align}\n",
    "for any $f \\in l^2_\\pi$.\n",
    "\n",
    "    **Hint:** Take $P(\\lambda) = (1-\\lambda)P_1 + \\lambda P_2, \\lambda \\in [0,1]$ and show that $\\frac{d}{d \\lambda} \\mathbb{V}(f,P(\\lambda)) \\geq 0$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
