{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "2_dqn_atari.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "eQx7oDGeeKWj"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLXw6zd-k3Xd"
   },
   "source": [
    "# Homework2 of Deep RL, KIT, 2022-23 WS (15Pts)\n",
    "\n",
    "Designed by Ge Li, ge.li@kit.edu, inspired by the official PyTorch DQN\n",
    "implementation and the RL Stable Baseline library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AcEJQ5NZVYW"
   },
   "source": [
    "All homeworks are self-contained. They can be completed in their respective notebooks.\n",
    "To edit and re-run code, you can therefore simply edit and restart the code cells below.\n",
    "There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window).\n",
    "This file should automatically be synced with your Google Drive. We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout.\n",
    " However, you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HBPnmbIPPyl",
    "outputId": "c9f8b25f-f30a-472f-f2ce-989a18cc64af"
   },
   "source": [
    "# Your work will be stored in a folder called `drl_ws22` by default to prevent Colab\n",
    "# instance timeouts from deleting your edits.\n",
    "# We do this by mounting your google drive on the virtual machine created in this colab\n",
    "# session. For this, you will likely need to sign in to your Google account and copy a\n",
    "# passcode into a field below\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OuCfTLJIx5nQ",
    "outputId": "dab2e6e9-0614-4e83-8f62-d9273fc19fb6"
   },
   "source": [
    "# Create paths in your google drive\n",
    "DRIVE_PATH = '/content/gdrive/My\\ Drive/drl_ws22'\n",
    "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
    "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
    "    ! mkdir $DRIVE_PATH\n",
    "\n",
    "# the space in `My Drive` causes some issues,\n",
    "# make a symlink to avoid this\n",
    "SYM_PATH = '/content/drl_ws22'\n",
    "if not os.path.exists(SYM_PATH):\n",
    "    !ln -s $DRIVE_PATH $SYM_PATH\n",
    "! cd $SYM_PATH"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SDMWmItLrL1R"
   },
   "source": [
    "# Install **python** packages\n",
    "!pip install matplotlib stable-baselines3[extra] unrar"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4GuoE0QTj4GJ"
   },
   "source": [
    "# Install **system** packages\n",
    "!apt update\n",
    "!apt install -y --no-install-recommends xvfb python-opengl ffmpeg > /dev/null 2>&1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UunygyDXrx7k"
   },
   "source": [
    "In this homework, we are going to implement the Deep Q-Network algorithm and\n",
    "apply it to the Atari 2600 game \"Breakout\". Atari was a popular game console\n",
    "during the 1980s, and Breakout is a game where few layers of bricks line the\n",
    "top of the screen and the goal is to destroy them all by repeatedly bouncing\n",
    "a ball off a paddle into them.\n",
    "Refer to <https://en.wikipedia.org/wiki/Breakout_(video_game)> for more details.\n",
    "\n",
    "\n",
    "OpenAI gym's Atari environment is used to simulate the game.\n",
    "In this homework, we simulate the original game console behaviour using all **4**\n",
    " game console buttons, with actions: **LEFT**, **RIGHT** and **RESTART** (when\n",
    " game over). The 4th button is useless in Breakout. The player starts each game with\n",
    "  5 lives, and loses a life whenever they miss an incoming ball. The player\n",
    "  gets one point per brick that they hit with the ball, after which this brick is\n",
    "  destroyed.\n",
    "  If the player loses all 5 lives, the game will be over. Then, only\n",
    "  when the \"RESTART\" button is pressed, the game will be restarted again.\n",
    "  The final score for each game is the accumulated score of all 5 lives.\n",
    "\n",
    "We start by importing all the necessary python modules and defining some helper\n",
    "functions which you do not need to change. Still, make sure you are aware of\n",
    "what they do.\n",
    "\n",
    "In the following blocks, you will see:\n",
    " * Import statements game file downloads\n",
    " * Utilities (PyTorch GPU, plotting functions, image pre-processes)\n",
    " * Environment setup and Gym state wrappers\n",
    " * Runtime arguments\n",
    " * Functional code (classes, functions, ...) and explanations:\n",
    "    * A utlity function to update the exploration rate\n",
    "    * Classes for Transitions and a ReplayMemory buffer\n",
    "    * A DQN in Pytorch using Convolutional Neural Networks and Fully Connected Layers\n",
    "    * **An action-selection function (Your job!)**\n",
    "    * **A function to optimize the policy network (Your job!)**\n",
    "    * **The main training loop containing rollouts, network updates and post-processing (Your job!)**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "enh5ZMHftEO7"
   },
   "source": [
    "# Imports and utility\n",
    "# DO NOT CHANGE THIS BLOCK!\n",
    "# DO NOT CHANGE THIS BLOCK!\n",
    "# DO NOT CHANGE THIS BLOCK!\n",
    "\n",
    "# Notebook utility for clear all output\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Some Python Buildin packages for random number and containers\n",
    "import random\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Although we use Deep Learning, we still need to use some CPU memory for the\n",
    "# Replay Buffer, since Image data normally takes a lot of space and GPU memory\n",
    "# is more expensive than CPU memory\n",
    "import numpy as np\n",
    "\n",
    "# Deep Learning Platform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# We use stable baseline to pre-process our Atari game data\n",
    "# detail Stable Baseline is a RL benchmark library with plenty of Algorithms,\n",
    "# here we borrow their wrappers for data preprocessing and video recording\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
    "\n",
    "# Download Atari game files\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar',\n",
    "                           'Roms.rar')\n",
    "!unrar x Roms.rar > /dev/null 2>&1\n",
    "!mkdir rars > /dev/null 2>&1\n",
    "!mv HC\\ ROMS.zip   rars > /dev/null 2>&1\n",
    "!mv ROMS.zip  rars > /dev/null 2>&1\n",
    "!python -m atari_py.import_roms rars > /dev/null 2>&1\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "env_seed = 0\n",
    "\n",
    "# Use GPU to speed up your training\n",
    "# detail: the code below will detect the hardware you have and set the\n",
    "# device to Nvidia \"cuda\" instead of \"cpu\".\n",
    "assert torch.cuda.is_available(), \"Change your Colab settings to use a GPU, as training an image-based network takes forever on a cpu.\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "def show_img(state) -> None:\n",
    "    \"\"\"\n",
    "    This is a helper function to plot the environment state.\n",
    "    Args:\n",
    "        state: environment state, either as a numpy image or a torch tensor\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if isinstance(state, np.ndarray):\n",
    "        state_numpy = state[0]\n",
    "    elif isinstance(state, torch.Tensor):\n",
    "        state_numpy = state[0].cpu().numpy()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    fig = plt.figure()\n",
    "    for i in range(4):\n",
    "        fig.add_subplot(2, 2, i + 1)\n",
    "        plt.imshow(state_numpy[..., i])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def normalize(data):\n",
    "    \"\"\"\n",
    "    Image Normalizer, normalize image data from the range [0, 255] to [0, 1]\n",
    "\n",
    "    Args:\n",
    "        data: integer data in either tensor or Numpy array, bounded by [0, 255]\n",
    "\n",
    "    Returns:\n",
    "        Float data bounded by [0, 1]\n",
    "    \"\"\"\n",
    "    return data / 255.0\n",
    "\n",
    "\n",
    "def show_avg_reward(list_num_game: list, list_avg_game_score: list) -> None:\n",
    "    \"\"\"\n",
    "    Plot average game score\n",
    "    Args:\n",
    "        list_num_game: x axis for different number of games\n",
    "        list_avg_game_score: y axis for average game scores\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    clear_output(True)\n",
    "    plt.figure()\n",
    "    plt.plot(list_num_game, list_avg_game_score)\n",
    "    plt.xlabel(\"Num of games\")\n",
    "    plt.ylabel(\"Avg_game_score\")\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Bj_5XyqHZVYZ"
   },
   "source": [
    "## Environment setup\n",
    "Get our environment and use some helper wrappers to pre-process the data.\n",
    "The wrapper will return a 4-frames-long game screenshots sequence as game state.\n",
    "For more preprocessing details, please refer:\n",
    "<https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7Dydj9zIzlIH"
   },
   "source": [
    "# DO NOT CHANGE THIS BLOCK!\n",
    "\n",
    "env = make_atari_env('BreakoutDeterministic-v4', n_envs=1, seed=env_seed)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Wrap env into a video recorder, since we do not have a display in Colab, we \n",
    "# cannot render the game. Instead, we record a video of the game and save it\n",
    "# to your Google Drive under \"My Drive/drl_ws22//DQN/video\"\n",
    "\n",
    "# Record a 500 steps long video in every 50000 training steps\n",
    "env = VecVideoRecorder(env, './DQN/video',\n",
    "                       record_video_trigger=lambda x: x % 50000 == 0,\n",
    "                       video_length=500,\n",
    "                       name_prefix=\"DQN_Atari\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "n36K8CFIZVYZ"
   },
   "source": [
    "## Initialize Runtime arguments and (hyper-)parameters\n",
    "Click and see details"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "imnAkQ6jryL7"
   },
   "source": [
    "# DO NOT CHANGE THIS BLOCK!\n",
    "# DO NOT CHANGE THIS BLOCK!\n",
    "# DO NOT CHANGE THIS BLOCK!\n",
    "\n",
    "class Args:\n",
    "    \"\"\"\n",
    "    Boilerplate for properly accessing the args\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def __setitem__(self, key, val):\n",
    "        setattr(self, key, val)\n",
    "\n",
    "    BATCH_SIZE = 32  #@param {type: \"int\"}, mini batch size\n",
    "    GAMMA = 0.99  #@param {type: \"float\"}, discount factor\n",
    "    EPS_START = 1.0  #@param {type: \"float\"}, exploration rate at the start\n",
    "    EPS_END = 0.05  #@param {type: \"float\"}, minimum exploration rate\n",
    "\n",
    "    EPS_FRACTION = 0.1  #@param {type: \"float\"}, exploration rate decreases\n",
    "    # during the first X% of the entire training process\n",
    "\n",
    "    TARGET_UPDATE_INTERVAL = 10000  #@param {type: \"int\"}, interval of\n",
    "    # updating the target network\n",
    "\n",
    "    LEARNING_RATE = 1e-4  #@param {type: \"float\"}, learning rate\n",
    "    BUFFER_SIZE = 100000  #@param {type: \"int\"}, replay buffer size\n",
    "    TOTAL_STEPS = 500000  #@param {type: \"int\"}, total time steps for training\n",
    "\n",
    "    MAX_GRAD_NORM = 10  #@param {type: \"int\"}, when do back-propagation, clip\n",
    "    # the gradient if it is greater than this value\n",
    "\n",
    "    TRAIN_FREQ = 4  #@param {type: \"int\"}, Optimize the NN after X-times of\n",
    "    # rollouts\n",
    "\n",
    "    NUM_ACTIONS = num_actions  #@param {type: \"int\"}, number of actions\n",
    "\n",
    "\n",
    "# Instantiate\n",
    "args = Args()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "v3l3mzBjZVYZ"
   },
   "source": [
    "## Update exploration rate\n",
    "The exploration rate will decrease from 1 to 0.05 during the first 10% of the\n",
    " training steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kWuhNkSArL1V"
   },
   "source": [
    "# DO NOT CHANGE THIS BLOCK!\n",
    "# DO NOT CHANGE THIS BLOCK!\n",
    "# DO NOT CHANGE THIS BLOCK!\n",
    "\n",
    "def update_eps(total_time_steps):\n",
    "    \"\"\"\n",
    "    This is a helper function to update exploration rate, the exploration rate\n",
    "    will decrease from 1 to 0.05 during the first 10% of the training steps\n",
    "\n",
    "    Args:\n",
    "        total_time_steps: total time steps from the start of the training\n",
    "\n",
    "    Returns:\n",
    "        eps: exploration rate\n",
    "\n",
    "    \"\"\"\n",
    "    return max(args.EPS_START - total_time_steps / (args.TOTAL_STEPS *\n",
    "                                                    args.EPS_FRACTION),\n",
    "               args.EPS_END)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "be9mXNKhZVYa"
   },
   "source": [
    "## Transition and ReplayMemory (buffer)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "fLnU1evmss4I"
   },
   "source": [
    "# DO NOT CHANGE THIS BLOCK!\n",
    "\n",
    "# Definition of transition stored by the reply buffer\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    Definition of reply buffer used by DQN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"Initialize the ReplayMemory with certain capacity\"\"\"\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample transitions and transfer them from numpy array to PyTorch tensor\n",
    "        Args:\n",
    "            batch_size: mini batch size\n",
    "\n",
    "        Returns:\n",
    "            5 batches data\n",
    "        \"\"\"\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # a detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*random.sample(self.memory, batch_size)))\n",
    "\n",
    "        # Get each individual batch prepared\n",
    "        done_b = torch.tensor(np.concatenate(batch.done), device=device,\n",
    "                              dtype=torch.bool).squeeze(-1)\n",
    "        state_b = torch.tensor(np.concatenate(batch.state),\n",
    "                               device=device, dtype=torch.float)\n",
    "        next_state_b = torch.tensor(np.concatenate(batch.next_state),\n",
    "                                    device=device, dtype=torch.float)\n",
    "        action_b = torch.tensor(np.concatenate(batch.action), device=device)\n",
    "        reward_b = torch.tensor(np.concatenate(batch.reward),\n",
    "                                device=device).squeeze(-1)\n",
    "        return state_b, action_b, next_state_b, reward_b, done_b\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Length of the ReplayMemory\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "# Instantiate\n",
    "memory = ReplayMemory(args.BUFFER_SIZE)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXPHcsDrZVYa"
   },
   "source": [
    "## Deep Q-Network with CNN\n",
    "In the following block, we define the DQN class. Each instance of this class\n",
    " is a Q network that computes the state action value function **Q(s, a)**.\n",
    "The input of this Network is a state **s**, and the output is the value of\n",
    "each action **a** given this state. Please note that for NN training, we\n",
    "normally feed data in a mini-batch manner. Since each state in current task\n",
    "is a 3rd order image tensor `(channel, height, width)`, the DQN expects input to be\n",
    "a 4th order tensor with shape `(mini-batch, channel, height, width)`. And the\n",
    "output is a 2nd order tensor with shape `(mini-batch, actions)`\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lj1WrUC9ZVYb"
   },
   "source": [
    "# DO NOT CHANGE THIS BLOCK!\n",
    "# DO NOT CHANGE THIS BLOCK!\n",
    "# DO NOT CHANGE THIS BLOCK!\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Q-Network with CNN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        \"\"\"\n",
    "        Initialize the network, which contains 2D convolution layers (image\n",
    "        process), Normalization layers (offer better numerical stability),\n",
    "        and Fully Connected layers\n",
    "        Args:\n",
    "            h: height of the image in pixel\n",
    "            w: width of the image in pixel\n",
    "            outputs: number of actions\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass function, given a state s, compute value of all actions\n",
    "        i.e. Q(s, a)\n",
    "        Called with either one element to determine next action, or a batch\n",
    "        during optimization.\n",
    "        Args:\n",
    "            x: state\n",
    "\n",
    "        Returns:\n",
    "            q_s_a = value of actions given this state\n",
    "        \"\"\"\n",
    "        # Shape of x:\n",
    "        # [batch_size or 1, height=84, width=84, channel=4]\n",
    "        #\n",
    "        # Shape of x after swapping the order of the data's axis:\n",
    "        # [batch_size or 1, channel=4, height=84, width=84]\n",
    "        #\n",
    "        # Shape of q_s_a:\n",
    "        # [batch_size or 1, num_actions=4]\n",
    "\n",
    "        x = x.to(device)  # to GPU\n",
    "\n",
    "        # Swap the order of the data's axis, from\n",
    "        # [batch, height, width, channel] to [batch, channel, height, width]\n",
    "        x = torch.einsum('...hwc->...chw', x)\n",
    "\n",
    "        # Forward pass\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        q_s_a = self.head(x.reshape([x.size(0), -1]))\n",
    "        return q_s_a\n",
    "\n",
    "\n",
    "# Instantiate policy net and its optimizer, note we send the networks to GPU\n",
    "policy_net = DQN(h=84, w=84, outputs=num_actions).to(device)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=args.LEARNING_RATE)\n",
    "\n",
    "# Instantiate target net using NN parameters of the policy net\n",
    "target_net = DQN(h=84, w=84, outputs=num_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# We do not need any gradient from target net, turn it into evaluation mode\n",
    "target_net = target_net.eval()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLORQjUQZVYb"
   },
   "source": [
    "## Sample action using epsilon-greedy exploration method (3pts)\n",
    "\n",
    "In the following block, you are going to select actions given a **state**\n",
    "mini-batch and an exploration rate **eps**(ilon). Each action is an integer 0,\n",
    " 1, 2 or 3.\n",
    "The workflow is:\n",
    "* Apply the epsilon-greedy method to decide if we go exploration or exploitation\n",
    "* If exploitation, use the policy net to compute the **Q(s,a)** and select the\n",
    " action with the highest value.\n",
    "* If exploration, randomly choose actions\n",
    "\n",
    "Hints:\n",
    "* The epsilon-greedy method can be achieved by an if-statement comparing eps\n",
    "    with a random float bounded by [0, 1], try `random.random()` to get a random number.\n",
    "* If exploitation, you do not need to compute gradient when you compute\n",
    "    the **Q(s,a)** (think about why!), in this case you can use a context scope\n",
    "    **with torch.no_grad():**. The operations in this scope will not contain\n",
    "    any gradient information (computational graph).\n",
    "* Before feed state into NN, call normalize() func to normalize it\n",
    "* If exploitation, you can apply **argmax()** to get the action, which is\n",
    "     the index of the max **Q(s,a)** along the action axis. You may need to\n",
    "     use the arguments **dim** and **keepdim** to get the desired result.\n",
    "* If exploration, randomly picking actions can be solved by generating a\n",
    "    random index tensor. **torch.randint()** can help you. Be careful with\n",
    "    the range and shape. As index in PyTorch, the data type **dtype** should be\n",
    "    **torch.long**. And don't forget **device=device** to get a GPU tensor!\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "k465rsv0rL1X"
   },
   "source": [
    "# Finish the implementation!\n",
    "\n",
    "def select_action(state: torch.Tensor, eps: float):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        state: state mini batch of shape [batch_size, height=84, width=84, num_channels=4]\n",
    "        eps: exploration rate\n",
    "\n",
    "    Returns:\n",
    "        selected_actions: Discrete action of shape [batch_size, 1] as a tensor batch, each action is one\n",
    "        integer from [0, 1, 2, 3]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ## TODO ##\n",
    "    ### Your code starts here ###\n",
    "\n",
    "    ### Your code ends here ###\n",
    "\n",
    "    # Some code to help check the validity of the output\n",
    "    assert selected_actions.dtype == torch.long\n",
    "    assert selected_actions.ndim == 2\n",
    "    assert selected_actions.shape[0] == state.shape[0]\n",
    "    assert selected_actions.shape[1] == 1\n",
    "\n",
    "    return selected_actions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfd3KIAlZVYb"
   },
   "source": [
    "## Optimize the model (7pts)\n",
    "\n",
    "In the following block, you are going to optimize the parameters of the network.\n",
    "\n",
    "The workflow is:\n",
    "* Sample a mini batch from the replay buffer\n",
    "* Compute an **unfinished masks batch** to distinguish if the $s'$ is terminal or\n",
    " not\n",
    "* Use **Policy net** and index with action batch to compute $Q(s, a)$\n",
    "* Compute $Q(s', a')$ using **Target net** if s' is not terminal state\n",
    "* apply **max** operation to get the $\\max_{a'} Q(s', a')$\n",
    "* If s' is terminal states, simply set  $\\max_{a'} Q(s', a')$ to 0\n",
    "* Compute $r + \\gamma~\\max_{a'} Q(s', a')$\n",
    "* Compute the Huber loss\n",
    "* Update the NN parameters through Back-propagation\n",
    "\n",
    "Hints:\n",
    "* If **a** is a tensor, you can call **a.shape** to get its shape\n",
    "* You can use torch.logical_not to compute **boolean not** operation\n",
    "* Before you feed the state into the NN, call `normalize()` to normalize it\n",
    "* To pick data from tensor A using tensor B as index, you can use **torch.gather()**\n",
    "* To remove a size 1 dimension of an input tensor, use **torch.squeeze()**\n",
    "* You can initialize a zeros tensor for $\\max_{a'} Q(s', a')$ and only\n",
    "    change the values of this tensor where s' is not a terminal state. Use\n",
    "    the unfinished masks to help you.\n",
    "* **torch.max()** can help you get the max value along a desired axis\n",
    "* If **a** is an output tensor of a NN, a.detach() will remove the\n",
    "    computational graph behind it.\n",
    "* **args.GAMMA** is the discount factor\n",
    "* You can instantiate a loss criterion using nn.HuberLoss() and\n",
    "    then compute the loss value by calling this instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rgiU2d1OrL1X"
   },
   "source": [
    "# Finish the implementation!\n",
    "\n",
    "def optimize_model():\n",
    "\n",
    "    # Train until we have enough data in the buffer\n",
    "    if len(memory) < args.BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    # Sample mini-batch\n",
    "    state_b, action_b, next_state_b, reward_b, done_b = memory.sample(args.BATCH_SIZE)\n",
    "\n",
    "    ## TODO ##\n",
    "    ### Your code starts here ###\n",
    "\n",
    "    ### Your code ends here ###\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), args.MAX_GRAD_NORM)\n",
    "    optimizer.step()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCwXxx7VZVYc"
   },
   "source": [
    "## Main loop (5pts)\n",
    "\n",
    "In the following block, you are going to finish the main loop of the training procedure.\n",
    "\n",
    "The workflow is:\n",
    "* Firstly initialize variables to record game status, like game score, remaining lives etc.\n",
    "* Start the game to get its first state\n",
    "* While loop until max training steps has been reached:\n",
    "    * While loop until **one life is wasted**:\n",
    "        * Get exploration rate updated\n",
    "        * Rollout transition:\n",
    "            * Transfer state from numpy array to tensor\n",
    "            * Select action\n",
    "            * Ask the environment to go one step further and get next\n",
    "                state, reward, done_flag and information (containing remaining\n",
    "                 lives)\n",
    "            * Save transition into replay buffer\n",
    "            * Move to the next state\n",
    "            * Update some game records\n",
    "        * Update policy net when necessary\n",
    "        * Update target net when necessary\n",
    "        * If one life is wasted, break\n",
    "    * If all 5 lives are wasted (game over), then do some post-processing job,\n",
    "    such as: update progress bar, plot average score curve for the past 10 games.\n",
    "\n",
    "Hint:\n",
    "* You do not need to take care of anything when 1 life is wasted or all 5\n",
    "lives are wasted (game over). The game environment will continue and get reset\n",
    "automatically.\n",
    "* If you are curious about how the state and next_state look like, you can\n",
    "plot them using the function: **show_img(state)**. You should only do this\n",
    "during debugging, as this will take up a lot of computation time if you call it\n",
    "too often.\n",
    "* When we rollout, we do not need any gradient info, so we use the context\n",
    "scope **with torch.no_grad():**.\n",
    "* You can call torch.tensor() to initialize a tensor, be careful with the device\n",
    "* If **a** is a GPU tensor, you can call **a.cpu()** to transfer it into a cpu\n",
    "tensor and further do **a.cpu().numpy()** to get a numpy array.\n",
    "* Use **env.step(action)** to perform an action. This function has 4 returns\n",
    "* When you start training, the early stage will be quite noisy. **You may\n",
    "need 15+ min to get the average score more than 5**. The entire training is\n",
    "about 2 hours. And the average score in the end should be around 20.\n",
    "* During the training procedure, we save a video for every 50000 time steps\n",
    "into your Google drive."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "_qQb789_syt0",
    "outputId": "4fdd1b8e-8a88-4c68-e3b6-9586ca89d718"
   },
   "source": [
    "# Finish the implementation and run it!\n",
    "\n",
    "!matplotlib inline\n",
    "\n",
    "list_num_game = []\n",
    "list_avg_game_score = []\n",
    "\n",
    "# Main Loop of training\n",
    "num_game = 0\n",
    "state = env.reset()\n",
    "remaining_lives = 5\n",
    "game_score = 0\n",
    "game_scores = deque([], maxlen=10)  # Store the score of the latest 10 games\n",
    "game_length = 0\n",
    "num_time_steps = 0\n",
    "\n",
    "# Progress bar\n",
    "with tqdm(total=args.TOTAL_STEPS, position=0, leave=True, unit=\"steps\") as pbar:\n",
    "    # Loop until total time steps has been reached\n",
    "    while num_time_steps < args.TOTAL_STEPS:\n",
    "\n",
    "        # Loop until one life is wasted\n",
    "        while True:\n",
    "\n",
    "            # Get exploration rate\n",
    "            eps = update_eps(num_time_steps)\n",
    "\n",
    "            # Rollout\n",
    "            with torch.no_grad():\n",
    "\n",
    "                ## TODO ##\n",
    "                ### Your code starts here ###\n",
    "\n",
    "                ### Your code ends here ###\n",
    "\n",
    "                # Update game score and length and total time steps\n",
    "                game_score += reward\n",
    "                game_length += 1\n",
    "                num_time_steps += 1\n",
    "\n",
    "            # Optimize model\n",
    "            if num_time_steps % args.TRAIN_FREQ == 0:\n",
    "                optimize_model()\n",
    "\n",
    "            # Update the target network\n",
    "            if num_time_steps % args.TARGET_UPDATE_INTERVAL == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            # One life is wasted\n",
    "            if done:\n",
    "                remaining_lives = info[0][\"lives\"]\n",
    "                break\n",
    "\n",
    "        # When one game is over, i.e. all 5 lives are wasted\n",
    "        if remaining_lives == 0:\n",
    "            num_game += 1\n",
    "            pbar.update(game_length)\n",
    "\n",
    "            # Print some result in the progress bar for every 10 games\n",
    "            if num_game % 10 == 0:\n",
    "                pbar.set_description(\n",
    "                    \"Game #{}-{}, Avg_score: {:.3f},\"\n",
    "                    \" eps: {:.3f}\".format(num_game - 9, num_game, np.asarray(\n",
    "                        game_scores).mean(), eps))\n",
    "\n",
    "            # Plot the average reward curve for every 50 games              \n",
    "            if num_game % 50 == 0:\n",
    "                list_num_game.append(num_game)\n",
    "                list_avg_game_score.append(np.asarray(game_scores).mean())\n",
    "                show_avg_reward(list_num_game, list_avg_game_score)\n",
    "\n",
    "            # Reset game score and length\n",
    "            game_scores.append(game_score)\n",
    "            game_score = 0\n",
    "            game_length = 0\n",
    "    pbar.update(game_length)\n",
    "    print(\"Finished!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Self-test question: Where and how can we change the model to get a Double DQN?\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
